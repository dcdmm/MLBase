{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c1a69d-e3c1-441f-8bbd-50cadecd77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce3999ee-2e3a-4594-8401-d770b80ec019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='./save_tokenizer/', vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "tokenizer_fast = BertTokenizerFast.from_pretrained('./save_tokenizer/')\n",
    "print(tokenizer_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56030e82-d1d1-4d95-854a-943676b038cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "胸廓对称，气管居中。所见骨骼骨质结构完整。双肺纹理清晰。两肺门影不大。心影横径增大，左心缘饱满。两侧膈面光整，两侧肋膈角锐利。1.两肺未见明显活动性病变，随诊。2.心影改变请结合临床。\n",
      "[[0, 2, '器官组织', '胸廓'], [2, 4, '阴性表现', '对称'], [5, 7, '器官组织', '气管'], [7, 9, '阴性表现', '居中'], [12, 16, '器官组织', '骨骼骨质'], [16, 18, '属性', '结构'], [18, 20, '阴性表现', '完整'], [21, 23, '器官组织', '双肺'], [23, 25, '属性', '纹理'], [25, 27, '阴性表现', '清晰'], [28, 32, '器官组织', '两肺门影'], [32, 34, '阴性表现', '不大'], [35, 37, '器官组织', '心影'], [37, 39, '属性', '横径'], [39, 41, '阳性表现', '增大'], [42, 45, '器官组织', '左心缘'], [45, 47, '阳性表现', '饱满'], [48, 52, '器官组织', '两侧膈面'], [52, 54, '阴性表现', '光整'], [55, 60, '器官组织', '两侧肋膈角'], [60, 62, '阴性表现', '锐利'], [65, 67, '器官组织', '两肺'], [67, 69, '否定描述', '未见'], [69, 71, '修饰描述', '明显'], [71, 74, '修饰描述', '活动性'], [74, 76, '异常现象', '病变'], [82, 84, '器官组织', '心影'], [84, 86, '异常现象', '改变']]\n",
      "\n",
      "廓\n",
      "称\n",
      "管\n",
      "中\n",
      "\n",
      "胸廓\n",
      "对称\n",
      "气\n",
      "居中\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/train.conll_convert.conll', 'r', encoding='utf-8') as f:\n",
    "    for sentence in f.readlines():\n",
    "        try:\n",
    "            sentence = json.loads(sentence)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        sent = sentence['sent'].replace(' ', '')\n",
    "        print(sent)\n",
    "        print(sentence['ners'], end='\\n\\n')\n",
    "\n",
    "        # 2 - 1表示结尾字符'廓'的未知\n",
    "        print(sent[2 - 1])\n",
    "        print(sent[4 - 1])\n",
    "        print(sent[7 - 1])\n",
    "        print(sent[9 - 1], end='\\n\\n')\n",
    "\n",
    "        # 字符串索引不包括结尾位置元素 \n",
    "        print(sent[0: 2])\n",
    "        print(sent[2: 4])\n",
    "        print(sent[5: 6])\n",
    "        print(sent[7: 9])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586e2e1f-54e4-40af-b313-0558f31fc3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144757/144757 [00:02<00:00, 54946.12it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "with open('datasets/train.conll_convert.conll', 'r', encoding='utf-8') as f:\n",
    "    for sentence in tqdm(f.readlines()):\n",
    "        try:\n",
    "            sentence = json.loads(sentence)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        offset_mapping_output = tokenizer_fast(sentence['sent'], return_offsets_mapping=True)[\"offset_mapping\"]\n",
    "        # 处理原句空格\n",
    "        offset_mapping = []\n",
    "        for i, (start, end) in enumerate(offset_mapping_output):\n",
    "            if (end > 0) and (i >= 2):\n",
    "                start -= (i - 1)\n",
    "                end -= (i - 1)\n",
    "            offset_mapping.append((start, end))\n",
    "\n",
    "        start_mapping = {j[0]: i for i, j in enumerate(offset_mapping) if j != (0, 0)}\n",
    "        # j[1] - 1表示该token结尾字符的位置\n",
    "        end_mapping = {j[1] - 1: i for i, j in enumerate(offset_mapping) if j != (0, 0)}\n",
    "\n",
    "        ent2token_spans = []\n",
    "        for ent in sentence['ners']:\n",
    "            start_idx, end_idx, entity_type, entity_text = ent[0], ent[1] - 1, ent[2], ent[3]\n",
    "            if start_idx in start_mapping and end_idx in end_mapping:\n",
    "                start_span = start_mapping[start_idx]\n",
    "                end_span = end_mapping[end_idx]\n",
    "            ent2token_spans.append([start_span, end_span, entity_type, entity_text])\n",
    "        sentence['spans'] = ent2token_spans\n",
    "        sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5304fdec-9ca7-4459-b6b3-65e7023ddbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/train_ner.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sentences, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392dafd0-0b88-4efc-a883-55d9bfa7713e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3812",
   "language": "python",
   "name": "env_3812"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}