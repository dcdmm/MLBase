{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since dair-ai/emotion couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'split' at C:\\Users\\duanm\\.cache\\huggingface\\datasets\\dair-ai___emotion\\split\\0.0.0\\cab853a1dbdf4c42c2b3ef2173804746df8825fe (last modified on Wed Jun 18 11:28:24 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['i didnt feel humiliated',\n",
       "  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       "  'im grabbing a minute to post i feel greedy wrong',\n",
       "  'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n",
       "  'i am feeling grouchy',\n",
       "  'ive been feeling a little burdened lately wasnt sure why that was',\n",
       "  'ive been taking or milligrams or times recommended amount and ive fallen asleep a lot faster but i also feel like so funny',\n",
       "  'i feel as confused about life as a teenager or as jaded as a year old man',\n",
       "  'i have been with petronas for years i feel that petronas has performed well and made a huge profit',\n",
       "  'i feel romantic too'],\n",
       " 'label': [0, 0, 3, 2, 3, 0, 5, 4, 1, 2]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_dataset(path='dair-ai/emotion')['train']\n",
    "train_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('E:\\huggingface_models\\Qwen3-0.6B')\n",
    "train_dataset = train_dataset.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "21\n",
      "10\n",
      "18\n",
      "5\n",
      "14\n",
      "24\n",
      "18\n",
      "23\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataset.select(range(0, 10)):\n",
    "    print(len(i['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'attention_mask', 'yy'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.remove_columns('text')\n",
    "yy_list = []\n",
    "for i in range(0, len(train_dataset)):\n",
    "    if i % batch_size == 0:\n",
    "        length =  random.randint(32, 64)\n",
    "    yy_list.append(list(range(0, length)))\n",
    "train_dataset = train_dataset.add_column('yy', yy_list)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['label', 'input_ids', 'attention_mask', 'yy', 'labels'])\n",
      "yy:  torch.Size([4, 50])\n",
      "label:  tensor([0, 0, 3, 2])\n",
      "labels:  tensor([[   72, 47607,  2666,  2784, 53773,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100],\n",
      "        [   72,   646,   728,   504,  8266,   773, 74223,   311,   773, 67365,\n",
      "         37550,  1101,   504,  1660,  2163,  4325,   879, 33572,   323,   374,\n",
      "         34347],\n",
      "        [  318, 48930,   264,  9383,   311,  1736,   600,  2666, 55980,  4969,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100],\n",
      "        [   72,  1079,  3512,  8266, 82274,   911,   279, 39411,   600,   686,\n",
      "          1414,   429,   432,   374,  2058,   389,   279,  3343,  -100,  -100,\n",
      "          -100]])\n",
      "labels.shape:  torch.Size([4, 21])\n",
      "input_ids:  tensor([[    72,  47607,   2666,   2784,  53773, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643],\n",
      "        [    72,    646,    728,    504,   8266,    773,  74223,    311,    773,\n",
      "          67365,  37550,   1101,    504,   1660,   2163,   4325,    879,  33572,\n",
      "            323,    374,  34347],\n",
      "        [   318,  48930,    264,   9383,    311,   1736,    600,   2666,  55980,\n",
      "           4969, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643],\n",
      "        [    72,   1079,   3512,   8266,  82274,    911,    279,  39411,    600,\n",
      "            686,   1414,    429,    432,    374,   2058,    389,    279,   3343,\n",
      "         151643, 151643, 151643]])\n",
      "input_ids.shape:  torch.Size([4, 21])\n",
      "attention_mask.shape:  torch.Size([4, 21])\n"
     ]
    }
   ],
   "source": [
    "dac = Data.DataLoader(train_dataset, \n",
    "                      # 保留所有字段(所有字段必须为数值类型)\n",
    "                      # tokenizer类型字段(如:input_ids,attention_mask)动态填充到最大长度\n",
    "                      # 其他字段每个batch_size的数据必须可以转换为一个tensor\n",
    "                      collate_fn=DataCollatorForLanguageModeling(tokenizer, \n",
    "                                                                 # Whether or not to use masked language modeling. If set to False, the labels are the same as the inputs with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked tokens and the value to predict for the masked token.\n",
    "                                                                 mlm=False), \n",
    "                      batch_size=batch_size)\n",
    "\n",
    "for i in dac:\n",
    "    print(i.keys())\n",
    "    print(\"yy: \", i[\"yy\"].shape)\n",
    "    print(\"label: \", i['label'])\n",
    "\n",
    "    # # labels内部计算实现(mlm=False时):\n",
    "    # labels = batch[\"input_ids\"].clone()\n",
    "    # if self.tokenizer.pad_token_id is not None:\n",
    "    #     labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "    # batch[\"labels\"] = labels\n",
    "    print(\"labels: \", i['labels'])\n",
    "    print(\"labels.shape: \", i['labels'].shape)\n",
    "    print('input_ids: ', i['input_ids'])\n",
    "    print(\"input_ids.shape: \", i['input_ids'].shape)\n",
    "    print(\"attention_mask.shape: \", i['attention_mask'].shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
