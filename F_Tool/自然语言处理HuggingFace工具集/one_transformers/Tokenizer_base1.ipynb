{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BertTokenizer(可通过`__init__`实例化): Construct a BERT tokenizer. Based on WordPiece.\n",
    "# from_pretrained(类方法): Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',\n",
    ")\n",
    "tokenizer  # 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.\n",
    "# This class cannot be instantiated directly using `__init__()` (throws an error).\n",
    "# 与上等价\n",
    "tokenizer_auto = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='bert-base-chinese')\n",
    "tokenizer_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, 100, 100]\n",
      "['月', '光', '的', '[UNK]', '[PAD]', '[CLS]', '新', '希', '望', '[', '[UNK]', ']', '<', 'e', '##op', '>']\n",
      "21128\n"
     ]
    }
   ],
   "source": [
    "text = '月光的[UNK][PAD][CLS]新希望[EOS]<eop>'\n",
    "\n",
    "# 未添加新tokens前的编码效果\n",
    "print(tokenizer.convert_tokens_to_ids(['月光', '希望', '[EOS]', '<eop>']))\n",
    "print(tokenizer.tokenize(text))\n",
    "print(len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "[EOS] 21130\n",
      "21133\n",
      "{'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<eop>', '<eod>']}\n",
      "['<eop>', '<eod>'] [21131, 21132]\n"
     ]
    }
   ],
   "source": [
    "# Add a list of new tokens to the tokenizer class.\n",
    "tokenizer.add_tokens(new_tokens=['月光', '希望'])\n",
    "\n",
    "#  Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes.\n",
    "#  If special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the current vocabulary).\n",
    "\n",
    "# Keys should be in the list of predefined special attributes:\n",
    "#   [`bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].\n",
    "print(tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '[EOS]',\n",
    "                                                  # Additional special tokens used by the tokenizer.\n",
    "                                                  'additional_special_tokens': [\"<eop>\", \"<eod>\"]})\n",
    "print(tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "print(len(tokenizer.get_vocab()))\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.additional_special_tokens, tokenizer.additional_special_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21128, 21129, 21130, 21131]\n",
      "['月光', '的', '[UNK]', '[PAD]', '[CLS]', '新', '希望', '[EOS]', '<eop>']\n",
      "21133\n"
     ]
    }
   ],
   "source": [
    "# 添加新tokens后的编码效果\n",
    "print(tokenizer.convert_tokens_to_ids(['月光', '希望', '[EOS]', '<eop>']))\n",
    "print(tokenizer.tokenize(text))\n",
    "print(len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../extra_dataset/save_tokenizer/tokenizer_config.json',\n",
       " '../extra_dataset/save_tokenizer/special_tokens_map.json',\n",
       " '../extra_dataset/save_tokenizer/vocab.txt',\n",
       " '../extra_dataset/save_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存分词器(包括新添加的tokens)\n",
    "tokenizer.save_pretrained(\"../extra_dataset/save_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='../extra_dataset/save_tokenizer/', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<eop>', '<eod>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21128: AddedToken(\"月光\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t21129: AddedToken(\"希望\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t21130: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21131: AddedToken(\"<eop>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21132: AddedToken(\"<eod>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地重新加载\n",
    "tokenizer1 = BertTokenizer.from_pretrained(\"../extra_dataset/save_tokenizer/\")\n",
    "tokenizer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21128, 21129, 21130, 21131]\n",
      "['月光', '的', '[UNK]', '[PAD]', '[CLS]', '新', '希望', '[EOS]', '<eop>']\n",
      "21133\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer1.convert_tokens_to_ids(['月光', '希望', '[EOS]', '<eop>']))\n",
    "print(tokenizer1.tokenize(text))\n",
    "print(len(tokenizer1.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1077\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m-> 1077\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1086\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:210\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    207\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 210\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    213\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model(**tokenizer(text, return_tensors='pt'))  # 报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(21128, 768, padding_idx=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_embedding维度为:21128 * 768(but此时tokenizer大小为:21133)\n",
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(21133, 768, padding_idx=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.5964,  0.7013,  0.0856,  ...,  0.6440, -0.2722, -0.0668],\n",
       "         [-0.2686,  0.4007, -0.0717,  ..., -0.0795, -0.4526, -0.3661],\n",
       "         [-0.4009,  0.5033,  0.4209,  ..., -0.1184, -0.0193, -0.2150],\n",
       "         ...,\n",
       "         [-0.6121,  0.4952, -0.2831,  ...,  0.5085, -0.2028, -0.3519],\n",
       "         [-0.3102,  0.6188,  0.1467,  ..., -0.0947, -0.4982, -0.2391],\n",
       "         [-0.5063,  0.7890,  0.2055,  ...,  0.5374, -0.7213,  0.1184]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.9988,  0.9998,  0.9968,  0.9804,  0.7700,  0.3574, -0.8768, -0.9041,\n",
       "          0.9960, -0.9994,  1.0000,  0.9996, -0.8934, -0.9461,  0.9999, -0.9994,\n",
       "         -0.7651,  0.9940,  0.9975,  0.0776,  0.9980, -1.0000, -0.9552, -0.7782,\n",
       "         -0.7619,  0.9963,  0.9376, -0.8758, -0.9999,  0.9988,  0.9848,  0.9996,\n",
       "          0.9788, -0.9999, -0.9993,  0.6910,  0.2432,  0.9935, -0.6462, -0.8489,\n",
       "         -0.9785, -0.6373, -0.4105, -0.9921, -0.9749,  0.5551, -1.0000, -1.0000,\n",
       "         -0.8434,  0.9996, -0.8961, -0.9999,  0.9168, -0.9178, -0.5836,  0.9802,\n",
       "         -0.9996,  0.9398,  1.0000,  0.8138,  0.9999, -0.9656,  0.6688, -0.9996,\n",
       "          1.0000, -0.9997, -0.9824, -0.1709,  0.9999,  1.0000, -0.9696,  0.9830,\n",
       "          1.0000,  0.2293,  0.2488,  0.9997, -0.9946,  0.6661, -1.0000,  0.5805,\n",
       "          1.0000,  0.9988, -0.9782,  0.9786, -0.9775, -1.0000, -0.9991,  0.9999,\n",
       "         -0.9187,  0.9993,  0.9933, -0.9994, -1.0000,  0.9996, -0.9993, -0.9976,\n",
       "         -0.9009,  0.9986, -0.9044, -0.8083, -0.5505,  0.6365, -0.9988, -0.9971,\n",
       "          0.9981,  0.9990,  0.7011, -0.9993,  0.9998,  0.4118, -1.0000, -0.9544,\n",
       "         -0.9999, -0.7758, -0.9938,  1.0000,  0.1719, -0.4864,  0.9998, -0.9987,\n",
       "          0.8906, -0.9999, -0.9271,  0.6481,  0.9997,  1.0000,  0.9987, -0.9988,\n",
       "          0.9996,  1.0000,  0.9936,  0.9658, -0.9961,  0.9874,  0.9282, -0.9972,\n",
       "         -0.0582, -0.7933,  1.0000,  0.9948,  0.9745, -0.9618,  0.9977, -0.9988,\n",
       "          1.0000, -1.0000,  0.9997, -1.0000, -0.9970,  0.9991,  0.9089,  1.0000,\n",
       "         -0.8853,  1.0000, -0.9881, -0.9997,  0.9969, -0.4482,  0.9852, -1.0000,\n",
       "          0.9605, -0.9241, -0.2237,  0.1335, -1.0000,  1.0000, -0.9365,  1.0000,\n",
       "          0.9996, -0.9968, -0.9950, -0.9987,  0.6479, -0.9976, -0.6120,  0.9956,\n",
       "         -0.1726,  0.9992, -0.8939, -0.9870,  0.9685,  0.4995, -0.9998,  0.9978,\n",
       "         -0.7525,  0.8253,  0.0535,  0.9373,  0.9977,  0.9797, -0.8657,  1.0000,\n",
       "          0.0703,  0.9958,  0.9960,  0.0920, -0.2925, -0.9954, -1.0000,  0.2323,\n",
       "          1.0000, -0.7213, -0.9990,  0.8618, -1.0000,  0.9684,  0.4296, -0.5630,\n",
       "         -0.9981, -0.9999,  0.9999, -0.9887, -0.9992,  0.9088, -0.9224,  0.0052,\n",
       "         -0.9998,  0.5005,  0.9980, -0.8190, -0.2229, -0.7358, -0.9978,  0.9957,\n",
       "         -0.9975,  0.9217,  0.9318,  1.0000,  0.9405, -0.9347, -0.9398,  0.9999,\n",
       "         -0.4333, -1.0000,  0.9143, -0.9946, -0.6915,  1.0000, -0.9992,  0.9784,\n",
       "          1.0000,  0.8232,  1.0000,  0.2083, -0.9991, -0.9982,  1.0000,  0.9989,\n",
       "          1.0000, -0.9988, -0.9970,  0.3168, -0.2485, -1.0000, -0.9993, -0.5069,\n",
       "          0.9985,  0.9998,  0.7325, -0.9984, -0.6949, -0.9995,  1.0000, -0.2698,\n",
       "          1.0000,  0.9975, -0.9807, -0.9627,  0.9007,  0.1407, -0.9998, -0.2522,\n",
       "         -0.9997, -0.9910, -1.0000,  0.9656, -0.9996, -1.0000,  0.7891,  1.0000,\n",
       "          0.2999, -0.9999,  0.9995,  0.9984, -0.9816, -0.9507,  0.9859, -1.0000,\n",
       "          1.0000, -0.9989,  0.6423, -0.9534, -0.9986, -0.8258,  0.9996,  0.9968,\n",
       "         -0.9941, -0.6025, -0.9886, -0.9818, -0.7563,  0.9231, -0.8056,  0.9490,\n",
       "         -0.9794, -0.9220,  0.9260, -0.9836, -1.0000,  0.3561,  1.0000, -0.1932,\n",
       "          1.0000,  0.9162,  1.0000, -0.9215, -0.9980,  0.9956, -0.6362, -0.8436,\n",
       "         -0.0034, -0.7628,  0.9045, -0.1817, -0.7972, -0.9998,  0.9999,  0.9532,\n",
       "          0.9321,  0.9744, -0.8783, -0.4931,  0.9814, -0.9955,  0.9996, -0.9997,\n",
       "         -0.9028,  0.9994,  1.0000,  0.9999,  0.5845, -0.8701,  0.9890, -0.9989,\n",
       "          0.9997, -0.9996,  0.9991, -0.9272,  0.7655, -0.7747, -0.9949,  1.0000,\n",
       "          0.9050,  0.8793,  1.0000, -0.9472,  0.9965,  0.9370,  0.9871,  0.9914,\n",
       "          0.5501,  0.9999, -0.9960, -0.9953, -0.5333, -0.9975, -0.9994, -1.0000,\n",
       "          0.0520, -0.9984, -0.9795, -0.3013,  0.4661,  0.7583, -0.9844, -0.9261,\n",
       "         -0.4061,  0.6638, -0.2253,  0.6797,  0.8912, -0.9995, -0.9864, -1.0000,\n",
       "         -0.9988,  0.5288,  0.9998, -0.9999,  0.9990, -1.0000, -0.7361,  0.2532,\n",
       "         -0.8619, -0.5035,  0.9999, -1.0000,  0.9524,  0.9999,  1.0000,  0.9998,\n",
       "          0.9995, -0.9914, -0.9997, -0.9997, -1.0000, -1.0000, -0.9992,  0.9422,\n",
       "          0.6560, -1.0000, -0.5413,  0.9747,  1.0000,  0.9961, -0.9998, -0.6684,\n",
       "         -0.9996, -0.9996,  0.9984, -0.9083, -0.9986,  0.9974, -0.6993,  1.0000,\n",
       "         -0.6458,  0.7792,  0.9714,  0.8488,  0.9044, -0.9999,  0.9751,  1.0000,\n",
       "          0.9758, -1.0000, -0.3925, -0.9135, -1.0000, -0.4002,  0.9511,  0.9997,\n",
       "         -0.9999, -0.1224, -0.9992,  0.8148,  0.9363,  0.9997,  0.9998,  0.9891,\n",
       "          0.9735,  0.9725,  0.4480,  1.0000,  0.4048, -0.9995,  0.9994, -0.2459,\n",
       "          0.6621, -0.9999,  0.9982,  0.9657,  1.0000,  0.9576, -0.3583, -0.9943,\n",
       "         -0.9729,  0.9969,  1.0000, -0.9103, -0.8587, -0.9998, -0.9997, -0.9987,\n",
       "         -0.7472, -0.8904, -0.9861, -0.9997,  0.2137,  0.6634,  1.0000,  1.0000,\n",
       "          0.9989, -0.9545, -0.9173,  0.9940,  0.8779,  0.9978, -0.6408, -1.0000,\n",
       "         -0.9920, -0.9998,  0.9997, -0.4048, -0.1978, -0.9646,  0.6064,  0.9664,\n",
       "         -0.9999, -0.9907, -0.9797,  0.1935,  1.0000, -0.9992,  0.9993, -0.9996,\n",
       "          0.3114,  0.8942,  0.7380,  0.9976, -0.8548, -0.0532, -0.7626,  0.8650,\n",
       "          0.9717,  0.9986, -0.9751,  0.8870,  0.9992, -0.9773,  0.9996,  0.3099,\n",
       "          0.9503,  0.9923,  1.0000,  0.6988,  0.9956,  0.9938,  1.0000,  1.0000,\n",
       "         -0.9938,  0.2832,  0.6148, -0.9230, -0.6016,  0.9055,  1.0000,  0.3129,\n",
       "         -0.9120, -0.9999,  0.9779,  0.9989,  1.0000, -0.9586,  0.9983, -0.6898,\n",
       "          0.8840,  0.9558,  0.7826,  0.3441,  0.5782,  0.9982,  0.9995, -0.9999,\n",
       "         -1.0000, -1.0000,  1.0000,  0.9995, -0.8732, -1.0000,  0.9997, -0.8406,\n",
       "          0.9944,  0.9988, -0.6140, -0.9473,  0.7506, -0.9998,  0.2026,  0.4467,\n",
       "          0.8076,  0.4175,  0.9992, -0.9998,  0.2979,  1.0000, -0.5048,  0.9998,\n",
       "          0.5176, -0.9998,  0.9991, -0.9989, -0.9999, -0.6094,  0.9998,  0.9998,\n",
       "         -0.6203,  0.4938,  0.9998, -0.9993,  1.0000, -1.0000,  0.8466, -0.9988,\n",
       "          0.9999, -0.9577, -0.9994, -0.7186,  0.6207,  0.9554, -0.8003,  1.0000,\n",
       "         -0.2030, -0.9275, -0.1536, -0.9616, -0.9991, -0.9952,  0.8568, -1.0000,\n",
       "          0.9338,  0.8916, -0.5277, -0.9361, -0.9999,  0.9999, -0.3499, -0.9818,\n",
       "          1.0000, -0.9936, -1.0000,  0.9503, -0.9937,  0.3389,  0.9945,  0.9141,\n",
       "          0.1330, -1.0000,  0.8519,  0.9999, -0.9989, -0.6685, -0.9832, -0.9831,\n",
       "          0.9766,  0.9942,  0.7194, -0.0114,  0.7644,  0.7991,  0.9108, -0.6421,\n",
       "          0.1912, -0.9998, -0.9981, -0.9725, -0.9981, -0.9998, -1.0000,  1.0000,\n",
       "          0.9995,  1.0000, -0.0958, -0.1816,  0.5994,  0.9893, -0.9998, -0.9630,\n",
       "          0.6982,  0.8781, -0.8686, -0.9994, -0.5385, -1.0000, -0.6284,  0.5400,\n",
       "         -0.9701, -0.7722,  1.0000,  0.9999, -0.9998, -0.9924, -0.9990, -0.9993,\n",
       "          1.0000,  0.9992,  0.9991, -0.4015, -0.9100,  0.9775, -0.7191, -0.3928,\n",
       "         -0.9994, -0.9969, -0.9999,  0.8800, -0.9990, -0.9999,  0.9981,  0.9999,\n",
       "          0.2634, -1.0000, -0.7590,  0.9999,  0.9989,  1.0000,  0.9539,  1.0000,\n",
       "         -0.9979,  0.9974, -0.9999,  1.0000, -1.0000,  1.0000,  1.0000,  0.9956,\n",
       "          0.9993, -0.9997,  0.9795, -0.0634, -0.8029,  0.9850, -0.6501, -0.9980,\n",
       "          0.3704,  0.9962, -0.9439,  1.0000,  0.8560,  0.6237,  0.8021, -0.1575,\n",
       "          0.9719, -0.4548, -0.9996,  0.9762,  0.9994,  0.9986,  1.0000,  0.9083,\n",
       "          0.9999, -0.9766, -0.9992,  0.6641, -0.6293, -0.3983, -0.9997,  0.9999,\n",
       "          0.9999, -0.9998, -0.9320,  0.3764,  0.2877,  0.9999,  0.9991,  0.8785,\n",
       "          0.9204,  0.0743,  0.9997, -0.9993,  0.8486, -0.9883, -0.9495,  0.9999,\n",
       "         -0.9656,  0.9994, -0.9977,  0.9997, -0.9954,  0.7667,  0.9984,  0.9932,\n",
       "         -0.9982,  1.0000,  0.7751, -0.9987, -0.9908, -0.9998, -0.9948,  0.8024]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tokenizer(text, return_tensors='pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
