{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "KE_TpNaSZQ5n",
    "jupyter": {},
    "outputId": "e100b424-e5d9-45e5-ba3a-abe466edeb16",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0hfGEe2LB9s",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\dcdmm\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\glue\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad (last modified on Wed Apr 27 20:43:06 2022) since it couldn't be found locally at glue., or remotely on the Hugging Face Hub.\n",
      "Reusing dataset glue (C:\\Users\\dcdmm\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6261c7cc4dc547c2a76ea7f360763510"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载mrpc数据集\n",
    "dataset = load_dataset('glue', 'mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4030B703046E478DBF8465C7106CF3E5",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = dataset['train'].to_pandas()\n",
    "df_val = dataset['test'].to_pandas()\n",
    "df_test = dataset['validation'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "nNs2FWNJSLSq",
    "jupyter": {},
    "outputId": "3157c71e-8b2d-4a86-9c3d-7433be8f7022",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3668, 4)\n",
      "(1725, 4)\n",
      "(408, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "irj7itV0UCF_",
    "jupyter": {},
    "outputId": "c7aedad8-f1c1-4407-f9d0-9c2d0b4b96b6",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                           sentence1  \\\n0  Amrozi accused his brother , whom he called \" ...   \n1  Yucaipa owned Dominick 's before selling the c...   \n2  They had published an advertisement on the Int...   \n3  Around 0335 GMT , Tab shares were up 19 cents ...   \n4  The stock rose $ 2.11 , or about 11 percent , ...   \n\n                                           sentence2  label  idx  \n0  Referring to him as only \" the witness \" , Amr...      1    0  \n1  Yucaipa bought Dominick 's in 1995 for $ 693 m...      0    1  \n2  On June 10 , the ship 's owners had published ...      1    2  \n3  Tab shares jumped 20 cents , or 4.6 % , to set...      0    3  \n4  PG & E Corp. shares jumped $ 1.63 or 8 percent...      1    4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n      <th>label</th>\n      <th>idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Amrozi accused his brother , whom he called \" ...</td>\n      <td>Referring to him as only \" the witness \" , Amr...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Yucaipa owned Dominick 's before selling the c...</td>\n      <td>Yucaipa bought Dominick 's in 1995 for $ 693 m...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>They had published an advertisement on the Int...</td>\n      <td>On June 10 , the ship 's owners had published ...</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Around 0335 GMT , Tab shares were up 19 cents ...</td>\n      <td>Tab shares jumped 20 cents , or 4.6 % , to set...</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The stock rose $ 2.11 , or about 11 percent , ...</td>\n      <td>PG &amp; E Corp. shares jumped $ 1.63 or 8 percent...</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()  # label:whether the sentences in the pair are semantically equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3668 entries, 0 to 3667\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentence1  3668 non-null   object\n",
      " 1   sentence2  3668 non-null   object\n",
      " 2   label      3668 non-null   int64 \n",
      " 3   idx        3668 non-null   int32 \n",
      "dtypes: int32(1), int64(1), object(2)\n",
      "memory usage: 100.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()  # 没有缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tc1GQh7yEm4C",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, max_length, pretrained_model='albert-base-v2'):\n",
    "        self.data = data\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sent1 = str(self.data.loc[index, 'sentence1'])\n",
    "        sent2 = str(self.data.loc[index, 'sentence2'])\n",
    "\n",
    "        encoded_pair = self.tokenizer(text=sent1, text_pair=sent2,\n",
    "                                      padding='max_length',\n",
    "                                      truncation=True,\n",
    "                                      max_length=self.max_length,\n",
    "                                      return_tensors='pt')\n",
    "\n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)\n",
    "        attn_masks = encoded_pair['attention_mask'].squeeze(0)\n",
    "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)\n",
    "\n",
    "        label = self.data.loc[index, 'label']\n",
    "        return token_ids, attn_masks, token_type_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"albert-base-v2\"  # 预训练模型名称\n",
    "maxlen = 128\n",
    "bs = 32  # 批次大小\n",
    "\n",
    "train_set = CustomDataset(df_train, maxlen, bert_model)\n",
    "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SentencePairClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model=\"albert-base-v2\", hidden_size=768):\n",
    "        super(SentencePairClassifier, self).__init__()\n",
    "        self.bert_layer = AutoModel.from_pretrained(pretrained_model)\n",
    "        self.hidden_size = hidden_size  # 不同预训练模型有不同的隐藏层大小\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.cls_layer = nn.Linear(self.hidden_size, 1)  # 下游分类任务\n",
    "\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
    "        outputs = self.bert_layer(input_ids, attn_masks, token_type_ids)\n",
    "        logits = self.cls_layer(self.dropout(outputs['pooler_output']))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "SentencePairClassifier(\n  (bert_layer): AlbertModel(\n    (embeddings): AlbertEmbeddings(\n      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (encoder): AlbertTransformer(\n      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n      (albert_layer_groups): ModuleList(\n        (0): AlbertLayerGroup(\n          (albert_layers): ModuleList(\n            (0): AlbertLayer(\n              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (attention): AlbertAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (attention_dropout): Dropout(p=0, inplace=False)\n                (output_dropout): Dropout(p=0, inplace=False)\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              )\n              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n              (activation): NewGELUActivation()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (pooler): Linear(in_features=768, out_features=768, bias=True)\n    (pooler_activation): Tanh()\n  )\n  (dropout): Dropout(p=0.5, inplace=False)\n  (cls_layer): Linear(in_features=768, out_features=1, bias=True)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = SentencePairClassifier()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n",
    "    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            # 学习率预热(线性增加)\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        # 学习率线性衰减(最小为0)\n",
    "        # num_training_steps后学习率恒为0\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    loss, count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, token_type_ids, labels in dataloader:\n",
    "            seq, attn_masks, token_type_ids, labels = seq.to(device), attn_masks.to(device), token_type_ids.to(\n",
    "                device), labels.to(device)\n",
    "            logits = net(seq, attn_masks, token_type_ids)\n",
    "            loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            count += 1\n",
    "\n",
    "    return loss / count  # 每轮的平均损失\n",
    "\n",
    "\n",
    "def train_bert(net, criterion, opti, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n",
    "    for ep in range(epochs):\n",
    "        net.train()\n",
    "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(train_loader):\n",
    "            seq, attn_masks, token_type_ids, labels = seq.to(device), attn_masks.to(device), token_type_ids.to(\n",
    "                device), labels.to(device)\n",
    "            logits = net(seq, attn_masks, token_type_ids)\n",
    "            # logits.shape=(32, 1)\n",
    "            loss = criterion(logits.squeeze(-1), labels.to(torch.float))\n",
    "            ###########################################################################\n",
    "            # 梯度累加可以先累加多个batch的梯度再进行一次参数更新,相当于增大了batch-size\n",
    "            loss = loss / iters_to_accumulate\n",
    "            loss.backward()  # 每次获取1个batch的数据,计算1次梯度,梯度不清空,不断累加\n",
    "            if (it + 1) % iters_to_accumulate == 0:  # 累加一定次数后,根据累加的梯度更新网络参数,然后清空梯度,进行下一次循环\n",
    "                opti.step()\n",
    "                lr_scheduler.step()\n",
    "                opti.zero_grad()\n",
    "            ###########################################################################\n",
    "        val_loss = evaluate_loss(net, device, criterion, val_loader)\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep + 1, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6f797e5377fe4ab281003d537c15f279",
      "9b46878367244033ac0d96f6f83766cd",
      "b9b102512ad548ac96f5560eead35201",
      "cb3cf3d7089347a092499762c552f785",
      "4ae80720349c409bba8dffa6ba429e9e",
      "c74fcaee125642908ab8fa8905f0cf6f",
      "6a229158e0db415b8561dc22a8a4ab47",
      "ef906f1d1e444404bd88e789e14e977a",
      "80d383da347f41a6927d659dc5e1a855",
      "26425569e06e4fc5b32405bcdba37dad",
      "c6897db24c284263879bcc92e6d56f39",
      "7468d3da8c8547d38e146b8adee8948e",
      "e90a00d220bc4546923517f13daf29be",
      "13b7985246244fcb89926039ded8eb20",
      "3e108b6d60874974aaf0eea761e7da59",
      "afe53d24858d446592d1825713664d9b",
      "ac95062870cd4eb3afc93dfcc8ef9a6e",
      "d79473c349744b6ba4d1532889a3dd3b",
      "bac3fa1e8ff04a1680267702e35c89b1",
      "c425744e85d44659b143679eccca5b0f",
      "a9c4247af57f46d485a4f509eafc79e3",
      "6a4d58a9a2a84f09b3d341529de530ed",
      "c07487bb1e8c4415986d31752fa0d7f5",
      "9a003cc981484793b31167d114362ea4"
     ]
    },
    "colab_type": "code",
    "id": "VZWGPomoryxy",
    "jupyter": {},
    "outputId": "25d2f91d-ed8e-4d59-b6cf-cffaff9aaf4b",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iters_to_accumulate = 2\n",
    "lr = 2e-5  # 学习率\n",
    "epochs = 5  # 训练轮数\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # 二分类交叉熵损失函数\n",
    "opti = optim.AdamW(net.parameters(), lr=lr, weight_decay=1e-2)  # 优化器\n",
    "t_total = (len(train_loader) // iters_to_accumulate) * epochs  # 总步数\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=0,\n",
    "                                               num_training_steps=t_total)  # 学习率策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete! Validation Loss : 0.46110450945518633\n",
      "Epoch 2 complete! Validation Loss : 0.34429635962954275\n",
      "Epoch 3 complete! Validation Loss : 0.33958045338039045\n",
      "Epoch 4 complete! Validation Loss : 0.36206909600231385\n",
      "Epoch 5 complete! Validation Loss : 0.375990265911376\n"
     ]
    }
   ],
   "source": [
    "train_bert(net, criterion, opti, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(net, device, dataloader):\n",
    "    net.eval()\n",
    "    probs_all = []\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, token_type_ids, _ in dataloader:\n",
    "            seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
    "            logits = net(seq, attn_masks, token_type_ids).squeeze(-1)\n",
    "            probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "            probs = probs.detach().cpu().numpy().squeeze(-1)\n",
    "            probs_all.extend(probs.tolist())\n",
    "    return np.array(probs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99298429 0.04565214 0.83503133 0.90706313 0.04442825 0.99266899\n",
      " 0.08452644 0.99114311 0.99298179 0.9937017  0.99283469 0.04185068\n",
      " 0.03460848 0.95590115 0.96891731 0.99206102 0.98882383 0.04016789\n",
      " 0.99308264 0.90801013 0.04755341 0.33021644 0.05235379 0.99285853\n",
      " 0.94888848 0.68846685 0.97375488 0.99145329 0.98986584 0.99317825\n",
      " 0.54840916 0.9933843  0.99123281 0.99275726 0.98958123 0.89708847\n",
      " 0.04817563 0.04130866 0.98649317 0.99266982 0.11675536 0.99151719\n",
      " 0.30865005 0.05822035 0.03599201 0.99234009 0.99354941 0.04328445\n",
      " 0.99353904 0.98876733 0.96335608 0.99286193 0.9889642  0.9933694\n",
      " 0.99088436 0.99284726 0.85534787 0.99275619 0.99353927 0.98396075\n",
      " 0.03855406 0.34536195 0.99367887 0.99213785 0.99090648 0.09596301\n",
      " 0.99262166 0.99033499 0.07382733 0.99459141 0.98917496 0.96623868\n",
      " 0.9934783  0.98693538 0.99254227 0.98829657 0.96153784 0.99395555\n",
      " 0.99337834 0.99188143 0.35985035 0.98949534 0.99277675 0.05196293\n",
      " 0.9846732  0.38236961 0.98823255 0.1154686  0.99321717 0.99366152\n",
      " 0.0459131  0.99305177 0.99385262 0.94947654 0.83379555 0.9939943\n",
      " 0.19406916 0.96616167 0.98586547 0.99053103 0.98840743 0.32325152\n",
      " 0.98385781 0.98979712 0.61333716 0.99348068 0.90689802 0.0492834\n",
      " 0.06363925 0.99161184 0.61851954 0.9601565  0.92483026 0.99325812\n",
      " 0.18594564 0.94111693 0.98588538 0.99349326 0.992634   0.98903883\n",
      " 0.99288428 0.07686723 0.09718512 0.99204552 0.99039793 0.9931584\n",
      " 0.99160039 0.04518995 0.99395132 0.99096745 0.98263562 0.08664613\n",
      " 0.99296898 0.0951099  0.98458314 0.99073046 0.03288184 0.10320324\n",
      " 0.99409384 0.9873116  0.99412537 0.67850995 0.03538942 0.99256092\n",
      " 0.08372881 0.98682308 0.99419701 0.99286735 0.97283834 0.9905827\n",
      " 0.03006536 0.78868574 0.98222625 0.99053383 0.98581308 0.99097693\n",
      " 0.99376833 0.9868623  0.89573115 0.99125415 0.98741871 0.99114519\n",
      " 0.99366498 0.99445713 0.96044046 0.99111432 0.96426147 0.03953684\n",
      " 0.98298025 0.59535557 0.07712698 0.14143769 0.98083478 0.93918681\n",
      " 0.32749444 0.98659199 0.99336505 0.0601652  0.9887135  0.99321365\n",
      " 0.68668854 0.98549056 0.98744428 0.99131471 0.9705556  0.98195618\n",
      " 0.71435767 0.93601125 0.08736943 0.99124873 0.99384224 0.03848556\n",
      " 0.98318553 0.99400169 0.51003706 0.97265536 0.99425215 0.08080769\n",
      " 0.9926247  0.98975742 0.99402076 0.08903327 0.94052172 0.97792715\n",
      " 0.99036193 0.02816598 0.98013937 0.99230087 0.8610884  0.1073999\n",
      " 0.09192418 0.98647404 0.96739244 0.97480744 0.99207348 0.99235415\n",
      " 0.97843915 0.99295771 0.05147683 0.99205583 0.59396207 0.98510355\n",
      " 0.98995471 0.06304695 0.97649515 0.99285549 0.99311465 0.98392248\n",
      " 0.99183249 0.991319   0.99141508 0.98019743 0.96062458 0.23999701\n",
      " 0.52850723 0.9765718  0.97967231 0.91745383 0.09378222 0.9032706\n",
      " 0.97108424 0.04106276 0.99136484 0.9880833  0.99171036 0.99213409\n",
      " 0.9893328  0.15770781 0.99347854 0.99186534 0.99295348 0.15569381\n",
      " 0.2659764  0.99236828 0.11123249 0.06147354 0.9930681  0.9910177\n",
      " 0.98387569 0.36292103 0.99084634 0.99035686 0.98993438 0.97605646\n",
      " 0.9349252  0.98399782 0.97926956 0.04677381 0.6674791  0.98966306\n",
      " 0.05931616 0.9905358  0.98837799 0.99387687 0.99304658 0.98417974\n",
      " 0.97079909 0.23422419 0.99253619 0.02861471 0.16493095 0.98982215\n",
      " 0.14943424 0.09471744 0.28767058 0.9932521  0.99270141 0.9749434\n",
      " 0.99264044 0.18023494 0.05744552 0.04953363 0.99262601 0.99297839\n",
      " 0.98714417 0.04537377 0.03663536 0.97810239 0.99352962 0.28544194\n",
      " 0.99426693 0.99297661 0.98483527 0.52224565 0.47201622 0.99282974\n",
      " 0.06058272 0.99162889 0.12454417 0.6249668  0.68870193 0.99206525\n",
      " 0.84998739 0.03876543 0.99368423 0.07494642 0.53694093 0.99013913\n",
      " 0.08972124 0.86778378 0.05639744 0.03104924 0.02298642 0.73001498\n",
      " 0.0320238  0.99215031 0.97354239 0.99102205 0.99179584 0.79115224\n",
      " 0.98879492 0.99206513 0.38972899 0.64693046 0.99277085 0.98830867\n",
      " 0.99251318 0.98579872 0.03752008 0.98616576 0.99331844 0.99195731\n",
      " 0.04822076 0.17717162 0.98896176 0.99280787 0.98647308 0.99176425\n",
      " 0.11359432 0.98995423 0.08483319 0.98947108 0.95379978 0.99359506\n",
      " 0.04742085 0.4700906  0.99046993 0.97811389 0.98193723 0.99342513\n",
      " 0.08809936 0.98349619 0.99434173 0.12234892 0.99327201 0.99293631\n",
      " 0.99028683 0.98669761 0.23548377 0.08148941 0.94977278 0.99286145\n",
      " 0.14098448 0.67337072 0.03682378 0.06948805 0.21377224 0.0612086\n",
      " 0.46903786 0.9942404  0.97921801 0.99325883 0.08765143 0.99120075\n",
      " 0.98200154 0.11958081 0.97727978 0.16496989 0.99307984 0.59092015\n",
      " 0.99404538 0.91277397 0.99163413 0.98763484 0.98954904 0.97799802\n",
      " 0.99372596 0.9337666  0.98912489 0.98943347 0.12625508 0.99425656\n",
      " 0.96346736 0.02607535 0.08421315 0.99349409 0.04805671 0.73338169]\n",
      "(408,)\n"
     ]
    }
   ],
   "source": [
    "test_set = CustomDataset(df_test, maxlen, bert_model)\n",
    "test_loader = DataLoader(test_set, batch_size=bs)\n",
    "test_probs_all = test_prediction(net, device, test_loader)\n",
    "print(test_probs_all)  # 概率向量\n",
    "print(test_probs_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n       1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n       1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n       0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n       1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n       0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.6  # 阈值\n",
    "test_bool_all = np.where(test_probs_all >= threshold, 1, 0)\n",
    "test_bool_all"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.74      0.77       129\n",
      "           1       0.89      0.91      0.90       279\n",
      "\n",
      "    accuracy                           0.86       408\n",
      "   macro avg       0.84      0.83      0.84       408\n",
      "weighted avg       0.86      0.86      0.86       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels_test = df_test['label']\n",
    "print(classification_report(labels_test, test_bool_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}