{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## xgboost主要调节参数\n",
    "\n",
    "1. 其他参数\n",
    "    * booster\n",
    "    * nthread\n",
    "    * objective\n",
    "    * num_class\n",
    "    * verbosity\n",
    "    * eval_metric\n",
    "    * *****************************\n",
    "    * dtrain\n",
    "    * num_boost_round\n",
    "    * evals\n",
    "    * early_stopping_rounds\n",
    "    * evals_result\n",
    "    * feval\n",
    "    * verbose_eval\n",
    "\n",
    "2. 树调节参数\n",
    "    * max_depth\n",
    "    * min_child_weight\n",
    "    * gamma/min_split_loss\n",
    "\n",
    "2. 防止过拟合参数\n",
    "    * eta/learning_rate\n",
    "    * subsample\n",
    "    * colsample_bytree\n",
    "    * colsample_bylevel\n",
    "    * reg_alpha/alpha\n",
    "    * reg_lambda/lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250, 54)\n",
      "(2250,)\n",
      "[1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "X = datasets.fetch_covtype().data[:3000]\n",
    "y = datasets.fetch_covtype().target[:3000]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(np.unique(y_train))  # 7分类任务"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "enc = OrdinalEncoder()\n",
    "y_train_enc = enc.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_enc = enc.transform(y_test.reshape(-1, 1))\n",
    "print(np.unique(y_train_enc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "train_dataset = xgb.DMatrix(data=X_train, label=y_train_enc)\n",
    "test_dataset = xgb.DMatrix(data=X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "(750, 7)"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Specify the learning task and the corresponding learning objective.\n",
    "objective [default=reg:squarederror]\n",
    "    reg:squarederror: regression with squared loss.\n",
    "    binary:logistic: logistic regression for binary classification, output probability\n",
    "    multi:softmax: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)\n",
    "    multi:softprob: same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix.\n",
    "        The result contains predicted probability of each data point belonging to each class.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Evaluation metrics for validation data, a default metric will be assigned according to objective (rmse for regression, and logloss for classification, mean average precision for ranking)\n",
    "User can add multiple evaluation metrics.\n",
    "    rmse: root mean square error\n",
    "    mae: mean absolute error\n",
    "    logloss: negative log-likelihood\n",
    "    error: Binary classification error rate.\n",
    "    merror: Multiclass classification error rate.\n",
    "    mlogloss: Multiclass logloss.\n",
    "    auc: Receiver Operating Characteristic Area under the Curve. Available for classification and learning-to-rank tasks.\n",
    "        * When used with binary classification, the objective should be binary:logistic or similar functions that work on probability.\n",
    "        * When used with multi-class classification, objective should be multi:softprob instead of multi:softmax, as the latter doesn’t output probability. Also the AUC is calculated by 1-vs-rest with reference class weighted by class prevalence.\n",
    "\"\"\"\n",
    "params = {'objective': 'multi:softprob',\n",
    "          \"eval_metric\": 'mlogloss',\n",
    "          'num_class': 7}  # 多分类类别数量(多分类任务时必须指定)\n",
    "model = xgb.train(params=params, dtrain=train_dataset)  # 分类问题中y标签必须从0开始\n",
    "model.predict(test_dataset).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "(750, 7)"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "booster [default= gbtree ]\n",
    "    Which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions.\n",
    "'''\n",
    "params = {'objective': 'multi:softprob',\n",
    "          \"eval_metric\": 'mlogloss',\n",
    "          'num_class': 7,\n",
    "          'booster': 'dart'}  # 若booster='gblinear',可调用sklearn API输出coef_,intercept_\n",
    "model = xgb.train(params=params, dtrain=train_dataset)\n",
    "model.predict(test_dataset).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 22 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 22 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 22 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 22 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 28 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 28 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 22 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16:44:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_prune.cc:101: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=6\n"
     ]
    },
    {
     "data": {
      "text/plain": "(750, 7)"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'objective': 'multi:softprob',\n",
    "          \"eval_metric\": 'mlogloss',\n",
    "          'num_class': 7,\n",
    "          \"nthread\": -1,  # default to maximum number of threads available if not set\n",
    "          # verbosity: Verbosity of printing messages. Valid values of 0 (silent), 1 (warning), 2 (info), and 3 (debug).\n",
    "          \"verbosity\": 2\n",
    "          }\n",
    "model = xgb.train(params=params, dtrain=train_dataset)\n",
    "model.predict(test_dataset).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "(750, 7)"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'objective': 'multi:softprob',\n",
    "          \"eval_metric\": 'mlogloss',\n",
    "          \"verbosity\": 0,\n",
    "          'num_class': 7,\n",
    "          # Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit\n",
    "          'max_depth': 6  # 默认max_depth=6\n",
    "          }\n",
    "model = xgb.train(params=params, dtrain=train_dataset)\n",
    "model.predict(test_dataset).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "(750, 7)"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'objective': 'multi:softprob',\n",
    "          \"eval_metric\": 'mlogloss',\n",
    "          'verbosity': 0,\n",
    "          'num_class': 7,\n",
    "          # Minimum sum of instance weight (hessian) needed in a child.\n",
    "          'min_child_weight': 2  # 默认min_child_weight=2\n",
    "          }\n",
    "model = xgb.train(params=params, dtrain=train_dataset)\n",
    "model.predict(test_dataset).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "(750, 7)"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "gamma [default=0, alias: min_split_loss]\n",
    "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "    The larger gamma is, the more conservative the algorithm will be.\n",
    "'''\n",
    "params = {'objective': 'multi:softprob',\n",
    "          \"eval_metric\": 'mlogloss',\n",
    "          'verbosity': 0,\n",
    "          'num_class': 7,\n",
    "          'gamma': 1\n",
    "          }\n",
    "model = xgb.train(params=params, dtrain=train_dataset)\n",
    "model.predict(test_dataset).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "(750, 7)"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "eta [default=0.3, alias: learning_rate]\n",
    "    Step size shrinkage used in update to prevents overfitting.\n",
    "'''\n",
    "params = {'objective': 'multi:softprob',\n",
    "          \"eval_metric\": 'mlogloss',\n",
    "          'verbosity': 0,\n",
    "          'num_class': 7,\n",
    "          'eta': 0.01\n",
    "          }\n",
    "model = xgb.train(params=params, dtrain=train_dataset)\n",
    "model.predict(test_dataset).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "(750, 7)"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "subsample [default=1]\n",
    "    Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "    range: (0,1]\n",
    "'''\n",
    "params = {'objective': 'multi:softprob',\n",
    "          \"eval_metric\": 'mlogloss',\n",
    "          'verbosity': 0,\n",
    "          'num_class': 7,\n",
    "          'subsample': 0.8\n",
    "          }\n",
    "model = xgb.train(params=params, dtrain=train_dataset)\n",
    "model.predict(test_dataset).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "(750, 7)"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# colsample_bytree is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n",
    "params = {'objective': 'multi:softprob',\n",
    "          \"eval_metric\": 'mlogloss',\n",
    "          'verbosity': 0,\n",
    "          'num_class': 7,\n",
    "          'colsample_bytree': 0.9\n",
    "          }\n",
    "model = xgb.train(params=params, dtrain=train_dataset)\n",
    "model.predict(test_dataset).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# colsample_bylevel is the subsample ratio of columns for each level.\n",
    "# Subsampling occurs once for every new depth level reached in a tree.\n",
    "# Columns are subsampled from the set of columns chosen for the current tree.\n",
    "params = {'objective': 'multi:softprob',\n",
    "          \"eval_metric\": 'mlogloss',\n",
    "          'verbosity': 0,\n",
    "          'num_class': 7,\n",
    "          'colsample_bylevel': 0.9\n",
    "          }\n",
    "model = xgb.train(params=params, dtrain=train_dataset)\n",
    "model.predict(test_dataset).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "(750, 7)"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "alpha [default=0, alias: reg_alpha]\n",
    "    L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "'''\n",
    "params = {'objective': 'multi:softprob',\n",
    "          \"eval_metric\": 'mlogloss',\n",
    "          'verbosity': 0,\n",
    "          'num_class': 7,\n",
    "          'alpha': 0.1\n",
    "          }\n",
    "model = xgb.train(params=params, dtrain=train_dataset)\n",
    "model.predict(test_dataset).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "(750, 7)"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "lambda [default=0, alias: reg_lambda]\n",
    "    L2 regularization term on weights.\n",
    "    Increasing this value will make model more conservative. Normalised to number of training examples.\n",
    "'''\n",
    "params = {'objective': 'multi:softprob',\n",
    "          \"eval_metric\": 'mlogloss',\n",
    "          'verbosity': 0,\n",
    "          'num_class': 7,\n",
    "          'lambda': 0.1\n",
    "          }\n",
    "model = xgb.train(params=params, dtrain=train_dataset)\n",
    "model.predict(test_dataset).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}