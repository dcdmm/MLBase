{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 感知机模型定义\n",
    "&emsp;&emsp;假设输入空间(特征空间)是$ \\mathcal{X} \\subseteq \\mathbf{R}^n $,输出空间\n",
    "是$ \\mathcal{Y} = \\{ +1, -1\\} $.输入$ \\mathbf{x} \\in \\mathcal{X} $表示实例的特征向量,对应于输入空间\n",
    "(特征空间)的点;输出$ y \\in \\mathcal{Y} $表示实例的类别.由输入空间到输出空间的如下函数:   \n",
    "$$ f(\\mathbf{x}) = \\mathrm{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b) $$   \n",
    "称为感知机.其中 $ \\mathbf{w} $和$ b$为感知机模型的参数,$ \\mathbf{w} \\in \\mathbf{R}^n $叫做权值(weight)或权值向量(weight vector),$ b \\in \\mathbf{R} $叫作\n",
    "偏置(bias), $ \\mathbf{w} \\cdot \\mathbf{x}  $表示$ \\mathbf{w} $和$ \\mathbf{x} $的内积.$ \\mathrm{sign} $是符号函数,即    \n",
    "\\begin{equation}\n",
    "\\mathrm{sign}(x)=\\begin{cases}\n",
    "\t\t    +1, & \\text{if} \\quad  x \\geq 0 \\\\\n",
    "            -1, & \\text{if} \\quad  x <0\n",
    "     \\end{cases}\n",
    "\\end{equation}    \n",
    "&emsp;&emsp;感知机是一种线性分类模型,属于判别模型     \n",
    "\n",
    "\n",
    "## 感知机模型损失函数\n",
    "&emsp;&emsp;给定训练数据集    \n",
    "$$ T = \\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2),\\dots,(\\mathbf{x}_N, y_N)\\} $$   \n",
    "其中$ \\mathbf{x}_i \\in \\mathcal{X} = \\mathbf{R}^n,y_i \\in \\mathcal{Y} =\\{-1,1\\},i=1,2,\\dots,N $.感知机$\\mathrm{sign}(w \\cdot \\mathbf{x} + b) $学习的损失函数定义为   \n",
    "$$ L(w,b) = -\\sum_{\\mathbf{x}_i \\in M} y_i(\\mathbf{w} \\cdot \\mathbf{x}_i +b) $$   \n",
    "其中$ M $为误分类点的集合.这个损失函数就是感知机学习的经验风险函数.                   \n",
    "&emsp;&emsp;显然,损失函数$L(\\mathbf{w}, b)$是非负的.如果没有误分类点,损失函数就是0.而且,误分类点越少,误分类点离超平面越近,损失函数\n",
    "值就越小.一个特定的样本点的损失函数:在误分类时是参数$\\mathbf{w},b$的线性函数,在正确分类时是0.因此,给定训练数据集$T$,损失函数$L(\\mathbf{w},b)$是$\\mathbf{w},b$的连续可导函数.      \n",
    "&emsp;&emsp;感知机学习的策略是在假设空间中选取使损失函数式最小的模型参数$\\mathbf{w},b$.\n",
    "\n",
    "\n",
    "## 感知机学习算法的原始形式\n",
    "<font color='red' size=4>算法(随机梯度下降法):</font>   \n",
    "输入:训练数据集$ T = \\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2),\\dots,(\\mathbf{x}_N, y_N)\\} $,其中$ \\mathbf{x}_i \\in \\mathcal{X} = \\mathbf{R}^n,y_i \\in \\mathcal{Y} =\\{-1,1\\},i=1,2,\\dots,N $;\n",
    "学习率$ \\eta(0< \\eta \\leq 1) $;     \n",
    "输出:$ \\mathbf{w},b$;感知机模型$ f(\\mathbf{x}) = \\mathrm{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b) $ \n",
    " \n",
    "1. 选取初值 $ \\mathbf{w}_0, b_0 $; \n",
    "2. 在训练集中选取数据($ \\mathbf{x}_i, y_i $)  \n",
    "3. 如果$ y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\leq 0 $   \n",
    "$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta y_i \\mathbf{x}_i $     \n",
    "$ b \\leftarrow b + \\eta y_i $\n",
    "4. 转至2,直到训练集中没有误分类点\n",
    "\n",
    "\n",
    "## 感知机学习算法的对偶形式\n",
    "&emsp;&emsp;对偶形式的基本想法是,将$ \\mathbf{w} $和$ b $表示为实例$ \\mathbf{x}_i $和标记$ y_i $的线性组合的形式,通过求解其系数而求\n",
    "得$\\mathbf{w}$和$b$.不失一般性,可假定初始值$w_0,b_0$均为0.对误分类点$(\\mathbf{x}_i, y_i)$通过          \n",
    "\\begin{align}   \n",
    "\\mathbf{w}   & \\leftarrow \\mathbf{w} + \\eta y_i \\mathbf{x}_i \\\\\n",
    "b   & \\leftarrow b + \\eta y_i\n",
    "\\end{align}      \n",
    "逐步修改$\\mathbf{w},b$,设修改$n$此(由随机梯度下降法得),则$\\mathbf{w},b$关于$(\\mathbf{x}_i, y_i)$的增量分别是$\\alpha_i y_i \\mathbf{x}_i$和$\\alpha_i y_i$,这里$\\alpha_i = n_i \\eta$.这样,从学习过程不难看出,\n",
    "最优学习到的$\\mathbf{w},b$可以分别表示为           \n",
    "\\begin{align}    \n",
    "\\mathbf{w}   & =  \\sum_{i=1}^{N} \\alpha_i y_i \\mathbf{x}_i     \\\\\n",
    "b   & =  \\sum_{i=1}^{N} \\alpha_i y_i     \n",
    "\\end{align}            \n",
    "这里$\\alpha_i \\leq 0, i=1,2,\\dots, N$当$\\eta = 1$时,表示第$i$个实例点由于误分而进行更新的次数.实例点更新次数越多,意味着它距离分离超平面越近,\n",
    "也就是越难正确分类,换句话说,这样的实例对学习结果影响最大.              \n",
    "\n",
    "<font color='red' size=4>算法:</font>   \n",
    "输入:训练数据集$ T = \\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2),\\dots,(\\mathbf{x}_N, y_N)\\} $,其中$ \\mathbf{x}_i \\in \\mathcal{X} = \\mathbf{R}^n,y_i \\in \\mathcal{Y} =\\{-1,1\\},i=1,2,\\dots,N $;\n",
    "学习率$ \\eta(0< \\eta \\leq 1) $;     \n",
    "输出:$ a,b$;感知机模型$ f(\\mathbf{x})=\\mathrm{sign}\\left(\\sum \\limits _{j=1}^N  \\alpha_j y_j \\mathbf{x}_j \\cdot \\mathbf{x} + b \\right) $,其中$ \\alpha=(\\alpha_1, \\alpha_2, \\dots, \\alpha_N)^N $\n",
    "1. $ \\alpha \\leftarrow 0, b \\leftarrow 0 $\n",
    "2. 在训练集中选取数据$ (\\mathbf{x}_i, y_i) $\n",
    "3. 如果$ y_i\\left(\\sum \\limits _{j=1}^N  \\alpha_j y_j \\mathbf{x}_j \\cdot \\mathbf{x}_i + b \\right) \\leq 0$  \n",
    "***\n",
    "<font color='red' size=4>分析</font>\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum \\limits _{j=1}^N  \\alpha_j y_j \\mathbf{x}_j \\cdot X=\\begin{cases}\n",
    "\t\t                                        \\alpha_1 y_1 \\mathbf{x}_1 \\cdot \\mathbf{x}_1 + \\alpha_2 y_2 \\mathbf{x}_2 \\cdot \\mathbf{x}_1 + \\dots + \\alpha_N y_N \\mathbf{x}_N \\cdot \\mathbf{x}_1     \\\\\n",
    "\t\t                                        \\alpha_1 y_1 \\mathbf{x}_1 \\cdot \\mathbf{x}_2 + \\alpha_2 y_2 \\mathbf{x}_2 \\cdot \\mathbf{x}_2 + \\dots + \\alpha_N y_N \\mathbf{x}_N \\cdot \\mathbf{x}_2     \\\\\n",
    "\t\t                                        \\dots                                                                                            \\\\\n",
    "\t\t                                        \\alpha_1 y_1 \\mathbf{x}_1 \\cdot \\mathbf{x}_N + \\alpha_2 y_2 \\mathbf{x}_2 \\cdot \\mathbf{x}_N + \\dots + \\alpha_N y_N \\mathbf{x}_N \\cdot \\mathbf{x}_N     \\\\\n",
    "                                                \\end{cases}\n",
    "\\end{equation}     \n",
    "将后面的内积计算$ \\mathbf{x}_1 \\cdot \\mathbf{x}_1 + \\mathbf{x}_2 \\cdot \\mathbf{x}_1, \\dots \\mathbf{x}_N \\cdot \\mathbf{x}_1 $等提取出来,组成的矩阵就是Gram矩阵,即    \n",
    "G = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}_1 \\cdot \\mathbf{x}_1 & \\dots  & \\mathbf{x}_N \\cdot \\mathbf{x}_1 \\\\\n",
    "\\mathbf{x}_1 \\cdot \\mathbf{x}_2 & \\dots  & \\mathbf{x}_N  \\cdot \\mathbf{x}_2 \\\\\n",
    "\\vdots        & \\vdots & \\vdots  \\\\\n",
    "\\mathbf{x}_1 \\cdot \\mathbf{x}_N &\t \\dots & x_N \\cdot x_N \n",
    "\\end{bmatrix}   \n",
    "将前面的常数组成行向量,即$\\left[ \\alpha_1 y_1, \\alpha_2 y_2, \\dots ,\\alpha_N y_N \\right]$  \n",
    "故$ \\sum \\limits _{j=1}^N  \\alpha_j y_j \\mathbf{x}_j \\cdot X $可以写成矩阵形式,即   \n",
    "$\\left[ \\alpha_1 y_1, \\alpha_2 y_2, \\dots ,\\alpha_N y_N \\right]$\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}_1 \\cdot \\mathbf{x}_1 & \\dots  & \\mathbf{x}_N \\cdot \\mathbf{x}_1 \\\\\n",
    "\\mathbf{x}_1 \\cdot \\mathbf{x}_2 & \\dots  & \\mathbf{x}_N  \\cdot \\mathbf{x}_2 \\\\\n",
    "\\vdots        & \\vdots & \\vdots  \\\\\n",
    "\\mathbf{x}_1 \\cdot \\mathbf{x}_N &\t \\dots & \\mathbf{x}_N \\cdot \\mathbf{x}_N \n",
    "\\end{bmatrix}   \n",
    "其中第一列就是计算$ \\mathbf{x}_{1} $时候的误分判别计算式,第$ i $个样本要计算的误分判别式在第$ i $列  \n",
    "***\n",
    "$ \\alpha_i \\leftarrow \\alpha_i + \\eta $      \n",
    "$ b \\leftarrow b + \\eta y_i $  \n",
    "4. 转至2直到没有误分类数据\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}