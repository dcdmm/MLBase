{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455eb723-2751-4458-884b-14b5cdfd8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from transformers import (BertTokenizerFast, BertModel)\n",
    "from torch.utils.data import (DataLoader, Dataset)\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import GlobalPointer\n",
    "from utils import loss_fun, MetricsCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d50efe1-fa9b-457e-8c87-7aaea11c4989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "seed = 2022\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58753c9-727b-46b1-b693-9f3a8ce7ebbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33437e9e-1ae7-4a9b-be88-a44eb1b0bf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sent': '胸 廓 对 称 ， 气 管 居 中 。 所 见 骨 骼 骨 质 结 构 完 整 。 双 肺 纹 理 清 晰 。 两 肺 门 影 不 大 。 心 影 横 径 增 大 ， 左 心 缘 饱 满 。 两 侧 膈 面 光 整 ， 两 侧 肋 膈 角 锐 利 。 1 . 两 肺 未 见 明 显 活 动 性 病 变 ， 随 诊 。 2 . 心 影 改 变 请 结 合 临 床 。', 'ners': [[0, 2, '器官组织', '胸廓'], [2, 4, '阴性表现', '对称'], [5, 7, '器官组织', '气管'], [7, 9, '阴性表现', '居中'], [12, 16, '器官组织', '骨骼骨质'], [16, 18, '属性', '结构'], [18, 20, '阴性表现', '完整'], [21, 23, '器官组织', '双肺'], [23, 25, '属性', '纹理'], [25, 27, '阴性表现', '清晰'], [28, 32, '器官组织', '两肺门影'], [32, 34, '阴性表现', '不大'], [35, 37, '器官组织', '心影'], [37, 39, '属性', '横径'], [39, 41, '阳性表现', '增大'], [42, 45, '器官组织', '左心缘'], [45, 47, '阳性表现', '饱满'], [48, 52, '器官组织', '两侧膈面'], [52, 54, '阴性表现', '光整'], [55, 60, '器官组织', '两侧肋膈角'], [60, 62, '阴性表现', '锐利'], [65, 67, '器官组织', '两肺'], [67, 69, '否定描述', '未见'], [69, 71, '修饰描述', '明显'], [71, 74, '修饰描述', '活动性'], [74, 76, '异常现象', '病变'], [82, 84, '器官组织', '心影'], [84, 86, '异常现象', '改变']], 'spans': [[1, 2, '器官组织', '胸廓'], [3, 4, '阴性表现', '对称'], [6, 7, '器官组织', '气管'], [8, 9, '阴性表现', '居中'], [13, 16, '器官组织', '骨骼骨质'], [17, 18, '属性', '结构'], [19, 20, '阴性表现', '完整'], [22, 23, '器官组织', '双肺'], [24, 25, '属性', '纹理'], [26, 27, '阴性表现', '清晰'], [29, 32, '器官组织', '两肺门影'], [33, 34, '阴性表现', '不大'], [36, 37, '器官组织', '心影'], [38, 39, '属性', '横径'], [40, 41, '阳性表现', '增大'], [43, 45, '器官组织', '左心缘'], [46, 47, '阳性表现', '饱满'], [49, 52, '器官组织', '两侧膈面'], [53, 54, '阴性表现', '光整'], [56, 60, '器官组织', '两侧肋膈角'], [61, 62, '阴性表现', '锐利'], [66, 67, '器官组织', '两肺'], [68, 69, '否定描述', '未见'], [70, 71, '修饰描述', '明显'], [72, 74, '修饰描述', '活动性'], [75, 76, '异常现象', '病变'], [83, 84, '器官组织', '心影'], [85, 86, '异常现象', '改变']]}\n",
      "\n",
      "3557\n",
      "396\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/train_ner.json', 'r', encoding='utf-8') as f:\n",
    "    train = json.load(f)  # 列表\n",
    "print(train[0], end='\\n\\n')\n",
    "\n",
    "# 划分训练/验证数据集\n",
    "train_data, valid_data = train_test_split(train, test_size=0.1, random_state=seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202078c3-00c5-4f39-870f-f4522f2a8e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ent2id': {'修饰描述': 0,\n",
       "  '否定描述': 1,\n",
       "  '器官组织': 2,\n",
       "  '属性': 3,\n",
       "  '异常现象': 4,\n",
       "  '手术': 5,\n",
       "  '指代': 6,\n",
       "  '数量': 7,\n",
       "  '期象': 8,\n",
       "  '检查手段': 9,\n",
       "  '测量值': 10,\n",
       "  '疾病': 11,\n",
       "  '病理分型': 12,\n",
       "  '病理分期': 13,\n",
       "  '病理分级': 14,\n",
       "  '累及部位': 15,\n",
       "  '阳性表现': 16,\n",
       "  '阴性表现': 17},\n",
       " 'id2ent': {'0': '修饰描述',\n",
       "  '1': '否定描述',\n",
       "  '2': '器官组织',\n",
       "  '3': '属性',\n",
       "  '4': '异常现象',\n",
       "  '5': '手术',\n",
       "  '6': '指代',\n",
       "  '7': '数量',\n",
       "  '8': '期象',\n",
       "  '9': '检查手段',\n",
       "  '10': '测量值',\n",
       "  '11': '疾病',\n",
       "  '12': '病理分型',\n",
       "  '13': '病理分期',\n",
       "  '14': '病理分级',\n",
       "  '15': '累及部位',\n",
       "  '16': '阳性表现',\n",
       "  '17': '阴性表现'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('datasets/ent_map_id.json', 'r', encoding='utf-8') as f:\n",
    "    ent_to_ot_id = json.load(f)\n",
    "ent_to_ot_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede3e891-9c48-4329-a013-e23d9b048289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "胸 腰 椎 C T 平 扫 + 三 维 重 建 所 示 层 面 的 胸 腰 段 椎 体 生 理 曲 度 存 在 ， 椎 体 序 列 尚 规 则 ， 胸 1 2 椎 体 骨 质 连 续 性 中 断 、 椎 体 高 度 变 扁 呈 楔 形 改 变 、 密 度 不 均 ； 余 所 示 诸 椎 体 边 缘 及 椎 小 关 节 可 见 骨 质 增 生 、 硬 化 影 ， 部 分 变 尖 ， 椎 间 隙 无 明 显 狭 窄 ， 腰 3 / 4 、 腰 4 / 5 椎 间 盘 向 周 围 隆 起 ， 硬 膜 囊 受 压 ， 椎 管 未 见 明 显 狭 窄 。 1 . 胸 1 2 椎 体 压 缩 性 骨 折 ； 2 . 胸 腰 椎 退 行 性 变 ； 腰 3 / 4 、 腰 4 / 5 椎 间 盘 膨 出 。\n",
      "[[0, 3, '器官组织', '胸腰椎'], [3, 7, '检查手段', 'CT平扫'], [8, 12, '检查手段', '三维重建'], [17, 22, '器官组织', '胸腰段椎体'], [22, 26, '属性', '生理曲度'], [26, 28, '阴性表现', '存在'], [29, 31, '器官组织', '椎体'], [31, 33, '属性', '序列'], [34, 36, '阴性表现', '规则'], [37, 44, '器官组织', '胸12椎体骨质'], [44, 47, '属性', '连续性'], [47, 49, '阳性表现', '中断'], [50, 52, '器官组织', '椎体'], [54, 56, '阳性表现', '变扁'], [57, 59, '修饰描述', '楔形'], [59, 61, '异常现象', '改变'], [62, 64, '属性', '密度'], [64, 66, '阳性表现', '不均'], [70, 73, '器官组织', '诸椎体'], [73, 75, '属性', '边缘'], [76, 80, '器官组织', '椎小关节'], [82, 86, '异常现象', '骨质增生'], [87, 90, '异常现象', '硬化影'], [91, 93, '指代', '部分'], [93, 95, '阳性表现', '变尖'], [96, 99, '器官组织', '椎间隙'], [99, 100, '否定描述', '无'], [100, 102, '修饰描述', '明显'], [102, 104, '阳性表现', '狭窄'], [105, 117, '器官组织', '腰3/4、腰4/5椎间盘'], [120, 122, '阳性表现', '隆起'], [123, 126, '累及部位', '硬膜囊'], [126, 128, '阳性表现', '受压'], [129, 131, '器官组织', '椎管'], [131, 133, '否定描述', '未见'], [133, 135, '修饰描述', '明显'], [135, 137, '阳性表现', '狭窄'], [140, 150, '疾病', '胸12椎体压缩性骨折'], [153, 160, '疾病', '胸腰椎退行性变'], [161, 173, '器官组织', '腰3/4、腰4/5椎间盘'], [173, 175, '阳性表现', '膨出']]\n",
      "[[1, 3, '器官组织', '胸腰椎'], [4, 7, '检查手段', 'CT平扫'], [9, 12, '检查手段', '三维重建'], [18, 22, '器官组织', '胸腰段椎体'], [23, 26, '属性', '生理曲度'], [27, 28, '阴性表现', '存在'], [30, 31, '器官组织', '椎体'], [32, 33, '属性', '序列'], [35, 36, '阴性表现', '规则'], [38, 44, '器官组织', '胸12椎体骨质'], [45, 47, '属性', '连续性'], [48, 49, '阳性表现', '中断'], [51, 52, '器官组织', '椎体'], [55, 56, '阳性表现', '变扁'], [58, 59, '修饰描述', '楔形'], [60, 61, '异常现象', '改变'], [63, 64, '属性', '密度'], [65, 66, '阳性表现', '不均'], [71, 73, '器官组织', '诸椎体'], [74, 75, '属性', '边缘'], [77, 80, '器官组织', '椎小关节'], [83, 86, '异常现象', '骨质增生'], [88, 90, '异常现象', '硬化影'], [92, 93, '指代', '部分'], [94, 95, '阳性表现', '变尖'], [97, 99, '器官组织', '椎间隙'], [100, 100, '否定描述', '无'], [101, 102, '修饰描述', '明显'], [103, 104, '阳性表现', '狭窄'], [106, 117, '器官组织', '腰3/4、腰4/5椎间盘'], [121, 122, '阳性表现', '隆起'], [124, 126, '累及部位', '硬膜囊'], [127, 128, '阳性表现', '受压'], [130, 131, '器官组织', '椎管'], [132, 133, '否定描述', '未见'], [134, 135, '修饰描述', '明显'], [136, 137, '阳性表现', '狭窄'], [141, 150, '疾病', '胸12椎体压缩性骨折'], [154, 160, '疾病', '胸腰椎退行性变'], [162, 173, '器官组织', '腰3/4、腰4/5椎间盘'], [174, 175, '阳性表现', '膨出']]\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, sentences):\n",
    "        self._sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self._sentences[index]\n",
    "        return {'text': sentence['sent'],\n",
    "                'tags': sentence['ners'],\n",
    "                'spans': sentence['spans']}\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(sentences=train_data)\n",
    "valid_dataset = CustomDataset(sentences=valid_data)\n",
    "\n",
    "for i in train_dataset:\n",
    "    # 调用__getitem__方法\n",
    "    print(i['text'])\n",
    "    print(i['tags'])\n",
    "    print(i['spans'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a70ede-fbab-41cb-b7b9-56db6db2d2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='./save_tokenizer/', vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325522432\n"
     ]
    }
   ],
   "source": [
    "tokenizer_fast = BertTokenizerFast.from_pretrained('./save_tokenizer/')\n",
    "print(tokenizer_fast)\n",
    "\n",
    "pretrained = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext-large')\n",
    "print(pretrained.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbcc4457-171c-483e-9137-94cbece6a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 4508, 4307, 5593, 1381, 1383,  123,  124,  155,  155,  190,  122,\n",
      "          130,  155,  155,  190,  125,  127,  155,  155, 8024, 2340, 1383,  122,\n",
      "          128,  155,  155,  190,  122,  127,  155,  155,  190,  125,  125,  155,\n",
      "          155, 8024, 2284, 6956, 1331,  124,  155,  155,  511, 4508, 4307, 5593,\n",
      "         1920, 2207,  510, 2501, 2578, 3633, 2382, 8024, 1726, 1898, 1772, 1258,\n",
      "         8024, 1259, 5606, 1045, 3146,  511,  145,  146,  148,  151, 8038, 5593,\n",
      "          860, 1079, 6224, 4157, 3340, 4307, 6117, 3837,  928, 1384,  511, 4508,\n",
      "         4307, 5593, 1079, 6224, 3144,  702,  856, 1726, 1898, 5310, 5688, 8024,\n",
      "         6804, 3926, 6226, 1156, 8024, 6804, 5357, 1045, 3146, 8024, 2340, 1383,\n",
      "         6772, 1920, 4638, 5276,  122,  124,  155,  155,  190,  130,  155,  155,\n",
      "         8024, 1381, 1383, 6772, 1920, 4638, 5276,  123,  129,  155,  155,  190,\n",
      "          122,  127,  155,  155,  117,  145,  146,  148,  151, 8038, 5310, 5688,\n",
      "         1079, 3313, 6224, 3209, 3227, 6117, 3837,  928, 1384,  511, 4508, 4307,\n",
      "         5593,  697, 1383, 1914, 1355,  856, 1726, 1898, 5310, 5688, 8020,  162,\n",
      "          151,  118,  160,  143,  146,  161, 1146, 5102, 8024,  124, 5102, 8021,\n",
      "         8024, 7390, 6393,  102],\n",
      "        [ 101, 2642, 5442, 5143,  100, 5511, 1304,  855,  100, 8038, 1352,  904,\n",
      "         1920, 5554, 1288, 4413, 2190, 4917, 8024, 5554, 2141, 6574, 3313, 6224,\n",
      "         3209, 3227, 2460, 2382,  928, 1384, 8024, 2207, 5554, 1350, 5554, 2397,\n",
      "         3313, 6224, 3209, 3227, 2460, 2382,  928, 1384, 8024, 1872, 2487, 2812,\n",
      "         2989, 3313, 6224, 3209, 3227, 2460, 2382, 2487, 1265,  928, 1384,  511,\n",
      "         5554, 2147, 5143, 5320, 3313, 6224, 3209, 3227, 2810, 1920, 1350, 1358,\n",
      "         1327, 8024, 5554, 3765,  510, 5554, 6162, 3187, 1872, 2160, 8024,  704,\n",
      "         5296, 5310, 3354, 2233,  704,  511, 1928, 7565,  155,  160, 2398, 2812,\n",
      "         1350, 1872, 2487, 3313, 6224, 3209, 3227, 2460, 2382, 8024, 2456, 6379,\n",
      "         7390, 6393,  511,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [ 101, 2414, 6804,  814, 1309,  855, 5541, 4275, 8024, 1920, 5636, 6225,\n",
      "         2175, 8038,  697, 5511, 5292, 4415, 1872, 1914,  510, 3563, 5128, 8024,\n",
      "         2340, 5511, 6224, 4275, 4307, 7770, 2166, 2428, 2512, 8024, 6804, 4518,\n",
      "         3612, 3926,  511, 5511, 7305, 2512, 1872, 3849,  511, 2552, 2512, 1439,\n",
      "         1309,  855, 3121, 1359, 8024, 2340, 2552, 5357, 3227, 4850,  679, 3926,\n",
      "          511, 2340,  904, 5599, 7481, 1350, 5490, 5599, 6235, 3227, 4850,  679,\n",
      "         3926, 8039, 1381, 5599, 7481, 1045, 3998, 8024, 1381,  904, 5490, 5599,\n",
      "         6235, 3313, 6224, 3209, 3227, 2460, 2382,  511,  677, 5579, 7474, 5549,\n",
      "         6624, 6121, 1277, 6224,  158,  151,  145,  145, 5052, 8024, 5052, 1928,\n",
      "         1920, 5636,  855,  754, 5018,  129, 1400, 5490,  677, 5357, 3717, 2398,\n",
      "          855, 5390,  511, 2340,  904, 5541, 5579, 4916, 3890, 8024, 7390, 6402,\n",
      "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [ 101,  678, 5579, 7474, 5549, 5052, 1880, 5310, 3354, 3926, 3251, 8024,\n",
      "         5052, 5579, 1079, 6851, 1898, 5679, 1962, 8024, 5592, 5579, 3667, 6956,\n",
      "         1146, 3227, 4850,  679, 3926, 8024, 2792, 6224, 3667, 1071, 1079, 3313,\n",
      "         6224, 2460, 2382, 2141, 2595, 1726, 1898,  511,  145,  146,  148,  151,\n",
      "         1350,  158,  165, 8038, 2792, 6224, 3667, 6117, 3837, 6858, 4517, 8024,\n",
      "         1041, 4659, 5679, 1962, 8024, 1439, 5632, 1355, 2595, 6117, 3837, 8024,\n",
      "         3837, 6862, 3633, 2382, 5745, 1741, 8024, 3313, 6224, 3209, 3227, 6819,\n",
      "         3837,  511,  678, 5579, 7474, 5549, 2792, 6224, 3667, 6117, 3837, 6858,\n",
      "         4517, 8024, 3313, 6224, 3209, 3227, 6117, 3410, 2501, 2768,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]])\n",
      "torch.Size([4, 184])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "torch.Size([4, 18, 184, 184])\n"
     ]
    }
   ],
   "source": [
    "def get_collate_fn(tokenizer, ent_map_id, max_len=512):\n",
    "    \"\"\"返回collate_fun函数(通过闭包函数引入形参)\"\"\"\n",
    "\n",
    "    ent2id = ent_map_id['ent2id']\n",
    "    ent_type_size = len(ent2id)  # 实体类型个数\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        sentences_list = [sentence['text'] for sentence in batch]\n",
    "        spans_list = [sentence['spans'] for sentence in batch]\n",
    "        outputs = tokenizer(sentences_list, truncation=True, max_length=max_len, padding=True, return_tensors='pt')\n",
    "        input_ids, attention_mask, token_type_ids = outputs.input_ids, outputs.attention_mask, outputs.token_type_ids\n",
    "\n",
    "        labels = np.zeros((input_ids.shape[0], ent_type_size, input_ids.shape[1], input_ids.shape[1]))  # 构造labels\n",
    "        for i, spans in enumerate(spans_list):\n",
    "            for start, end, ent_type, ent_text in spans:\n",
    "                labels[i, ent2id[ent_type], start, end] = 1\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=4,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=get_collate_fn(tokenizer_fast, ent_to_ot_id))\n",
    "dataloader_valid = DataLoader(dataset=valid_dataset,\n",
    "                              batch_size=4,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=get_collate_fn(tokenizer_fast, ent_to_ot_id))\n",
    "\n",
    "for input_ids, attention_mask, token_type_ids, labels in dataloader_train:\n",
    "    print(input_ids)\n",
    "    print(input_ids.shape)\n",
    "    print(attention_mask)\n",
    "    print(token_type_ids)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89037807-e844-447d-a1dd-99c90bca175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GlobalPointer(copy.deepcopy(pretrained), len(ent_to_ot_id['ent2id']), 64).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c562b722-24c6-47e6-afd2-10541e33b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    for idx, (input_ids, attention_mask, token_type_ids, labels) in enumerate(dataloader):\n",
    "        # 数据设备切换\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        # labels.shape=[batch_size, ent_type_size, seq_len, seq_len]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # logits.shape=[batch_size, ent_type_size, seq_len, seq_len]\n",
    "        logits = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = loss_fun(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0 and idx > 0:\n",
    "            mc = MetricsCalculator()  # 计算实体的查准率、查全率、F1 score\n",
    "            mc.calc_confusion_matrix_ner(logits, labels)\n",
    "            print('| step {:5d} | loss {:8.5f} | precision {:8.5f} | recall {:8.5f} | f1 {:8.5f} |'.format(idx,\n",
    "                                                                                                           loss.item(),\n",
    "                                                                                                           mc.precision,\n",
    "                                                                                                           mc.recall,\n",
    "                                                                                                           mc.f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64c2a06c-11e0-4507-9230-440af9d25f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型验证\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    mc = MetricsCalculator()\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, token_type_ids, labels in dataloader:\n",
    "            # 数据设备切换\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            # logits.shape=[batch_size, ent_type_size, seq_len, seq_len]\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            mc.calc_confusion_matrix_ner(logits, labels)\n",
    "    return mc.precision, mc.recall, mc.f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83cc1c41-5fe7-4937-a663-6d8a147e7efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step   100 | loss  1.45807 | precision  0.87500 | recall  0.79333 | f1  0.83217 |\n",
      "| step   200 | loss  0.61724 | precision  0.96241 | recall  0.93431 | f1  0.94815 |\n",
      "| step   300 | loss  0.33895 | precision  0.96522 | recall  0.96522 | f1  0.96522 |\n",
      "| step   400 | loss  0.67916 | precision  0.95139 | recall  0.91946 | f1  0.93515 |\n",
      "| step   500 | loss  0.20465 | precision  0.97059 | recall  0.96117 | f1  0.96585 |\n",
      "| step   600 | loss  0.38906 | precision  0.95420 | recall  0.93284 | f1  0.94340 |\n",
      "| step   700 | loss  0.41365 | precision  0.97248 | recall  0.92174 | f1  0.94643 |\n",
      "| step   800 | loss  1.07702 | precision  0.90972 | recall  0.86184 | f1  0.88514 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch:     0 | time: 458.36s | valid precision  0.93032 | valid recall  0.94425 | valid f1  0.93723 | train f1  0.95313 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss  0.09637 | precision  0.99020 | recall  1.00000 | f1  0.99507 |\n",
      "| step   200 | loss  0.18875 | precision  0.98810 | recall  0.99401 | f1  0.99104 |\n",
      "| step   300 | loss  0.28054 | precision  0.95035 | recall  0.97101 | f1  0.96057 |\n",
      "| step   400 | loss  0.36505 | precision  0.97753 | recall  0.97753 | f1  0.97753 |\n",
      "| step   500 | loss  0.27530 | precision  0.92793 | recall  0.93636 | f1  0.93213 |\n",
      "| step   600 | loss  0.60064 | precision  0.93182 | recall  0.93893 | f1  0.93536 |\n",
      "| step   700 | loss  0.46263 | precision  0.93985 | recall  0.93985 | f1  0.93985 |\n",
      "| step   800 | loss  0.44518 | precision  0.96350 | recall  0.94286 | f1  0.95307 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch:     1 | time: 460.00s | valid precision  0.93380 | valid recall  0.95076 | valid f1  0.94220 | train f1  0.96381 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss  0.32077 | precision  0.90435 | recall  0.93694 | f1  0.92035 |\n",
      "| step   200 | loss  0.26745 | precision  0.95652 | recall  0.97778 | f1  0.96703 |\n",
      "| step   300 | loss  0.12374 | precision  0.98315 | recall  0.99432 | f1  0.98870 |\n",
      "| step   400 | loss  0.28385 | precision  0.97345 | recall  0.95652 | f1  0.96491 |\n",
      "| step   500 | loss  0.39511 | precision  0.95041 | recall  0.95041 | f1  0.95041 |\n",
      "| step   600 | loss  0.20865 | precision  0.98693 | recall  0.98052 | f1  0.98371 |\n",
      "| step   700 | loss  0.29842 | precision  0.96795 | recall  0.98693 | f1  0.97735 |\n",
      "| step   800 | loss  0.22513 | precision  1.00000 | recall  0.97794 | f1  0.98885 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch:     2 | time: 463.10s | valid precision  0.93995 | valid recall  0.94728 | valid f1  0.94360 | train f1  0.97088 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, dataloader_train, optimizer, device)\n",
    "    _, _, train_f1 = evaluate(model, dataloader_train, device)\n",
    "    valid_precision, valid_recall, valid_f1 = evaluate(model, dataloader_valid, device)\n",
    "    print('-' * 123)\n",
    "    print('| epoch: {:5d} | time: {:5.2f}s '\n",
    "          '| valid precision {:8.5f} '\n",
    "          '| valid recall {:8.5f} '\n",
    "          '| valid f1 {:8.5f} | train f1 {:8.5f} |'.format(epoch,\n",
    "                                                           time.time() - epoch_start_time,\n",
    "                                                           valid_precision,\n",
    "                                                           valid_recall,\n",
    "                                                           valid_f1,\n",
    "                                                           train_f1))\n",
    "    print('-' * 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "654a7985-3ce1-4d63-8af9-8298f6d2c90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:14<00:00, 13.35it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_pred_all = []\n",
    "\n",
    "with open('datasets/testB.conll_sent.conll', 'r', encoding='utf-8') as f:\n",
    "    testB_sentences = f.readlines()\n",
    "    for sentence in tqdm(testB_sentences):  # 每次预测一条数据\n",
    "        sentence = sentence.strip()\n",
    "        sentence_pred = {\"sent\": sentence}\n",
    "        output = tokenizer_fast([sentence], return_offsets_mapping=True, max_length=512, truncation=True, padding=True)\n",
    "        input_ids = torch.tensor(output['input_ids'], dtype=torch.int64).to(device)\n",
    "        token_type_ids = torch.tensor(output['token_type_ids'], dtype=torch.int64).to(device)\n",
    "        attention_mask = torch.tensor(output['attention_mask'], dtype=torch.int64).to(device)\n",
    "\n",
    "        # 处理原句空格\n",
    "        offset_mapping = []\n",
    "        for i, (start, end) in enumerate(output[\"offset_mapping\"][0]):\n",
    "            if (end > 0) and (i >= 2):\n",
    "                start -= (i - 1)\n",
    "                end -= (i - 1)\n",
    "            offset_mapping.append((start, end))\n",
    "\n",
    "        sentence = sentence.replace(' ', '')\n",
    "\n",
    "        ent_list = []\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, attention_mask, token_type_ids).cpu()\n",
    "            for _, l, start, end in zip(*torch.where(logits > 0.0)):  # 阈值(threshold)设置为0.0\n",
    "                ent_type = ent_to_ot_id['id2ent'][str(l.item())]\n",
    "                ent_char_span = [offset_mapping[start.item()][0], offset_mapping[end.item()][1]]\n",
    "                ent_text = sentence[ent_char_span[0]: ent_char_span[1]]\n",
    "                ent_list.append([ent_char_span[0], ent_char_span[1], ent_type, ent_text])\n",
    "        ent_list = sorted(ent_list, key=lambda x: x[0])\n",
    "        sentence_pred['ners'] = ent_list\n",
    "        sentence_pred_all.append(sentence_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a53f587d-2ce5-40e6-88cd-363901568213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent': '幽 门 : 呈 圆 形 , 开 闭 尚 可 , 粘 膜 皱 襞 光 滑 , 色 泽 淡 红 , 未 见 出 血 及 溃 疡 。',\n",
       " 'ners': [[0, 2, '器官组织', '幽门'],\n",
       "  [4, 6, '阴性表现', '圆形'],\n",
       "  [7, 9, '属性', '开闭'],\n",
       "  [10, 11, '阴性表现', '可'],\n",
       "  [12, 16, '器官组织', '粘膜皱襞'],\n",
       "  [16, 18, '阴性表现', '光滑'],\n",
       "  [19, 21, '属性', '色泽'],\n",
       "  [24, 26, '否定描述', '未见'],\n",
       "  [26, 28, '阳性表现', '出血'],\n",
       "  [29, 31, '阳性表现', '溃疡']]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_pred_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "941c8763-ace4-4f3e-89d4-b161ac07a59f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('result_data/ner.json', 'w') as fp:\n",
    "    json.dump(sentence_pred_all, fp, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3812",
   "language": "python",
   "name": "env_3812"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}