{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color='red' size=4>算法:</font>   \n",
    "输入:训练集$ D = \\{(x^{(n)}, y^{(n)})\\}_{n=1}^{N}$验证集$ V$,学习率$ \\alpha $   \n",
    "随机初始化$ \\theta $         \n",
    "**repeat**   \n",
    "&emsp;&emsp;对训练$D $中的样本随机重新排序     \n",
    "&emsp;&emsp;**for**  $ n= 1 \\dots N $ **do**       \n",
    "&emsp;&emsp;&emsp;从训练集$D $中选取样本$ (x^{(n)}, y^{(n)}) $  \n",
    "&emsp;&emsp;&emsp;$$ \\theta \\leftarrow \\theta - \\alpha \\frac{\\partial L(y^{(n)}, f(x^{(n)}, \\theta))}{\\partial \\theta} $$    注意: $ L(y^{(n)}, f(x^{(n)}, \\theta)) $ 是模型$ f(X, \\theta) $ 关于训练数据集的平均损失(即经验风险)    \n",
    "**until**模型$ f(\\mathbf{x}, \\theta) $在验证集$ V$上的错误率不再下降  \n",
    "输出: $ \\theta $\n",
    "\n",
    "\n",
    "<font color='red' size=4>结论:</font>   \n",
    "1. 经过足够次数的迭代时,随机梯度下降法(也叫增量梯度下降)也可以收敛到局部最优解\n",
    "2. 批量梯度下降和随机梯度下降之间的区别在于每次迭代的优化目标是对所有样本的平均损失函数还是单个样本的损失函数\n",
    "3. 随机梯度下降收敛速度非常快\n",
    "4. 随机梯度下降无法充分利用计算机的并行计算能力(mini-batch gradient descent可以利用到计算机的并行计算能力,又不至于计算开销过大)\n",
    "5. 随机梯度下降相当于在批量梯度下降的梯度上引入了<font color='red'>随机噪声</font>.当目标函数非凸时,反而可以使其逃离局部最优点"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}