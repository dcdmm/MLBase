{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# TODO ★★★★torchtext已停止更新维护,请替换\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "# TODO ★★★★torchtext已停止更新维护,请替换\n",
    "from torchtext.datasets import IMDB\n",
    "# TODO ★★★★英文分词请使用nltk\n",
    "import spacy\n",
    "import torch\n",
    "# TODO ★★★★torchtext已停止更新维护,请替换\n",
    "import torchtext\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 使用torchtext加载斯坦福⼤学的⼤型电影评论数据集\n",
    "train_iter = IMDB(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield [tok.text for tok in spacy_en.tokenizer(text)]  # 分词\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter))  # Build a Vocab from an iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab.insert_token(\"<unk>\", 0)\n",
    "vocab.insert_token(\"<pad>\", 1)\n",
    "vocab.insert_token(\"<SOS>\", 2)\n",
    "vocab.insert_token(\"<EOS>\", 3)\n",
    "vocab.set_default_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 200])\n",
      "tensor([[-0.0715,  0.0935,  0.0237,  ...,  0.3362,  0.0306,  0.2558],\n",
      "        [ 0.1765,  0.2921, -0.0021,  ..., -0.2077, -0.2319, -0.1081],\n",
      "        [ 0.1229,  0.5804, -0.0696,  ..., -0.0392, -0.1624, -0.0967],\n",
      "        ...,\n",
      "        [-0.0020,  0.0202, -0.0244,  ...,  0.0142, -0.8224, -0.3703],\n",
      "        [ 0.1291, -0.2605,  0.0139,  ...,  0.1384, -0.0146,  0.4337],\n",
      "        [-0.7300,  0.5164, -0.5798,  ...,  0.3581,  1.1576,  0.2573]])\n"
     ]
    }
   ],
   "source": [
    "# 预训练词向量\n",
    "vec1 = torchtext.vocab.Vectors(name=\"glove.6B.200d.txt\",\n",
    "                               max_vectors=25000,\n",
    "                               cache=r'C:\\Users\\dcdmm\\Music\\GitHubProjects\\MLNote\\E_PyTorch\\高阶操作及深度学习相关理论\\torchtext自然语言处理\\.vector_cache')\n",
    "\n",
    "print(vec1.vectors.shape)\n",
    "print(vec1.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121068, 200])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = vec1.get_vecs_by_tokens(vocab.get_itos())\n",
    "\n",
    "print(pretrained_embeddings.shape)\n",
    "print(pretrained_embeddings)  # 模型词向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def to_map_style_dataset(iter_data):\n",
    "    r\"\"\"Convert iterable-style dataset to map-style dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    class _MapStyleDataset(Data.Dataset):\n",
    "\n",
    "        def __init__(self, iter_data):\n",
    "            self._data = list(iter_data)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self._data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self._data[idx]\n",
    "\n",
    "    return _MapStyleDataset(iter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "\n",
    "train_data = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_train = int(len(train_data) * 0.7)\n",
    "train_dataset, valid_dataset = random_split(train_data,\n",
    "                                            [num_train, len(train_data) - num_train])  # 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_transform = lambda x: [vocab['<SOS>']] + [vocab[token] for token in\n",
    "                                               [tok.text for tok in spacy_en.tokenizer(x)]] + [vocab['<EOS>']]\n",
    "label_transform = lambda x: 1 if x == 'pos' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,     2,     2,  ...,     2,     2,     2],\n",
      "        [  642,    13,   984,  ...,    13, 30057,  1966],\n",
      "        [13731,   440,  5844,  ...,   162,    14,    11],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0]])\n",
      "tensor([130, 241, 305, 156, 262, 212, 155, 154, 123, 160, 562, 288, 299, 981,\n",
      "        204, 117, 243, 216,  56, 251, 278, 270, 164, 240, 127, 151, 211, 231,\n",
      "         63, 213, 169,  79])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    对文本标签和文本内容进行处理使之可以用于pack_padded_sequence操作\n",
    "    Parameters\n",
    "    ---------\n",
    "    batch : 每个batch数据\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    label_tensor : 每个batch数据文本标签的处理输出\n",
    "    text_pad : 每个batch数据文本内容的处理输出\n",
    "    lengths : 每个batch数据文本内容的真实长度\n",
    "    \"\"\"\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_transform(_label))\n",
    "        processed_text = torch.tensor(text_transform(_text))\n",
    "        lengths.append(len(processed_text))\n",
    "        text_list.append(processed_text)\n",
    "    label_tensor = torch.tensor(label_list)\n",
    "    text_pad = pad_sequence(text_list, batch_first=False, padding_value=0)\n",
    "    lengths = torch.tensor(lengths)  # 真实长度\n",
    "    return text_pad, lengths, label_tensor\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
    "                              collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "for text, length, label in train_dataloader:\n",
    "    print(text)\n",
    "    print(length)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocal_size, embedding_size = pretrained_embeddings.shape\n",
    "hidden_size = 256\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "out_size = 2\n",
    "num_layers = 2\n",
    "lr = 0.001  # 学习率\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from textrnn_model_torch import TextRNN\n",
    "%run textrnn_model_torch.py\n",
    "\n",
    "net = TextRNN(vocab_size=vocal_size,\n",
    "              embedding_size=embedding_size,\n",
    "              hidden_size=hidden_size,\n",
    "              num_layers=num_layers,\n",
    "              dropout_ratio=dropout,\n",
    "              bidirectional=True,\n",
    "              out_size=out_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用模型预训练词向量矩阵\n",
    "net.embed.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from train_evaluate_c import Trainer\n",
    "%run train_evaluate_c.py\n",
    "\n",
    "t_and_v = Trainer(net, optimizer, criterion, 5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics_acc(predict_all, y_true):\n",
    "    predict = predict_all.argmax(-1)\n",
    "    label = y_true\n",
    "    acc = accuracy_score(label, predict)\n",
    "    return {\"acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  [0    /17500 (0  %)]\tLoss: 0.691854\tacc: 0.562500\n",
      "Train Epoch: 0  [1600 /17500 (9  %)]\tLoss: 0.666550\tacc: 0.593750\n",
      "Train Epoch: 0  [3200 /17500 (18 %)]\tLoss: 0.676721\tacc: 0.593750\n",
      "Train Epoch: 0  [4800 /17500 (27 %)]\tLoss: 0.604364\tacc: 0.687500\n",
      "Train Epoch: 0  [6400 /17500 (37 %)]\tLoss: 0.630378\tacc: 0.625000\n",
      "Train Epoch: 0  [8000 /17500 (46 %)]\tLoss: 0.542636\tacc: 0.718750\n",
      "Train Epoch: 0  [9600 /17500 (55 %)]\tLoss: 0.726545\tacc: 0.500000\n",
      "Train Epoch: 0  [11200/17500 (64 %)]\tLoss: 0.665629\tacc: 0.593750\n",
      "Train Epoch: 0  [12800/17500 (73 %)]\tLoss: 0.723559\tacc: 0.468750\n",
      "Train Epoch: 0  [14400/17500 (82 %)]\tLoss: 0.665595\tacc: 0.562500\n",
      "Train Epoch: 0  [16000/17500 (91 %)]\tLoss: 0.704983\tacc: 0.531250\n",
      "Train Epoch: 0  [17500/17500 (100%)]\tLoss: 0.709275\tacc: 0.500000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train Epoch: 1  [0    /17500 (0  %)]\tLoss: 0.645284\tacc: 0.656250\n",
      "Train Epoch: 1  [1600 /17500 (9  %)]\tLoss: 0.486097\tacc: 0.843750\n",
      "Train Epoch: 1  [3200 /17500 (18 %)]\tLoss: 0.628508\tacc: 0.687500\n",
      "Train Epoch: 1  [4800 /17500 (27 %)]\tLoss: 0.472436\tacc: 0.843750\n",
      "Train Epoch: 1  [6400 /17500 (37 %)]\tLoss: 0.697179\tacc: 0.718750\n",
      "Train Epoch: 1  [8000 /17500 (46 %)]\tLoss: 0.525694\tacc: 0.656250\n",
      "Train Epoch: 1  [9600 /17500 (55 %)]\tLoss: 0.477984\tacc: 0.812500\n",
      "Train Epoch: 1  [11200/17500 (64 %)]\tLoss: 0.491635\tacc: 0.750000\n",
      "Train Epoch: 1  [12800/17500 (73 %)]\tLoss: 0.431555\tacc: 0.781250\n",
      "Train Epoch: 1  [14400/17500 (82 %)]\tLoss: 0.445295\tacc: 0.718750\n",
      "Train Epoch: 1  [16000/17500 (91 %)]\tLoss: 0.530126\tacc: 0.718750\n",
      "Train Epoch: 1  [17500/17500 (100%)]\tLoss: 0.197912\tacc: 0.928571\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train Epoch: 2  [0    /17500 (0  %)]\tLoss: 0.405289\tacc: 0.718750\n",
      "Train Epoch: 2  [1600 /17500 (9  %)]\tLoss: 0.131344\tacc: 0.968750\n",
      "Train Epoch: 2  [3200 /17500 (18 %)]\tLoss: 0.290552\tacc: 0.875000\n",
      "Train Epoch: 2  [4800 /17500 (27 %)]\tLoss: 0.329363\tacc: 0.875000\n",
      "Train Epoch: 2  [6400 /17500 (37 %)]\tLoss: 0.229410\tacc: 0.906250\n",
      "Train Epoch: 2  [8000 /17500 (46 %)]\tLoss: 0.325490\tacc: 0.812500\n",
      "Train Epoch: 2  [9600 /17500 (55 %)]\tLoss: 0.412388\tacc: 0.906250\n",
      "Train Epoch: 2  [11200/17500 (64 %)]\tLoss: 0.107358\tacc: 0.937500\n",
      "Train Epoch: 2  [12800/17500 (73 %)]\tLoss: 0.375012\tacc: 0.843750\n",
      "Train Epoch: 2  [14400/17500 (82 %)]\tLoss: 0.106429\tacc: 0.968750\n",
      "Train Epoch: 2  [16000/17500 (91 %)]\tLoss: 0.122467\tacc: 0.968750\n",
      "Train Epoch: 2  [17500/17500 (100%)]\tLoss: 0.230889\tacc: 0.892857\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train Epoch: 3  [0    /17500 (0  %)]\tLoss: 0.024734\tacc: 1.000000\n",
      "Train Epoch: 3  [1600 /17500 (9  %)]\tLoss: 0.083464\tacc: 0.968750\n",
      "Train Epoch: 3  [3200 /17500 (18 %)]\tLoss: 0.129877\tacc: 0.968750\n",
      "Train Epoch: 3  [4800 /17500 (27 %)]\tLoss: 0.249152\tacc: 0.937500\n",
      "Train Epoch: 3  [6400 /17500 (37 %)]\tLoss: 0.191486\tacc: 0.937500\n",
      "Train Epoch: 3  [8000 /17500 (46 %)]\tLoss: 0.099083\tacc: 0.937500\n",
      "Train Epoch: 3  [9600 /17500 (55 %)]\tLoss: 0.271704\tacc: 0.968750\n",
      "Train Epoch: 3  [11200/17500 (64 %)]\tLoss: 0.062199\tacc: 0.968750\n",
      "Train Epoch: 3  [12800/17500 (73 %)]\tLoss: 0.050682\tacc: 1.000000\n",
      "Train Epoch: 3  [14400/17500 (82 %)]\tLoss: 0.200773\tacc: 0.906250\n",
      "Train Epoch: 3  [16000/17500 (91 %)]\tLoss: 0.254443\tacc: 0.937500\n",
      "Train Epoch: 3  [17500/17500 (100%)]\tLoss: 0.067096\tacc: 0.964286\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train Epoch: 4  [0    /17500 (0  %)]\tLoss: 0.081545\tacc: 0.968750\n",
      "Train Epoch: 4  [1600 /17500 (9  %)]\tLoss: 0.084400\tacc: 0.968750\n",
      "Train Epoch: 4  [3200 /17500 (18 %)]\tLoss: 0.088245\tacc: 1.000000\n",
      "Train Epoch: 4  [4800 /17500 (27 %)]\tLoss: 0.042122\tacc: 0.968750\n",
      "Train Epoch: 4  [6400 /17500 (37 %)]\tLoss: 0.046831\tacc: 1.000000\n",
      "Train Epoch: 4  [8000 /17500 (46 %)]\tLoss: 0.014792\tacc: 1.000000\n",
      "Train Epoch: 4  [9600 /17500 (55 %)]\tLoss: 0.020350\tacc: 1.000000\n",
      "Train Epoch: 4  [11200/17500 (64 %)]\tLoss: 0.149883\tacc: 0.968750\n",
      "Train Epoch: 4  [12800/17500 (73 %)]\tLoss: 0.016347\tacc: 1.000000\n",
      "Train Epoch: 4  [14400/17500 (82 %)]\tLoss: 0.285006\tacc: 0.968750\n",
      "Train Epoch: 4  [16000/17500 (91 %)]\tLoss: 0.042467\tacc: 0.968750\n",
      "Train Epoch: 4  [17500/17500 (100%)]\tLoss: 0.026661\tacc: 1.000000\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Training loss': [0.6237832903862,\n",
       "  0.24200162291526794,\n",
       "  0.12398403882980347,\n",
       "  0.039155032485723495,\n",
       "  0.024145226925611496],\n",
       " 'Training acc': [0.6600571428571429,\n",
       "  0.9157142857142857,\n",
       "  0.9568571428571429,\n",
       "  0.9895428571428572,\n",
       "  0.9941714285714286],\n",
       " 'Validation loss': [0.6412099599838257,\n",
       "  0.33504730463027954,\n",
       "  0.3482036888599396,\n",
       "  0.36343148350715637,\n",
       "  0.4002194106578827],\n",
       " 'Validation acc': [0.6350666666666667,\n",
       "  0.864,\n",
       "  0.872,\n",
       "  0.8876,\n",
       "  0.8797333333333334]}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = t_and_v.train(train_dataloader, valid_dataloader, compute_metrics=compute_metrics_acc, verbose=50)\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    \"\"\"预测句子的评价\"\"\"\n",
    "    model.eval()\n",
    "    processed_text = torch.tensor(text_transform(sentence)).to(device)\n",
    "    processed_text = processed_text.unsqueeze(1)\n",
    "    length = [len(processed_text)]\n",
    "    prediction = torch.sigmoid(model(processed_text, length))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9321, 0.0544]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, \"fuck, garbage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9282, 0.0610]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, \"This film is terrible\")  # 倾向于负面评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0341, 0.9604]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, \"This film is great\")  # 倾向于正面评价\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
