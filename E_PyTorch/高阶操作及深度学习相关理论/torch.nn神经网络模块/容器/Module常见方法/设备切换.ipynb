{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "tensor相关属性.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "LOKQqMBk-muu",
    "colab_type": "code",
    "colab": {},
    "executionInfo": {
     "status": "ok",
     "timestamp": 1592749609350,
     "user_tz": -480,
     "elapsed": 926,
     "user": {
      "displayName": "chao duan",
      "photoUrl": "",
      "userId": "16986074214670067064"
     }
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as Data\n",
    "import torch"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i5L4RepM_qLG",
    "colab_type": "code",
    "colab": {},
    "executionInfo": {
     "status": "ok",
     "timestamp": 1592749609351,
     "user_tz": -480,
     "elapsed": 906,
     "user": {
      "displayName": "chao duan",
      "photoUrl": "",
      "userId": "16986074214670067064"
     }
    }
   },
   "source": [
    "batch_size = 200\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "\n",
    "train_data = datasets.MNIST('../../../../../Other/datasets/PyTorch',\n",
    "                            train=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                            ]))\n",
    "\n",
    "test_data = datasets.MNIST('../../../../../Other/datasets/PyTorch',\n",
    "                           train=False,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.1307,), (0.3081,))\n",
    "                           ]))\n",
    "\n",
    "# 手写数字训练数据集\n",
    "train_loader = Data.DataLoader(dataset=train_data,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True)\n",
    "\n",
    "# 手写数字测试数据集\n",
    "test_loader = Data.DataLoader(dataset=test_data,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6RyfGus4_2vG",
    "colab_type": "code",
    "colab": {},
    "executionInfo": {
     "status": "ok",
     "timestamp": 1592749609351,
     "user_tz": -480,
     "elapsed": 899,
     "user": {
      "displayName": "chao duan",
      "photoUrl": "",
      "userId": "16986074214670067064"
     }
    }
   },
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.featrues = nn.Sequential(  # 内部实现了forward函数;各模块顺序执行\n",
    "            nn.Conv2d(1, 6, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(400, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10),  # 10分类问题\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.featrues(x)\n",
    "        x = x.reshape(x.size()[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xoVSFbbN_7mV",
    "colab_type": "code",
    "colab": {},
    "executionInfo": {
     "status": "ok",
     "timestamp": 1592749817861,
     "user_tz": -480,
     "elapsed": 197551,
     "user": {
      "displayName": "chao duan",
      "photoUrl": "",
      "userId": "16986074214670067064"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 所有的模型,数据必须都转换到GPU上\n",
    "criteon = nn.CrossEntropyLoss().to(device=device,\n",
    "                                   dtype=torch.float32)  # 定义损失函数为交叉熵并转换module的device和dtype(nn.module为in-place)\n",
    "net = LeNet()\n",
    "net.to(device=device, dtype=torch.float32)  # 转换module的devic和于dtype(in-place)\n",
    "\n",
    "# 多参数组的SGD优化器\n",
    "optimizer = optim.SGD([dict(params=net.featrues.parameters(), momentum=0.9),\n",
    "                       dict(params=net.classifier.parameters(),\n",
    "                            momentum=0.95,  # 动量参数\\alpha;默认momentum=0\n",
    "                            nesterov=True)],  # 是否启用Nesterov动量\n",
    "                      lr=learning_rate)  # 学习率"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([60000, 1, 28, 28])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms_data = []\n",
    "for i, j in train_data:\n",
    "    transforms_data.append(i.tolist())\n",
    "transforms_data_ten = torch.tensor(transforms_data)\n",
    "transforms_data_ten.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "transforms_data_loader = Data.DataLoader(transforms_data_ten, batch_size=200, shuffle=True)\n",
    "y_label = train_loader.dataset.targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from torch_model.train_evaluate import Train_Evaluate\n",
    "\n",
    "t_and_v = Train_Evaluate(model=net, optimizer=optimizer, criterion=criteon, epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  [0    /60000 (0  %)]\tLoss: 2.300406\n",
      "Train Epoch: 0  [2000 /60000 (3  %)]\tLoss: 2.291150\n",
      "Train Epoch: 0  [4000 /60000 (7  %)]\tLoss: 2.271410\n",
      "Train Epoch: 0  [6000 /60000 (10 %)]\tLoss: 2.245275\n",
      "Train Epoch: 0  [8000 /60000 (13 %)]\tLoss: 2.143907\n",
      "Train Epoch: 0  [10000/60000 (17 %)]\tLoss: 1.713377\n",
      "Train Epoch: 0  [12000/60000 (20 %)]\tLoss: 0.927804\n",
      "Train Epoch: 0  [14000/60000 (23 %)]\tLoss: 1.283235\n",
      "Train Epoch: 0  [16000/60000 (27 %)]\tLoss: 0.703749\n",
      "Train Epoch: 0  [18000/60000 (30 %)]\tLoss: 0.462573\n",
      "Train Epoch: 0  [20000/60000 (33 %)]\tLoss: 0.433865\n",
      "Train Epoch: 0  [22000/60000 (37 %)]\tLoss: 0.367559\n",
      "Train Epoch: 0  [24000/60000 (40 %)]\tLoss: 0.330594\n",
      "Train Epoch: 0  [26000/60000 (43 %)]\tLoss: 0.342789\n",
      "Train Epoch: 0  [28000/60000 (47 %)]\tLoss: 0.234067\n",
      "Train Epoch: 0  [30000/60000 (50 %)]\tLoss: 0.170483\n",
      "Train Epoch: 0  [32000/60000 (53 %)]\tLoss: 0.313330\n",
      "Train Epoch: 0  [34000/60000 (57 %)]\tLoss: 0.245354\n",
      "Train Epoch: 0  [36000/60000 (60 %)]\tLoss: 0.198010\n",
      "Train Epoch: 0  [38000/60000 (63 %)]\tLoss: 0.152679\n",
      "Train Epoch: 0  [40000/60000 (67 %)]\tLoss: 0.149037\n",
      "Train Epoch: 0  [42000/60000 (70 %)]\tLoss: 0.192794\n",
      "Train Epoch: 0  [44000/60000 (73 %)]\tLoss: 0.212679\n",
      "Train Epoch: 0  [46000/60000 (77 %)]\tLoss: 0.188447\n",
      "Train Epoch: 0  [48000/60000 (80 %)]\tLoss: 0.159845\n",
      "Train Epoch: 0  [50000/60000 (83 %)]\tLoss: 0.126159\n",
      "Train Epoch: 0  [52000/60000 (87 %)]\tLoss: 0.155594\n",
      "Train Epoch: 0  [54000/60000 (90 %)]\tLoss: 0.128343\n",
      "Train Epoch: 0  [56000/60000 (93 %)]\tLoss: 0.189579\n",
      "Train Epoch: 0  [58000/60000 (97 %)]\tLoss: 0.143611\n",
      "Train Epoch: 0  [60000/60000 (100%)]\tLoss: 0.103648\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train Epoch: 1  [0    /60000 (0  %)]\tLoss: 0.076052\n",
      "Train Epoch: 1  [2000 /60000 (3  %)]\tLoss: 0.087243\n",
      "Train Epoch: 1  [4000 /60000 (7  %)]\tLoss: 0.131304\n",
      "Train Epoch: 1  [6000 /60000 (10 %)]\tLoss: 0.097849\n",
      "Train Epoch: 1  [8000 /60000 (13 %)]\tLoss: 0.132636\n",
      "Train Epoch: 1  [10000/60000 (17 %)]\tLoss: 0.103128\n",
      "Train Epoch: 1  [12000/60000 (20 %)]\tLoss: 0.079552\n",
      "Train Epoch: 1  [14000/60000 (23 %)]\tLoss: 0.153088\n",
      "Train Epoch: 1  [16000/60000 (27 %)]\tLoss: 0.113462\n",
      "Train Epoch: 1  [18000/60000 (30 %)]\tLoss: 0.054073\n",
      "Train Epoch: 1  [20000/60000 (33 %)]\tLoss: 0.125906\n",
      "Train Epoch: 1  [22000/60000 (37 %)]\tLoss: 0.132438\n",
      "Train Epoch: 1  [24000/60000 (40 %)]\tLoss: 0.101384\n",
      "Train Epoch: 1  [26000/60000 (43 %)]\tLoss: 0.128797\n",
      "Train Epoch: 1  [28000/60000 (47 %)]\tLoss: 0.079278\n",
      "Train Epoch: 1  [30000/60000 (50 %)]\tLoss: 0.060177\n",
      "Train Epoch: 1  [32000/60000 (53 %)]\tLoss: 0.092840\n",
      "Train Epoch: 1  [34000/60000 (57 %)]\tLoss: 0.053032\n",
      "Train Epoch: 1  [36000/60000 (60 %)]\tLoss: 0.053531\n",
      "Train Epoch: 1  [38000/60000 (63 %)]\tLoss: 0.132254\n",
      "Train Epoch: 1  [40000/60000 (67 %)]\tLoss: 0.045825\n",
      "Train Epoch: 1  [42000/60000 (70 %)]\tLoss: 0.075038\n",
      "Train Epoch: 1  [44000/60000 (73 %)]\tLoss: 0.066319\n",
      "Train Epoch: 1  [46000/60000 (77 %)]\tLoss: 0.117448\n",
      "Train Epoch: 1  [48000/60000 (80 %)]\tLoss: 0.105964\n",
      "Train Epoch: 1  [50000/60000 (83 %)]\tLoss: 0.091954\n",
      "Train Epoch: 1  [52000/60000 (87 %)]\tLoss: 0.083317\n",
      "Train Epoch: 1  [54000/60000 (90 %)]\tLoss: 0.092693\n",
      "Train Epoch: 1  [56000/60000 (93 %)]\tLoss: 0.042917\n",
      "Train Epoch: 1  [58000/60000 (97 %)]\tLoss: 0.066296\n",
      "Train Epoch: 1  [60000/60000 (100%)]\tLoss: 0.077251\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "[[10.471982955932617], [11.857095718383789]]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_reslut = t_and_v.train_eval(train_loader=train_loader, valid_sets=[(transforms_data_loader, y_label)])\n",
    "metric_reslut"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}