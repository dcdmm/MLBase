{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='bert-base-chinese') \n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, 100, 100]\n",
      "['月', '光', '的', '[UNK]', '[PAD]', '[CLS]', '新', '希', '望', '[', '[UNK]', ']', '<', 'e', '##op', '>']\n",
      "21128\n"
     ]
    }
   ],
   "source": [
    "text = '月光的[UNK][PAD][CLS]新希望[EOS]<eop>'\n",
    "\n",
    "# 未添加新tokens前的编码效果\n",
    "print(tokenizer.convert_tokens_to_ids(['月光', '希望', '[EOS]', '<eop>']))\n",
    "print(tokenizer.tokenize(text))\n",
    "print(len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "[EOS] 21130\n",
      "21133\n",
      "{'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<eop>', '<eod>']}\n",
      "['<eop>', '<eod>'] [21131, 21132]\n"
     ]
    }
   ],
   "source": [
    "# Add a list of new tokens to the tokenizer class.\n",
    "tokenizer.add_tokens(new_tokens=['月光', '希望'])\n",
    "\n",
    "# Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes.\n",
    "# If special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the current vocabulary).\n",
    "print(tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "tokenizer.add_special_tokens(\n",
    "    # Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].\n",
    "    special_tokens_dict={'eos_token': '[EOS]', 'additional_special_tokens': [\"<eop>\", \"<eod>\"]})\n",
    "print(tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "print(len(tokenizer.get_vocab()))\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.additional_special_tokens, tokenizer.additional_special_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21128, 21129, 21130, 21131]\n",
      "[' 月  光 ', '的', '[UNK]', '[PAD]', '[CLS]', '新', ' 希  望 ', '[EOS]', '<eop>']\n",
      "21133\n"
     ]
    }
   ],
   "source": [
    "# 添加新tokens后的编码效果\n",
    "print(tokenizer.convert_tokens_to_ids(['月光', '希望', '[EOS]', '<eop>']))\n",
    "print(tokenizer.tokenize(text))\n",
    "print(len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../extra_dataset/save_tokenizer/tokenizer_config.json',\n",
       " '../extra_dataset/save_tokenizer/special_tokens_map.json',\n",
       " '../extra_dataset/save_tokenizer/vocab.txt',\n",
       " '../extra_dataset/save_tokenizer/added_tokens.json',\n",
       " '../extra_dataset/save_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存分词器(包括新添加的tokens)\n",
    "tokenizer.save_pretrained(\"../extra_dataset/save_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='../extra_dataset/save_tokenizer/', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<eop>', '<eod>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21128: AddedToken(\"月光\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t21129: AddedToken(\"希望\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t21130: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21131: AddedToken(\"<eop>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21132: AddedToken(\"<eod>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地重新加载\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"../extra_dataset/save_tokenizer/\")\n",
    "tokenizer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21128, 21129, 21130, 21131]\n",
      "[' 月  光 ', '的', '[UNK]', '[PAD]', '[CLS]', '新', ' 希  望 ', '[EOS]', '<eop>']\n",
      "21133\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer1.convert_tokens_to_ids(['月光', '希望', '[EOS]', '<eop>']))\n",
    "print(tokenizer1.tokenize(text))\n",
    "print(len(tokenizer1.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:952\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    950\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 952\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:178\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    175\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    181\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model(**tokenizer(text, return_tensors='pt'))  # 报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(21128, 768, padding_idx=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_embedding维度为:21128 * 768(but此时tokenizer大小为:21133)\n",
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(21133, 768, padding_idx=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3367,  0.7263,  0.5332,  ...,  0.3420, -0.5114, -0.0175],\n",
       "         [-0.3379,  0.1868,  0.1348,  ...,  0.0349, -0.7228, -0.4014],\n",
       "         [-0.5350,  0.4015,  0.6540,  ..., -0.2214, -0.3078,  0.0057],\n",
       "         ...,\n",
       "         [-0.5030,  0.4545,  0.6126,  ...,  0.5991, -0.3347, -0.3338],\n",
       "         [-0.4078,  0.8154,  0.7629,  ..., -0.2217, -0.5943,  0.0385],\n",
       "         [-0.3751,  0.7748,  0.1946,  ...,  0.6225, -0.9416,  0.1327]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.9987,  0.9997,  0.9953,  0.9943,  0.6537, -0.1105, -0.8766, -0.9217,\n",
       "          0.9982, -0.9998,  1.0000,  0.9986, -0.7282, -0.9041,  0.9998, -0.9984,\n",
       "         -0.6943,  0.9976,  0.9933,  0.0379,  0.9960, -1.0000, -0.9166, -0.8110,\n",
       "         -0.5432,  0.9988,  0.8344, -0.9641, -0.9999,  0.9990,  0.9801,  0.9995,\n",
       "          0.8950, -0.9999, -0.9996,  0.7325,  0.3060,  0.9951, -0.3898, -0.9223,\n",
       "         -0.9804, -0.8721, -0.1568, -0.9924, -0.9964,  0.6245, -1.0000, -0.9999,\n",
       "         -0.8280,  0.9992, -0.8497, -0.9995,  0.9011, -0.9199, -0.0195,  0.9893,\n",
       "         -0.9994,  0.9753,  1.0000,  0.8749,  0.9999, -0.9863,  0.6518, -0.9998,\n",
       "          1.0000, -0.9991, -0.9908,  0.5311,  0.9998,  1.0000, -0.9720,  0.9854,\n",
       "          1.0000,  0.4059,  0.4810,  0.9989, -0.9969,  0.6743, -1.0000,  0.8824,\n",
       "          1.0000,  0.9978, -0.9754,  0.9785, -0.9653, -1.0000, -0.9980,  0.9997,\n",
       "         -0.8819,  0.9986,  0.9962, -0.9991, -1.0000,  0.9995, -0.9983, -0.9985,\n",
       "         -0.9735,  0.9978, -0.9091, -0.7337, -0.3587,  0.8948, -0.9978, -0.9950,\n",
       "          0.9986,  0.9952,  0.1430, -0.9993,  0.9997,  0.0516, -1.0000, -0.9464,\n",
       "         -0.9996, -0.7332, -0.9921,  0.9999, -0.7794, -0.6769,  0.9998, -0.9976,\n",
       "          0.9343, -0.9996, -0.9151,  0.6236,  0.9978,  0.9999,  0.9971, -0.9954,\n",
       "          0.9984,  1.0000,  0.9826,  0.9709, -0.9947,  0.9760,  0.9625, -0.9923,\n",
       "          0.0875, -0.7507,  1.0000,  0.9972,  0.9757, -0.9543,  0.9974, -0.9979,\n",
       "          1.0000, -1.0000,  0.9998, -1.0000, -0.9939,  0.9963,  0.9161,  1.0000,\n",
       "         -0.8885,  1.0000, -0.9878, -0.9992,  0.9906, -0.2406,  0.9894, -1.0000,\n",
       "          0.9010, -0.8876,  0.1953, -0.6174, -1.0000,  1.0000, -0.9350,  1.0000,\n",
       "          0.9989, -0.9917, -0.9931, -0.9974,  0.6509, -0.9969, -0.6100,  0.9964,\n",
       "         -0.2036,  0.9996, -0.8205, -0.9779,  0.9804,  0.5548, -0.9998,  0.9944,\n",
       "         -0.5600,  0.9102,  0.4470,  0.9278,  0.9929,  0.9529, -0.8552,  1.0000,\n",
       "          0.3573,  0.9859,  0.9969,  0.0422, -0.5854, -0.9962, -1.0000, -0.2445,\n",
       "          0.9999, -0.5518, -0.9992,  0.6274, -1.0000,  0.8612, -0.0569, -0.4607,\n",
       "         -0.9980, -0.9999,  0.9995, -0.9699, -0.9987,  0.8330, -0.8346, -0.3573,\n",
       "         -0.9991,  0.3943,  0.9947, -0.6040,  0.0584, -0.7692, -0.9990,  0.9953,\n",
       "         -0.9964,  0.9334,  0.7706,  1.0000,  0.9179, -0.8965, -0.9098,  0.9997,\n",
       "         -0.1625, -1.0000,  0.9735, -0.9957, -0.0067,  1.0000, -0.9979,  0.9093,\n",
       "          1.0000,  0.7292,  1.0000, -0.1055, -0.9987, -0.9983,  1.0000,  0.9979,\n",
       "          1.0000, -0.9924, -0.9946,  0.7035, -0.6409, -1.0000, -0.9981, -0.6919,\n",
       "          0.9987,  0.9998,  0.7408, -0.9958, -0.8170, -0.9994,  1.0000, -0.8736,\n",
       "          1.0000,  0.9969, -0.9897, -0.5150,  0.5632, -0.6576, -0.9997, -0.3561,\n",
       "         -0.9998, -0.9817, -1.0000,  0.9696, -0.9994, -1.0000,  0.7750,  1.0000,\n",
       "         -0.4070, -0.9999,  0.9987,  0.9978, -0.9281, -0.9785,  0.9757, -1.0000,\n",
       "          1.0000, -0.9994,  0.7415, -0.9737, -0.9974, -0.7730,  0.9997,  0.9866,\n",
       "         -0.9788, -0.7634, -0.9965, -0.9658, -0.7353,  0.9151, -0.9118,  0.9236,\n",
       "         -0.9631, -0.8142,  0.9372, -0.9436, -0.9999,  0.7848,  1.0000, -0.4127,\n",
       "          1.0000,  0.9662,  1.0000, -0.9839, -0.9944,  0.9945, -0.4965, -0.8307,\n",
       "          0.6024, -0.8545,  0.9144, -0.3811, -0.8899, -0.9999,  0.9999,  0.9379,\n",
       "          0.9522,  0.7590, -0.9122,  0.0678,  0.9832, -0.9844,  0.9995, -0.9995,\n",
       "         -0.7094,  0.9999,  1.0000,  0.9997,  0.7646, -0.6172,  0.9945, -0.9981,\n",
       "          0.9998, -0.9999,  0.9990, -0.9421,  0.8125, -0.7360, -0.9754,  1.0000,\n",
       "          0.6171,  0.8853,  0.9998, -0.9105,  0.9940,  0.9471,  0.9917,  0.9648,\n",
       "          0.8713,  0.9999, -0.9911, -0.9928, -0.2365, -0.9955, -0.9986, -1.0000,\n",
       "          0.0994, -0.9986, -0.9835, -0.0260,  0.4125,  0.9139, -0.9887, -0.9821,\n",
       "         -0.5561,  0.7329, -0.2783,  0.4836,  0.8733, -0.9986, -0.9880, -1.0000,\n",
       "         -0.9957,  0.4943,  0.9999, -0.9998,  0.9976, -1.0000, -0.8648,  0.5103,\n",
       "         -0.9192, -0.8853,  0.9999, -1.0000,  0.9300,  0.9998,  1.0000,  0.9994,\n",
       "          0.9997, -0.9663, -0.9997, -0.9995, -1.0000, -1.0000, -0.9975,  0.9119,\n",
       "          0.5733, -1.0000,  0.3500,  0.9254,  1.0000,  0.9970, -0.9996, -0.7973,\n",
       "         -0.9990, -0.9990,  0.9966, -0.9203, -0.9995,  0.9975, -0.7271,  1.0000,\n",
       "         -0.6957,  0.5327,  0.9837,  0.8489,  0.9019, -0.9998,  0.9906,  1.0000,\n",
       "          0.9851, -1.0000, -0.0497, -0.9371, -0.9998, -0.3150,  0.9503,  0.9988,\n",
       "         -0.9999, -0.1579, -0.9991,  0.7296,  0.9245,  0.9995,  0.9994,  0.9775,\n",
       "          0.9789,  0.9885, -0.0062,  1.0000,  0.1343, -0.9995,  0.9989, -0.2813,\n",
       "          0.8352, -0.9997,  0.9993,  0.9681,  1.0000,  0.9190, -0.6895, -0.9966,\n",
       "         -0.9898,  0.9951,  1.0000, -0.9784, -0.5356, -0.9998, -0.9994, -0.9972,\n",
       "         -0.6537, -0.6810, -0.9895, -0.9993, -0.0769,  0.6399,  1.0000,  1.0000,\n",
       "          0.9993, -0.9693, -0.8746,  0.9888,  0.9077,  0.9952, -0.8753, -1.0000,\n",
       "         -0.9920, -0.9988,  0.9993, -0.6398, -0.2969, -0.9235,  0.6555,  0.9594,\n",
       "         -0.9999, -0.9861, -0.9654,  0.5671,  1.0000, -0.9990,  0.9991, -0.9983,\n",
       "          0.6521,  0.7845,  0.7698,  0.9957, -0.8346, -0.2635, -0.8310,  0.6356,\n",
       "          0.9681,  0.9955, -0.9547,  0.8755,  0.9990, -0.9712,  0.9996,  0.5153,\n",
       "          0.9814,  0.9912,  1.0000,  0.8780,  0.9871,  0.9691,  0.9999,  1.0000,\n",
       "         -0.9844,  0.4882,  0.7532, -0.9726, -0.6847,  0.9192,  1.0000,  0.0572,\n",
       "         -0.2061, -0.9999,  0.9924,  0.9981,  1.0000, -0.9409,  0.9964, -0.6769,\n",
       "          0.9035,  0.9526,  0.7005,  0.5253,  0.6554,  0.9976,  0.9991, -0.9998,\n",
       "         -1.0000, -1.0000,  1.0000,  0.9965, -0.8655, -1.0000,  0.9993, -0.7807,\n",
       "          0.9889,  0.9982, -0.4978, -0.9838,  0.9597, -0.9995,  0.3278,  0.2981,\n",
       "          0.8544,  0.1254,  0.9995, -0.9998,  0.6792,  1.0000, -0.6673,  0.9999,\n",
       "          0.6529, -0.9998,  0.9991, -0.9990, -0.9998, -0.6737,  0.9997,  0.9996,\n",
       "         -0.1905,  0.3238,  0.9997, -0.9985,  1.0000, -1.0000,  0.8522, -0.9969,\n",
       "          0.9999, -0.9837, -0.9991, -0.6637,  0.9441,  0.9225, -0.8686,  1.0000,\n",
       "         -0.0479, -0.8034,  0.1028, -0.9864, -0.9969, -0.9895,  0.7223, -0.9999,\n",
       "          0.9041,  0.9099, -0.8658, -0.8747, -0.9998,  0.9997, -0.7987, -0.9575,\n",
       "          1.0000, -0.9907, -1.0000,  0.9244, -0.9922,  0.6910,  0.9948,  0.8247,\n",
       "          0.1517, -1.0000,  0.8658,  0.9999, -0.9966, -0.8210, -0.9743, -0.9909,\n",
       "          0.9811,  0.9949,  0.9391,  0.1788,  0.9075,  0.3431,  0.9350, -0.7945,\n",
       "         -0.1904, -0.9999, -0.9973, -0.9474, -0.9959, -0.9998, -1.0000,  1.0000,\n",
       "          0.9997,  0.9999, -0.8286, -0.4552,  0.5763,  0.9830, -0.9997, -0.9728,\n",
       "          0.6971,  0.9013, -0.8189, -0.9989, -0.6452, -1.0000, -0.7196,  0.5494,\n",
       "         -0.9352, -0.9158,  0.9999,  0.9999, -0.9992, -0.9812, -0.9978, -0.9995,\n",
       "          0.9999,  0.9994,  0.9982, -0.6769, -0.8336,  0.9803, -0.8929, -0.4700,\n",
       "         -0.9995, -0.9933, -0.9998,  0.8846, -0.9964, -0.9998,  0.9878,  0.9999,\n",
       "         -0.0301, -1.0000, -0.7145,  0.9999,  0.9973,  1.0000,  0.8677,  0.9999,\n",
       "         -0.9967,  0.9913, -0.9999,  1.0000, -1.0000,  1.0000,  1.0000,  0.9902,\n",
       "          0.9990, -0.9995,  0.9918, -0.5643, -0.5104,  0.9204, -0.8109, -0.9969,\n",
       "          0.1652,  0.9925, -0.9560,  1.0000,  0.6244,  0.2143,  0.7871,  0.3510,\n",
       "          0.9887, -0.6475, -0.9987,  0.9519,  0.9997,  0.9966,  1.0000,  0.8819,\n",
       "          0.9999, -0.9631, -0.9974,  0.6394, -0.5349, -0.7970, -0.9997,  0.9999,\n",
       "          0.9998, -0.9999, -0.9210,  0.5587,  0.5367,  0.9999,  0.9971,  0.9235,\n",
       "          0.9423,  0.7085,  0.9954, -0.9980,  0.7608, -0.9909, -0.8820,  0.9999,\n",
       "         -0.9808,  0.9988, -0.9982,  0.9990, -0.9669, -0.1814,  0.9963,  0.9875,\n",
       "         -0.9976,  0.9999,  0.8134, -0.9978, -0.9814, -0.9997, -0.9980,  0.7019]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tokenizer(text, return_tensors='pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
