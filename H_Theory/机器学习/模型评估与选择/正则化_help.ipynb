{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;正则化是结构风险最小化策略的实现,是在经验风险上加一个正则化项(regularizer)或罚项(penalty term).正则化\n",
    "项一般是模型复杂度的单调递增函数,模型越复杂,正则化值就越大.比如,正则化项可以是模型参数向量的范数.\n",
    "\n",
    "&emsp;&emsp;正则化一般具有如下形式:    \n",
    "\n",
    "$$ \\min_{f \\in F}  \\frac{1}{N} \\sum_{i=1}^{N}L(y_i, f(\\mathbf{x}_i))  + \\lambda J(f) $$\n",
    "\n",
    "其中,第一项是经验风险,第二项是正则化项目,$ \\lambda \\geq 0 $为调整两者之间关系的系数.\n",
    "\n",
    "&emsp;&emsp;正则化项可以取不同的形式.例如,在回归问题中,损失函数是平方损失,正则化项可以是参数向量的$ L_2 $范数:\n",
    "\n",
    "$$ L(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(f(\\mathbf{x}_i;\\mathbf{w}) - y_i \\right)^2  + \\frac{\\lambda}{2} ||\\mathbf{w}||^2 \\tag{1}$$\n",
    "\n",
    "这里,$ ||\\mathbf{w}|| $表示参数向量$\\mathbf{w}$的$L_2$范数.\n",
    "\n",
    "&emsp;&emsp;正则化项可以是参数向量的$ L_1 $ 范数:\n",
    "\n",
    "$$ L(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(f(\\mathbf{x}_i;\\mathbf{w}) - y_i \\right)^2  + \\frac{\\lambda}{2} ||\\mathbf{w}||_1 \\tag{2}$$\n",
    "\n",
    "这里,$ ||\\mathbf{w}|| $表示参数向量$\\mathbf{w}$的$ L_1$范数.\n",
    "\n",
    "&emsp;&emsp;第一项的经验风险较小的模型可能较复杂(有多个非零参数),这时第二项的模型复杂度会较大.正则化的作用是选择经验风险与\n",
    "模型复杂度同时较小的模型.     \n",
    "\n",
    "&emsp;&emsp;正则化符号奥卡姆( Occam's razor)原理.奥卡姆剃刀原理应用于模型选择时变成以下想法:在所有可能选择的模型中,能够\n",
    "很好的解释已知数据并且十分简单才是最好的模型,也就是应该选择的模型.从贝叶斯估计的角度来看,正则化项对于与模型的先验概率.可以假定\n",
    "复杂的模型有较小的先验概率,简单的模型有较大的先验概率.       \n",
    "\n",
    "\n",
    "&emsp;&emsp;$L_1$范数和$L_2$范数正则化都有助于降低过拟合风险,但前者还会带来一个额外的好处:它比后者更容易获得\"稀疏解\",\n",
    "即它求得的$\\mathbf{w}$会有更少的非零分量.\n",
    "\n",
    "&emsp;&emsp;为了理解这一点,我们来看一个直观的例子:假设$\\mathbf{x}$仅有两个属性,于是无论式(1)还是式(2)解出的$\\mathbf{w}$都只有两个分量,\n",
    "即$w_1, w_2$我们将其作为两个坐标轴,然后在图中绘制出式(1)与式(2)的第一项的\"等值线\",即在$(w_1, w_2)$空间中平方误差项取值相同的点的连线,\n",
    "再分别绘制出$L_1$范数与$L_2$范数的等值线,\n",
    "\n",
    "\n",
    "<img src='../../../Other/img/L_1andL_2.png?raw=true' style=\"width:550px;height:400px;float:bottom\">\n",
    "\n",
    "\n",
    "即在$(w_1,w_2)$空间中$L_1$范数取值相同的点的连线,以及$L_2$范数取值相同的点的连线,如上图所示.式(1)与式(2)的解要在平方误差项与正则化项之间折中,\n",
    "即出现在图中平方误差项等值线与正则化等值线相交处.有上图可以看出,采用$L_1$范数时平方误差与正则化项等值线的交点常出现在坐标轴上,\n",
    "即$w_1$或$w_2$为0,而采用$L_2$范数时,两者的交点常出现在某个象限中,即$w_1$或$w_2$均非0;换言之,采用$L_1$范数比$L_2$范数更容易得到稀疏解.\n",
    "\n",
    "&emsp;&emsp;注意到$\\mathbf{w}$取得稀疏解意味着初始的$d$个特征中仅有对应着$\\mathbf{w}$的非零分量的特征才会出现在最终模型上,\n",
    "于是,求解$L_1$范数正则化的结果是得到了仅采用一部分初始特征的模型;换言之,基于$L_1$正则化的学习方法就是一种嵌入式特征选择方法,\n",
    "其特征选择过程与学习器训练过程融为一体,同时完成(如Lasso回归)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}