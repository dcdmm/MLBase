#### 交叉熵损失和均方误差梯度更新比较(激活函数为sigmoid时)

1. 均方误差(存在梯度消失问题)

$$
J=\frac{(y - \sigma(\mathrm{x}^T \mathrm{w}))^{2}}{2}
$$

$$
\frac{\partial J}{\partial \mathrm{w}} = \left(\sigma(\mathrm{x}^T \mathrm{w}) - y\right) \sigma^{\prime}(\mathrm{x}^T \mathrm{w}) \mathrm{x}^T
$$

其中$J$表示损失函数,$\mathrm{x}$表示样本, $y$表示真实值,$\sigma(x) = \frac{1}{1 + \exp(-x)}  $,$ \sigma'(x) = \sigma(x)(1 - \sigma(x))  \in [0, 0.25] $

2. 交叉熵损失
   $$
   J=-[y \ln \sigma(\mathrm{x}^T \mathrm{w}) + (1-y) \ln (1-\sigma(\mathrm{x}^T \mathrm{w}))]
   $$

   $$
   \begin{aligned}
   \frac{\partial J}{\partial \mathrm{w}} &= -\left(\frac{y}{\sigma(\mathrm{x}^T \mathrm{w})}-\frac{(1-y)}{1-\sigma(\mathrm{x}^T \mathrm{w})}\right) \sigma^{\prime}(\mathrm{x}^T \mathrm{w}) \mathrm{x} \\
   &= \frac{\sigma^{\prime}(\mathrm{x}^T \mathrm{w}) \mathrm{x}}{\sigma(\mathrm{x}^T \mathrm{w})\left(1-\sigma(\mathrm{x}^T \mathrm{w})\right)}\left(\sigma(\mathrm{x}^T \mathrm{w})-y\right)
   \\
   &= \left(\sigma(\mathrm{x}^T \mathrm{w})-y\right) \mathrm{x}^T
   \end{aligned}
   $$

   #### XGBoost与GBDT的区别与联系

1. 在使用CART作为基分类器时,XGBoost显式地加入了正则项来控制模型的复杂度,有利于防止过拟合,从而提高模型的泛化能力
2. GBDT在模型训练时只使用了代价函数的一阶导数信息,XGBoost对代价函数进行二阶泰勒展开,可以同时使用一阶和二阶导数
3. 传统的GBDT采用CART作为基分类器,XGBoost支持多种类型的基分类识器,比如线性分类器
4. 传统的GBDT在每轮迭代时使用全部的数据,XGBoost则采用了与随机森林相似的策略,支持对数据进行采样。
5. 传统的GBDT没有设计对缺失值进行处理,XGBoost能够自动学习出缺失值的处理策略。

 