{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from datasets import load_from_disk\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(Data.Dataset):\n",
    "    \"\"\"定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, split):\n",
    "        dataset_init = load_from_disk(\"dataset\")  # 从HuggingFace保存的数据集\n",
    "        if split == 'train':\n",
    "            self.dataset = dataset_init['train']\n",
    "        elif split == 'validation':\n",
    "            self.dataset = dataset_init['validation']\n",
    "        else:\n",
    "            self.dataset = dataset_init['test']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"定义索引方式\"\"\"\n",
    "        text = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "        return text, label\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    # 批量编码句子\n",
    "    data = token(text=sents,\n",
    "                 truncation=True,\n",
    "                 padding='max_length',\n",
    "                 max_length=512,\n",
    "                 return_token_type_ids=True,\n",
    "                 return_attention_mask=True,\n",
    "                 return_tensors='pt')\n",
    "\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \"\"\"下游训练任务模型\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)  # 二分类任务\n",
    "        self.pretrained = pretrained_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        out = self.pretrained(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.fc(out.pooler_output)\n",
    "        out = out.softmax(dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('train')\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=32,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model(pretrained)\n",
    "model = model.to(device)\n",
    "\n",
    "# 不同网络层拥有不同的优化器参数\n",
    "optimizer = optim.AdamW([dict(params=model.fc.parameters(), lr=2e-5),\n",
    "                         dict(params=model.pretrained.parameters(), lr=5e-6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FGM():\n",
    "    \"\"\"Fast Gradient Sign Method\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self,\n",
    "               emb_name,  # 添加扰动的embedding层名称\n",
    "               epsilon=1.0):  # 扰动项中的\\epsilon\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and name == emb_name:\n",
    "                self.backup[name] = param.detach().clone()\n",
    "                norm = torch.linalg.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm  # \\epsilon * (g / ||g||_2)\n",
    "                    param.data.add_(r_at)  # embedding层参数增加扰动\\Delta x\n",
    "\n",
    "    def restore(self, emb_name):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and name == emb_name:\n",
    "                param.data = self.backup[name]  # 恢复embedding层原有参数值\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7277681827545166 0.25\n",
      "10 0.6629133224487305 0.75\n",
      "20 0.6543726325035095 0.6875\n",
      "30 0.5911693572998047 0.9375\n",
      "40 0.5501372814178467 0.8125\n",
      "50 0.5407504439353943 0.8125\n",
      "60 0.46793609857559204 0.84375\n",
      "70 0.44919583201408386 0.875\n",
      "80 0.42310693860054016 0.9375\n",
      "90 0.47143206000328064 0.84375\n",
      "100 0.3642938435077667 0.96875\n",
      "110 0.396707147359848 0.90625\n",
      "120 0.4522962272167206 0.84375\n",
      "130 0.42607593536376953 0.875\n",
      "140 0.43155309557914734 0.875\n",
      "150 0.4146895110607147 0.875\n",
      "160 0.3909030258655548 0.9375\n",
      "170 0.41944530606269836 0.90625\n",
      "180 0.37423813343048096 0.9375\n",
      "190 0.4125397801399231 0.90625\n",
      "200 0.4017783999443054 0.90625\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# step 1. 初始化\n",
    "fgm = FGM(model)\n",
    "###########################################################################\n",
    "\n",
    "model.train()\n",
    "for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    token_type_ids = token_type_ids.to(device)\n",
    "    labels = labels.to(device)\n",
    "    out = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()  # 未进行对抗训练的梯度\n",
    "\n",
    "    ###########################################################################\n",
    "    # step 2. 对抗训练\n",
    "    fgm.attack(emb_name='pretrained.embeddings.word_embeddings.weight', epsilon=1.)\n",
    "    out_adv = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "    loss_adv = criterion(out_adv, labels)\n",
    "    loss_adv.backward()  # 对抗训练的梯度\n",
    "    fgm.restore(emb_name='pretrained.embeddings.word_embeddings.weight')  # 恢复embedding层原有参数值\n",
    "    ###########################################################################\n",
    "\n",
    "    optimizer.step()  # 梯度累加\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels).sum().item() / len(labels)\n",
    "        print(i, loss.item(), accuracy)\n",
    "\n",
    "    if i == 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89375"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型验证\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader_test):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if i == 5:\n",
    "            break\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# 未添加对抗训练:0.88125\n",
    "# 添加对抗训练后:0.89375\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}