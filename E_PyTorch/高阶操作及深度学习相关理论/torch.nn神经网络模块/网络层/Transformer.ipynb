{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "TransformerEncoderLayer(\n  (self_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n  )\n  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (dropout1): Dropout(p=0.1, inplace=False)\n  (dropout2): Dropout(p=0.1, inplace=False)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 编码器Block\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    # the number of expected features in the input (required).\n",
    "    d_model=512,  # 特征数目(E_q = E_k = E_v)\n",
    "    # 多头数\n",
    "    nhead=8,\n",
    "    # the dimension of the feedforward network model\n",
    "    dim_feedforward=2048,  # 默认dim_feedforward=2048\n",
    "    dropout=0.1,  # 默认dropout=0.1\n",
    "    activation='relu',  # 默认activattion='relu'\n",
    "    # the eps value in layer normalization components\n",
    "    layer_norm_eps=1e-05,  # 默认layer_norm_eps=1e05\n",
    "    # f True, layer norm is done prior to attention and feedforward operations, respectivaly. Otherwise it’s done after\n",
    "    norm_first=False,  # 默认norm_first=False\n",
    "    device=None)\n",
    "encoder_layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.7469e+00,  4.2051e-02,  1.9362e+00,  ..., -5.9903e-02,\n           1.4711e-01, -7.2252e-01],\n         [-2.2480e+00,  1.3206e+00, -7.8952e-01,  ...,  1.6642e+00,\n           7.3171e-01, -1.2751e+00],\n         [-1.6488e+00,  4.6468e-01,  1.5653e+00,  ...,  4.3992e-01,\n          -2.5424e-01,  1.1500e+00],\n         ...,\n         [-7.3992e-02, -8.0366e-01,  8.1588e-01,  ...,  4.3400e-01,\n           1.6023e+00, -7.6043e-01],\n         [-1.3871e+00, -7.2627e-01,  1.7174e-02,  ...,  1.1562e-01,\n           1.1124e+00,  8.4239e-01],\n         [-8.9786e-01,  1.4180e+00,  1.1189e+00,  ..., -6.4031e-01,\n           1.2422e+00, -4.6449e-01]],\n\n        [[-2.4380e+00, -3.9573e-01,  1.4947e+00,  ...,  1.8465e-01,\n          -7.9765e-02,  9.4587e-01],\n         [-1.1278e+00,  1.5586e+00,  1.5435e+00,  ..., -3.9909e-02,\n           4.9932e-01, -1.2963e-01],\n         [ 1.4141e-01,  2.2784e-01,  2.2294e-01,  ...,  6.3093e-01,\n           8.1622e-01, -5.5462e-01],\n         ...,\n         [-1.1984e+00,  1.2676e-02,  1.4903e+00,  ...,  1.7632e+00,\n           1.5094e-01,  6.2887e-02],\n         [-7.2920e-02, -4.4969e-01,  1.9441e+00,  ...,  8.5199e-02,\n           1.1490e+00, -1.0108e+00],\n         [-1.4386e-03,  6.5788e-01,  1.7506e+00,  ..., -4.5701e-02,\n           1.2410e+00,  1.2832e+00]],\n\n        [[-1.6299e+00,  1.4094e-01, -7.9496e-01,  ...,  1.9623e-01,\n           1.4874e+00, -4.9053e-01],\n         [-1.0628e+00, -7.6779e-01, -7.7738e-01,  ...,  6.2595e-01,\n           8.9955e-02, -2.2098e-01],\n         [-6.5677e-01,  1.1141e+00,  1.7514e+00,  ...,  1.1291e+00,\n           9.6894e-01, -3.4182e-01],\n         ...,\n         [-1.2384e+00,  1.4519e+00,  2.1188e+00,  ...,  1.0044e+00,\n           8.9539e-01,  4.1349e-01],\n         [-1.2514e+00, -1.2132e-01, -4.0526e-01,  ...,  1.7211e+00,\n           1.6971e+00,  5.2895e-01],\n         [-1.6992e+00, -4.7149e-01,  1.9356e-01,  ...,  1.7469e+00,\n           2.6707e-01,  6.8559e-01]],\n\n        ...,\n\n        [[-1.1533e+00,  5.9681e-01, -4.2738e-01,  ..., -1.3402e-04,\n           1.5876e+00,  4.1027e-01],\n         [-1.1854e+00,  5.2959e-01,  1.9775e-01,  ...,  7.3217e-02,\n          -1.0123e+00,  7.8206e-01],\n         [ 4.0017e-01,  4.8808e-01,  6.7198e-01,  ...,  1.0921e+00,\n           1.5051e+00, -1.2826e+00],\n         ...,\n         [ 8.1956e-01,  1.2781e+00,  1.0909e+00,  ...,  1.1804e+00,\n          -1.8109e-01, -1.2053e+00],\n         [-1.9728e+00, -2.7475e-01,  5.7076e-01,  ..., -1.2725e-01,\n          -6.4466e-01,  4.1623e-01],\n         [-2.0685e+00, -5.9218e-01, -2.0694e-01,  ...,  1.5253e+00,\n           1.4449e+00,  7.2832e-01]],\n\n        [[-2.0460e+00,  1.1082e+00, -7.9863e-01,  ...,  1.5413e+00,\n           6.7048e-01, -5.9215e-02],\n         [-1.0745e+00,  6.6847e-01, -7.5658e-01,  ..., -6.1765e-01,\n           3.5379e-01,  1.2001e+00],\n         [-1.3378e+00, -1.7666e-01,  7.0898e-01,  ..., -9.0232e-01,\n           9.2811e-01,  1.2121e+00],\n         ...,\n         [ 9.5975e-02, -2.6366e-02,  2.0160e+00,  ..., -6.0404e-01,\n           1.4033e+00,  1.2132e+00],\n         [ 2.2565e-02,  1.9296e-01,  1.8825e+00,  ...,  6.9400e-02,\n           5.2070e-01,  3.1887e-01],\n         [-1.0020e+00, -6.5246e-01,  2.8337e-01,  ..., -4.4035e-01,\n           7.6042e-01, -3.9966e-01]],\n\n        [[-2.3463e-02,  1.0845e+00,  1.1076e+00,  ..., -7.2160e-01,\n           1.0946e+00, -1.0846e-01],\n         [-9.5533e-02,  7.9028e-01,  6.8079e-01,  ..., -1.0921e-01,\n           5.3925e-01, -1.0050e+00],\n         [-6.4165e-01, -1.0739e-01, -1.4703e-01,  ...,  1.2395e+00,\n           2.2397e-02, -4.9300e-01],\n         ...,\n         [-1.3180e+00,  1.3059e-01,  1.5718e+00,  ..., -6.6078e-01,\n           9.7304e-01,  8.6991e-01],\n         [-4.7056e-01,  1.3954e+00,  1.4912e+00,  ...,  1.3337e+00,\n          -6.1451e-01,  1.0152e+00],\n         [-2.5083e+00,  5.7301e-01,  4.7689e-01,  ..., -5.8160e-01,\n          -5.0873e-01,  8.0732e-01]]], grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.rand(10, 32, 512)  # 输入形状: (batch, seq, feature)\n",
    "encoder_layer(\n",
    "    # the sequence to the encoder layer (required)\n",
    "    src=src,\n",
    "    # the mask for the src sequence (optional)\n",
    "    src_mask=None)  # 多头注意力中的attn_mask参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.7223e+00, -2.5422e-01,  5.2391e-01,  ..., -3.6043e-01,\n          -8.4727e-01, -1.3284e+00],\n         [-1.6210e+00,  7.2879e-01, -8.3492e-01,  ...,  7.5971e-01,\n          -5.4969e-01, -1.7766e+00],\n         [-1.3749e+00, -4.2977e-01,  1.2220e+00,  ..., -1.3658e+00,\n           1.3468e-01, -1.0810e+00],\n         ...,\n         [-9.4071e-01, -1.3170e+00, -1.9088e-01,  ...,  3.6259e-01,\n           3.9407e-02, -1.5182e+00],\n         [-8.6168e-01, -6.0159e-01,  1.0581e-02,  ..., -6.0681e-01,\n          -1.4985e-01,  1.0571e-01],\n         [-6.1878e-01,  6.9865e-01,  2.1431e-03,  ...,  1.0071e-01,\n           4.0756e-01, -1.5029e+00]],\n\n        [[-8.3908e-01, -8.5193e-01,  1.0251e-01,  ..., -1.0720e+00,\n          -9.3323e-02, -5.5439e-01],\n         [-9.5127e-01,  2.8391e-01,  4.8230e-01,  ...,  3.4062e-01,\n           6.7189e-02, -1.3187e+00],\n         [-1.7839e+00, -7.0469e-01,  3.7584e-01,  ..., -1.1501e+00,\n           2.8152e-01, -1.0479e+00],\n         ...,\n         [-1.2833e+00, -5.1886e-01,  6.0077e-01,  ..., -1.3259e-01,\n          -2.7059e-01, -2.7963e-01],\n         [-3.1558e-02, -2.9144e-01,  7.2217e-01,  ...,  4.4234e-02,\n          -1.0201e+00, -6.7827e-01],\n         [-1.5815e+00,  5.7702e-01,  6.6650e-01,  ...,  1.6105e-01,\n           6.4505e-01, -6.0613e-01]],\n\n        [[-1.1613e+00, -6.0297e-01, -1.7249e-01,  ..., -3.2547e-01,\n          -2.6698e-01, -1.4961e+00],\n         [-6.9795e-01, -1.5169e-01, -6.8515e-01,  ...,  7.8591e-01,\n          -5.1842e-01, -2.1328e+00],\n         [-8.0613e-01, -4.2318e-01,  1.0370e+00,  ..., -5.4488e-01,\n          -1.6635e-01, -8.5307e-01],\n         ...,\n         [-1.7063e+00,  2.1882e-01,  5.6279e-01,  ...,  6.8499e-01,\n           7.2562e-01, -2.8916e-01],\n         [-1.3178e-01, -1.4429e+00, -2.8256e-01,  ..., -2.2282e-01,\n          -2.3851e-01, -2.7005e-01],\n         [-1.8130e+00,  1.4584e-01,  1.5314e-02,  ...,  2.9110e-01,\n           9.2667e-03, -9.5109e-01]],\n\n        ...,\n\n        [[-1.0691e+00, -8.1965e-01,  3.4310e-01,  ..., -4.6827e-01,\n          -7.5791e-01, -5.3724e-01],\n         [-1.0901e+00, -4.3945e-01, -1.5436e-01,  ..., -4.4131e-02,\n          -3.7833e-01, -1.2931e+00],\n         [-5.5982e-01, -3.5211e-01,  1.2470e+00,  ..., -1.3829e+00,\n           1.7281e-01, -8.1595e-01],\n         ...,\n         [-9.1514e-01,  9.7673e-02,  1.2878e-01,  ...,  5.3718e-01,\n           5.6711e-02, -1.1557e+00],\n         [-1.2614e+00,  2.1530e-01,  7.7731e-01,  ..., -6.5355e-01,\n          -1.6190e+00, -1.7196e-01],\n         [-1.3528e+00,  2.9144e-01, -6.4961e-01,  ...,  2.6262e-01,\n           5.8031e-01, -1.2040e+00]],\n\n        [[-1.3571e+00, -2.6142e-01,  3.0821e-02,  ..., -1.5746e-01,\n          -4.6343e-01, -1.4464e+00],\n         [-1.6904e+00, -3.5765e-01, -5.1079e-01,  ...,  8.6208e-02,\n          -8.1141e-01, -1.3584e+00],\n         [-1.0363e+00, -6.5788e-01,  1.1947e+00,  ..., -1.4560e+00,\n           1.3135e-01, -1.8045e-01],\n         ...,\n         [-1.3149e+00, -7.7788e-01,  1.5476e-01,  ..., -3.5121e-01,\n          -2.2977e-01, -7.7834e-01],\n         [-4.8572e-01, -2.5248e-01,  1.0708e+00,  ..., -4.3239e-01,\n          -8.2132e-01,  4.8209e-02],\n         [-1.5669e+00,  7.2839e-01,  3.2084e-01,  ...,  2.9042e-02,\n          -8.8511e-02, -2.2258e+00]],\n\n        [[-9.8588e-01,  1.0323e-01,  1.0286e+00,  ..., -4.2682e-01,\n           3.0681e-01, -8.4870e-01],\n         [ 2.5159e-01,  1.8268e-01,  5.0220e-01,  ..., -2.4969e-01,\n           2.8607e-01, -1.8682e+00],\n         [-1.3763e+00, -1.2097e+00, -2.0409e-01,  ..., -4.4161e-01,\n          -4.4290e-02, -1.3003e+00],\n         ...,\n         [-1.4273e+00, -7.4360e-01, -5.2631e-03,  ..., -7.7312e-01,\n           5.8989e-02, -1.5135e-01],\n         [-6.9729e-01, -1.7747e-02,  6.0029e-01,  ...,  1.5614e-01,\n          -1.1069e+00, -3.5260e-01],\n         [-2.1755e+00,  2.6882e-01,  1.0647e-01,  ..., -2.8181e-01,\n           4.9631e-01, -1.5884e+00]]], grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 编码器\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layer,\n",
    "                                            num_layers=6)\n",
    "out_src_en = transformer_encoder(src, mask=None)\n",
    "out_src_en"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "TransformerDecoderLayer(\n  (self_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n  )\n  (multihead_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n  )\n  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (dropout1): Dropout(p=0.1, inplace=False)\n  (dropout2): Dropout(p=0.1, inplace=False)\n  (dropout3): Dropout(p=0.1, inplace=False)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 解码器Block\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512,\n",
    "                                           nhead=8,\n",
    "                                           dim_feedforward=2048,  # 默认dim_feedforward=2048\n",
    "                                           dropout=0.1,  # 默认dropout=0.1\n",
    "                                           activation='relu',  # 默认activattion='relu'\n",
    "                                           layer_norm_eps=1e-05,  # 默认layer_norm_eps=1e05\n",
    "                                           norm_first=False,  # 默认norm_first=False\n",
    "                                           device=None)\n",
    "decoder_layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-2.4136e-01, -7.0166e-01, -1.8993e+00,  ...,  1.2587e+00,\n           2.0225e-01,  1.7014e+00],\n         [ 7.7665e-01,  9.8012e-01, -8.4711e-01,  ...,  1.6112e+00,\n           2.2498e-01, -1.2817e-01],\n         [ 7.2620e-01, -4.3400e-01, -1.2945e+00,  ..., -4.5379e-01,\n           1.7600e-01,  3.5164e-01],\n         ...,\n         [-1.0200e+00,  5.1918e-02, -1.8676e+00,  ...,  1.3874e+00,\n           3.5616e-01, -1.9928e-01],\n         [ 3.5279e-01,  1.8399e+00, -5.3174e-01,  ..., -8.7508e-01,\n          -9.7843e-01,  1.2661e+00],\n         [-1.2432e-01, -2.9244e-01,  7.4572e-01,  ...,  1.9856e-01,\n           1.9546e+00,  8.1695e-01]],\n\n        [[ 8.1090e-01,  1.2328e+00,  5.7557e-01,  ..., -5.4578e-01,\n           1.2031e+00, -2.5842e-01],\n         [-5.2259e-01, -1.1667e+00, -1.1536e+00,  ..., -6.1386e-02,\n           6.3571e-01,  1.4593e+00],\n         [ 6.1783e-01,  1.3427e+00, -1.2045e+00,  ...,  3.3734e-02,\n           1.7217e+00,  1.5385e+00],\n         ...,\n         [ 4.5740e-01,  9.4524e-01, -1.7662e+00,  ...,  5.8659e-01,\n          -7.1761e-01, -3.6190e-01],\n         [-1.4478e+00, -8.4610e-02, -9.4968e-01,  ...,  8.1836e-01,\n           1.1578e+00,  4.5201e-01],\n         [ 1.4220e-01, -2.6973e-01,  6.5624e-01,  ..., -4.8634e-01,\n           2.2894e-01,  8.4649e-01]],\n\n        [[-2.7605e-01,  6.9825e-01,  4.0851e-01,  ...,  1.7650e+00,\n           2.1904e-01,  7.2362e-01],\n         [-1.4831e-01, -2.9023e-01,  6.0013e-01,  ...,  1.9139e-01,\n          -4.8335e-01,  3.6201e-01],\n         [ 1.2158e+00,  1.1482e+00, -1.8580e+00,  ..., -1.3979e-01,\n           1.0830e+00, -4.2991e-01],\n         ...,\n         [-8.1494e-01, -7.0477e-01,  4.2447e-01,  ..., -3.4162e-03,\n           2.9883e-01,  1.8608e+00],\n         [-4.0345e-01,  6.3179e-01, -4.5593e-01,  ...,  1.8495e-01,\n           4.2540e-01,  1.9797e+00],\n         [-2.7372e-01,  3.9148e-01, -2.7850e-01,  ...,  1.6374e+00,\n           1.2622e+00,  7.1603e-01]],\n\n        ...,\n\n        [[-8.7152e-01,  1.2871e+00, -7.4270e-01,  ...,  1.5052e-01,\n           1.6731e-01,  1.5078e+00],\n         [-1.3095e+00,  1.6728e+00, -3.1614e-01,  ..., -3.3810e-01,\n          -1.3710e-01,  3.1982e-01],\n         [-9.0693e-01,  6.0833e-01, -1.0712e+00,  ...,  1.1550e+00,\n           1.0824e+00,  5.8891e-01],\n         ...,\n         [ 4.1693e-01,  8.6422e-01, -1.0321e+00,  ...,  1.7695e+00,\n          -5.0398e-01,  1.5764e+00],\n         [-6.0460e-04, -3.7012e-01, -1.9123e+00,  ...,  1.4691e+00,\n          -8.2221e-01, -1.6822e-01],\n         [ 8.4848e-01,  2.4084e-01, -1.5637e+00,  ...,  1.0282e+00,\n           9.9634e-01,  4.3258e-01]],\n\n        [[ 4.8132e-01, -5.7729e-01, -1.2617e+00,  ...,  8.3073e-01,\n           1.0300e+00,  1.2873e+00],\n         [-1.4883e-01,  1.7466e+00, -6.4612e-01,  ...,  1.1643e+00,\n           5.4632e-01, -1.0289e-01],\n         [-4.1533e-01, -8.9660e-01, -1.8826e+00,  ...,  9.2827e-01,\n           1.4270e+00, -3.9254e-01],\n         ...,\n         [-2.1205e-02,  1.6511e+00,  4.1710e-01,  ...,  9.5419e-01,\n           1.0421e+00, -1.2134e-01],\n         [-1.6524e+00,  1.4548e+00, -6.4976e-01,  ...,  4.1366e-01,\n          -2.3954e-01,  6.5493e-02],\n         [-6.2735e-01,  1.8674e-01, -5.7368e-01,  ..., -7.4533e-01,\n          -1.3857e-01,  9.1332e-02]],\n\n        [[ 9.3012e-03,  1.8521e+00, -2.8972e-02,  ..., -6.4958e-01,\n          -3.8026e-01,  4.7666e-01],\n         [-9.4808e-01, -9.3978e-02, -1.7271e+00,  ..., -4.8424e-01,\n          -1.3397e-01,  9.1032e-01],\n         [ 8.4184e-01, -2.1781e-01, -4.9957e-02,  ...,  6.6543e-01,\n          -4.2498e-01,  1.3317e+00],\n         ...,\n         [ 1.1958e+00, -5.7047e-01, -1.5996e+00,  ...,  2.8613e-01,\n           1.1338e+00,  1.5684e-01],\n         [-1.9764e-01, -7.1815e-01,  2.1976e-01,  ..., -3.3816e-01,\n           1.4317e+00,  6.7155e-01],\n         [-7.1228e-01,  4.6064e-01, -1.2672e+00,  ..., -1.1986e+00,\n           1.3581e+00,  8.2937e-01]]], grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "decoder_layer(\n",
    "    # the sequence to the decoder layer (required).\n",
    "    tgt=tgt,\n",
    "    # the sequence from the last layer of the encoder (required).\n",
    "    memory=memory,\n",
    "    # the mask for the tgt sequence (optional)\n",
    "    tgt_mask=None,\n",
    "    # the mask for the memory sequence (optional)\n",
    "    memory_mask=None\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.4741,  1.3153,  0.7335,  ...,  1.7569, -0.5978,  1.0754],\n         [-0.5324,  1.2446,  0.8768,  ...,  1.4507, -0.5487, -0.2084],\n         [-1.0658,  1.2898,  0.3674,  ...,  1.0603, -0.1856,  0.9397],\n         ...,\n         [-1.7659,  1.6570,  0.4399,  ...,  1.8238, -0.5380, -0.0103],\n         [-0.7136,  2.4370,  0.9044,  ...,  1.7245, -1.3035,  0.7737],\n         [-1.4178,  0.9352,  1.2538,  ...,  0.4309,  0.4570,  1.4094]],\n\n        [[-0.7885,  1.8838,  0.8437,  ...,  1.5711,  0.1666,  1.0607],\n         [-1.7347,  0.9000,  1.0103,  ...,  1.0445, -0.0289,  0.5028],\n         [-2.0434,  1.9087,  0.6508,  ...,  1.4442, -0.5380,  0.7605],\n         ...,\n         [-0.9334,  1.2041,  0.4968,  ...,  1.7545, -0.8955,  0.0577],\n         [-1.3306,  0.7884,  0.7297,  ...,  1.8367,  0.1691,  0.5927],\n         [-1.4414,  0.7141,  1.3668,  ...,  0.8071, -0.3070,  0.8092]],\n\n        [[-0.7734,  1.8164,  1.3775,  ...,  1.8063, -0.5843,  1.4923],\n         [-1.7300,  0.8537,  1.2114,  ...,  1.6073, -1.1539,  0.2831],\n         [-1.3946,  1.3345,  0.1261,  ...,  1.0498, -0.7756,  0.2110],\n         ...,\n         [-1.3264,  1.0245,  1.5602,  ...,  2.0557, -0.3241,  1.1643],\n         [-1.6698,  1.6495,  0.4557,  ...,  1.9442, -0.1072,  1.4274],\n         [-1.2763,  1.1018,  0.5629,  ...,  1.6640, -0.0221,  0.8163]],\n\n        ...,\n\n        [[-1.9317,  1.6086,  1.3594,  ...,  1.5268, -0.5776,  1.3043],\n         [-1.8985,  1.3776,  1.1808,  ...,  0.9465, -0.6986,  0.3530],\n         [-1.8019,  1.0359,  0.4625,  ...,  1.2997, -0.4975,  0.5883],\n         ...,\n         [-1.0293,  1.3598,  0.4967,  ...,  2.1406, -1.1292,  1.0172],\n         [-0.9943,  0.9948,  0.4011,  ...,  1.3064, -0.9917,  0.5262],\n         [-1.1388,  0.9073,  0.3826,  ...,  1.3236, -0.0112,  0.8454]],\n\n        [[-0.9373,  2.0431,  1.2504,  ...,  1.8538, -0.7505,  1.4701],\n         [-1.1779,  1.3612,  1.2331,  ...,  1.2292, -0.3772,  0.0942],\n         [-1.4253,  0.8565,  0.0987,  ...,  1.2822, -0.6694,  0.3489],\n         ...,\n         [-1.6066,  2.0137,  1.5932,  ...,  1.1926, -0.6875, -0.0955],\n         [-1.9591,  1.4884,  0.8329,  ...,  1.0883, -0.7721,  0.8288],\n         [-1.5678,  1.5559,  1.0907,  ...,  0.9559, -0.1363,  0.5803]],\n\n        [[-1.1826,  1.7357,  1.2140,  ...,  1.3749, -0.0480,  1.2066],\n         [-1.5870,  0.6278,  0.3101,  ...,  1.0758, -0.8284,  0.7863],\n         [-0.5999,  1.5179,  0.3074,  ...,  1.8502, -0.7423,  0.8227],\n         ...,\n         [-0.5072,  1.3940,  1.0896,  ...,  1.8741, -0.3324,  0.5506],\n         [-0.8966,  1.4382,  1.4880,  ...,  1.4378, -0.2623,  1.0895],\n         [-0.9708,  1.0302,  0.7248,  ...,  0.6085,  0.0475,  0.8565]]],\n       grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 编码器\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer=decoder_layer,\n",
    "                                            num_layers=6)\n",
    "out_src_de = transformer_decoder(tgt=tgt, memory=memory,\n",
    "                                 tgt_mask=None, memory_mask=None)  # 多头注意力中的attn_mask参数\n",
    "out_src_de"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Transformer(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (4): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (5): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (4): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (5): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformer默认参数如下\n",
    "transformer_model_default = nn.Transformer(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    activation='relu',\n",
    "    custom_encoder=None,\n",
    "    custom_decoder=None,\n",
    "    layer_norm_eps=1e-05,\n",
    "    norm_first=False,\n",
    "    device=None)\n",
    "transformer_model_default"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Transformer(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (4): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (5): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (4): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (5): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model_obj = nn.Transformer(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    activation='relu',\n",
    "    custom_encoder=transformer_encoder,  # 自定义编码器\n",
    "    custom_decoder=transformer_decoder,  # 自定义解码器\n",
    "    layer_norm_eps=1e-05,\n",
    "    norm_first=False,\n",
    "    device=None)\n",
    "transformer_model_obj"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.6244, -2.0542,  0.4478,  ...,  1.9199,  1.0156, -1.2590],\n         [-0.4608, -1.4375,  1.2078,  ...,  1.5706, -0.0928, -1.7533],\n         [ 0.0137, -0.9193,  2.2957,  ...,  1.2349, -0.0572, -1.6597],\n         ...,\n         [ 0.5563, -0.7968,  1.5184,  ...,  0.8981, -0.6524, -1.3620],\n         [ 0.4151, -0.3644,  2.3248,  ...,  1.2957,  0.2861, -1.4721],\n         [ 1.1312, -1.4982,  1.1165,  ...,  0.5287,  0.3034, -0.9692]],\n\n        [[ 0.3215, -2.2888,  2.7028,  ...,  1.6844,  0.8484, -1.4221],\n         [-0.2087, -2.3712,  0.8356,  ...,  1.8050,  0.9232, -1.4095],\n         [ 0.3716, -1.3159,  2.3826,  ...,  1.9608, -0.0205, -0.4565],\n         ...,\n         [ 0.1239, -0.6204,  1.2942,  ...,  0.9238, -0.7001, -1.2035],\n         [-0.1830, -1.4763,  1.5067,  ...,  1.8243,  0.0708, -1.7185],\n         [ 0.5813, -2.1217,  1.4111,  ...,  1.0835,  0.0330, -1.0250]],\n\n        [[ 0.7165, -1.1891,  1.9710,  ...,  2.2829,  0.7236, -2.8863],\n         [-0.0607, -1.6457,  0.7646,  ...,  2.2327, -0.6189, -2.4069],\n         [ 0.5613, -1.4158,  2.0501,  ...,  1.5423, -0.5519, -0.8693],\n         ...,\n         [-0.0490, -1.2012,  1.7036,  ...,  1.4454, -0.8382, -1.9636],\n         [-0.8079, -1.6278,  1.7425,  ...,  1.1416, -0.4107, -1.4430],\n         [ 0.9649, -1.3233,  1.0242,  ...,  1.0272, -0.3727, -1.6806]],\n\n        ...,\n\n        [[ 0.3129, -1.1066,  1.9030,  ...,  1.8460, -0.0843, -1.9922],\n         [-0.8722, -1.5008,  1.1056,  ...,  2.2173,  0.0253, -1.8833],\n         [ 0.0508, -1.2826,  1.4461,  ...,  1.8631,  0.4277, -0.9736],\n         ...,\n         [ 0.2035, -0.9849,  0.4305,  ...,  1.9990, -1.1014, -0.6909],\n         [ 0.3529, -1.4640,  2.4189,  ...,  2.1591, -0.6332, -1.5004],\n         [ 1.1443, -1.1137,  1.5257,  ...,  1.4136,  0.4310, -1.8588]],\n\n        [[ 0.3097, -2.1129,  2.4281,  ...,  1.9128,  0.4348, -2.7663],\n         [-0.9308, -2.1003,  1.4611,  ...,  2.7839, -0.6851, -2.2687],\n         [-0.8269, -1.3547,  1.9148,  ...,  1.5921,  0.4520, -1.8876],\n         ...,\n         [ 0.0908, -1.5836,  1.9330,  ...,  0.2019, -0.5385, -1.2134],\n         [ 0.0856, -1.4842,  1.6637,  ...,  1.3513,  0.3258, -0.7885],\n         [ 1.1862, -1.6167,  1.2133,  ...,  1.0132,  0.4821, -2.1323]],\n\n        [[-0.1492, -1.1775,  2.0587,  ...,  1.9086,  0.2706, -1.7075],\n         [-0.7982, -1.5737,  1.6610,  ...,  1.8863, -0.5376, -0.6903],\n         [-0.0344, -1.3215,  1.7370,  ...,  0.7094,  0.2724, -1.3385],\n         ...,\n         [ 0.6908, -1.0853,  1.4585,  ...,  1.4319,  0.3644, -0.6717],\n         [ 0.2399, -1.3204,  0.8425,  ...,  1.8031,  0.0551, -1.9668],\n         [ 1.1738, -1.8217,  1.1415,  ...,  1.5580,  0.5256, -1.2287]]],\n       grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model_default(src=src,\n",
    "                          tgt=tgt,\n",
    "                          tgt_mask=None,\n",
    "                          memory_mask=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}