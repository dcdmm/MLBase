{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 30])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 100)\n",
    "\n",
    "rnn = nn.RNN(input_size=100, hidden_size=30)\n",
    "\n",
    "out, h = rnn(x)  # 单层,序列长度为1\n",
    "\n",
    "print(out.shape)\n",
    "print(out - h)  # 此时out=h;即相当于RNNCell(只输出h)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 10]) torch.Size([2, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(input_size=100,  # The number of expected features in the input x\n",
    "             hidden_size=10,  # 隐含变量的维度大小(即权重矩阵W_{ih}、W_{hh}中的hidden_size)\n",
    "             num_layers=2,  # 循环神经网络层数.若num_layers=2,则第2层的输入是第1层的输出(层与层之间的参数相互独立)\n",
    "             nonlinearity='tanh',  # 非线性激活函数类型,也可以设置为'relu'.默认nonlinearity='tanh'\n",
    "             bias=True,  # 是否添加偏置.默认bias=True\n",
    "             batch_first=False,  # 如果batch_first=True,则输入张量大小为(N,T,C),而不是(T,N,C).默认batch_first=False\n",
    "             dropout=0.5)  # 如果这个值非零,则在循环神经网络最后输出的基础上加上丢弃层,丢弃的概率由输入的dropout确定.默认dropout=0\n",
    "\n",
    "x = torch.randn(20, 3, 100)  # 输入的默认形状为(T,N,C),其中T为序列的长度,N为min-batch的大小,C为输入的特征数目\n",
    "h_0 = torch.ones((2, 3, 10))  # (L*D, N, hidden_size),其中L为循环神经网络层数,D为1(单向)或2(双向)\n",
    "out, h = rnn(x, hx=h_0)  # 自定义h_0,默认h_0为全0张量\n",
    "print(out.shape, h.shape)  # out.shape=(T, N, hidden_size);h.shape=(L*D, N, hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0   shape= torch.Size([10, 100])\n",
      "weight_hh_l0   shape= torch.Size([10, 10])\n",
      "bias_ih_l0   shape= torch.Size([10])\n",
      "bias_hh_l0   shape= torch.Size([10])\n",
      "weight_ih_l1   shape= torch.Size([10, 10])\n",
      "weight_hh_l1   shape= torch.Size([10, 10])\n",
      "bias_ih_l1   shape= torch.Size([10])\n",
      "bias_hh_l1   shape= torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "~RNN.weight_ih_l[k] –\n",
    "    the learnable input-hidden weights of the k-th layer, of shape (hidden_size, input_size) for k = 0.\n",
    "    Otherwise, the shape is (hidden_size, num_directions * hidden_size)\n",
    "~RNN.weight_hh_l[k] –\n",
    "    the learnable hidden-hidden weights of the k-th layer, of shape (hidden_size, hidden_size)\n",
    "~RNN.bias_ih_l[k] –\n",
    "    the learnable input-hidden bias of the k-th layer, of shape (hidden_size)\n",
    "~RNN.bias_hh_l[k] –\n",
    "    the learnable hidden-hidden bias of the k-th layer, of shape (hidden_size)\n",
    "'''\n",
    "for name, param in rnn.named_parameters():\n",
    "    print(name, '  shape=', param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 30]) torch.Size([4, 3, 15])\n"
     ]
    }
   ],
   "source": [
    "b_run = nn.RNN(input_size=100, hidden_size=15, num_layers=2,\n",
    "               bidirectional=True)  # 是否为设置为双向循环神经网络,默认为False\n",
    "\n",
    "# 此时b_h_0为正向hx(shape=(2, 3, 15))和反向hx(shape=(2, 3, 15))在第0个维度的拼接\n",
    "b_h_0 = torch.ones((4, 3, 15))\n",
    "\n",
    "# ★★★★★b_out中T(训练的长度)信息仍然保留\n",
    "b_out, b_h = b_run(x, hx=b_h_0)\n",
    "# b_out为每个序列最后一层的输出(双向则第二维度*2);b_h为最后一个序列每层的输出(双向则第0维度*2)\n",
    "print(b_out.shape, b_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0   shape= torch.Size([15, 100])\n",
      "weight_hh_l0   shape= torch.Size([15, 15])\n",
      "bias_ih_l0   shape= torch.Size([15])\n",
      "bias_hh_l0   shape= torch.Size([15])\n",
      "weight_ih_l0_reverse   shape= torch.Size([15, 100])\n",
      "weight_hh_l0_reverse   shape= torch.Size([15, 15])\n",
      "bias_ih_l0_reverse   shape= torch.Size([15])\n",
      "bias_hh_l0_reverse   shape= torch.Size([15])\n",
      "weight_ih_l1   shape= torch.Size([15, 30])\n",
      "weight_hh_l1   shape= torch.Size([15, 15])\n",
      "bias_ih_l1   shape= torch.Size([15])\n",
      "bias_hh_l1   shape= torch.Size([15])\n",
      "weight_ih_l1_reverse   shape= torch.Size([15, 30])\n",
      "weight_hh_l1_reverse   shape= torch.Size([15, 15])\n",
      "bias_ih_l1_reverse   shape= torch.Size([15])\n",
      "bias_hh_l1_reverse   shape= torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "for name, param in b_run.named_parameters():\n",
    "    '''正向和反向两个方向的循环神经网络有各自的相互独立的参数'''\n",
    "    print(name, '  shape=', param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-0.1624,  0.2138,  0.0355, -0.1030,  0.0249,  0.0346, -0.1337, -0.2361,\n         -0.2403, -0.0956,  0.0726,  0.0127,  0.0639,  0.1459,  0.1096],\n        [ 0.0793, -0.2393,  0.2392,  0.0268,  0.1901, -0.0037, -0.1741, -0.0875,\n         -0.0806,  0.1782, -0.2402, -0.0513,  0.0246,  0.2576, -0.0746],\n        [ 0.0227, -0.2464, -0.0191, -0.1724,  0.0934,  0.1226,  0.1112, -0.1264,\n         -0.2379,  0.0968, -0.0834,  0.0482, -0.0331,  0.0499,  0.2456],\n        [-0.0639, -0.1552, -0.0647,  0.0254, -0.0868, -0.2272,  0.1727,  0.1697,\n          0.2208,  0.1287, -0.2386,  0.0543,  0.2541, -0.0613,  0.2509],\n        [ 0.0355, -0.0166, -0.2445, -0.0880,  0.2539,  0.0606, -0.2037, -0.0882,\n          0.1818,  0.1172, -0.0297, -0.0860, -0.1478, -0.2366, -0.0389],\n        [ 0.0048, -0.1967, -0.1605, -0.2279,  0.1017,  0.1661,  0.1931, -0.1015,\n         -0.0006, -0.0360,  0.1823, -0.2241, -0.0922, -0.1130, -0.1167],\n        [ 0.0159,  0.0018, -0.1755,  0.2395, -0.0436,  0.1319, -0.1562, -0.0255,\n          0.0395, -0.2497,  0.1402, -0.1998, -0.2422,  0.1644,  0.1398],\n        [ 0.0935,  0.2046,  0.2146, -0.2015, -0.0710,  0.0417, -0.1449,  0.2468,\n         -0.1057,  0.0666, -0.2491,  0.0125,  0.0418, -0.1538,  0.1627],\n        [ 0.2532,  0.1421,  0.0665,  0.1694,  0.1288,  0.1380,  0.1464,  0.0021,\n          0.0723, -0.2570, -0.0299, -0.2442, -0.0724,  0.1818,  0.2219],\n        [-0.1269,  0.1899, -0.2087,  0.0711,  0.0948, -0.0687, -0.0236,  0.0798,\n         -0.1976, -0.1280,  0.0417,  0.0985,  0.1979, -0.1378, -0.1433],\n        [-0.0472,  0.1681,  0.2433, -0.0633,  0.1100,  0.1032, -0.2203,  0.2284,\n          0.2343,  0.1171,  0.2083, -0.0262,  0.1795,  0.0566, -0.0732],\n        [-0.0503,  0.0458,  0.1990,  0.0921,  0.1068,  0.2459,  0.0613, -0.0747,\n          0.1106, -0.0787, -0.1187, -0.1162,  0.2147,  0.2056, -0.0081],\n        [ 0.0621, -0.1000, -0.2091, -0.0199,  0.2140,  0.0902,  0.2396,  0.2293,\n          0.2148,  0.1011, -0.0149, -0.0872, -0.1751, -0.2554, -0.2334],\n        [-0.0275, -0.0138, -0.0446, -0.2510,  0.1870,  0.0024,  0.0505,  0.1317,\n         -0.0429,  0.2018,  0.0011, -0.0966,  0.0992, -0.0249,  0.0802],\n        [-0.1000,  0.2563,  0.1499,  0.2349,  0.2439, -0.1547, -0.0825,  0.2163,\n          0.2123, -0.1934,  0.1192, -0.0535,  0.0893, -0.2447, -0.0156]],\n       requires_grad=True)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_run.weight_hh_l0  # all the weights and biases are initialized from U(-\\sqrt{k}, \\sqrt{k}), where k=1/hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 0.1266, -0.1950, -0.0558, -0.0238, -0.1321, -0.0923,  0.1650, -0.1078,\n         -0.0907,  0.2191, -0.0831, -0.2206,  0.1373,  0.2469,  0.2197,  0.0520,\n          0.1646, -0.0422,  0.1956, -0.0193,  0.0213, -0.1220, -0.1972, -0.1268,\n         -0.1419, -0.0045, -0.0702,  0.1078, -0.0327,  0.1466],\n        [ 0.0743,  0.1339,  0.1481,  0.1415, -0.1055,  0.1244,  0.0323, -0.0247,\n          0.2374, -0.1276, -0.1891, -0.0081,  0.0392,  0.1447,  0.1944, -0.2141,\n         -0.0024,  0.1767,  0.0129, -0.0213,  0.2110,  0.0730, -0.1559,  0.0101,\n         -0.1155,  0.0776,  0.2073,  0.1121, -0.0066,  0.1654],\n        [ 0.0278,  0.0303,  0.0553, -0.2115, -0.1622,  0.2455,  0.1116, -0.2505,\n          0.0621,  0.2028,  0.0170, -0.0820, -0.1857,  0.0681, -0.2511,  0.1831,\n          0.0192, -0.0645, -0.0982, -0.0984, -0.1612, -0.2012,  0.1480,  0.0177,\n          0.1261,  0.2217, -0.2058,  0.0563, -0.0779,  0.0942],\n        [-0.1648,  0.0622,  0.2400, -0.0613, -0.2388, -0.0705,  0.0160,  0.1696,\n         -0.0460, -0.2065,  0.1782, -0.1228, -0.1034, -0.1251,  0.2389, -0.1304,\n         -0.1991,  0.0725,  0.0814, -0.0965,  0.0037, -0.0050, -0.1836,  0.2328,\n          0.2193,  0.2397,  0.0924, -0.0513, -0.1526,  0.1072],\n        [ 0.1611,  0.0635,  0.1969,  0.2033,  0.2412, -0.0130,  0.0293,  0.2407,\n          0.0724, -0.0716, -0.1612, -0.2377, -0.1030, -0.2178,  0.0276,  0.2127,\n         -0.0997,  0.2124, -0.0495, -0.2437, -0.0116,  0.1870,  0.0476, -0.2255,\n         -0.0674,  0.2479, -0.1822, -0.0872, -0.1064, -0.1091],\n        [ 0.1004,  0.2018, -0.1488, -0.1361,  0.0860,  0.0670, -0.1824,  0.2444,\n          0.1106,  0.0307, -0.0485, -0.2253, -0.1764, -0.0962,  0.1270,  0.1589,\n         -0.1460,  0.2168,  0.1395,  0.1921, -0.2089, -0.1723, -0.1766,  0.1886,\n          0.0599, -0.2193, -0.2506, -0.1105, -0.0722,  0.1509],\n        [-0.0911, -0.0979, -0.0948,  0.0942, -0.0020,  0.2075, -0.1163,  0.1936,\n         -0.1093,  0.2046, -0.0421, -0.1739, -0.0842, -0.0558,  0.0557, -0.1098,\n          0.0063, -0.2165,  0.0049,  0.0447,  0.0656,  0.2096, -0.1875,  0.2040,\n          0.0850, -0.0270,  0.1901,  0.0629, -0.2382, -0.1379],\n        [-0.1433, -0.1707, -0.0930, -0.0228, -0.0920,  0.0535,  0.2387,  0.0603,\n          0.1843, -0.0746,  0.1241,  0.0036, -0.0154,  0.1568,  0.0048, -0.1808,\n         -0.0091,  0.1682, -0.0893, -0.0407, -0.0459,  0.2502,  0.0245, -0.1758,\n         -0.1240,  0.1050,  0.0160, -0.1013,  0.1761, -0.1728],\n        [-0.1060,  0.0812,  0.1106, -0.2274,  0.1599, -0.1683,  0.0070, -0.1776,\n         -0.0731,  0.0520,  0.2206, -0.0069, -0.2520, -0.1759,  0.0277,  0.2462,\n         -0.0345,  0.0571,  0.1664, -0.0195,  0.2489,  0.1592, -0.1525, -0.1570,\n         -0.0786,  0.0051, -0.1939,  0.1787, -0.1479,  0.0655],\n        [ 0.1442,  0.0910,  0.1667, -0.2092,  0.1590,  0.0980,  0.0373, -0.1829,\n         -0.1794,  0.0065,  0.1616,  0.1317, -0.1274, -0.0267,  0.0232, -0.1681,\n          0.1138, -0.2001,  0.1805,  0.0602, -0.0681, -0.1818, -0.0433,  0.0486,\n          0.1964,  0.0543, -0.2168,  0.2462,  0.2142,  0.0827],\n        [-0.2411,  0.2418, -0.0510,  0.0577, -0.2139, -0.1682,  0.0376,  0.1604,\n          0.1162,  0.1855,  0.1792,  0.1422, -0.1963,  0.2202,  0.2080,  0.0215,\n          0.1639,  0.1847,  0.0451,  0.1380,  0.2032, -0.1501, -0.1245, -0.0497,\n          0.0076, -0.0820,  0.2376, -0.1863,  0.1635, -0.0642],\n        [-0.2570,  0.0438,  0.1213, -0.1206, -0.2457,  0.0890, -0.0099, -0.0727,\n          0.1536, -0.0396, -0.1412,  0.1941,  0.0516,  0.2253, -0.0517, -0.2218,\n         -0.1893, -0.0215,  0.1485,  0.1790, -0.2094, -0.1441,  0.2512,  0.0268,\n          0.1428,  0.0358, -0.0980, -0.1470, -0.0332, -0.0464],\n        [-0.1180, -0.0724,  0.2259, -0.1994, -0.1109, -0.1964, -0.1962, -0.1521,\n          0.1200,  0.0542, -0.0671,  0.0955, -0.1598,  0.2502,  0.2543, -0.1898,\n         -0.0991, -0.1042, -0.1904,  0.2528,  0.0741, -0.0546,  0.1037,  0.1459,\n         -0.2350, -0.2068, -0.2033,  0.0795, -0.1133, -0.0582],\n        [ 0.0883,  0.1133,  0.2498, -0.0634, -0.2330, -0.0822,  0.1737, -0.1853,\n          0.0643, -0.0966, -0.1682,  0.1092, -0.0862, -0.0515, -0.1577, -0.0475,\n          0.0523, -0.2448, -0.1495,  0.0244, -0.0297, -0.1898,  0.1985,  0.0341,\n         -0.1616, -0.2124, -0.1727,  0.0718, -0.1424,  0.0869],\n        [ 0.1313,  0.0383,  0.1887,  0.2435, -0.1031, -0.1306, -0.0410,  0.0785,\n         -0.1988, -0.1084,  0.1987, -0.0007,  0.2431, -0.0140,  0.2314,  0.2571,\n          0.0223, -0.2328,  0.2382,  0.1834,  0.1190,  0.2473,  0.1673, -0.1128,\n         -0.1497, -0.1512, -0.1133, -0.1228,  0.0791,  0.2137]],\n       requires_grad=True)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_run.weight_ih_l1  # 参数全部都requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}