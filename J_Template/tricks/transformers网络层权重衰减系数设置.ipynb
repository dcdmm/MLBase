{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class Bert_base(torch.nn.Module):\n",
    "    \"\"\"Bert + Linear基础模型(transformer实现训练过程)\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model, num_class, dropout_ratio=0.2):\n",
    "        super().__init__()\n",
    "        self.pretrained = pretrained_model\n",
    "        self.hidden_size = pretrained_model.config.hidden_size\n",
    "        self.fc = torch.nn.Linear(self.hidden_size, num_class)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        model_output = self.pretrained(input_ids=input_ids,\n",
    "                                       attention_mask=attention_mask,\n",
    "                                       token_type_ids=token_type_ids)\n",
    "\n",
    "        # model_output.pooler_output.shape=[batch_size, self.hidden_size]\n",
    "        out = self.fc(self.dropout(model_output.pooler_output))  # 最后一个序列的信息\n",
    "        out = out.softmax(dim=1)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained.embeddings.word_embeddings.weight\n",
      "pretrained.embeddings.position_embeddings.weight\n",
      "pretrained.embeddings.token_type_embeddings.weight\n",
      "pretrained.embeddings.LayerNorm.weight\n",
      "pretrained.embeddings.LayerNorm.bias\n",
      "pretrained.encoder.layer.0.attention.self.query.weight\n",
      "pretrained.encoder.layer.0.attention.self.query.bias\n",
      "pretrained.encoder.layer.0.attention.self.key.weight\n",
      "pretrained.encoder.layer.0.attention.self.key.bias\n",
      "pretrained.encoder.layer.0.attention.self.value.weight\n",
      "pretrained.encoder.layer.0.attention.self.value.bias\n",
      "pretrained.encoder.layer.0.attention.output.dense.weight\n",
      "pretrained.encoder.layer.0.attention.output.dense.bias\n",
      "pretrained.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.0.intermediate.dense.weight\n",
      "pretrained.encoder.layer.0.intermediate.dense.bias\n",
      "pretrained.encoder.layer.0.output.dense.weight\n",
      "pretrained.encoder.layer.0.output.dense.bias\n",
      "pretrained.encoder.layer.0.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.0.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.1.attention.self.query.weight\n",
      "pretrained.encoder.layer.1.attention.self.query.bias\n",
      "pretrained.encoder.layer.1.attention.self.key.weight\n",
      "pretrained.encoder.layer.1.attention.self.key.bias\n",
      "pretrained.encoder.layer.1.attention.self.value.weight\n",
      "pretrained.encoder.layer.1.attention.self.value.bias\n",
      "pretrained.encoder.layer.1.attention.output.dense.weight\n",
      "pretrained.encoder.layer.1.attention.output.dense.bias\n",
      "pretrained.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.1.intermediate.dense.weight\n",
      "pretrained.encoder.layer.1.intermediate.dense.bias\n",
      "pretrained.encoder.layer.1.output.dense.weight\n",
      "pretrained.encoder.layer.1.output.dense.bias\n",
      "pretrained.encoder.layer.1.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.1.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.2.attention.self.query.weight\n",
      "pretrained.encoder.layer.2.attention.self.query.bias\n",
      "pretrained.encoder.layer.2.attention.self.key.weight\n",
      "pretrained.encoder.layer.2.attention.self.key.bias\n",
      "pretrained.encoder.layer.2.attention.self.value.weight\n",
      "pretrained.encoder.layer.2.attention.self.value.bias\n",
      "pretrained.encoder.layer.2.attention.output.dense.weight\n",
      "pretrained.encoder.layer.2.attention.output.dense.bias\n",
      "pretrained.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.2.intermediate.dense.weight\n",
      "pretrained.encoder.layer.2.intermediate.dense.bias\n",
      "pretrained.encoder.layer.2.output.dense.weight\n",
      "pretrained.encoder.layer.2.output.dense.bias\n",
      "pretrained.encoder.layer.2.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.2.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.3.attention.self.query.weight\n",
      "pretrained.encoder.layer.3.attention.self.query.bias\n",
      "pretrained.encoder.layer.3.attention.self.key.weight\n",
      "pretrained.encoder.layer.3.attention.self.key.bias\n",
      "pretrained.encoder.layer.3.attention.self.value.weight\n",
      "pretrained.encoder.layer.3.attention.self.value.bias\n",
      "pretrained.encoder.layer.3.attention.output.dense.weight\n",
      "pretrained.encoder.layer.3.attention.output.dense.bias\n",
      "pretrained.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.3.intermediate.dense.weight\n",
      "pretrained.encoder.layer.3.intermediate.dense.bias\n",
      "pretrained.encoder.layer.3.output.dense.weight\n",
      "pretrained.encoder.layer.3.output.dense.bias\n",
      "pretrained.encoder.layer.3.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.3.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.4.attention.self.query.weight\n",
      "pretrained.encoder.layer.4.attention.self.query.bias\n",
      "pretrained.encoder.layer.4.attention.self.key.weight\n",
      "pretrained.encoder.layer.4.attention.self.key.bias\n",
      "pretrained.encoder.layer.4.attention.self.value.weight\n",
      "pretrained.encoder.layer.4.attention.self.value.bias\n",
      "pretrained.encoder.layer.4.attention.output.dense.weight\n",
      "pretrained.encoder.layer.4.attention.output.dense.bias\n",
      "pretrained.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.4.intermediate.dense.weight\n",
      "pretrained.encoder.layer.4.intermediate.dense.bias\n",
      "pretrained.encoder.layer.4.output.dense.weight\n",
      "pretrained.encoder.layer.4.output.dense.bias\n",
      "pretrained.encoder.layer.4.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.4.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.5.attention.self.query.weight\n",
      "pretrained.encoder.layer.5.attention.self.query.bias\n",
      "pretrained.encoder.layer.5.attention.self.key.weight\n",
      "pretrained.encoder.layer.5.attention.self.key.bias\n",
      "pretrained.encoder.layer.5.attention.self.value.weight\n",
      "pretrained.encoder.layer.5.attention.self.value.bias\n",
      "pretrained.encoder.layer.5.attention.output.dense.weight\n",
      "pretrained.encoder.layer.5.attention.output.dense.bias\n",
      "pretrained.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.5.intermediate.dense.weight\n",
      "pretrained.encoder.layer.5.intermediate.dense.bias\n",
      "pretrained.encoder.layer.5.output.dense.weight\n",
      "pretrained.encoder.layer.5.output.dense.bias\n",
      "pretrained.encoder.layer.5.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.5.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.6.attention.self.query.weight\n",
      "pretrained.encoder.layer.6.attention.self.query.bias\n",
      "pretrained.encoder.layer.6.attention.self.key.weight\n",
      "pretrained.encoder.layer.6.attention.self.key.bias\n",
      "pretrained.encoder.layer.6.attention.self.value.weight\n",
      "pretrained.encoder.layer.6.attention.self.value.bias\n",
      "pretrained.encoder.layer.6.attention.output.dense.weight\n",
      "pretrained.encoder.layer.6.attention.output.dense.bias\n",
      "pretrained.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.6.intermediate.dense.weight\n",
      "pretrained.encoder.layer.6.intermediate.dense.bias\n",
      "pretrained.encoder.layer.6.output.dense.weight\n",
      "pretrained.encoder.layer.6.output.dense.bias\n",
      "pretrained.encoder.layer.6.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.6.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.7.attention.self.query.weight\n",
      "pretrained.encoder.layer.7.attention.self.query.bias\n",
      "pretrained.encoder.layer.7.attention.self.key.weight\n",
      "pretrained.encoder.layer.7.attention.self.key.bias\n",
      "pretrained.encoder.layer.7.attention.self.value.weight\n",
      "pretrained.encoder.layer.7.attention.self.value.bias\n",
      "pretrained.encoder.layer.7.attention.output.dense.weight\n",
      "pretrained.encoder.layer.7.attention.output.dense.bias\n",
      "pretrained.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.7.intermediate.dense.weight\n",
      "pretrained.encoder.layer.7.intermediate.dense.bias\n",
      "pretrained.encoder.layer.7.output.dense.weight\n",
      "pretrained.encoder.layer.7.output.dense.bias\n",
      "pretrained.encoder.layer.7.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.7.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.8.attention.self.query.weight\n",
      "pretrained.encoder.layer.8.attention.self.query.bias\n",
      "pretrained.encoder.layer.8.attention.self.key.weight\n",
      "pretrained.encoder.layer.8.attention.self.key.bias\n",
      "pretrained.encoder.layer.8.attention.self.value.weight\n",
      "pretrained.encoder.layer.8.attention.self.value.bias\n",
      "pretrained.encoder.layer.8.attention.output.dense.weight\n",
      "pretrained.encoder.layer.8.attention.output.dense.bias\n",
      "pretrained.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.8.intermediate.dense.weight\n",
      "pretrained.encoder.layer.8.intermediate.dense.bias\n",
      "pretrained.encoder.layer.8.output.dense.weight\n",
      "pretrained.encoder.layer.8.output.dense.bias\n",
      "pretrained.encoder.layer.8.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.8.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.9.attention.self.query.weight\n",
      "pretrained.encoder.layer.9.attention.self.query.bias\n",
      "pretrained.encoder.layer.9.attention.self.key.weight\n",
      "pretrained.encoder.layer.9.attention.self.key.bias\n",
      "pretrained.encoder.layer.9.attention.self.value.weight\n",
      "pretrained.encoder.layer.9.attention.self.value.bias\n",
      "pretrained.encoder.layer.9.attention.output.dense.weight\n",
      "pretrained.encoder.layer.9.attention.output.dense.bias\n",
      "pretrained.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.9.intermediate.dense.weight\n",
      "pretrained.encoder.layer.9.intermediate.dense.bias\n",
      "pretrained.encoder.layer.9.output.dense.weight\n",
      "pretrained.encoder.layer.9.output.dense.bias\n",
      "pretrained.encoder.layer.9.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.9.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.10.attention.self.query.weight\n",
      "pretrained.encoder.layer.10.attention.self.query.bias\n",
      "pretrained.encoder.layer.10.attention.self.key.weight\n",
      "pretrained.encoder.layer.10.attention.self.key.bias\n",
      "pretrained.encoder.layer.10.attention.self.value.weight\n",
      "pretrained.encoder.layer.10.attention.self.value.bias\n",
      "pretrained.encoder.layer.10.attention.output.dense.weight\n",
      "pretrained.encoder.layer.10.attention.output.dense.bias\n",
      "pretrained.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.10.intermediate.dense.weight\n",
      "pretrained.encoder.layer.10.intermediate.dense.bias\n",
      "pretrained.encoder.layer.10.output.dense.weight\n",
      "pretrained.encoder.layer.10.output.dense.bias\n",
      "pretrained.encoder.layer.10.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.10.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.11.attention.self.query.weight\n",
      "pretrained.encoder.layer.11.attention.self.query.bias\n",
      "pretrained.encoder.layer.11.attention.self.key.weight\n",
      "pretrained.encoder.layer.11.attention.self.key.bias\n",
      "pretrained.encoder.layer.11.attention.self.value.weight\n",
      "pretrained.encoder.layer.11.attention.self.value.bias\n",
      "pretrained.encoder.layer.11.attention.output.dense.weight\n",
      "pretrained.encoder.layer.11.attention.output.dense.bias\n",
      "pretrained.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "pretrained.encoder.layer.11.intermediate.dense.weight\n",
      "pretrained.encoder.layer.11.intermediate.dense.bias\n",
      "pretrained.encoder.layer.11.output.dense.weight\n",
      "pretrained.encoder.layer.11.output.dense.bias\n",
      "pretrained.encoder.layer.11.output.LayerNorm.weight\n",
      "pretrained.encoder.layer.11.output.LayerNorm.bias\n",
      "pretrained.pooler.dense.weight\n",
      "pretrained.pooler.dense.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "pretrained = AutoModel.from_pretrained('bert-base-uncased')\n",
    "model = Bert_base(pretrained, 2)\n",
    "for n, p in model.named_parameters():\n",
    "    print(n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def get_parameter_names(model, forbidden_layer_types):\n",
    "    \"\"\"\n",
    "    Returns the names of the model parameters that are not inside a forbidden layer.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for name, child in model.named_children():\n",
    "        result += [\n",
    "            f\"{name}.{n}\"\n",
    "            for n in get_parameter_names(child, forbidden_layer_types)\n",
    "            if not isinstance(child, tuple(forbidden_layer_types))\n",
    "        ]\n",
    "    # Add model specific parameters (defined with nn.Parameter) since they are not in any child.\n",
    "    result += list(model._parameters.keys())\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_parameters(opt_model, weight_decay=0.0):\n",
    "    decay_parameters = get_parameter_names(opt_model, [nn.LayerNorm])\n",
    "    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "    optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if n in decay_parameters],\n",
    "                    \"weight_decay\": weight_decay,  # 权重衰减系数\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if n not in decay_parameters],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ]\n",
    "    return optimizer_grouped_parameters, decay_parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "['pretrained.embeddings.word_embeddings.weight',\n 'pretrained.embeddings.position_embeddings.weight',\n 'pretrained.embeddings.token_type_embeddings.weight',\n 'pretrained.encoder.layer.0.attention.self.query.weight',\n 'pretrained.encoder.layer.0.attention.self.key.weight',\n 'pretrained.encoder.layer.0.attention.self.value.weight',\n 'pretrained.encoder.layer.0.attention.output.dense.weight',\n 'pretrained.encoder.layer.0.intermediate.dense.weight',\n 'pretrained.encoder.layer.0.output.dense.weight',\n 'pretrained.encoder.layer.1.attention.self.query.weight',\n 'pretrained.encoder.layer.1.attention.self.key.weight',\n 'pretrained.encoder.layer.1.attention.self.value.weight',\n 'pretrained.encoder.layer.1.attention.output.dense.weight',\n 'pretrained.encoder.layer.1.intermediate.dense.weight',\n 'pretrained.encoder.layer.1.output.dense.weight',\n 'pretrained.encoder.layer.2.attention.self.query.weight',\n 'pretrained.encoder.layer.2.attention.self.key.weight',\n 'pretrained.encoder.layer.2.attention.self.value.weight',\n 'pretrained.encoder.layer.2.attention.output.dense.weight',\n 'pretrained.encoder.layer.2.intermediate.dense.weight',\n 'pretrained.encoder.layer.2.output.dense.weight',\n 'pretrained.encoder.layer.3.attention.self.query.weight',\n 'pretrained.encoder.layer.3.attention.self.key.weight',\n 'pretrained.encoder.layer.3.attention.self.value.weight',\n 'pretrained.encoder.layer.3.attention.output.dense.weight',\n 'pretrained.encoder.layer.3.intermediate.dense.weight',\n 'pretrained.encoder.layer.3.output.dense.weight',\n 'pretrained.encoder.layer.4.attention.self.query.weight',\n 'pretrained.encoder.layer.4.attention.self.key.weight',\n 'pretrained.encoder.layer.4.attention.self.value.weight',\n 'pretrained.encoder.layer.4.attention.output.dense.weight',\n 'pretrained.encoder.layer.4.intermediate.dense.weight',\n 'pretrained.encoder.layer.4.output.dense.weight',\n 'pretrained.encoder.layer.5.attention.self.query.weight',\n 'pretrained.encoder.layer.5.attention.self.key.weight',\n 'pretrained.encoder.layer.5.attention.self.value.weight',\n 'pretrained.encoder.layer.5.attention.output.dense.weight',\n 'pretrained.encoder.layer.5.intermediate.dense.weight',\n 'pretrained.encoder.layer.5.output.dense.weight',\n 'pretrained.encoder.layer.6.attention.self.query.weight',\n 'pretrained.encoder.layer.6.attention.self.key.weight',\n 'pretrained.encoder.layer.6.attention.self.value.weight',\n 'pretrained.encoder.layer.6.attention.output.dense.weight',\n 'pretrained.encoder.layer.6.intermediate.dense.weight',\n 'pretrained.encoder.layer.6.output.dense.weight',\n 'pretrained.encoder.layer.7.attention.self.query.weight',\n 'pretrained.encoder.layer.7.attention.self.key.weight',\n 'pretrained.encoder.layer.7.attention.self.value.weight',\n 'pretrained.encoder.layer.7.attention.output.dense.weight',\n 'pretrained.encoder.layer.7.intermediate.dense.weight',\n 'pretrained.encoder.layer.7.output.dense.weight',\n 'pretrained.encoder.layer.8.attention.self.query.weight',\n 'pretrained.encoder.layer.8.attention.self.key.weight',\n 'pretrained.encoder.layer.8.attention.self.value.weight',\n 'pretrained.encoder.layer.8.attention.output.dense.weight',\n 'pretrained.encoder.layer.8.intermediate.dense.weight',\n 'pretrained.encoder.layer.8.output.dense.weight',\n 'pretrained.encoder.layer.9.attention.self.query.weight',\n 'pretrained.encoder.layer.9.attention.self.key.weight',\n 'pretrained.encoder.layer.9.attention.self.value.weight',\n 'pretrained.encoder.layer.9.attention.output.dense.weight',\n 'pretrained.encoder.layer.9.intermediate.dense.weight',\n 'pretrained.encoder.layer.9.output.dense.weight',\n 'pretrained.encoder.layer.10.attention.self.query.weight',\n 'pretrained.encoder.layer.10.attention.self.key.weight',\n 'pretrained.encoder.layer.10.attention.self.value.weight',\n 'pretrained.encoder.layer.10.attention.output.dense.weight',\n 'pretrained.encoder.layer.10.intermediate.dense.weight',\n 'pretrained.encoder.layer.10.output.dense.weight',\n 'pretrained.encoder.layer.11.attention.self.query.weight',\n 'pretrained.encoder.layer.11.attention.self.key.weight',\n 'pretrained.encoder.layer.11.attention.self.value.weight',\n 'pretrained.encoder.layer.11.attention.output.dense.weight',\n 'pretrained.encoder.layer.11.intermediate.dense.weight',\n 'pretrained.encoder.layer.11.output.dense.weight',\n 'pretrained.pooler.dense.weight',\n 'fc.weight']"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, dp = get_parameters(model)\n",
    "dp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}