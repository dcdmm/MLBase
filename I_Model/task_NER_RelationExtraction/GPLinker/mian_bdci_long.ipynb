{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa3c2ab-22ea-4eb8-aa00-67834dd2be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import copy\n",
    "\n",
    "from model import RawGlobalPointer, ERENet\n",
    "from utils import sparse_multilabel_categorical_crossentropy, MetricsCalculator_bdci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c58614-db6c-4826-ba07-ed60b054c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b1cd15-57b0-4fb3-a4a5-9fc7e98f233d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff540639-02de-4bf3-b393-e2ccde267a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': '部件故障', '1': '性能故障', '2': '检测工具', '3': '组成'}\n",
      "{'部件故障': 0, '性能故障': 1, '检测工具': 2, '组成': 3}\n"
     ]
    }
   ],
   "source": [
    "with open('datasets_bdci/rel2id.json', 'r', encoding='utf-8') as f:\n",
    "    id_to_rel, rel_to_id = json.load(f)\n",
    "    print(id_to_rel)\n",
    "    print(rel_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84bee069-3d7e-4df3-b091-480d8e43629f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '62号汽车故障报告综合情况:故障现象:加速后，丢开油门，发动机熄火。', 'spo_list': [('发动机', [28, 31], '熄火', [31, 33], '部件故障')]}\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    # example=[{'text': text0, 'spo_list': [(h_name00, hpos00, t_name00, tpos00, r00), (h_name01, hpos01, t_name01, tpos01, r01), xxxxxx]}, xxxxxx]\n",
    "    D = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line)\n",
    "            D.append({\n",
    "                \"text\": line[\"text\"],\n",
    "                \"spo_list\": [(spo['h']['name'], spo['h']['pos'], spo['t']['name'], spo['t']['pos'], spo['relation']) for\n",
    "                             spo in line[\"spo_list\"]]})\n",
    "        return D\n",
    "\n",
    "\n",
    "data = load_data('datasets_bdci/train_bdci.json')\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04fe119b-af01-44fe-9459-68640493db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, items):\n",
    "        self._items = items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._items)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self._items[index]\n",
    "\n",
    "        return {'text': item['text'],\n",
    "                'spo_list': item['spo_list']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "828046de-c9ef-48cd-b939-d2c410908680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='IDEA-CCNL/Erlangshen-DeBERTa-v2-320M-Chinese', vocab_size=12800, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at IDEA-CCNL/Erlangshen-DeBERTa-v2-320M-Chinese were not used when initializing DebertaV2Model: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319094784\n"
     ]
    }
   ],
   "source": [
    "tokenizer_fast = AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-320M-Chinese', use_fast=False)\n",
    "tokenizer_fast.add_tokens(new_tokens=['[SP]'])\n",
    "print(tokenizer_fast)\n",
    "\n",
    "pretrained = AutoModel.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-320M-Chinese')\n",
    "pretrained.resize_token_embeddings(len(tokenizer_fast))\n",
    "print(pretrained.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2621cba-696a-4e91-88eb-da70b901b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 971])\n",
      "torch.Size([2, 2, 11, 2])\n",
      "torch.Size([2, 4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "def padding_space(d):\n",
    "    \"\"\"将句子转换为字符列表,并将列表中的空格(' ')替换为'[SP]'\"\"\"\n",
    "    if d.find(' ') == -1:\n",
    "        return list(d)\n",
    "    else:\n",
    "        d_arr = np.array(list(d))\n",
    "        d_arr = np.where(d_arr == ' ', '[SP]', d_arr).tolist()\n",
    "        return d_arr\n",
    "\n",
    "\n",
    "def get_collate_fn(tokenizer, max_len=2048):\n",
    "    \"\"\"返回collate_fun函数(通过闭包函数引入形参)\"\"\"\n",
    "\n",
    "    def collate_fn(data):\n",
    "        batch_size = len(data)\n",
    "        texts = [padding_space(i['text']) for i in data]\n",
    "        spo_lists = [i['spo_list'] for i in data]\n",
    "\n",
    "        encoder_text = tokenizer(texts, padding=True, max_length=max_len, truncation=True, is_split_into_words=True,\n",
    "                                 return_tensors='pt')\n",
    "        input_ids, token_type_ids, attention_mask = encoder_text.values()\n",
    "\n",
    "        entity_labels, head_labels, tail_labels = [], [], []\n",
    "        for i in range(batch_size):\n",
    "            entity_labels_temp, head_labels_temp, tail_labels_temp = [[], []], [[], [], [], []], [[], [], [], []]\n",
    "            if spo_lists[i]:\n",
    "                for _, p_index, _, o_index, r in spo_lists[i]:\n",
    "                    entity_labels_temp[0].append((p_index[0], p_index[1] - 1))\n",
    "                    entity_labels_temp[1].append((o_index[0], o_index[1] - 1))\n",
    "                    head_labels_temp[rel_to_id[r]].append((p_index[0], o_index[0]))\n",
    "                    tail_labels_temp[rel_to_id[r]].append((p_index[1] - 1, o_index[1] - 1))\n",
    "            else:\n",
    "                # spo_lists为空列表时\n",
    "                entity_labels_temp[0].append((0, 0))\n",
    "                entity_labels_temp[1].append((0, 0))\n",
    "\n",
    "            _, _ = [i.append((0, 0)) for i in head_labels_temp if not i], [i.append((0, 0)) for i in tail_labels_temp if\n",
    "                                                                           not i]\n",
    "\n",
    "            entity_labels_temp = torch.transpose(torch.tensor(entity_labels_temp), 0, 1)\n",
    "            entity_labels.append(entity_labels_temp)\n",
    "\n",
    "            head_labels_temp = [torch.tensor(i) for i in head_labels_temp]\n",
    "            head_labels_temp = torch.transpose(pad_sequence(head_labels_temp, batch_first=True), 0, 1)\n",
    "            head_labels.append(head_labels_temp)\n",
    "            tail_labels_temp = [torch.tensor(i) for i in tail_labels_temp]\n",
    "            tail_labels_temp = torch.transpose(pad_sequence(tail_labels_temp, batch_first=True), 0, 1)\n",
    "            tail_labels.append(tail_labels_temp)\n",
    "\n",
    "        entity_labels = torch.transpose(pad_sequence(entity_labels, batch_first=True), 1, 2)\n",
    "        head_labels = torch.transpose(pad_sequence(head_labels, batch_first=True), 1, 2)\n",
    "        tail_labels = torch.transpose(pad_sequence(tail_labels, batch_first=True), 1, 2)\n",
    "        return input_ids, attention_mask, token_type_ids, entity_labels, head_labels, tail_labels, texts, spo_lists\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "train_loader = DataLoader(CustomDataset(items=data), batch_size=2, shuffle=True,\n",
    "                          collate_fn=get_collate_fn(tokenizer_fast))\n",
    "for i in train_loader:\n",
    "    print(i[0].shape)\n",
    "    print(i[3].shape)\n",
    "    print(i[4].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4dde6a4-3558-460b-821f-68a5b1b85fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = pretrained.config.hidden_size\n",
    "mention_detect = RawGlobalPointer(hidden_size, 2, 64).to(device)  # 不提取实体类型(只识别subject、object对应的实体)\n",
    "s_o_head = RawGlobalPointer(hidden_size, len(id_to_rel), 64, RoPE=False, tril_mask=False).to(\n",
    "    device)  # 不需要设置tril_mask=False\n",
    "s_o_tail = RawGlobalPointer(hidden_size, len(id_to_rel), 64, RoPE=False, tril_mask=False).to(\n",
    "    device)  # 不需要设置tril_mask=False\n",
    "net = ERENet(copy.deepcopy(pretrained), mention_detect, s_o_head, s_o_tail).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "015e1746-f036-457f-8ec2-5c77ea069626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spoes(logits1, logits2, logits3, texts, id2predicate):\n",
    "    logits1 = logits1.data.cpu().numpy()\n",
    "    logits2 = logits2.data.cpu().numpy()\n",
    "    logits3 = logits3.data.cpu().numpy()\n",
    "    batch_size = logits1.shape[0]\n",
    "\n",
    "    # 序列开头与结尾特殊token('[CLS]', '[SEP]')处元素设置为无穷小\n",
    "    logits1[:, :, [0, -1]] -= np.inf\n",
    "    logits1[:, :, :, [0, -1]] -= np.inf\n",
    "    subjects, objects = [[] for _ in range(batch_size)], [[] for _ in range(batch_size)]\n",
    "    for b, l, h, t in zip(*np.where(logits1 > 0.0)):  # 阈值(threshold)设置为0.0\n",
    "        if l == 0:  # 不提取实体类型(只识别subject、objects对应的实体)\n",
    "            subjects[b].append((int(h), int(t)))\n",
    "        else:\n",
    "            objects[b].append((int(h), int(t)))\n",
    "\n",
    "    spoes = [[] for _ in range(batch_size)]\n",
    "    for b in range(batch_size):\n",
    "        text_b = np.array(texts[b])\n",
    "        text_b = np.where(text_b == '[SP]', ' ', text_b).tolist()  # 重新恢复为' '\n",
    "        # 计算subjects[b]与objects[b]所有可能关系的笛卡尔组合\n",
    "        # 解析:subjects[b]:((s00, s01), (s10, s11), (s20, s21), (s30, s31)), objects[b]:((o00, o01), (o10, o11), (o20, o21))\n",
    "        # 无法确定(s00, s01)必定对应(o00, o01),可能(o00, o01)前有未被解析;也有可能(s10, s11)才是subjects[b]的第一个实体,(s00, s01)为解析错误\n",
    "        # 故仍采用笛卡尔组合的形式\n",
    "        for sh, st in subjects[b]:\n",
    "            for oh, ot in objects[b]:\n",
    "                p1s = np.where(logits2[b, :, sh, oh] > 0.0)[0]  # 阈值(threshold)设置为0.0\n",
    "                p2s = np.where(logits3[b, :, st, ot] > 0.0)[0]  # 阈值(threshold)设置为0.0\n",
    "                ps = set(p1s) & set(p2s)\n",
    "                for p in ps:\n",
    "                    sht_str = ''.join(text_b[sh: st + 1])\n",
    "                    oht_str = ''.join(text_b[oh: ot + 1])\n",
    "                    spoes[b].append((sht_str, (sh, st + 1), oht_str, (oh, ot + 1),\n",
    "                                     id2predicate[str(p)]))  # 添加预测结果:(h_name, hpos, t_name, tpos, r)\n",
    "    return spoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6789da71-a1dd-420b-b03e-bd59546836f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(model, dataloader, optimizer, id2predicate, device):\n",
    "    model.train()\n",
    "\n",
    "    for idx, (\n",
    "            input_ids, attention_mask, token_type_ids, entity_labels, head_labels, tail_labels, texts,\n",
    "            spo_lists) in enumerate(\n",
    "        dataloader, start=1):\n",
    "        # 数据设备切换\n",
    "        # input_ids.shape=[batch_size, seq_len]\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        entity_labels = entity_labels.to(device)\n",
    "        head_labels = head_labels.to(device)\n",
    "        tail_labels = tail_labels.to(device)\n",
    "\n",
    "        # logits1.shape=[batch_size, 2, seq_len, seq_len]\n",
    "        # logits2.shape=[batch_size, len(schema) seq_len, seq_len]\n",
    "        # logits3.shape=[batch_size, len(schema), seq_len, seq_len]\n",
    "        logits1, logits2, logits3 = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        loss1 = sparse_multilabel_categorical_crossentropy(y_true=entity_labels, y_pred=logits1)\n",
    "        loss2 = sparse_multilabel_categorical_crossentropy(y_true=head_labels, y_pred=logits2)\n",
    "        loss3 = sparse_multilabel_categorical_crossentropy(y_true=tail_labels, y_pred=logits3)\n",
    "        loss = sum([loss1, loss2, loss3]) / 3  # entities和relations之间的信息共享和交互\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            y_pred = extract_spoes(logits1, logits2, logits3, texts, id2predicate)\n",
    "            mc = MetricsCalculator_bdci()  # 计算查准率、查全率、F1 score \n",
    "            mc.calc_confusion_matrix(y_pred, spo_lists)\n",
    "            print('| step {:5d} | loss {:9.5f} | precision {:8.5f} | recall {:8.5f} | f1 {:8.5f} |'.format(idx,\n",
    "                                                                                                           loss.item(),\n",
    "                                                                                                           mc.precision,\n",
    "                                                                                                           mc.recall,\n",
    "                                                                                                           mc.f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e179acaf-b292-42b7-90d8-7fbb9d6349e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------0--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/env_3812/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / torch.tensor(\n",
      "/root/miniconda3/envs/env_3812/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:829: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += c2p_att / torch.tensor(scale, dtype=c2p_att.dtype)\n",
      "/root/miniconda3/envs/env_3812/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:852: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step   100 | loss  22.03539 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   200 | loss  22.05331 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   300 | loss  11.94513 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   400 | loss  -2.82410 | precision  1.00000 | recall  0.66667 | f1  0.80000 |\n",
      "| step   500 | loss  28.19783 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   600 | loss   4.90600 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   700 | loss  17.45040 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "--------------------------------------------------1--------------------------------------------------\n",
      "| step   100 | loss   4.86326 | precision  0.40000 | recall  0.28571 | f1  0.33333 |\n",
      "| step   200 | loss   3.64981 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   300 | loss  17.02134 | precision  0.22222 | recall  0.18182 | f1  0.20000 |\n",
      "| step   400 | loss  14.05476 | precision  0.50000 | recall  0.16667 | f1  0.25000 |\n",
      "| step   500 | loss   7.90815 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   600 | loss  14.68390 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   700 | loss  -1.71729 | precision  1.00000 | recall  0.28571 | f1  0.44444 |\n",
      "--------------------------------------------------2--------------------------------------------------\n",
      "| step   100 | loss   0.79103 | precision  0.20000 | recall  0.12500 | f1  0.15385 |\n",
      "| step   200 | loss   2.96917 | precision  0.18182 | recall  0.36364 | f1  0.24242 |\n",
      "| step   300 | loss  13.29199 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   400 | loss  13.02540 | precision  1.00000 | recall  0.20000 | f1  0.33333 |\n",
      "| step   500 | loss   5.65643 | precision  0.25000 | recall  0.10000 | f1  0.14286 |\n",
      "| step   600 | loss  -2.31976 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   700 | loss  -1.70474 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "--------------------------------------------------3--------------------------------------------------\n",
      "| step   100 | loss   2.81968 | precision  0.50000 | recall  0.50000 | f1  0.50000 |\n",
      "| step   200 | loss   4.98289 | precision  1.00000 | recall  0.40000 | f1  0.57143 |\n",
      "| step   300 | loss   2.29770 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   400 | loss   5.19214 | precision  0.60000 | recall  0.75000 | f1  0.66667 |\n",
      "| step   500 | loss   8.27075 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   600 | loss  -5.92627 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   700 | loss  19.38801 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "--------------------------------------------------4--------------------------------------------------\n",
      "| step   100 | loss   4.36903 | precision  1.00000 | recall  0.09091 | f1  0.16667 |\n",
      "| step   200 | loss  10.45065 | precision  1.00000 | recall  0.09091 | f1  0.16667 |\n",
      "| step   300 | loss  -4.31743 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   400 | loss   3.18070 | precision  1.00000 | recall  0.33333 | f1  0.50000 |\n",
      "| step   500 | loss  15.60828 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   600 | loss   3.64153 | precision  0.33333 | recall  0.14286 | f1  0.20000 |\n",
      "| step   700 | loss   1.83661 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "--------------------------------------------------5--------------------------------------------------\n",
      "| step   100 | loss   0.64133 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss   2.79529 | precision  0.80000 | recall  0.50000 | f1  0.61538 |\n",
      "| step   300 | loss  11.19973 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   400 | loss   4.52155 | precision  1.00000 | recall  0.40000 | f1  0.57143 |\n",
      "| step   500 | loss   0.93033 | precision  0.66667 | recall  0.20000 | f1  0.30769 |\n",
      "| step   600 | loss   5.02198 | precision  1.00000 | recall  0.20000 | f1  0.33333 |\n",
      "| step   700 | loss  12.25590 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "--------------------------------------------------6--------------------------------------------------\n",
      "| step   100 | loss   6.76471 | precision  1.00000 | recall  0.08333 | f1  0.15385 |\n",
      "| step   200 | loss  -2.07144 | precision  0.66667 | recall  0.33333 | f1  0.44444 |\n",
      "| step   300 | loss   3.77773 | precision  1.00000 | recall  0.60000 | f1  0.75000 |\n",
      "| step   400 | loss   2.55574 | precision  0.80000 | recall  0.30769 | f1  0.44444 |\n",
      "| step   500 | loss   0.54191 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   600 | loss   4.61886 | precision  1.00000 | recall  0.11111 | f1  0.20000 |\n",
      "| step   700 | loss  27.13734 | precision  0.33333 | recall  0.09091 | f1  0.14286 |\n",
      "--------------------------------------------------7--------------------------------------------------\n",
      "| step   100 | loss   4.28311 | precision  0.41176 | recall  0.50000 | f1  0.45161 |\n",
      "| step   200 | loss  11.99741 | precision  0.50000 | recall  0.40000 | f1  0.44444 |\n",
      "| step   300 | loss  -2.48705 | precision  1.00000 | recall  0.80000 | f1  0.88889 |\n",
      "| step   400 | loss   7.16336 | precision  1.00000 | recall  0.09091 | f1  0.16667 |\n",
      "| step   500 | loss   6.72755 | precision  0.66667 | recall  0.50000 | f1  0.57143 |\n",
      "| step   600 | loss   4.08305 | precision  1.00000 | recall  0.11111 | f1  0.20000 |\n",
      "| step   700 | loss  -5.19883 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "--------------------------------------------------8--------------------------------------------------\n",
      "| step   100 | loss  -2.73167 | precision  1.00000 | recall  0.42857 | f1  0.60000 |\n",
      "| step   200 | loss   3.82495 | precision  1.00000 | recall  0.75000 | f1  0.85714 |\n",
      "| step   300 | loss   3.07367 | precision  1.00000 | recall  0.66667 | f1  0.80000 |\n",
      "| step   400 | loss   2.05182 | precision  1.00000 | recall  0.66667 | f1  0.80000 |\n",
      "| step   500 | loss   6.87869 | precision  1.00000 | recall  0.11111 | f1  0.20000 |\n",
      "| step   600 | loss   1.46532 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   700 | loss   9.88894 | precision  0.66667 | recall  0.66667 | f1  0.66667 |\n",
      "--------------------------------------------------9--------------------------------------------------\n",
      "| step   100 | loss   3.39504 | precision  1.00000 | recall  0.85714 | f1  0.92308 |\n",
      "| step   200 | loss   1.60326 | precision  1.00000 | recall  0.80000 | f1  0.88889 |\n",
      "| step   300 | loss  13.25039 | precision  0.50000 | recall  0.08571 | f1  0.14634 |\n",
      "| step   400 | loss  -3.73399 | precision  1.00000 | recall  0.75000 | f1  0.85714 |\n",
      "| step   500 | loss  -1.24370 | precision  0.80000 | recall  0.72727 | f1  0.76190 |\n",
      "| step   600 | loss  26.60623 | precision  0.54545 | recall  0.24000 | f1  0.33333 |\n",
      "| step   700 | loss  -3.82486 | precision  1.00000 | recall  0.60000 | f1  0.75000 |\n",
      "--------------------------------------------------10--------------------------------------------------\n",
      "| step   100 | loss  -0.32598 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   200 | loss   1.74669 | precision  0.88889 | recall  0.57143 | f1  0.69565 |\n",
      "| step   300 | loss  -1.48006 | precision  1.00000 | recall  0.60000 | f1  0.75000 |\n",
      "| step   400 | loss   0.46329 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   500 | loss  -0.03670 | precision  1.00000 | recall  0.18182 | f1  0.30769 |\n",
      "| step   600 | loss   4.61766 | precision  1.00000 | recall  0.66667 | f1  0.80000 |\n",
      "| step   700 | loss  -7.69420 | precision  1.00000 | recall  0.66667 | f1  0.80000 |\n",
      "--------------------------------------------------11--------------------------------------------------\n",
      "| step   100 | loss  -4.54772 | precision  0.75000 | recall  1.00000 | f1  0.85714 |\n",
      "| step   200 | loss -10.71643 | precision  1.00000 | recall  0.90909 | f1  0.95238 |\n",
      "| step   300 | loss   0.09723 | precision  1.00000 | recall  0.66667 | f1  0.80000 |\n",
      "| step   400 | loss   0.14785 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   500 | loss   0.34875 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   600 | loss   0.07222 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   700 | loss   0.03240 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "--------------------------------------------------12--------------------------------------------------\n",
      "| step   100 | loss  -2.70020 | precision  1.00000 | recall  0.09091 | f1  0.16667 |\n",
      "| step   200 | loss   6.80583 | precision  1.00000 | recall  0.33333 | f1  0.50000 |\n",
      "| step   300 | loss   2.93651 | precision  0.66667 | recall  0.66667 | f1  0.66667 |\n",
      "| step   400 | loss  12.33932 | precision  1.00000 | recall  0.47059 | f1  0.64000 |\n",
      "| step   500 | loss   0.35131 | precision  1.00000 | recall  0.61538 | f1  0.76190 |\n",
      "| step   600 | loss   7.53639 | precision  0.85714 | recall  0.40000 | f1  0.54545 |\n",
      "| step   700 | loss  -5.05702 | precision  0.92308 | recall  0.50000 | f1  0.64865 |\n",
      "--------------------------------------------------13--------------------------------------------------\n",
      "| step   100 | loss   8.74772 | precision  0.75000 | recall  0.09375 | f1  0.16667 |\n",
      "| step   200 | loss   4.20880 | precision  0.83333 | recall  0.83333 | f1  0.83333 |\n",
      "| step   300 | loss  -2.66631 | precision  0.88889 | recall  0.53333 | f1  0.66667 |\n",
      "| step   400 | loss   0.70909 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   500 | loss   1.13104 | precision  0.90000 | recall  0.36000 | f1  0.51429 |\n",
      "| step   600 | loss   1.68929 | precision  0.75000 | recall  1.00000 | f1  0.85714 |\n",
      "| step   700 | loss   0.37243 | precision  1.00000 | recall  0.66667 | f1  0.80000 |\n",
      "--------------------------------------------------14--------------------------------------------------\n",
      "| step   100 | loss  -7.72718 | precision  1.00000 | recall  0.61538 | f1  0.76190 |\n",
      "| step   200 | loss  19.78253 | precision  0.40000 | recall  0.28571 | f1  0.33333 |\n",
      "| step   300 | loss   3.96542 | precision  0.83333 | recall  1.00000 | f1  0.90909 |\n",
      "| step   400 | loss  13.74627 | precision  1.00000 | recall  0.37500 | f1  0.54545 |\n",
      "| step   500 | loss  10.43531 | precision  1.00000 | recall  0.60000 | f1  0.75000 |\n",
      "| step   600 | loss  -0.61932 | precision  0.66667 | recall  0.66667 | f1  0.66667 |\n",
      "| step   700 | loss  -5.49106 | precision  0.87500 | recall  0.43750 | f1  0.58333 |\n",
      "--------------------------------------------------15--------------------------------------------------\n",
      "| step   100 | loss   2.51653 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss  -3.42285 | precision  1.00000 | recall  0.81818 | f1  0.90000 |\n",
      "| step   300 | loss   4.14832 | precision  1.00000 | recall  0.58333 | f1  0.73684 |\n",
      "| step   400 | loss   0.82579 | precision  1.00000 | recall  0.50000 | f1  0.66667 |\n",
      "| step   500 | loss  -5.69282 | precision  0.87500 | recall  0.58333 | f1  0.70000 |\n",
      "| step   600 | loss   4.74273 | precision  1.00000 | recall  0.50000 | f1  0.66667 |\n",
      "| step   700 | loss   3.67351 | precision  1.00000 | recall  0.55556 | f1  0.71429 |\n",
      "--------------------------------------------------16--------------------------------------------------\n",
      "| step   100 | loss  -6.36100 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss   0.72095 | precision  1.00000 | recall  0.75000 | f1  0.85714 |\n",
      "| step   300 | loss   3.20418 | precision  0.80000 | recall  0.80000 | f1  0.80000 |\n",
      "| step   400 | loss   8.34971 | precision  0.76471 | recall  0.65000 | f1  0.70270 |\n",
      "| step   500 | loss   0.74948 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   600 | loss  -9.52373 | precision  1.00000 | recall  0.80000 | f1  0.88889 |\n",
      "| step   700 | loss   0.46042 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "--------------------------------------------------17--------------------------------------------------\n",
      "| step   100 | loss   4.69273 | precision  0.50000 | recall  0.50000 | f1  0.50000 |\n",
      "| step   200 | loss   1.62600 | precision  1.00000 | recall  0.50000 | f1  0.66667 |\n",
      "| step   300 | loss   3.42848 | precision  0.66667 | recall  1.00000 | f1  0.80000 |\n",
      "| step   400 | loss  -0.65930 | precision  1.00000 | recall  0.63636 | f1  0.77778 |\n",
      "| step   500 | loss   0.07076 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   600 | loss  -3.50620 | precision  1.00000 | recall  0.75000 | f1  0.85714 |\n",
      "| step   700 | loss  -3.74730 | precision  1.00000 | recall  0.88889 | f1  0.94118 |\n",
      "--------------------------------------------------18--------------------------------------------------\n",
      "| step   100 | loss   0.19562 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss   2.13306 | precision  1.00000 | recall  0.90000 | f1  0.94737 |\n",
      "| step   300 | loss  -6.24973 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   400 | loss   4.97651 | precision  0.87500 | recall  0.87500 | f1  0.87500 |\n",
      "| step   500 | loss   0.66808 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   600 | loss  -3.89046 | precision  0.83333 | recall  0.83333 | f1  0.83333 |\n",
      "| step   700 | loss  -0.79918 | precision  1.00000 | recall  0.60000 | f1  0.75000 |\n",
      "--------------------------------------------------19--------------------------------------------------\n",
      "| step   100 | loss   0.73245 | precision  1.00000 | recall  0.66667 | f1  0.80000 |\n",
      "| step   200 | loss  -5.14998 | precision  1.00000 | recall  0.75000 | f1  0.85714 |\n",
      "| step   300 | loss   0.81174 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   400 | loss   0.41734 | precision  1.00000 | recall  0.66667 | f1  0.80000 |\n",
      "| step   500 | loss   0.05321 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   600 | loss   0.57198 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   700 | loss  -4.96608 | precision  1.00000 | recall  0.50000 | f1  0.66667 |\n",
      "--------------------------------------------------20--------------------------------------------------\n",
      "| step   100 | loss   0.54844 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss  -6.69243 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   300 | loss -12.06088 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   400 | loss   8.35680 | precision  1.00000 | recall  0.75000 | f1  0.85714 |\n",
      "| step   500 | loss -11.36814 | precision  1.00000 | recall  0.64706 | f1  0.78571 |\n",
      "| step   600 | loss  -0.15485 | precision  0.91667 | recall  0.91667 | f1  0.91667 |\n",
      "| step   700 | loss  -8.63433 | precision  0.85714 | recall  0.31579 | f1  0.46154 |\n",
      "--------------------------------------------------21--------------------------------------------------\n",
      "| step   100 | loss  -4.72177 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss   0.06119 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   300 | loss   0.19596 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   400 | loss   0.26591 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   500 | loss  11.28254 | precision  0.80000 | recall  0.57143 | f1  0.66667 |\n",
      "| step   600 | loss   5.06121 | precision  1.00000 | recall  0.87500 | f1  0.93333 |\n",
      "| step   700 | loss  -4.57205 | precision  1.00000 | recall  0.42857 | f1  0.60000 |\n",
      "--------------------------------------------------22--------------------------------------------------\n",
      "| step   100 | loss -14.22106 | precision  0.88889 | recall  0.88889 | f1  0.88889 |\n",
      "| step   200 | loss   0.88464 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   300 | loss   0.03590 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   400 | loss   2.53113 | precision  1.00000 | recall  0.44444 | f1  0.61538 |\n",
      "| step   500 | loss -15.69595 | precision  1.00000 | recall  0.38462 | f1  0.55556 |\n",
      "| step   600 | loss  -6.92573 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   700 | loss  -4.77970 | precision  1.00000 | recall  0.87500 | f1  0.93333 |\n",
      "--------------------------------------------------23--------------------------------------------------\n",
      "| step   100 | loss   0.39284 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss   0.45951 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   300 | loss  -0.72473 | precision  0.50000 | recall  0.60000 | f1  0.54545 |\n",
      "| step   400 | loss  -5.37033 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   500 | loss  -2.92994 | precision  1.00000 | recall  0.50000 | f1  0.66667 |\n",
      "| step   600 | loss  -8.72980 | precision  1.00000 | recall  0.83333 | f1  0.90909 |\n",
      "| step   700 | loss  -3.81626 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "--------------------------------------------------24--------------------------------------------------\n",
      "| step   100 | loss   1.96987 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss  -2.79596 | precision  1.00000 | recall  0.84615 | f1  0.91667 |\n",
      "| step   300 | loss   0.07971 | precision  0.87500 | recall  0.87500 | f1  0.87500 |\n",
      "| step   400 | loss   0.05995 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   500 | loss   4.17972 | precision  1.00000 | recall  0.75000 | f1  0.85714 |\n",
      "| step   600 | loss  -6.26223 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   700 | loss   9.88168 | precision  1.00000 | recall  0.60000 | f1  0.75000 |\n",
      "--------------------------------------------------25--------------------------------------------------\n",
      "| step   100 | loss   0.22677 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss   3.12606 | precision  0.90909 | recall  0.90909 | f1  0.90909 |\n",
      "| step   300 | loss   4.43914 | precision  0.90000 | recall  0.90000 | f1  0.90000 |\n",
      "| step   400 | loss   0.04685 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   500 | loss  -0.00768 | precision  1.00000 | recall  0.89474 | f1  0.94444 |\n",
      "| step   600 | loss   0.06334 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   700 | loss   0.05598 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "--------------------------------------------------26--------------------------------------------------\n",
      "| step   100 | loss  -5.00182 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss   7.52265 | precision  0.14286 | recall  0.25000 | f1  0.18182 |\n",
      "| step   300 | loss  -0.16690 | precision  0.88889 | recall  0.66667 | f1  0.76190 |\n",
      "| step   400 | loss  -2.24641 | precision  0.93333 | recall  1.00000 | f1  0.96552 |\n",
      "| step   500 | loss -10.51972 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   600 | loss -17.66946 | precision  0.84615 | recall  0.68750 | f1  0.75862 |\n",
      "| step   700 | loss   0.78614 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "--------------------------------------------------27--------------------------------------------------\n",
      "| step   100 | loss   0.03205 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss  -6.51982 | precision  1.00000 | recall  0.53846 | f1  0.70000 |\n",
      "| step   300 | loss   5.53658 | precision  0.85714 | recall  0.85714 | f1  0.85714 |\n",
      "| step   400 | loss  -9.43557 | precision  1.00000 | recall  0.77778 | f1  0.87500 |\n",
      "| step   500 | loss   0.47944 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   600 | loss   0.10425 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   700 | loss  -2.67530 | precision  0.83333 | recall  0.83333 | f1  0.83333 |\n",
      "--------------------------------------------------28--------------------------------------------------\n",
      "| step   100 | loss   0.30111 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss -12.53873 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   300 | loss   0.13987 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   400 | loss   0.95368 | precision  0.75000 | recall  0.75000 | f1  0.75000 |\n",
      "| step   500 | loss  -6.13390 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   600 | loss   1.64446 | precision  1.00000 | recall  0.83333 | f1  0.90909 |\n",
      "| step   700 | loss -13.19946 | precision  0.93333 | recall  0.66667 | f1  0.77778 |\n",
      "--------------------------------------------------29--------------------------------------------------\n",
      "| step   100 | loss   1.34070 | precision  1.00000 | recall  0.75000 | f1  0.85714 |\n",
      "| step   200 | loss  -2.97326 | precision  0.86957 | recall  0.95238 | f1  0.90909 |\n",
      "| step   300 | loss   0.27830 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   400 | loss   0.39395 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   500 | loss   0.03885 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   600 | loss -13.44295 | precision  0.82609 | recall  0.79167 | f1  0.80851 |\n",
      "| step   700 | loss  -4.59902 | precision  0.85714 | recall  1.00000 | f1  0.92308 |\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    print('-' * 50 + str(epoch) + '-' * 50)\n",
    "    train(net, train_loader, optimizer, id_to_rel, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da3b6602-4fe0-4f8c-8eaa-f1a90dd4c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测\n",
    "def predict(model, filename, tokenizer, id2predicate, device):\n",
    "    model.eval()\n",
    "\n",
    "    predict_list = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line)  # 每次预测一条数据\n",
    "\n",
    "            ID, text = line[\"ID\"], line[\"text\"]\n",
    "            texts = [padding_space(text)]\n",
    "\n",
    "            temp_dict = {}\n",
    "            temp_dict[\"ID\"] = ID\n",
    "            temp_dict[\"text\"] = text\n",
    "\n",
    "            text_encode = tokenizer(texts, is_split_into_words=True, return_tensors='pt')\n",
    "            input_ids = text_encode['input_ids'].to(device)\n",
    "            attention_mask = text_encode['attention_mask'].to(device)\n",
    "            token_type_ids = text_encode['attention_mask'].to(device)\n",
    "\n",
    "            logits1, logits2, logits3 = model(input_ids, attention_mask, token_type_ids)\n",
    "            y_pred = extract_spoes(logits1, logits2, logits3, texts, id2predicate)\n",
    "\n",
    "            temp_spo_list = []\n",
    "            for i in y_pred[0]:\n",
    "                temp_spo = {\"h\": {\"name\": i[0], \"pos\": list(i[1])},\n",
    "                            \"t\": {\"name\": i[2], \"pos\": list(i[3])},\n",
    "                            \"relation\": i[-1]}\n",
    "                temp_spo_list.append(temp_spo)\n",
    "\n",
    "            temp_dict[\"spo_list\"] = temp_spo_list\n",
    "            predict_list.append(temp_dict)\n",
    "    return predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35f8a2da-9af9-4123-af52-42040f33362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result = predict(net, 'datasets_bdci/evalA.json', tokenizer_fast, id_to_rel, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baf613a3-49a4-4364-a3ea-b3957332f72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 'AE0001',\n",
       " 'text': '三、故障排除(1)发生电缆分支箱带电后,先断开电缆分支箱电源。(2)及时汇报上级部门,并由95598抢修平台发布停电信息。(3)分析故障原因。一般情况下设备外壳带电是由于相线接地、零线断线或三相负荷不平衡和接地电阻不符合要求引起。应先分析用户侧用电使用情况,如用户侧用电正常,则说明相线不完全接地或三相负荷不平衡和接地电阻不符合要求引起,如用户侧用电不正常,则说明相线完全接地或零线断线等。(4)根据分析结果制定查找故障点方法,正确填写配电故障紧急抢修单,选择查找故障所需的工器具和材料。(5)查找故障点时为确保作业人身安全,操作时应按带电作业的安全要求进行。(6)查找故障点。首先确认该电缆分支箱已停电,出线负荷已断开;检查电缆分支箱内的设备(导线)与分支箱外壳有无明显导通,可用万用表欧姆挡测量各相与分支箱外壳是否导通;再检查零线连接是否牢固或有无断线;最后用接地电阻摇表测量接地电阻是否合格,确定故障点位置。(7)落实安全组织措施后,工作许可人应做好线路停电、验电、挂设接地线、悬挂标志牌等安全技术措施,并向工作负责人办理许可手续。(8)施工前工作负责人向全体工作班成员进行\"三交三查\",班组人员确认签名。(9)故障点处理。使用合格的工器具,做好防触电等安全措施,正确处理故障点。(10)工作负责人对施工质量进行验收,并符合设计要求。(11)拆除现场安全围栏等设施,收回工器具、材料并清理现场,工作负责人召开站班会,组织抢修人员撤离作业现场。(12)工作负责人向工作许可人汇报工作结束,工作许可人拆除所有安全措施后,按操作步骤进行送电。(13)工作总结,并由 95598 抢修平台发布送电信息。',\n",
       " 'spo_list': [{'h': {'name': '电缆分支箱', 'pos': [11, 16]},\n",
       "   't': {'name': '带电', 'pos': [16, 18]},\n",
       "   'relation': '部件故障'},\n",
       "  {'h': {'name': '电缆分支箱', 'pos': [11, 16]},\n",
       "   't': {'name': '断线', 'pos': [92, 94]},\n",
       "   'relation': '部件故障'},\n",
       "  {'h': {'name': '相线', 'pos': [85, 87]},\n",
       "   't': {'name': '接地', 'pos': [87, 89]},\n",
       "   'relation': '部件故障'},\n",
       "  {'h': {'name': '零线', 'pos': [90, 92]},\n",
       "   't': {'name': '带电', 'pos': [16, 18]},\n",
       "   'relation': '部件故障'},\n",
       "  {'h': {'name': '零线', 'pos': [90, 92]},\n",
       "   't': {'name': '断线', 'pos': [92, 94]},\n",
       "   'relation': '部件故障'},\n",
       "  {'h': {'name': '三相', 'pos': [95, 97]},\n",
       "   't': {'name': '负荷不平衡', 'pos': [97, 102]},\n",
       "   'relation': '部件故障'},\n",
       "  {'h': {'name': '接地电阻', 'pos': [103, 107]},\n",
       "   't': {'name': '不符合要求', 'pos': [107, 112]},\n",
       "   'relation': '性能故障'},\n",
       "  {'h': {'name': '相线', 'pos': [141, 143]},\n",
       "   't': {'name': '不完全接地', 'pos': [143, 148]},\n",
       "   'relation': '部件故障'},\n",
       "  {'h': {'name': '三相', 'pos': [149, 151]},\n",
       "   't': {'name': '负荷不平衡', 'pos': [151, 156]},\n",
       "   'relation': '部件故障'},\n",
       "  {'h': {'name': '接地电阻', 'pos': [157, 161]},\n",
       "   't': {'name': '不符合要求', 'pos': [161, 166]},\n",
       "   'relation': '性能故障'},\n",
       "  {'h': {'name': '接地电阻', 'pos': [157, 161]},\n",
       "   't': {'name': '断开', 'pos': [308, 310]},\n",
       "   'relation': '性能故障'},\n",
       "  {'h': {'name': '相线', 'pos': [182, 184]},\n",
       "   't': {'name': '接地', 'pos': [186, 188]},\n",
       "   'relation': '部件故障'},\n",
       "  {'h': {'name': '零线', 'pos': [189, 191]},\n",
       "   't': {'name': '断线', 'pos': [191, 193]},\n",
       "   'relation': '部件故障'},\n",
       "  {'h': {'name': '电缆分支箱', 'pos': [294, 299]},\n",
       "   't': {'name': '停电', 'pos': [300, 302]},\n",
       "   'relation': '部件故障'},\n",
       "  {'h': {'name': '出线负荷', 'pos': [303, 307]},\n",
       "   't': {'name': '不符合要求', 'pos': [161, 166]},\n",
       "   'relation': '性能故障'},\n",
       "  {'h': {'name': '出线负荷', 'pos': [303, 307]},\n",
       "   't': {'name': '断开', 'pos': [308, 310]},\n",
       "   'relation': '性能故障'},\n",
       "  {'h': {'name': '接地电阻摇表', 'pos': [382, 388]},\n",
       "   't': {'name': '接地电阻', 'pos': [390, 394]},\n",
       "   'relation': '检测工具'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65b2926b-38f1-45ae-96e5-a855feb0cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON字符串的保存\n",
    "f = open('evalResult.json', 'w', encoding='utf-8')\n",
    "for i in predict_result:\n",
    "    s = json.dumps(i, ensure_ascii=False)\n",
    "    f.write(s + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd59be3-6ab5-4652-a512-50f225b7f993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3812",
   "language": "python",
   "name": "env_3812"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}