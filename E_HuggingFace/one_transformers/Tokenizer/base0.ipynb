{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': 'C:\\\\Users\\\\duanm\\\\.cache\\\\huggingface\\\\hub\\\\models--Qwen--Qwen2.5-7B-Instruct\\\\snapshots\\\\a09a35458c702b33eeacc393d103063234e8bc28\\\\vocab.json',\n",
       " 'merges_file': 'C:\\\\Users\\\\duanm\\\\.cache\\\\huggingface\\\\hub\\\\models--Qwen--Qwen2.5-7B-Instruct\\\\snapshots\\\\a09a35458c702b33eeacc393d103063234e8bc28\\\\merges.txt',\n",
       " 'unk_token': None,\n",
       " 'bos_token': None,\n",
       " 'eos_token': AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'pad_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'add_bos_token': False,\n",
       " 'add_prefix_space': False,\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>'],\n",
       " 'chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n',\n",
       " 'clean_up_tokenization_spaces': False,\n",
       " 'errors': 'replace',\n",
       " 'model_max_length': 131072,\n",
       " 'split_special_tokens': False,\n",
       " 'name_or_path': 'Qwen/Qwen2.5-7B-Instruct'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the AutoTokenizer.from_pretrained() class method.\n",
    "# This class cannot be instantiated directly using __init__() (throws an error).\n",
    "# tokenizer_llm = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")  # 加载预训练模型分词器(类方法)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")  # 与上等价\n",
    "tokenizer.init_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151643\n",
      "131072\n",
      "['input_ids', 'attention_mask']\n",
      "right\n",
      "right\n",
      "<class 'dict'>\n",
      "151665\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 词汇量\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "# model’s maximum context size\n",
    "print(tokenizer.model_max_length)\n",
    "\n",
    "# 模型前向传递(forward)中需要输入的字段名称\n",
    "print(tokenizer.model_input_names)\n",
    "\n",
    "print(tokenizer.padding_side)\n",
    "print(tokenizer.truncation_side)\n",
    "\n",
    "# 词典\n",
    "tokenizer_dict = tokenizer.get_vocab()\n",
    "print(type(tokenizer_dict))  # <class 'dict'>\n",
    "print(len(tokenizer_dict))\n",
    "print('love' in tokenizer_dict)\n",
    "print('china' in tokenizer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "<|im_end|> 151645\n",
      "None None\n",
      "None None\n",
      "<|endoftext|> 151643\n",
      "None None\n",
      "None None\n",
      "['<|im_end|>', '<|endoftext|>', '<|im_start|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>'] [151645, 151643, 151644, 151646, 151647, 151648, 151649, 151650, 151651, 151652, 151653, 151654, 151655, 151656]\n",
      "['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>'] [151644, 151645, 151646, 151647, 151648, 151649, 151650, 151651, 151652, 151653, 151654, 151655, 151656]\n",
      "{'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}\n"
     ]
    }
   ],
   "source": [
    "# 特殊符号\n",
    "print(tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id)\n",
    "print(tokenizer.sep_token, tokenizer.sep_token_id)\n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(tokenizer.mask_token, tokenizer.mask_token_id)\n",
    "print(tokenizer.all_special_tokens, tokenizer.all_special_ids)\n",
    "print(tokenizer.additional_special_tokens, tokenizer.additional_special_tokens_ids)\n",
    "print(tokenizer.special_tokens_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
