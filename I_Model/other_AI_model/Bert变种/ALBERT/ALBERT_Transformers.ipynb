{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask']\n",
      "['[CLS]', '[SEP]', '<unk>', '<pad>', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "主要预训练模型:\n",
    "'albert-base-v2'\n",
    "'albert-xxlarge-v2'\n",
    "'''\n",
    "tokenizer = AutoTokenizer.from_pretrained('albert-base-v2')\n",
    "print(tokenizer.model_input_names)\n",
    "print(tokenizer.all_special_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "AlbertModel(\n  (embeddings): AlbertEmbeddings(\n    (word_embeddings): Embedding(30000, 128, padding_idx=0)\n    (position_embeddings): Embedding(512, 128)\n    (token_type_embeddings): Embedding(2, 128)\n    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (encoder): AlbertTransformer(\n    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n    (albert_layer_groups): ModuleList(\n      (0): AlbertLayerGroup(\n        (albert_layers): ModuleList(\n          (0): AlbertLayer(\n            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (attention): AlbertAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (attention_dropout): Dropout(p=0, inplace=False)\n              (output_dropout): Dropout(p=0, inplace=False)\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            )\n            (ffn): Linear(in_features=768, out_features=3072, bias=True)\n            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): NewGELUActivation()\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (pooler): Linear(in_features=768, out_features=768, bias=True)\n  (pooler_activation): Tanh()\n)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"albert-base-v2\")\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "AlbertConfig {\n  \"_name_or_path\": \"albert-base-v2\",\n  \"architectures\": [\n    \"AlbertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 2,\n  \"classifier_dropout_prob\": 0.1,\n  \"down_scale_factor\": 1,\n  \"embedding_size\": 128,\n  \"eos_token_id\": 3,\n  \"gap_size\": 0,\n  \"hidden_act\": \"gelu_new\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"inner_group_num\": 1,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"albert\",\n  \"net_structure_type\": 0,\n  \"num_attention_heads\": 12,\n  \"num_hidden_groups\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_memory_blocks\": 0,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.17.0\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30000\n}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  2,  31, 339,  42,   3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "[CLS] i love you[SEP]\n"
     ]
    }
   ],
   "source": [
    "text = \"i love you\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "print(encoded_input.keys())\n",
    "print(tokenizer.decode(encoded_input['input_ids'].tolist()[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 1.9883,  1.4943, -1.2583,  ...,  0.4002, -0.5958,  0.5466],\n         [ 0.9151,  0.8466, -0.8045,  ...,  0.2197,  0.8086, -0.3149],\n         [ 0.0617, -0.9907,  0.4586,  ...,  1.3911,  1.4486, -0.2087],\n         [ 0.9766,  0.2713,  0.9242,  ...,  0.0440,  1.8477, -1.3263],\n         [ 0.0424,  0.1538, -0.0686,  ..., -0.0756,  0.1219,  0.2168]]],\n       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.2241, -0.4457,  0.0199, -0.7580,  0.9498, -0.8606,  0.4836, -0.3872,\n          0.5382, -0.9992,  0.9414,  0.2750, -0.2783, -0.9876, -0.9850, -0.4512,\n          0.5003,  0.4480,  0.9311, -0.3789, -0.9957, -0.9947,  0.9961,  0.9849,\n         -0.4534, -0.4747,  0.4794, -0.8809, -0.9997, -0.4315, -1.0000,  0.4048,\n          0.5014,  0.4136,  0.5041, -0.3707,  0.4273,  0.8567, -0.4759,  0.3681,\n          0.3939, -0.9777, -0.5081,  0.3430,  0.3633,  0.3827,  0.9802, -0.9726,\n          0.8978, -0.3908, -0.3559, -0.2761, -0.3745, -0.9644,  0.8471,  0.4966,\n         -0.4473, -0.2838,  0.9929, -0.2326,  0.3259, -0.4732,  0.4677,  0.8550,\n         -0.4095,  0.3931,  0.2831,  0.9833, -0.3248,  0.9942,  0.2426,  0.4475,\n          0.3276, -0.5164,  0.6747,  0.3718,  0.9940,  0.3792,  0.9751, -0.9964,\n          0.4471,  0.5006, -0.4341,  0.4325, -0.8894, -0.9943,  0.4717, -1.0000,\n          0.4193,  0.9857,  0.3870,  0.4825, -0.3675, -0.9998,  0.4473, -0.3932,\n         -0.9791,  0.5140,  0.3511, -0.4329,  0.9894, -0.3152,  0.4081, -0.3959,\n         -0.2337, -0.3914,  0.4486,  0.0510,  0.3759,  0.9972, -0.9735, -0.4645,\n          0.3726,  0.9986, -0.4333,  0.3794, -0.4508,  0.9618, -0.9823,  0.3993,\n          0.3845,  0.7561,  0.4183, -0.5814,  0.4065, -0.7814,  0.2518, -0.5846,\n          0.9860, -0.9843,  0.4464,  0.3690, -0.9541,  0.1644,  0.4545,  0.9930,\n         -0.3325, -0.4445, -0.4639,  0.5175,  0.9862, -0.3864,  0.4395, -0.4902,\n         -0.3574,  0.3689,  0.8058, -0.2637,  0.2935,  0.8273, -0.4023,  0.9484,\n         -0.8220, -0.6564, -0.9983,  0.0182,  0.9977, -0.7832,  0.7757, -0.4338,\n         -0.3538, -0.8304, -0.9973, -0.4259, -0.9322, -0.4762,  0.9997, -0.0258,\n          0.9986, -0.9395, -0.2862,  0.3894, -0.3664,  0.8299,  0.4353,  0.4952,\n          0.3454, -0.8538, -0.4799, -0.2793,  0.9422, -0.9976,  0.3372,  0.5038,\n          0.9890,  0.5023,  0.3993, -0.9462,  0.3089,  0.1874, -0.4704, -0.5389,\n          0.3732,  0.9466,  1.0000, -0.5716,  0.9980, -0.8579,  0.9907, -0.9996,\n         -0.4997,  0.8632,  0.3815,  0.3565,  0.4159,  0.6581, -0.9574, -0.9107,\n         -0.9806, -0.4446, -0.9148,  0.9604, -0.9453,  0.4312, -0.9541,  0.9830,\n          0.9840, -0.4033,  0.9992, -0.3988,  0.4879,  0.4103, -0.9999,  0.8780,\n          0.2911,  0.3657,  0.2512, -0.4749,  0.8679, -0.9979, -0.5626, -0.9508,\n          0.4228, -0.9990, -0.3507, -0.5142,  0.4550,  0.4840,  0.4573, -0.9999,\n          0.9989,  0.4771, -0.3436, -0.4402, -0.3571,  0.9997, -0.9829,  0.2869,\n         -0.5285,  0.9985,  0.9161,  0.4564,  0.5173, -0.3820,  0.9917, -0.2453,\n         -0.9878, -0.2430, -0.9890,  0.3959, -0.9921,  0.5653, -0.2174, -0.9771,\n         -0.5174,  0.9629,  0.9978,  0.9979,  0.4796, -0.5719,  0.2969, -0.3432,\n          0.8450,  0.0579,  0.9881, -0.8773, -0.6610,  0.4182,  0.5859,  0.3349,\n         -0.4648, -0.3123,  0.3430, -0.4364,  0.9637, -0.7071,  0.9998, -0.8864,\n         -0.9998,  0.3850, -0.3672, -0.4110,  0.9922, -0.4210, -0.9995, -0.9994,\n          0.7267,  0.9265,  0.9754, -0.9993,  0.4061, -0.5444, -0.4100,  0.9953,\n          0.3931, -0.4249, -0.2162,  0.3706,  0.3230,  0.1084, -0.9877, -0.4087,\n         -0.4595, -0.9967, -0.4257, -0.8944, -0.3730, -0.9862,  0.9563,  0.9961,\n         -0.3464, -0.5210,  0.9987, -0.9970,  0.4051, -0.9996,  0.4675, -0.9970,\n         -0.9995,  0.5078,  0.4184, -0.4371, -0.5444, -0.9737,  0.4075, -0.9997,\n          0.2214, -0.5160,  0.9840,  0.9870, -0.9993, -0.2694, -0.2253,  0.4186,\n          0.4267,  0.4554, -0.6675, -0.9582,  0.3138, -0.9902,  0.3784,  0.6138,\n         -0.4228,  0.9427, -0.5046,  0.3779, -0.9902, -0.9174, -0.4062,  0.9661,\n          0.9961,  0.2792, -0.3395,  0.4554, -0.9537,  0.9755, -0.7934,  0.9908,\n         -0.9940, -0.4670, -0.9965,  0.9998,  0.8682,  0.9906, -0.4870, -0.9418,\n         -0.9988,  0.4226, -0.4316, -0.4321, -0.4190,  0.6571,  0.4298,  0.2664,\n         -0.7494,  0.4482,  0.3306,  0.7949, -0.9880,  0.9924,  0.4704, -0.4414,\n          0.7334,  0.9997, -0.4175,  0.3915, -0.8867, -0.9898, -0.4210,  0.4411,\n          0.9987, -0.3812, -0.7985,  0.9671,  0.9704, -0.9836,  0.4254,  0.9700,\n          0.9376,  0.5335,  0.4715,  0.7363,  0.7669,  0.4090,  0.9423, -0.3986,\n          0.9900, -0.9987, -0.9983,  0.9971, -0.3761,  0.9981, -0.6040,  0.3932,\n         -0.2809,  0.4238, -0.4239, -0.5503,  0.4249,  0.0084,  0.5562,  0.9993,\n          0.3854, -0.9993, -0.9995,  0.9427,  0.5264,  0.4949, -0.3539,  0.9463,\n         -0.4285, -0.7044, -0.5785, -0.4873,  0.5178, -0.9999,  0.9984, -0.9667,\n          0.9744, -0.3177, -0.1203,  0.4022,  0.8511, -0.3941, -0.9999, -0.5188,\n         -0.9978,  0.5007, -0.3830, -0.8875, -0.3788,  0.9627, -0.4192,  0.7989,\n          0.8037, -0.5162, -0.3748,  0.9999, -0.9810, -0.9992,  0.4459,  0.4494,\n          0.3462,  0.5249,  0.4244,  0.3512, -0.6832, -0.3817,  0.9595, -0.9975,\n          0.0724, -0.9462,  0.9707, -0.5763,  0.3758,  0.9172,  0.9988,  0.9857,\n          0.1886, -1.0000, -0.9825, -0.9955, -0.9546,  0.3725, -0.7861,  0.7617,\n         -0.4414, -0.3483,  0.9267,  0.8264, -0.4705, -0.4785, -0.9668, -0.9818,\n         -0.3210, -0.9217,  0.4271, -0.9928, -0.5576, -0.9894, -0.9982,  0.1292,\n         -0.9950,  0.3672, -0.9996, -0.4543, -0.3928,  0.6005, -0.3033,  0.3993,\n          0.3564, -0.4214,  0.5342,  0.9939, -0.4916,  0.9998,  0.9998,  0.7789,\n          0.4009, -0.8989, -0.4325,  0.9959,  0.2520, -0.9948,  0.9478, -0.5468,\n          0.9948, -0.9950,  0.9046, -0.9188, -0.6495, -0.9992, -0.2672, -0.9978,\n         -0.9932, -0.9982,  0.4712,  0.9766, -0.9369,  1.0000,  0.4799, -0.8362,\n          0.9975,  0.4142, -0.9527,  0.3846, -0.5949,  0.9999, -0.4592,  0.9879,\n         -0.3469, -0.9916,  0.9919,  0.9522, -0.3144, -0.3316, -0.9495, -0.6941,\n          0.9366, -0.4208,  0.4375,  0.9987, -0.9236,  0.3030, -0.4825,  0.4410,\n         -0.8465,  0.3852, -0.6401,  0.8508, -0.9868, -0.9949,  0.9987,  0.3895,\n          0.5140,  0.4305,  0.6232, -0.2603,  0.9985, -0.9956, -0.4453,  0.3506,\n         -0.0602,  0.9790, -0.4085, -0.5126,  0.3924,  0.9473, -0.3741,  0.9965,\n         -0.1273,  0.4898,  0.3356,  0.4032,  0.4232,  0.9821,  0.3609, -0.9954,\n         -0.3485, -0.3674, -0.3931,  0.9986,  0.9976, -0.9563, -0.1104, -0.3568,\n         -0.9959,  0.9972,  0.8513,  0.9914,  0.9998,  0.6018, -0.2942, -0.9784,\n          0.9973,  0.9353, -0.4969,  0.2746,  0.5945, -0.3184,  0.9304,  0.9628,\n         -0.9871, -0.0075,  0.4458,  0.9977,  0.9874, -0.4024,  0.9867, -0.9997,\n         -0.4828,  0.9905, -0.9005,  0.5546,  0.9993, -0.9996, -0.4184, -0.4179,\n          0.9152,  0.9567,  0.3621,  0.3217, -0.3462, -0.4836,  0.9931,  0.3864,\n         -0.4905,  0.9801, -0.3705, -0.3327,  0.4124,  0.5165,  0.9982,  0.9882,\n         -0.9852, -0.5003,  0.5250, -0.4589,  0.9942, -0.9999, -0.3111,  0.2404,\n         -0.6223,  0.2977,  0.8822,  0.5393, -0.5013, -0.9928,  0.4357,  0.4294,\n          0.3578,  0.9961,  0.2914, -0.5009,  0.6495,  0.7959, -0.4626,  0.7449,\n         -0.9202,  0.3230, -0.5748,  0.4454, -0.4381, -0.9786, -0.5745,  0.4611,\n          0.9952,  0.4593,  0.5250,  0.8768, -0.3917,  0.5401,  0.3958,  0.3924,\n         -0.4199, -0.5055, -0.4072, -0.9992,  0.1356,  0.3818, -0.3051,  0.5182,\n         -0.2600, -0.4298,  0.4160, -0.4382,  0.4686,  0.0884, -0.9999, -0.5036,\n         -0.9337, -0.5019,  0.4022, -0.5517,  0.5332, -0.6610, -0.9928, -0.6921,\n          0.3785, -0.9113,  0.3783, -0.8048,  0.3548,  0.9983,  1.0000, -0.9969,\n          0.3699, -0.9994, -0.3749, -0.3908, -0.9991,  0.3083,  0.9997,  0.7994,\n          0.3397, -0.9825,  0.0100,  0.9987, -0.7666, -0.4834,  0.5306, -0.3637,\n         -0.9997,  0.7155,  0.4730,  0.4127,  0.5030, -0.8938, -0.5452,  0.7423,\n         -0.9951,  0.4026,  0.9981, -0.3468,  0.3501,  0.3486, -0.9880,  0.4395]],\n       grad_fn=<TanhBackward0>), hidden_states=None, attentions=None)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model(**encoded_input)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.9883,  1.4943, -1.2583,  ...,  0.4002, -0.5958,  0.5466],\n",
      "         [ 0.9151,  0.8466, -0.8045,  ...,  0.2197,  0.8086, -0.3149],\n",
      "         [ 0.0617, -0.9907,  0.4586,  ...,  1.3911,  1.4486, -0.2087],\n",
      "         [ 0.9766,  0.2713,  0.9242,  ...,  0.0440,  1.8477, -1.3263],\n",
      "         [ 0.0424,  0.1538, -0.0686,  ..., -0.0756,  0.1219,  0.2168]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.2241, -0.4457,  0.0199, -0.7580,  0.9498, -0.8606,  0.4836, -0.3872,\n",
      "          0.5382, -0.9992,  0.9414,  0.2750, -0.2783, -0.9876, -0.9850, -0.4512,\n",
      "          0.5003,  0.4480,  0.9311, -0.3789, -0.9957, -0.9947,  0.9961,  0.9849,\n",
      "         -0.4534, -0.4747,  0.4794, -0.8809, -0.9997, -0.4315, -1.0000,  0.4048,\n",
      "          0.5014,  0.4136,  0.5041, -0.3707,  0.4273,  0.8567, -0.4759,  0.3681,\n",
      "          0.3939, -0.9777, -0.5081,  0.3430,  0.3633,  0.3827,  0.9802, -0.9726,\n",
      "          0.8978, -0.3908, -0.3559, -0.2761, -0.3745, -0.9644,  0.8471,  0.4966,\n",
      "         -0.4473, -0.2838,  0.9929, -0.2326,  0.3259, -0.4732,  0.4677,  0.8550,\n",
      "         -0.4095,  0.3931,  0.2831,  0.9833, -0.3248,  0.9942,  0.2426,  0.4475,\n",
      "          0.3276, -0.5164,  0.6747,  0.3718,  0.9940,  0.3792,  0.9751, -0.9964,\n",
      "          0.4471,  0.5006, -0.4341,  0.4325, -0.8894, -0.9943,  0.4717, -1.0000,\n",
      "          0.4193,  0.9857,  0.3870,  0.4825, -0.3675, -0.9998,  0.4473, -0.3932,\n",
      "         -0.9791,  0.5140,  0.3511, -0.4329,  0.9894, -0.3152,  0.4081, -0.3959,\n",
      "         -0.2337, -0.3914,  0.4486,  0.0510,  0.3759,  0.9972, -0.9735, -0.4645,\n",
      "          0.3726,  0.9986, -0.4333,  0.3794, -0.4508,  0.9618, -0.9823,  0.3993,\n",
      "          0.3845,  0.7561,  0.4183, -0.5814,  0.4065, -0.7814,  0.2518, -0.5846,\n",
      "          0.9860, -0.9843,  0.4464,  0.3690, -0.9541,  0.1644,  0.4545,  0.9930,\n",
      "         -0.3325, -0.4445, -0.4639,  0.5175,  0.9862, -0.3864,  0.4395, -0.4902,\n",
      "         -0.3574,  0.3689,  0.8058, -0.2637,  0.2935,  0.8273, -0.4023,  0.9484,\n",
      "         -0.8220, -0.6564, -0.9983,  0.0182,  0.9977, -0.7832,  0.7757, -0.4338,\n",
      "         -0.3538, -0.8304, -0.9973, -0.4259, -0.9322, -0.4762,  0.9997, -0.0258,\n",
      "          0.9986, -0.9395, -0.2862,  0.3894, -0.3664,  0.8299,  0.4353,  0.4952,\n",
      "          0.3454, -0.8538, -0.4799, -0.2793,  0.9422, -0.9976,  0.3372,  0.5038,\n",
      "          0.9890,  0.5023,  0.3993, -0.9462,  0.3089,  0.1874, -0.4704, -0.5389,\n",
      "          0.3732,  0.9466,  1.0000, -0.5716,  0.9980, -0.8579,  0.9907, -0.9996,\n",
      "         -0.4997,  0.8632,  0.3815,  0.3565,  0.4159,  0.6581, -0.9574, -0.9107,\n",
      "         -0.9806, -0.4446, -0.9148,  0.9604, -0.9453,  0.4312, -0.9541,  0.9830,\n",
      "          0.9840, -0.4033,  0.9992, -0.3988,  0.4879,  0.4103, -0.9999,  0.8780,\n",
      "          0.2911,  0.3657,  0.2512, -0.4749,  0.8679, -0.9979, -0.5626, -0.9508,\n",
      "          0.4228, -0.9990, -0.3507, -0.5142,  0.4550,  0.4840,  0.4573, -0.9999,\n",
      "          0.9989,  0.4771, -0.3436, -0.4402, -0.3571,  0.9997, -0.9829,  0.2869,\n",
      "         -0.5285,  0.9985,  0.9161,  0.4564,  0.5173, -0.3820,  0.9917, -0.2453,\n",
      "         -0.9878, -0.2430, -0.9890,  0.3959, -0.9921,  0.5653, -0.2174, -0.9771,\n",
      "         -0.5174,  0.9629,  0.9978,  0.9979,  0.4796, -0.5719,  0.2969, -0.3432,\n",
      "          0.8450,  0.0579,  0.9881, -0.8773, -0.6610,  0.4182,  0.5859,  0.3349,\n",
      "         -0.4648, -0.3123,  0.3430, -0.4364,  0.9637, -0.7071,  0.9998, -0.8864,\n",
      "         -0.9998,  0.3850, -0.3672, -0.4110,  0.9922, -0.4210, -0.9995, -0.9994,\n",
      "          0.7267,  0.9265,  0.9754, -0.9993,  0.4061, -0.5444, -0.4100,  0.9953,\n",
      "          0.3931, -0.4249, -0.2162,  0.3706,  0.3230,  0.1084, -0.9877, -0.4087,\n",
      "         -0.4595, -0.9967, -0.4257, -0.8944, -0.3730, -0.9862,  0.9563,  0.9961,\n",
      "         -0.3464, -0.5210,  0.9987, -0.9970,  0.4051, -0.9996,  0.4675, -0.9970,\n",
      "         -0.9995,  0.5078,  0.4184, -0.4371, -0.5444, -0.9737,  0.4075, -0.9997,\n",
      "          0.2214, -0.5160,  0.9840,  0.9870, -0.9993, -0.2694, -0.2253,  0.4186,\n",
      "          0.4267,  0.4554, -0.6675, -0.9582,  0.3138, -0.9902,  0.3784,  0.6138,\n",
      "         -0.4228,  0.9427, -0.5046,  0.3779, -0.9902, -0.9174, -0.4062,  0.9661,\n",
      "          0.9961,  0.2792, -0.3395,  0.4554, -0.9537,  0.9755, -0.7934,  0.9908,\n",
      "         -0.9940, -0.4670, -0.9965,  0.9998,  0.8682,  0.9906, -0.4870, -0.9418,\n",
      "         -0.9988,  0.4226, -0.4316, -0.4321, -0.4190,  0.6571,  0.4298,  0.2664,\n",
      "         -0.7494,  0.4482,  0.3306,  0.7949, -0.9880,  0.9924,  0.4704, -0.4414,\n",
      "          0.7334,  0.9997, -0.4175,  0.3915, -0.8867, -0.9898, -0.4210,  0.4411,\n",
      "          0.9987, -0.3812, -0.7985,  0.9671,  0.9704, -0.9836,  0.4254,  0.9700,\n",
      "          0.9376,  0.5335,  0.4715,  0.7363,  0.7669,  0.4090,  0.9423, -0.3986,\n",
      "          0.9900, -0.9987, -0.9983,  0.9971, -0.3761,  0.9981, -0.6040,  0.3932,\n",
      "         -0.2809,  0.4238, -0.4239, -0.5503,  0.4249,  0.0084,  0.5562,  0.9993,\n",
      "          0.3854, -0.9993, -0.9995,  0.9427,  0.5264,  0.4949, -0.3539,  0.9463,\n",
      "         -0.4285, -0.7044, -0.5785, -0.4873,  0.5178, -0.9999,  0.9984, -0.9667,\n",
      "          0.9744, -0.3177, -0.1203,  0.4022,  0.8511, -0.3941, -0.9999, -0.5188,\n",
      "         -0.9978,  0.5007, -0.3830, -0.8875, -0.3788,  0.9627, -0.4192,  0.7989,\n",
      "          0.8037, -0.5162, -0.3748,  0.9999, -0.9810, -0.9992,  0.4459,  0.4494,\n",
      "          0.3462,  0.5249,  0.4244,  0.3512, -0.6832, -0.3817,  0.9595, -0.9975,\n",
      "          0.0724, -0.9462,  0.9707, -0.5763,  0.3758,  0.9172,  0.9988,  0.9857,\n",
      "          0.1886, -1.0000, -0.9825, -0.9955, -0.9546,  0.3725, -0.7861,  0.7617,\n",
      "         -0.4414, -0.3483,  0.9267,  0.8264, -0.4705, -0.4785, -0.9668, -0.9818,\n",
      "         -0.3210, -0.9217,  0.4271, -0.9928, -0.5576, -0.9894, -0.9982,  0.1292,\n",
      "         -0.9950,  0.3672, -0.9996, -0.4543, -0.3928,  0.6005, -0.3033,  0.3993,\n",
      "          0.3564, -0.4214,  0.5342,  0.9939, -0.4916,  0.9998,  0.9998,  0.7789,\n",
      "          0.4009, -0.8989, -0.4325,  0.9959,  0.2520, -0.9948,  0.9478, -0.5468,\n",
      "          0.9948, -0.9950,  0.9046, -0.9188, -0.6495, -0.9992, -0.2672, -0.9978,\n",
      "         -0.9932, -0.9982,  0.4712,  0.9766, -0.9369,  1.0000,  0.4799, -0.8362,\n",
      "          0.9975,  0.4142, -0.9527,  0.3846, -0.5949,  0.9999, -0.4592,  0.9879,\n",
      "         -0.3469, -0.9916,  0.9919,  0.9522, -0.3144, -0.3316, -0.9495, -0.6941,\n",
      "          0.9366, -0.4208,  0.4375,  0.9987, -0.9236,  0.3030, -0.4825,  0.4410,\n",
      "         -0.8465,  0.3852, -0.6401,  0.8508, -0.9868, -0.9949,  0.9987,  0.3895,\n",
      "          0.5140,  0.4305,  0.6232, -0.2603,  0.9985, -0.9956, -0.4453,  0.3506,\n",
      "         -0.0602,  0.9790, -0.4085, -0.5126,  0.3924,  0.9473, -0.3741,  0.9965,\n",
      "         -0.1273,  0.4898,  0.3356,  0.4032,  0.4232,  0.9821,  0.3609, -0.9954,\n",
      "         -0.3485, -0.3674, -0.3931,  0.9986,  0.9976, -0.9563, -0.1104, -0.3568,\n",
      "         -0.9959,  0.9972,  0.8513,  0.9914,  0.9998,  0.6018, -0.2942, -0.9784,\n",
      "          0.9973,  0.9353, -0.4969,  0.2746,  0.5945, -0.3184,  0.9304,  0.9628,\n",
      "         -0.9871, -0.0075,  0.4458,  0.9977,  0.9874, -0.4024,  0.9867, -0.9997,\n",
      "         -0.4828,  0.9905, -0.9005,  0.5546,  0.9993, -0.9996, -0.4184, -0.4179,\n",
      "          0.9152,  0.9567,  0.3621,  0.3217, -0.3462, -0.4836,  0.9931,  0.3864,\n",
      "         -0.4905,  0.9801, -0.3705, -0.3327,  0.4124,  0.5165,  0.9982,  0.9882,\n",
      "         -0.9852, -0.5003,  0.5250, -0.4589,  0.9942, -0.9999, -0.3111,  0.2404,\n",
      "         -0.6223,  0.2977,  0.8822,  0.5393, -0.5013, -0.9928,  0.4357,  0.4294,\n",
      "          0.3578,  0.9961,  0.2914, -0.5009,  0.6495,  0.7959, -0.4626,  0.7449,\n",
      "         -0.9202,  0.3230, -0.5748,  0.4454, -0.4381, -0.9786, -0.5745,  0.4611,\n",
      "          0.9952,  0.4593,  0.5250,  0.8768, -0.3917,  0.5401,  0.3958,  0.3924,\n",
      "         -0.4199, -0.5055, -0.4072, -0.9992,  0.1356,  0.3818, -0.3051,  0.5182,\n",
      "         -0.2600, -0.4298,  0.4160, -0.4382,  0.4686,  0.0884, -0.9999, -0.5036,\n",
      "         -0.9337, -0.5019,  0.4022, -0.5517,  0.5332, -0.6610, -0.9928, -0.6921,\n",
      "          0.3785, -0.9113,  0.3783, -0.8048,  0.3548,  0.9983,  1.0000, -0.9969,\n",
      "          0.3699, -0.9994, -0.3749, -0.3908, -0.9991,  0.3083,  0.9997,  0.7994,\n",
      "          0.3397, -0.9825,  0.0100,  0.9987, -0.7666, -0.4834,  0.5306, -0.3637,\n",
      "         -0.9997,  0.7155,  0.4730,  0.4127,  0.5030, -0.8938, -0.5452,  0.7423,\n",
      "         -0.9951,  0.4026,  0.9981, -0.3468,  0.3501,  0.3486, -0.9880,  0.4395]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(result.last_hidden_state)\n",
    "print(result.pooler_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(result.last_hidden_state.shape)\n",
    "print(result.pooler_output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}