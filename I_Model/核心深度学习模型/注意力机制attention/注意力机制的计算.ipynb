{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"../../../Other/img/注意力机制.png\"  style=\"width:750px;height:400px;float:bottom\">\n",
    "\n",
    "其中$a$表示注意力评分函数\n",
    "\n",
    "假设有一个查询$ \\mathrm{q} \\in \\mathbb{R}^{q} $ 和$m$个\"键-值\"对$ \\left(\\mathrm{k}_{1}, \\mathrm{v}_{1}\\right), \\ldots,\\left(\\mathrm{k}_{m}, \\mathrm{v}_{m}\\right) $,其中$ \\mathrm{k}_{i} \\in \\mathbb{R}^{k}, \\mathrm{v}_{i} \\in \\mathbb{R}^{v}$.注意力汇聚函数$f$就被表示成值的加权和:\n",
    "\n",
    "$$ f\\left(\\mathrm{q},\\left(\\mathrm{k}_{1}, \\mathrm{v}_{1}\\right), \\ldots,\\left(\\mathrm{k}_{m}, \\mathrm{v}_{m}\\right)\\right)=\\sum^{m} \\alpha\\left(\\mathrm{q}, \\mathrm{k}_{i}\\right) \\mathrm{v}_{i} \\in \\mathbb{R}^{v} $$\n",
    "\n",
    "其中查询$\\mathrm{q}$和键$\\mathrm{k}_i$的注意力权重是通过注意力评分函数$a$将两个向量映射为标量,再经过softmax运算得到:\n",
    "\n",
    "$$ \\alpha\\left(\\mathrm{q}, \\mathrm{k}_{i}\\right)=\\operatorname{softmax}\\left(a\\left(\\mathrm{q}, \\mathrm{k}_{i}\\right)\\right)=\\frac{\\exp \\left(a\\left(\\mathrm{q}, \\mathrm{k}_{i}\\right)\\right)}{\\sum_{j=1}^{m} \\exp \\left(a\\left(\\mathrm{q}, \\mathbf{k}_{j}\\right)\\right)} \\in \\mathbb{R} $$\n",
    "\n",
    "选择不同的注意力评分函数$ a $会导致不同的注意力汇聚操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens=None):\n",
    "    \"\"\"通过在最后⼀个轴上遮蔽元素来执⾏softmax操作\"\"\"\n",
    "\n",
    "    def sequence_mask(X, valid_len, value=0):\n",
    "        \"\"\"Mask irrelevant entries in sequences\"\"\"\n",
    "        maxlen = X.size(1)\n",
    "        # 广播机制\n",
    "        mask = torch.arange(maxlen, device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = value\n",
    "        return X\n",
    "\n",
    "    if valid_lens is None:\n",
    "        return F.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # 被遮蔽的元素使用⼀个非常大的负值替换,使其softmax输出为0\n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
    "                          value=-1e6)\n",
    "        return F.softmax(X.reshape(shape), dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2002, 0.2938, 0.2681, 0.2379],\n",
      "         [0.3241, 0.2578, 0.2362, 0.1819]],\n",
      "\n",
      "        [[0.2096, 0.2820, 0.3411, 0.1672],\n",
      "         [0.2291, 0.3357, 0.2534, 0.1817]]])\n",
      "\n",
      "tensor([[[0.5132, 0.4868, 0.0000, 0.0000],\n",
      "         [0.3840, 0.6160, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3501, 0.3823, 0.2676, 0.0000],\n",
      "         [0.4300, 0.2426, 0.3274, 0.0000]]])\n",
      "\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2068, 0.4308, 0.3625, 0.0000]],\n",
      "\n",
      "        [[0.4520, 0.5480, 0.0000, 0.0000],\n",
      "         [0.3348, 0.1653, 0.3201, 0.1798]]])\n"
     ]
    }
   ],
   "source": [
    "print(masked_softmax(torch.rand(2, 2, 4)), end='\\n\\n')\n",
    "print(masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3])), end='\\n\\n')\n",
    "print(masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加性注意力\n",
    "\n",
    "当查询和键是不同长度的矢量时,可以使⽤加性注意力作为评分函数.给定查询$ \\mathrm{q} \\in \\mathbb{R}^{1 \\times q} $和键$\\mathrm{k} \\in \\mathbb{R}^{1 \\times k} $,加性注意力(additive attention)的评分函数为:\n",
    "\n",
    "$$ a(\\mathrm{q}, \\mathrm{k})=\\tanh \\left(\\mathrm{q} W_q^T + \\mathrm{k} W_k^T\\right) \\mathrm{w}_{v} \\in \\mathbb{R} $$\n",
    "\n",
    "其中可学习的参数是$ W_q \\in \\mathbb{R}^{h \\times q}, W_k \\in \\mathbb{R}^{h \\times k} $和$\\mathrm{w}_v \\in \\mathbb{R}^{h \\times 1}$.\n",
    "然后将查询和键连接起来后输⼊到⼀个多层感知机(MLP)中,感知机包含⼀个隐藏层,其隐藏单元数是⼀个超参数$h$.通过使用tanh作为激活函数,并且禁用偏置项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"加性注意⼒\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # 键特征数目\n",
    "                 key_size,\n",
    "                 # 查询特征数据\n",
    "                 query_size,\n",
    "                 num_hiddens, dropout):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        # self.W_k.weight.shape = (num_hiddens, key_size)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n",
    "        # self.W_q.weight.shape = (num_hiddens, query_size)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n",
    "        # self.W_v.weight.shape = (1, num_hiddens)\n",
    "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        \"\"\"\n",
    "        queries: 查询\n",
    "        keys: 键\n",
    "        values: 值\n",
    "        valid_lens: 计算attention_weights的有效长度\n",
    "        \"\"\"\n",
    "        # queries:(b, ?q, query_size) x (query_size, num_hiddens) = (b, ?q, num_hiddens)\n",
    "        # keys:(b, ?k, key_size) x (key_size, num_hiddens) = (b, ?k, num_hiddens)\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # queries.unsqueeze(2).shape=(b, ?q, 1, num_hiddens)\n",
    "        # keys.unsqueeze(1).shape=(b, 1, ?k, num_hiddens)\n",
    "        # features.shape=(b, ?q, ?k, num_hiddens)\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # scores:(b, ?q, ?k, num_hiddens) x (num_hiddens, 1) = (b, ?q, ?k, 1)\n",
    "        # scores.squeeze(-1).shape=(b, ?q, ?k)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # values.shape=(b, ?k, ?v)\n",
    "        # 返回值:(b, ?q, ?k) x (b, ?k, ?v) = (b, ?q, ?v)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n\n        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# queries.shape(2, 1, 10)\n",
    "# keys.shape=(2, 10, 2)\n",
    "# values.shape=(2, 10, 4)\n",
    "queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))\n",
    "values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(2, 1, 1)\n",
    "valid_lens = torch.tensor([2, 6])\n",
    "# 最终结果.shape=(2, 1, 4)\n",
    "attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,\n",
    "                              dropout=0.1)\n",
    "attention.eval()\n",
    "attention(queries, keys, values, valid_lens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000]],\n\n        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n          0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(33.0, 0.5, 'Queries')"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 注意力权重\n",
    "_, axes = plt.subplots(1, 1)\n",
    "sns.heatmap(torch.squeeze(attention.attention_weights.detach(), 1), cmap='Reds', ax=axes)\n",
    "axes.set_xlabel('Keys')\n",
    "axes.set_ylabel('Queries')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 缩放点积注意力\n",
    "\n",
    "使用点积可以得到计算效率更⾼的评分函数.但是点积操作要求查询和键具有相同的⻓度$ d $.\n",
    "\n",
    "缩放点注意力(scaled dot-product attention)评分函数:\n",
    "\n",
    "$$ a(\\mathrm{q}, \\mathrm{k})=\\mathrm{q} \\mathrm{k}^T / \\sqrt{d} $$\n",
    "\n",
    "向量化版本:\n",
    "\n",
    "1. $$ Q \\in \\mathbb{R}^{n \\times d}, \\quad K \\in \\mathbb{R}^{m \\times d}, V \\in \\mathbb{R}^{m \\times v} $$\n",
    "\n",
    "2. 注意力分数: $$ a(Q, K)=Q K^{T} / \\sqrt{d} \\in \\mathbb{R}^{n \\times m}$$\n",
    "\n",
    "3. 注意力汇聚:$$ f=\\operatorname{softmax}(a(Q,K)) V \\in \\mathbb{R}^{n \\times v} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力(Attention ls All You Need的注意力计算方式)\"\"\"\n",
    "\n",
    "    def __init__(self, dropout):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        \"\"\"\n",
    "        queries: 查询\n",
    "        keys: 键\n",
    "        values: 值\n",
    "        valid_lens: 计算attention_weights的有效长度\n",
    "        \"\"\"\n",
    "        # queries.shape = (b, ?q, d)\n",
    "        # keys.shape = (b, ?k, d)\n",
    "        # scores.shape = (b, ?q, d) x (b, d, ?k) = (b, ?q, ?k)\n",
    "        d = queries.shape[-1]\n",
    "        # 除以d的平方根\n",
    "        # 原因:当维度很大时,点积结果会很大,会导致softmax的梯度很小(见softmax-Softmax.ipynb).为了减轻这个影响,对点积进行缩放\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # values.shape=(b, ?k, ?v)\n",
    "        # 返回值.shape=(b, ?q, ?k) x (b, ?k, ?v) = (b, ?q, ?v)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n\n        [[10.0000, 11.0000, 12.0000, 13.0000]]])"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = torch.normal(0, 1, (2, 1, 2))\n",
    "keys = torch.ones((2, 10, 2))\n",
    "values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(2, 1, 1)\n",
    "valid_lens = torch.tensor([2, 6])\n",
    "# 最终结果.shape=(2, 1, 4)\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "\n",
    "attention.eval()\n",
    "attention(queries, keys, values, valid_lens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(33.0, 0.5, 'Queries')"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 注意力权重\n",
    "_, axes = plt.subplots(1, 1)\n",
    "sns.heatmap(torch.squeeze(attention.attention_weights.detach(), 1), cmap='Reds', ax=axes)\n",
    "axes.set_xlabel('Keys')\n",
    "axes.set_ylabel('Queries')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "a = nn.Linear(3, 1, bias=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 3])"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # 查询特征数目(E_q)\n",
    "                 query_size,\n",
    "                 # 键特征数目(E_k)\n",
    "                 key_size,\n",
    "                 # 值特征数目(E_v)\n",
    "                 value_size,\n",
    "                 # 多头数\n",
    "                 num_heads, dropout, bias=False):  # 模仿pytorch的参数组成\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert query_size % num_heads == 0, \"query_size must be divisible by num_heads\"\n",
    "        self.W_q = nn.Linear(query_size, query_size, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, query_size, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, query_size, bias=bias)\n",
    "        self.W_o = nn.Linear(query_size, query_size, bias=bias)\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def transpose_qkv(X, num_heads):\n",
    "        # 输入:X.shape=(N, L or S, E_q)\n",
    "        # X.shape=(N, L or S, num_heads, E_q / num_heads)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "        # X.shape=(N, num_heads, L or S, E_q / num_heads)\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        # 返回值.shape=(N * num_heads, L or S, E_q / num_heads)\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        \"\"\"\n",
    "        queries: 查询\n",
    "        keys: 键\n",
    "        values: 值\n",
    "        valid_lens: 计算attention_weights的有效长度\n",
    "        \"\"\"\n",
    "        # queries.shape=(N, L, E_q)\n",
    "        # self.W_q(queries).shape=(N, L, E_q)\n",
    "        # queries.shape=(N * num_heads, L, E_q / num_heads)\n",
    "\n",
    "        # keys.shape=(N, S, E_k)\n",
    "        # self.W_k(queries).shape=(N, S, E_q)\n",
    "        # keys.shape=(N * num_heads, S, E_q / num_heads)\n",
    "\n",
    "        # values.shape=(N, S, E_v)\n",
    "        # self.W_v(values).shape=(N, S, E_q)\n",
    "        # values.shape=(N * num_heads, S, E_q / num_heads)\n",
    "        queries = self.transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = self.transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = self.transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # E_q维度信息增加到batch_size维度上\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # output.shape=(N * num_heads, L, E_q / num_heads)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # output.shape=(N, num_heads, L, E_q / num_heads)\n",
    "        output = output.reshape(-1, self.num_heads, output.shape[1], output.shape[2])\n",
    "        # output.shape=(N, L, num_heads, E_q / num_heads)\n",
    "        output = output.permute(0, 2, 1, 3)\n",
    "        # output.shape=(N, L, E_q)\n",
    "        output_concat = output.reshape(output.shape[0], output.shape[1], -1)\n",
    "        # 返回值.shape=(N, L, E_q)\n",
    "        return self.W_o(output_concat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "data": {
      "text/plain": "MultiHeadAttention(\n  (W_q): Linear(in_features=100, out_features=100, bias=False)\n  (W_k): Linear(in_features=200, out_features=100, bias=False)\n  (W_v): Linear(in_features=200, out_features=100, bias=False)\n  (W_o): Linear(in_features=100, out_features=100, bias=False)\n  (attention): DotProductAttention(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n)"
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_size, key_size, value_size, num_heads = 100, 200, 200, 5\n",
    "multi_head_atten = MultiHeadAttention(query_size=query_size,\n",
    "                                      key_size=key_size,\n",
    "                                      value_size=value_size,\n",
    "                                      num_heads=num_heads,\n",
    "                                      dropout=0.1)\n",
    "multi_head_atten.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 4, 100])"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_queries, num_kvpairs, valid_lens = 2, 4, 6, torch.tensor([3, 2])\n",
    "X = torch.randn((batch_size, num_queries, query_size))\n",
    "Y = torch.randn((batch_size, num_kvpairs, key_size))\n",
    "multi_head_atten(X, Y, Y, valid_lens).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[0.3167, 0.3271, 0.3562, 0.0000, 0.0000, 0.0000],\n          [0.3244, 0.3725, 0.3031, 0.0000, 0.0000, 0.0000],\n          [0.4037, 0.3623, 0.2340, 0.0000, 0.0000, 0.0000],\n          [0.3666, 0.3143, 0.3192, 0.0000, 0.0000, 0.0000]],\n\n         [[0.2817, 0.4388, 0.2795, 0.0000, 0.0000, 0.0000],\n          [0.1589, 0.3706, 0.4705, 0.0000, 0.0000, 0.0000],\n          [0.4145, 0.2617, 0.3239, 0.0000, 0.0000, 0.0000],\n          [0.5017, 0.3187, 0.1796, 0.0000, 0.0000, 0.0000]],\n\n         [[0.4196, 0.3384, 0.2421, 0.0000, 0.0000, 0.0000],\n          [0.2021, 0.2033, 0.5945, 0.0000, 0.0000, 0.0000],\n          [0.3431, 0.3324, 0.3246, 0.0000, 0.0000, 0.0000],\n          [0.3291, 0.2590, 0.4119, 0.0000, 0.0000, 0.0000]],\n\n         [[0.4122, 0.2506, 0.3372, 0.0000, 0.0000, 0.0000],\n          [0.3988, 0.2652, 0.3359, 0.0000, 0.0000, 0.0000],\n          [0.2977, 0.3857, 0.3166, 0.0000, 0.0000, 0.0000],\n          [0.4420, 0.2143, 0.3437, 0.0000, 0.0000, 0.0000]],\n\n         [[0.2211, 0.2842, 0.4947, 0.0000, 0.0000, 0.0000],\n          [0.2742, 0.3320, 0.3938, 0.0000, 0.0000, 0.0000],\n          [0.3414, 0.3327, 0.3259, 0.0000, 0.0000, 0.0000],\n          [0.3238, 0.2988, 0.3774, 0.0000, 0.0000, 0.0000]]],\n\n\n        [[[0.4016, 0.5984, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.4732, 0.5268, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.4108, 0.5892, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.3951, 0.6049, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.4906, 0.5094, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.6347, 0.3653, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.6041, 0.3959, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.5225, 0.4775, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.3086, 0.6914, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.4767, 0.5233, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.3666, 0.6334, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.4092, 0.5908, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.4036, 0.5964, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.5153, 0.4847, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.5371, 0.4629, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.5267, 0.4733, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.5712, 0.4288, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.4377, 0.5623, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.4127, 0.5873, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.5350, 0.4650, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n       grad_fn=<ReshapeAliasBackward0>)"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意力输出权重.shape=(N * num_heads, L, S)\n",
    "# mul_head_att_weight.reshape=(N, num_heads, L, S)\n",
    "mul_head_att_weight = multi_head_atten.attention.attention_weights.reshape(batch_size, num_heads, num_queries,\n",
    "                                                                           num_kvpairs)\n",
    "mul_head_att_weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}