{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BertTokenizer(可通过`__init__`实例化): Construct a BERT tokenizer. Based on WordPiece.\n",
    "# from_pretrained(类方法): Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',\n",
    ")\n",
    "tokenizer  # 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "21128"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizerFast(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library\n",
    "#   when created with the [`AutoTokenizer.from_pretrained`] class method.\n",
    "#   This class cannot be instantiated directly using `__init__()` (throws an error).\n",
    "# 与上等价\n",
    "tokenizer_auto = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='bert-base-chinese')\n",
    "tokenizer_auto"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP]\n",
      "选 择 珠 江 花 园 的 原 因 就 是 方 便 。\n"
     ]
    }
   ],
   "source": [
    "list_of_token = [101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102]\n",
    "\n",
    "# Convert a list of lists of token ids into a list of strings by calling decode.\n",
    "print(tokenizer.decode(list_of_token))\n",
    "\n",
    "# skip_special_tokens:Whether or not to remove special tokens in the decoding.\n",
    "# If these tokens are already part of the vocabulary, it just let the Tokenizer know about them. If they don’t exist, the Tokenizer creates them, giving them a new id.\n",
    "# These special tokens will never be processed by the model (ie won’t be split into multiple tokens), and they can be removed from the output when decoding.\n",
    "print(tokenizer.decode(list_of_token, skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, 100, 100]\n",
      "['[UNK]', '[UNK]', '[UNK]', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "#  Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the vocabulary.\n",
    "print(tokenizer.convert_tokens_to_ids(['月光', '希望', '[EOS', '<eop>']))\n",
    "\n",
    "# Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and added tokens.\n",
    "print(tokenizer.convert_ids_to_tokens([21128, 21128, 21130, 21131]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 3299, 1045, 4638, 3173, 2361, 3307, 138, 100, 140, 133, 147, 9133, 135, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] 月 光 的 新 希 望 [ [UNK] ] < eop > [SEP]\n",
      "['[CLS]', '月', '光', '的', '新', '希', '望', '[', '[UNK]', ']', '<', 'e', '##op', '>', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "result = tokenizer(text='月光的新希望[EOS]<eop>')\n",
    "\n",
    "# 未添加新tokens前的编码效果\n",
    "print(result)\n",
    "\n",
    "print(tokenizer.decode(result['input_ids']))  # 解码\n",
    "print(tokenizer.convert_ids_to_tokens(result['input_ids']))  # 只是ids到tokens的转换"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21133\n"
     ]
    }
   ],
   "source": [
    "# Add a list of new tokens to the tokenizer class.\n",
    "tokenizer.add_tokens(new_tokens=['月光', '希望'])\n",
    "\n",
    "#  Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes.\n",
    "#  If special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the current vocabulary).\n",
    "\n",
    "# Keys should be in the list of predefined special attributes:\n",
    "#   [`bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].\n",
    "tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '[EOS]',\n",
    "                                                  # Additional special tokens used by the tokenizer.\n",
    "                                                  'additional_special_tokens': [\"<eop>\", \"<eod>\"]})\n",
    "print(len(tokenizer.get_vocab()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 21128, 4638, 3173, 21129, 21130, 21131, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] 月光 的 新 希望 [EOS] <eop> [SEP]\n",
      "['[CLS]', '月', '光', '的', '新', '希', '望', '[', '[UNK]', ']', '<', 'e', '##op', '>', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "result1 = tokenizer(text='月光的新希望[EOS]<eop>')\n",
    "\n",
    "# 添加新tokens后的编码效果\n",
    "print(result1)\n",
    "\n",
    "print(tokenizer.decode(result1['input_ids']))  # 分词效果达到预期\n",
    "print(tokenizer.convert_ids_to_tokens(result['input_ids']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "('save_tokenizer/tokenizer_config.json',\n 'save_tokenizer/special_tokens_map.json',\n 'save_tokenizer/vocab.txt',\n 'save_tokenizer/added_tokens.json')"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存分词器(包括新添加的tokens)\n",
    "tokenizer.save_pretrained(\"save_tokenizer/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='save_tokenizer/', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<eop>', '<eod>']})"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地重新加载\n",
    "tokenizer1 = BertTokenizer.from_pretrained(\"save_tokenizer/\")\n",
    "tokenizer1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 21128, 4638, 3173, 21129, 21130, 21131, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] 月光 的 新 希望 [EOS] <eop> [SEP]\n"
     ]
    }
   ],
   "source": [
    "result2 = tokenizer1(text='月光的新希望[EOS]<eop>')\n",
    "\n",
    "print(result2)\n",
    "print(tokenizer1.decode(result2['input_ids']))  # 分词效果与上等价"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}