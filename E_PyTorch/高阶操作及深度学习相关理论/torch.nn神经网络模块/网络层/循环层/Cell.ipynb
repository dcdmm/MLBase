{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[ 0.6434, -0.1808, -0.7593,  0.8360, -0.8499, -0.4159,  0.9616, -0.9498,\n           0.8111, -0.6140, -0.1622,  0.2313,  0.8592,  0.9843,  0.5759, -0.6067,\n          -0.7575, -0.6723,  0.4485,  0.7207],\n         [-0.5760,  0.3550, -0.2389,  0.2893,  0.3561,  0.1091,  0.7203, -0.8221,\n           0.1464, -0.8327,  0.3760,  0.7806,  0.6498,  0.6565, -0.2575, -0.7315,\n          -0.6968,  0.8181,  0.7018, -0.4305],\n         [ 0.1176,  0.0899, -0.7593, -0.5440, -0.0137, -0.2884, -0.6252, -0.7871,\n           0.0138, -0.8440, -0.0936,  0.5796,  0.2432,  0.5899,  0.7322, -0.0883,\n           0.2807,  0.1261,  0.0546, -0.6091]], grad_fn=<TanhBackward>),\n tensor([[ 0.2420, -0.0255, -0.7812,  0.8089,  0.0644, -0.8222,  0.1228, -0.1012,\n           0.7978,  0.0817, -0.0480,  0.7764,  0.1722,  0.3278,  0.0674, -0.4624,\n          -0.5477, -0.5941, -0.2604,  0.6680],\n         [-0.5474, -0.8532, -0.1522,  0.6226, -0.6826, -0.7079,  0.4523,  0.4899,\n           0.2708,  0.6929, -0.7240,  0.7414, -0.2014,  0.1786, -0.6039,  0.4054,\n           0.5819,  0.5057, -0.0574,  0.4619],\n         [-0.6972, -0.0788, -0.5389,  0.3436, -0.4893,  0.3937,  0.0720, -0.6721,\n          -0.2852, -0.0769, -0.5771,  0.1145,  0.0416, -0.1363, -0.3196, -0.1398,\n          -0.8986, -0.4217, -0.3199,  0.4541]], grad_fn=<TanhBackward>),\n tensor([[ 0.8117,  0.5724, -0.7942,  0.4067, -0.0497, -0.1822,  0.1750,  0.1205,\n           0.4977,  0.3159,  0.1373,  0.7899,  0.4503,  0.3134,  0.6167,  0.0921,\n          -0.1596, -0.1603, -0.0572, -0.0087],\n         [-0.2984, -0.2395,  0.0518,  0.4403, -0.4077, -0.6183,  0.7360, -0.6495,\n          -0.4433,  0.2477, -0.2065, -0.3554,  0.5601,  0.2859, -0.4096, -0.1069,\n           0.2623, -0.5664, -0.2667,  0.5628],\n         [ 0.3882, -0.2428, -0.8729,  0.3936,  0.1961, -0.4993,  0.2097, -0.5247,\n          -0.0218, -0.7267, -0.5234, -0.2244,  0.4232,  0.3706, -0.5657,  0.1779,\n          -0.8561, -0.0937,  0.1494,  0.6212]], grad_fn=<TanhBackward>),\n tensor([[ 0.7024,  0.3712,  0.3743, -0.5227, -0.1539, -0.0827,  0.4766, -0.0868,\n          -0.2407,  0.3901, -0.6540,  0.4800, -0.5292, -0.5069,  0.0476, -0.4646,\n           0.3781,  0.2480, -0.1372, -0.3994],\n         [ 0.5228,  0.7209, -0.3659,  0.3342, -0.0445,  0.1109, -0.1271,  0.8235,\n           0.1672, -0.3885, -0.0489,  0.7868,  0.4681,  0.2492, -0.3981,  0.6782,\n           0.2776,  0.5158, -0.2544, -0.2666],\n         [ 0.4216,  0.2379, -0.6754,  0.7696, -0.1782, -0.7068,  0.3230,  0.4904,\n          -0.0218, -0.4971, -0.3288,  0.6117, -0.1744,  0.1673, -0.4150,  0.3027,\n          -0.4606,  0.0874,  0.7955, -0.2694]], grad_fn=<TanhBackward>),\n tensor([[ 0.2568, -0.4192,  0.2987, -0.1048, -0.3337, -0.5144,  0.5714, -0.5396,\n          -0.3211,  0.5566, -0.0915,  0.5594,  0.2031,  0.0190,  0.0209, -0.2734,\n          -0.1071, -0.1456, -0.4505,  0.0198],\n         [ 0.3155, -0.3270, -0.3032,  0.1883, -0.8570, -0.0778,  0.1506,  0.4445,\n           0.2161, -0.0352,  0.2920,  0.6837,  0.4259, -0.1580, -0.2642,  0.5015,\n           0.9072, -0.1358, -0.0315, -0.3476],\n         [ 0.3683, -0.7214,  0.2224,  0.6384, -0.5308, -0.6242,  0.7550, -0.1431,\n           0.9134, -0.0614,  0.1612,  0.0339,  0.5875,  0.7749, -0.5519, -0.2122,\n           0.2371,  0.2089,  0.6421,  0.4032]], grad_fn=<TanhBackward>),\n tensor([[ 0.1422,  0.2735,  0.2278,  0.4873, -0.2482, -0.0173,  0.5767,  0.5200,\n          -0.2317,  0.0264,  0.2008,  0.3869,  0.2352, -0.1433, -0.2432,  0.0582,\n           0.3779,  0.1424, -0.3284, -0.3611],\n         [ 0.5942,  0.5396, -0.2585,  0.7569, -0.6411,  0.3190,  0.4748,  0.4466,\n           0.5037,  0.6452,  0.5197, -0.2576,  0.4072,  0.0792,  0.1951,  0.5652,\n          -0.3157, -0.5209,  0.7585,  0.2643],\n         [ 0.4869,  0.3360,  0.0992, -0.4786,  0.3566, -0.6768, -0.7552,  0.6293,\n           0.4901, -0.6555, -0.4698,  0.8040,  0.2826,  0.1099, -0.6207,  0.0026,\n           0.6948,  0.7327, -0.6076,  0.0099]], grad_fn=<TanhBackward>)]"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参数含义完全类似RNN\n",
    "rnn = nn.RNNCell(input_size=10,\n",
    "                 hidden_size=20,\n",
    "                 bias=True,\n",
    "                 nonlinearity='tanh')\n",
    "\n",
    "entry = torch.randn(6, 3, 10)\n",
    "h = torch.randn(3, 20)\n",
    "\n",
    "output = []\n",
    "for i in range(6):\n",
    "    # 前一个序列的输出h,作为下次序列的一个输入\n",
    "    h = rnn(entry[i], hx=h)  # 输入是(N, C)的张量,输出为h\n",
    "    output.append(h)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "# 双层循环神经网络RNN\n",
    "entry = torch.randn(10, 3, 100)\n",
    "\n",
    "cell1 = nn.RNNCell(100, 30)  # 第一层\n",
    "cell2 = nn.RNNCell(30, 10)  # 第二层\n",
    "h1 = torch.zeros(3, 30)\n",
    "h2 = torch.zeros(3, 10)\n",
    "\n",
    "for i in range(6):\n",
    "    h1 = cell1(entry[i], h1)\n",
    "    h2 = cell2(h1, h2)\n",
    "print(h2.shape)  # 最后一个序列的输出"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "# 双层循环神经网络LSTM\n",
    "entry = torch.randn(10, 3, 100)\n",
    "\n",
    "cell1 = nn.LSTMCell(input_size=100,\n",
    "                    hidden_size=30,\n",
    "                    bias=True)  # 第一层\n",
    "cell2 = nn.LSTMCell(30, 10)  # 第二层\n",
    "h1 = torch.zeros(3, 30)\n",
    "c1 = torch.ones(3, 30)\n",
    "h2 = torch.zeros(3, 10)\n",
    "c2 = torch.ones(3, 10)\n",
    "\n",
    "for i in range(6):\n",
    "    h1, c1 = cell1(entry[i], hx=(h1, c1))\n",
    "    h2, c2 = cell2(h1, hx=(h2, c2))\n",
    "print(h2.shape)  # 最后一个序列的输出"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "# 双层循环神经网络GRU\n",
    "entry = torch.randn(10, 3, 100)\n",
    "\n",
    "cell1 = nn.GRUCell(input_size=100,\n",
    "                   hidden_size=30,\n",
    "                   bias=True)  # 第一层\n",
    "cell2 = nn.GRUCell(30, 10)  # 第二层\n",
    "h1 = torch.zeros(3, 30)\n",
    "h2 = torch.zeros(3, 10)\n",
    "\n",
    "for i in range(6):\n",
    "    h1 = cell1(entry[i], hx=h1)\n",
    "    h2 = cell2(h1, h2)\n",
    "print(h2.shape)  # 最后一个序列的输出"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}