{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### bert模型及其组件"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 24])\n"
     ]
    }
   ],
   "source": [
    "def masked_softmax(X, valid_lens=None):\n",
    "    \"\"\"通过在最后⼀个轴上遮蔽元素来执⾏softmax操作\"\"\"\n",
    "\n",
    "    def sequence_mask(X, valid_len, value=0):\n",
    "        \"\"\"Mask irrelevant entries in sequences\"\"\"\n",
    "        maxlen = X.size(1)\n",
    "        # 广播机制\n",
    "        mask = torch.arange(maxlen, device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = value\n",
    "        return X\n",
    "\n",
    "    if valid_lens is None:\n",
    "        return F.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # 被遮蔽的元素使用⼀个非常大的负值替换,使其softmax输出为0\n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
    "                          value=-1e6)\n",
    "        return F.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力\"\"\"\n",
    "\n",
    "    def __init__(self, dropout):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        # queries.shape = (b, ?q, d)\n",
    "        # keys.shape = (b, ?k, d)\n",
    "        # scores.shape = (b, ?q, d) x (b, d, ?k) = (b, ?q, ?k)\n",
    "        d = queries.shape[-1]\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # values.shape=(b, ?k, ?v)\n",
    "        # 返回值.shape=(b, ?q, ?k) x (b, ?k, ?v) = (b, ?q, ?v)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # 查询特征数目(E_q)\n",
    "                 query_size,\n",
    "                 # 键特征数目(E_k)\n",
    "                 key_size,\n",
    "                 # 值特征数目(E_v)\n",
    "                 value_size,\n",
    "                 # 多头数目\n",
    "                 num_heads, dropout, bias=False):  # 模仿pytorch的参数组成\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert query_size % num_heads == 0, \"query_size must be divisible by num_heads\"\n",
    "        self.W_q = nn.Linear(query_size, query_size, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, query_size, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, query_size, bias=bias)\n",
    "        self.W_o = nn.Linear(query_size, query_size, bias=bias)\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def transpose_qkv(X, num_heads):\n",
    "        # 输入:X.shape=(N, L or S, E_q)\n",
    "        # X.shape=(N, L or S, num_heads, E_q / num_heads)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "        # X.shape=(N, num_heads, L or S, E_q / num_heads)\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        # 返回值.shape=(N * num_heads, L or S, E_q / num_heads)\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        \"\"\"\n",
    "        queries: 查询\n",
    "        keys: 键\n",
    "        values: 值\n",
    "        valid_lens: 计算attention_weights的有效长度\n",
    "        \"\"\"\n",
    "        # queries.shape=(N, L, E_q)\n",
    "        # self.W_q(queries).shape=(N, L, E_q)\n",
    "        # queries.shape=(N * num_heads, L, E_q / num_heads)\n",
    "\n",
    "        # keys.shape=(N, S, E_k)\n",
    "        # self.W_k(queries).shape=(N, S, E_q)\n",
    "        # keys.shape=(N * num_heads, S, E_q / num_heads)\n",
    "\n",
    "        # values.shape=(N, S, E_v)\n",
    "        # self.W_v(values).shape=(N, S, E_q)\n",
    "        # values.shape=(N * num_heads, S, E_q / num_heads)\n",
    "        queries = self.transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = self.transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = self.transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # E_q维度信息增加到batch_size维度上\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # output.shape=(N * num_heads, L, E_q / num_heads)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # output.shape=(N, num_heads, L, E_q / num_heads)\n",
    "        output = output.reshape(-1, self.num_heads, output.shape[1], output.shape[2])\n",
    "        # output.shape=(N, L, num_heads, E_q / num_heads)\n",
    "        output = output.permute(0, 2, 1, 3)\n",
    "        # output.shape=(N, L, E_q)\n",
    "        output_concat = output.reshape(output.shape[0], output.shape[1], -1)\n",
    "        # 返回值.shape=(N, L, E_q)\n",
    "        return self.W_o(output_concat)\n",
    "\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"残差连接和层归一化\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # 除mini-batch维度之外其他维度的列表(即进行层归一化的维度)\n",
    "                 normalized_shape,\n",
    "                 dropout):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 原因:句子长度不一致,并且各个batch的信息没什么关系\n",
    "        self.ln = nn.LayerNorm(normalized_shape)  # Normalized_shape is input.size()[1:]\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        # self.ln内部为残差连接\n",
    "        return self.ln(self.dropout(Y) + X)\n",
    "\n",
    "\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Networks\"\"\"\n",
    "\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs):\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"transformer编码器Block\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # 查询特征数目(E_q)\n",
    "                 query_size,\n",
    "                 # 键特征数目(E_k)\n",
    "                 key_size,\n",
    "                 # 值特征数目(E_v)\n",
    "                 value_size,\n",
    "                 # 除mini-batch维度之外其他维度的列表(即进行层归一化的维度)\n",
    "                 norm_shape,\n",
    "                 ffn_num_hiddens,\n",
    "                 # 多头数\n",
    "                 num_heads, dropout, use_bias=False):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(query_size, key_size, value_size, num_heads, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        # 多头注意力`forward`返回值的shape为:(N, L, E_q)\n",
    "        # 故PositionWiseFFN第一个线性层的输入维度为E_q\n",
    "        self.ffn = PositionWiseFFN(query_size, ffn_num_hiddens, query_size)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        # transformer编码器Block结构为:\n",
    "        # =+残差连接=>多头注意力==>add & Norm=+残差连接=>Feed Forward=>add & Norm\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))  # 多头自注意力\n",
    "        return self.addnorm2(Y, self.ffn(Y))\n",
    "\n",
    "\n",
    "encoder_blk = EncoderBlock(query_size=24,\n",
    "                           key_size=24,\n",
    "                           value_size=24,\n",
    "                           norm_shape=[100, 24],\n",
    "                           ffn_num_hiddens=48,\n",
    "                           num_heads=8,\n",
    "                           dropout=0.5)\n",
    "encoder_blk.eval()\n",
    "\n",
    "X = torch.ones((2, 100, 24))\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "\n",
    "# 可以看出Transformer编码器中的任何层都不会改变其输⼊的形状(可叠加多层)\n",
    "print(encoder_blk(X, valid_lens).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 8, 768])"
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT encoder\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # 单词表的单词数目\n",
    "                 vocab_size,\n",
    "                 # 除mini-batch维度之外其他维度的列表(即进行层归一化的维度)\n",
    "                 norm_shape,\n",
    "                 # PositionWiseFFN层隐藏层大小\n",
    "                 ffn_num_hiddens,\n",
    "                 # 多头数\n",
    "                 num_heads,\n",
    "                 # bert编码器Block数\n",
    "                 num_layers,\n",
    "                 dropout,\n",
    "                 # 支持的最大序列长度\n",
    "                 max_len=1000,\n",
    "                 key_size=768, query_size=768, value_size=768):\n",
    "        super(BERTEncoder, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, query_size)\n",
    "        self.segment_embedding = nn.Embedding(2, query_size)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\", EncoderBlock(\n",
    "                query_size=query_size,\n",
    "                key_size=key_size,\n",
    "                value_size=value_size,\n",
    "                norm_shape=norm_shape,\n",
    "                num_heads=num_heads,\n",
    "                ffn_num_hiddens=ffn_num_hiddens,\n",
    "                dropout=dropout,\n",
    "                use_bias=True))\n",
    "\n",
    "        # BERT中,位置嵌⼊是可学习的\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, query_size))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # tokens.shape=(N,  T)\n",
    "        # segments.shape=(N, T)\n",
    "        # X.shape=(N, T, vocab_size)\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        # 返回值:X.shape=(N, T, vocab_size)\n",
    "        return X\n",
    "\n",
    "\n",
    "vocab_size, ffn_num_hiddens, num_heads = 10000, 1024, 4\n",
    "norm_shape, num_layers, dropout = [768], 2, 0.2\n",
    "encoder = BERTEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    norm_shape=norm_shape,\n",
    "    ffn_num_hiddens=ffn_num_hiddens,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout)\n",
    "\n",
    "tokens = torch.randint(0, vocab_size, (2, 8))  # 序列长度为8\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "# 不会改变其输⼊的形状\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 10000])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    \"\"\"BERT的遮蔽语⾔模型任务\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768):\n",
    "        super(MaskLM, self).__init__()\n",
    "        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.Linear(num_hiddens, vocab_size))\n",
    "\n",
    "    def forward(self,\n",
    "                # BERTEncoder的编码结果\n",
    "                X,\n",
    "                # 用于预测的词元位置\n",
    "                pred_positions):\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        # 假设batch_size=2, num_pred_positions=3\n",
    "        # 则batch_idx为tensor([0, 0, 0, 1, 1, 1])\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "        # 整数索引\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        # 输入:X.shaep=(N, L, query_size)\n",
    "        # 输出:mlm_Y_hat.shape=(N, len(flatten()), vocab_size)\n",
    "        return mlm_Y_hat\n",
    "\n",
    "\n",
    "mlm = MaskLM(10000, 768)\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)  # encoded_X.shape=(2, 8, 768)\n",
    "print(mlm_Y_hat.shape)\n",
    "\n",
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))  # 计算损失\n",
    "print(mlm_l.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"BERT的下⼀句预测任务\"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs):\n",
    "        super(NextSentencePred, self).__init__()\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X.shape=(batch size, hum_hiddens)\n",
    "        return self.output(X)\n",
    "\n",
    "\n",
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "nsp = NextSentencePred(encoded_X.shape[-1])\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "print(nsp_Y_hat.shape)\n",
    "\n",
    "nsp_y = torch.tensor([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "print(nsp_l.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 3, 10000])\n",
      "tensor([[-0.0104,  0.1527]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    \"\"\"BERT模型\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # 单词表的单词数目\n",
    "                 vocab_size,\n",
    "                 num_hiddens,\n",
    "                 # 除mini-batch维度之外其他维度的列表(即进行层归一化的维度)\n",
    "                 norm_shape,\n",
    "                 ffn_num_hiddens,\n",
    "                 # 多头数\n",
    "                 num_heads,\n",
    "                 # bert编码器Block数\n",
    "                 num_layers,\n",
    "                 dropout,\n",
    "                 max_len=1000,\n",
    "                 key_size=768, query_size=768, value_size=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            norm_shape=norm_shape,\n",
    "            ffn_num_hiddens=ffn_num_hiddens,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            max_len=max_len,\n",
    "            key_size=key_size,\n",
    "            query_size=query_size,\n",
    "            value_size=value_size)\n",
    "        self.mlm = MaskLM(\n",
    "            vocab_size=vocab_size,\n",
    "            num_hiddens=num_hiddens,\n",
    "            num_inputs=query_size)\n",
    "        self.hidden = nn.Sequential(nn.Linear(query_size, num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.nsp = NextSentencePred(num_hiddens)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        # tokens.shape=(N, T)\n",
    "        # segments.shape=(N, T)\n",
    "        # encoded_X.shape=(N, T, query_size)\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            # mlm_Y_hat.shape=(N, len(flatten()), vocab_size)\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # encoded_X[:, 0, :].shape=(N, query_size)\n",
    "        # self.hidden(encoded_X[:, 0, :])).shape=(N, num_hiddens)\n",
    "        # nsp_Y_hat.shape=(N, 2)\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))  # 0为\"<cls>\"标记的索引\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat\n",
    "\n",
    "\n",
    "vocab_size, ffn_num_hiddens, num_heads = 10000, 1024, 4\n",
    "norm_shape, num_layers, dropout = [768], 2, 0.2\n",
    "\n",
    "bert_model = BERTModel(vocab_size=vocab_size,\n",
    "                       num_hiddens=1024,\n",
    "                       norm_shape=norm_shape,\n",
    "                       ffn_num_hiddens=ffn_num_hiddens,\n",
    "                       num_heads=num_heads,\n",
    "                       num_layers=num_layers,\n",
    "                       dropout=dropout)\n",
    "\n",
    "tokens = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8]])\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1]])\n",
    "mlm_positions = torch.tensor([[1, 5, 2]])\n",
    "\n",
    "encoded_X, mlm_Y_hat, nsp_Y_hat = bert_model(tokens, segments, pred_positions=mlm_positions)\n",
    "print(encoded_X.shape)\n",
    "print(mlm_Y_hat.shape)\n",
    "print(nsp_Y_hat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 数据处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "outputs": [
    {
     "data": {
      "text/plain": "[['shortly before the third leg of the <unk> tour , the september 11 attacks occurred in new york city and washington d.c. during the band \\'s first show in new york city following the attacks , the band performed \" where the streets have no name \" , and when the stage lights illuminated the audience , the band saw tears streaming down the faces of many fans',\n  'the experience was one inspiration for the song \" city of <unk> lights \"',\n  'the band paid tribute to the 9 / 11 victims during their performance of the song at the super bowl <unk> halftime show on 3 february 2002',\n  'the performance featured the names of the september 11 victims projected onto a large white banner behind the band',\n  'u2 \\'s appearance was later ranked number 1 on sports illustrated \\'s list of \" top 10 super bowl <unk> <unk> \" .'],\n ['for australians and new zealanders the gallipoli campaign came to symbolise an important milestone in the emergence of both nations as independent actors on the world stage and the development of a sense of national identity',\n  'today , the date of the initial landings , 25 april , is known as anzac day in australia and new zealand and every year thousands of people gather at memorials in both nations , as well as turkey , to honour the bravery and sacrifice of the original <unk> , and of all those who have subsequently lost their lives in war .'],\n ['in july 2014 the international astronomical union launched a process for giving proper names to certain exoplanets and their host stars',\n  'the process involved public nomination and voting for the new names',\n  'in december 2015 , the iau announced the winning name was <unk> for this planet',\n  'the winning name was submitted by the vega <unk> club of morocco and honours the 11th century astronomer ibn al @-@ <unk> of muslim spain .'],\n ['the australian mainland came under attack during 1942 and 1943',\n  'japanese submarines operated off australia from may to august 1942 and january to june 1943',\n  'these attacks sought to cut the allied supply lines between australia and the us and australia and new guinea , but were unsuccessful',\n  'japanese aircraft also conducted air raids against allied bases in northern australia which were being used to mount the north western area campaign against japanese positions in the netherlands east indies ( <unk> ) .'],\n ['returning to australia , <unk> became deputy chief of the air staff ( <unk> ) on 26 january 1965',\n  'he was appointed a companion of the order of the bath ( cb ) \" in recognition of distinguished service in the borneo territories \" on 22 june',\n  \"his tenure as <unk> coincided with the most significant <unk> program the air force had undertaken since world war ii , and with manpower shortages stemming from this expansion and from australia 's increasing involvement in the security of south east asia\",\n  'the first raaf helicopters were committed to the vietnam war towards the end of his term , and he travelled to saigon with the chief of the general staff , lieutenant general sir john <unk> , in march 1966 to plan the deployment',\n  'the year before , <unk> had recommended to air marshal murdoch , the chief of the air staff , that two iroquois be sent to vietnam for <unk> purposes ; murdoch had rebuffed <unk> , and the raaf helicopter squadron was considered <unk> for its army co @-@ operation role when it finally did deploy',\n  '<unk> succeeded air vice marshal douglas candy as aoc support command , melbourne , on 8 august 1966',\n  \"support command had been formed in 1959 , by merging the raaf 's former training and maintenance <unk>\",\n  'on 1 january 1968 , <unk> was posted to london as head of the australian joint services staff',\n  'he served as an extra gentleman usher to the queen from 17 november 1970 to 5 june 1971 .'],\n ['richmond started 1982 without a ride before getting a one @-@ race deal to drive for billie harvey at the <unk> track',\n  'richmond completed 112 laps of the <unk> @-@ lap event to finish 31st , retiring from the race with engine problems',\n  \"for the following event , richmond was hired to drive <unk> stacy 's no. 2 car\",\n  'in his first race for the team , richmond earned his first career top 5 finish when he placed fifth at darlington <unk>',\n  'returning to <unk> , he finished second , before winning his first race on the road course at riverside , california the following week',\n  'later that season , he earned his first pole position at bristol',\n  'the tour returned to riverside for the final race of the season where richmond won his second race , sweeping both events at the track',\n  'benny parsons said that \" watching richmond go through the <unk> was unbelievable \"',\n  'for the season , richmond had twelve top 10s , two wins , and one pole to finish 26th in points .'],\n ['the <unk> were removed from the stone floor , which was then repaired',\n  'one of the main beams was found to have been attacked by death watch beetles , and over a third of the <unk> beam was replaced with oak',\n  'the six supports for the <unk> frame were repaired one at a time , as they bore the whole of the weight of the machinery above',\n  'one post required replacement',\n  'it had <unk> and the floor had sunk by 4 inches ( 100 mm ) causing the second main floor beam to fracture',\n  'when this work had been completed , the stone floor was removed , including the second main floor beam , which was removed in one piece so that the most suitable route to install its replacement could be assessed .'],\n ['<unk> <unk> president joseph <unk> iii convinced richmond to make the change to stock car racing on the <unk> circuit',\n  'richmond made his first <unk> start two months after winning the indianapolis 500 rookie of the year award',\n  'he debuted at the coca @-@ cola 500 at <unk> on july 27 , 1980 , finishing 12th in a d. k. <unk> @-@ owned <unk>',\n  'that season , he competed in five events , with two <unk> ( did not finish ) and three 12th @-@ place finishes',\n  'overall , he finished the 1980 season 41st in points .'],\n ['the <unk> cap sits on top of the tower , giving the mill an overall height of 45 feet ( 13 @.@ 72 m ) to the <unk>',\n  'it houses the cast @-@ iron <unk> and 7 feet 2 inches ( 2 @.@ 18 m ) diameter wooden brake wheel internally',\n  '<unk> the four double patent sails span 64 feet ( 19 @.@ 51 m )',\n  'they are 9 feet ( 2 @.@ 74 m ) wide and can develop 30 horsepower ( 22 kw )',\n  'the eight <unk> <unk> keeps the mill turned into wind .'],\n ['the prevailing winds are from the west @-@ southwest , and they are normally strongest in march and april',\n  '<unk> , intense thunderstorms are common between april and october',\n  '<unk> in the summer months tend to be more isolated and often produce dry lightning strikes .']]"
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _read_wiki(file_name):\n",
    "    with open(file_name, 'r', encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # 在WikiText-2数据集中,每行代表⼀个段落\n",
    "    # 保留至少有两句话的段落\n",
    "    # 大写字母转换为小写字母\n",
    "    # 使用句号作为分隔符来拆分句子\n",
    "    paragraphs = [line.strip().lower().split(' . ') for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    # paragraphs[0]表示一个段落\n",
    "    # paragraphsp[0][0]表示段落的第一句话\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "file_name = \"WikiText2/wiki.valid.tokens\"\n",
    "test_read_wiki = _read_wiki(file_name)[0:10]\n",
    "test_read_wiki"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "outputs": [
    {
     "data": {
      "text/plain": "[[['shortly',\n   'before',\n   'the',\n   'third',\n   'leg',\n   'of',\n   'the',\n   '<unk>',\n   'tour',\n   ',',\n   'the',\n   'september',\n   '11',\n   'attacks',\n   'occurred',\n   'in',\n   'new',\n   'york',\n   'city',\n   'and',\n   'washington',\n   'd.c.',\n   'during',\n   'the',\n   'band',\n   \"'s\",\n   'first',\n   'show',\n   'in',\n   'new',\n   'york',\n   'city',\n   'following',\n   'the',\n   'attacks',\n   ',',\n   'the',\n   'band',\n   'performed',\n   '\"',\n   'where',\n   'the',\n   'streets',\n   'have',\n   'no',\n   'name',\n   '\"',\n   ',',\n   'and',\n   'when',\n   'the',\n   'stage',\n   'lights',\n   'illuminated',\n   'the',\n   'audience',\n   ',',\n   'the',\n   'band',\n   'saw',\n   'tears',\n   'streaming',\n   'down',\n   'the',\n   'faces',\n   'of',\n   'many',\n   'fans'],\n  ['the',\n   'experience',\n   'was',\n   'one',\n   'inspiration',\n   'for',\n   'the',\n   'song',\n   '\"',\n   'city',\n   'of',\n   '<unk>',\n   'lights',\n   '\"'],\n  ['the',\n   'band',\n   'paid',\n   'tribute',\n   'to',\n   'the',\n   '9',\n   '/',\n   '11',\n   'victims',\n   'during',\n   'their',\n   'performance',\n   'of',\n   'the',\n   'song',\n   'at',\n   'the',\n   'super',\n   'bowl',\n   '<unk>',\n   'halftime',\n   'show',\n   'on',\n   '3',\n   'february',\n   '2002'],\n  ['the',\n   'performance',\n   'featured',\n   'the',\n   'names',\n   'of',\n   'the',\n   'september',\n   '11',\n   'victims',\n   'projected',\n   'onto',\n   'a',\n   'large',\n   'white',\n   'banner',\n   'behind',\n   'the',\n   'band'],\n  ['u2',\n   \"'s\",\n   'appearance',\n   'was',\n   'later',\n   'ranked',\n   'number',\n   '1',\n   'on',\n   'sports',\n   'illustrated',\n   \"'s\",\n   'list',\n   'of',\n   '\"',\n   'top',\n   '10',\n   'super',\n   'bowl',\n   '<unk>',\n   '<unk>',\n   '\"',\n   '.']],\n [['for',\n   'australians',\n   'and',\n   'new',\n   'zealanders',\n   'the',\n   'gallipoli',\n   'campaign',\n   'came',\n   'to',\n   'symbolise',\n   'an',\n   'important',\n   'milestone',\n   'in',\n   'the',\n   'emergence',\n   'of',\n   'both',\n   'nations',\n   'as',\n   'independent',\n   'actors',\n   'on',\n   'the',\n   'world',\n   'stage',\n   'and',\n   'the',\n   'development',\n   'of',\n   'a',\n   'sense',\n   'of',\n   'national',\n   'identity'],\n  ['today',\n   ',',\n   'the',\n   'date',\n   'of',\n   'the',\n   'initial',\n   'landings',\n   ',',\n   '25',\n   'april',\n   ',',\n   'is',\n   'known',\n   'as',\n   'anzac',\n   'day',\n   'in',\n   'australia',\n   'and',\n   'new',\n   'zealand',\n   'and',\n   'every',\n   'year',\n   'thousands',\n   'of',\n   'people',\n   'gather',\n   'at',\n   'memorials',\n   'in',\n   'both',\n   'nations',\n   ',',\n   'as',\n   'well',\n   'as',\n   'turkey',\n   ',',\n   'to',\n   'honour',\n   'the',\n   'bravery',\n   'and',\n   'sacrifice',\n   'of',\n   'the',\n   'original',\n   '<unk>',\n   ',',\n   'and',\n   'of',\n   'all',\n   'those',\n   'who',\n   'have',\n   'subsequently',\n   'lost',\n   'their',\n   'lives',\n   'in',\n   'war',\n   '.']],\n [['in',\n   'july',\n   '2014',\n   'the',\n   'international',\n   'astronomical',\n   'union',\n   'launched',\n   'a',\n   'process',\n   'for',\n   'giving',\n   'proper',\n   'names',\n   'to',\n   'certain',\n   'exoplanets',\n   'and',\n   'their',\n   'host',\n   'stars'],\n  ['the',\n   'process',\n   'involved',\n   'public',\n   'nomination',\n   'and',\n   'voting',\n   'for',\n   'the',\n   'new',\n   'names'],\n  ['in',\n   'december',\n   '2015',\n   ',',\n   'the',\n   'iau',\n   'announced',\n   'the',\n   'winning',\n   'name',\n   'was',\n   '<unk>',\n   'for',\n   'this',\n   'planet'],\n  ['the',\n   'winning',\n   'name',\n   'was',\n   'submitted',\n   'by',\n   'the',\n   'vega',\n   '<unk>',\n   'club',\n   'of',\n   'morocco',\n   'and',\n   'honours',\n   'the',\n   '11th',\n   'century',\n   'astronomer',\n   'ibn',\n   'al',\n   '@-@',\n   '<unk>',\n   'of',\n   'muslim',\n   'spain',\n   '.']],\n [['the',\n   'australian',\n   'mainland',\n   'came',\n   'under',\n   'attack',\n   'during',\n   '1942',\n   'and',\n   '1943'],\n  ['japanese',\n   'submarines',\n   'operated',\n   'off',\n   'australia',\n   'from',\n   'may',\n   'to',\n   'august',\n   '1942',\n   'and',\n   'january',\n   'to',\n   'june',\n   '1943'],\n  ['these',\n   'attacks',\n   'sought',\n   'to',\n   'cut',\n   'the',\n   'allied',\n   'supply',\n   'lines',\n   'between',\n   'australia',\n   'and',\n   'the',\n   'us',\n   'and',\n   'australia',\n   'and',\n   'new',\n   'guinea',\n   ',',\n   'but',\n   'were',\n   'unsuccessful'],\n  ['japanese',\n   'aircraft',\n   'also',\n   'conducted',\n   'air',\n   'raids',\n   'against',\n   'allied',\n   'bases',\n   'in',\n   'northern',\n   'australia',\n   'which',\n   'were',\n   'being',\n   'used',\n   'to',\n   'mount',\n   'the',\n   'north',\n   'western',\n   'area',\n   'campaign',\n   'against',\n   'japanese',\n   'positions',\n   'in',\n   'the',\n   'netherlands',\n   'east',\n   'indies',\n   '(',\n   '<unk>',\n   ')',\n   '.']],\n [['returning',\n   'to',\n   'australia',\n   ',',\n   '<unk>',\n   'became',\n   'deputy',\n   'chief',\n   'of',\n   'the',\n   'air',\n   'staff',\n   '(',\n   '<unk>',\n   ')',\n   'on',\n   '26',\n   'january',\n   '1965'],\n  ['he',\n   'was',\n   'appointed',\n   'a',\n   'companion',\n   'of',\n   'the',\n   'order',\n   'of',\n   'the',\n   'bath',\n   '(',\n   'cb',\n   ')',\n   '\"',\n   'in',\n   'recognition',\n   'of',\n   'distinguished',\n   'service',\n   'in',\n   'the',\n   'borneo',\n   'territories',\n   '\"',\n   'on',\n   '22',\n   'june'],\n  ['his',\n   'tenure',\n   'as',\n   '<unk>',\n   'coincided',\n   'with',\n   'the',\n   'most',\n   'significant',\n   '<unk>',\n   'program',\n   'the',\n   'air',\n   'force',\n   'had',\n   'undertaken',\n   'since',\n   'world',\n   'war',\n   'ii',\n   ',',\n   'and',\n   'with',\n   'manpower',\n   'shortages',\n   'stemming',\n   'from',\n   'this',\n   'expansion',\n   'and',\n   'from',\n   'australia',\n   \"'s\",\n   'increasing',\n   'involvement',\n   'in',\n   'the',\n   'security',\n   'of',\n   'south',\n   'east',\n   'asia'],\n  ['the',\n   'first',\n   'raaf',\n   'helicopters',\n   'were',\n   'committed',\n   'to',\n   'the',\n   'vietnam',\n   'war',\n   'towards',\n   'the',\n   'end',\n   'of',\n   'his',\n   'term',\n   ',',\n   'and',\n   'he',\n   'travelled',\n   'to',\n   'saigon',\n   'with',\n   'the',\n   'chief',\n   'of',\n   'the',\n   'general',\n   'staff',\n   ',',\n   'lieutenant',\n   'general',\n   'sir',\n   'john',\n   '<unk>',\n   ',',\n   'in',\n   'march',\n   '1966',\n   'to',\n   'plan',\n   'the',\n   'deployment'],\n  ['the',\n   'year',\n   'before',\n   ',',\n   '<unk>',\n   'had',\n   'recommended',\n   'to',\n   'air',\n   'marshal',\n   'murdoch',\n   ',',\n   'the',\n   'chief',\n   'of',\n   'the',\n   'air',\n   'staff',\n   ',',\n   'that',\n   'two',\n   'iroquois',\n   'be',\n   'sent',\n   'to',\n   'vietnam',\n   'for',\n   '<unk>',\n   'purposes',\n   ';',\n   'murdoch',\n   'had',\n   'rebuffed',\n   '<unk>',\n   ',',\n   'and',\n   'the',\n   'raaf',\n   'helicopter',\n   'squadron',\n   'was',\n   'considered',\n   '<unk>',\n   'for',\n   'its',\n   'army',\n   'co',\n   '@-@',\n   'operation',\n   'role',\n   'when',\n   'it',\n   'finally',\n   'did',\n   'deploy'],\n  ['<unk>',\n   'succeeded',\n   'air',\n   'vice',\n   'marshal',\n   'douglas',\n   'candy',\n   'as',\n   'aoc',\n   'support',\n   'command',\n   ',',\n   'melbourne',\n   ',',\n   'on',\n   '8',\n   'august',\n   '1966'],\n  ['support',\n   'command',\n   'had',\n   'been',\n   'formed',\n   'in',\n   '1959',\n   ',',\n   'by',\n   'merging',\n   'the',\n   'raaf',\n   \"'s\",\n   'former',\n   'training',\n   'and',\n   'maintenance',\n   '<unk>'],\n  ['on',\n   '1',\n   'january',\n   '1968',\n   ',',\n   '<unk>',\n   'was',\n   'posted',\n   'to',\n   'london',\n   'as',\n   'head',\n   'of',\n   'the',\n   'australian',\n   'joint',\n   'services',\n   'staff'],\n  ['he',\n   'served',\n   'as',\n   'an',\n   'extra',\n   'gentleman',\n   'usher',\n   'to',\n   'the',\n   'queen',\n   'from',\n   '17',\n   'november',\n   '1970',\n   'to',\n   '5',\n   'june',\n   '1971',\n   '.']],\n [['richmond',\n   'started',\n   '1982',\n   'without',\n   'a',\n   'ride',\n   'before',\n   'getting',\n   'a',\n   'one',\n   '@-@',\n   'race',\n   'deal',\n   'to',\n   'drive',\n   'for',\n   'billie',\n   'harvey',\n   'at',\n   'the',\n   '<unk>',\n   'track'],\n  ['richmond',\n   'completed',\n   '112',\n   'laps',\n   'of',\n   'the',\n   '<unk>',\n   '@-@',\n   'lap',\n   'event',\n   'to',\n   'finish',\n   '31st',\n   ',',\n   'retiring',\n   'from',\n   'the',\n   'race',\n   'with',\n   'engine',\n   'problems'],\n  ['for',\n   'the',\n   'following',\n   'event',\n   ',',\n   'richmond',\n   'was',\n   'hired',\n   'to',\n   'drive',\n   '<unk>',\n   'stacy',\n   \"'s\",\n   'no.',\n   '2',\n   'car'],\n  ['in',\n   'his',\n   'first',\n   'race',\n   'for',\n   'the',\n   'team',\n   ',',\n   'richmond',\n   'earned',\n   'his',\n   'first',\n   'career',\n   'top',\n   '5',\n   'finish',\n   'when',\n   'he',\n   'placed',\n   'fifth',\n   'at',\n   'darlington',\n   '<unk>'],\n  ['returning',\n   'to',\n   '<unk>',\n   ',',\n   'he',\n   'finished',\n   'second',\n   ',',\n   'before',\n   'winning',\n   'his',\n   'first',\n   'race',\n   'on',\n   'the',\n   'road',\n   'course',\n   'at',\n   'riverside',\n   ',',\n   'california',\n   'the',\n   'following',\n   'week'],\n  ['later',\n   'that',\n   'season',\n   ',',\n   'he',\n   'earned',\n   'his',\n   'first',\n   'pole',\n   'position',\n   'at',\n   'bristol'],\n  ['the',\n   'tour',\n   'returned',\n   'to',\n   'riverside',\n   'for',\n   'the',\n   'final',\n   'race',\n   'of',\n   'the',\n   'season',\n   'where',\n   'richmond',\n   'won',\n   'his',\n   'second',\n   'race',\n   ',',\n   'sweeping',\n   'both',\n   'events',\n   'at',\n   'the',\n   'track'],\n  ['benny',\n   'parsons',\n   'said',\n   'that',\n   '\"',\n   'watching',\n   'richmond',\n   'go',\n   'through',\n   'the',\n   '<unk>',\n   'was',\n   'unbelievable',\n   '\"'],\n  ['for',\n   'the',\n   'season',\n   ',',\n   'richmond',\n   'had',\n   'twelve',\n   'top',\n   '10s',\n   ',',\n   'two',\n   'wins',\n   ',',\n   'and',\n   'one',\n   'pole',\n   'to',\n   'finish',\n   '26th',\n   'in',\n   'points',\n   '.']],\n [['the',\n   '<unk>',\n   'were',\n   'removed',\n   'from',\n   'the',\n   'stone',\n   'floor',\n   ',',\n   'which',\n   'was',\n   'then',\n   'repaired'],\n  ['one',\n   'of',\n   'the',\n   'main',\n   'beams',\n   'was',\n   'found',\n   'to',\n   'have',\n   'been',\n   'attacked',\n   'by',\n   'death',\n   'watch',\n   'beetles',\n   ',',\n   'and',\n   'over',\n   'a',\n   'third',\n   'of',\n   'the',\n   '<unk>',\n   'beam',\n   'was',\n   'replaced',\n   'with',\n   'oak'],\n  ['the',\n   'six',\n   'supports',\n   'for',\n   'the',\n   '<unk>',\n   'frame',\n   'were',\n   'repaired',\n   'one',\n   'at',\n   'a',\n   'time',\n   ',',\n   'as',\n   'they',\n   'bore',\n   'the',\n   'whole',\n   'of',\n   'the',\n   'weight',\n   'of',\n   'the',\n   'machinery',\n   'above'],\n  ['one', 'post', 'required', 'replacement'],\n  ['it',\n   'had',\n   '<unk>',\n   'and',\n   'the',\n   'floor',\n   'had',\n   'sunk',\n   'by',\n   '4',\n   'inches',\n   '(',\n   '100',\n   'mm',\n   ')',\n   'causing',\n   'the',\n   'second',\n   'main',\n   'floor',\n   'beam',\n   'to',\n   'fracture'],\n  ['when',\n   'this',\n   'work',\n   'had',\n   'been',\n   'completed',\n   ',',\n   'the',\n   'stone',\n   'floor',\n   'was',\n   'removed',\n   ',',\n   'including',\n   'the',\n   'second',\n   'main',\n   'floor',\n   'beam',\n   ',',\n   'which',\n   'was',\n   'removed',\n   'in',\n   'one',\n   'piece',\n   'so',\n   'that',\n   'the',\n   'most',\n   'suitable',\n   'route',\n   'to',\n   'install',\n   'its',\n   'replacement',\n   'could',\n   'be',\n   'assessed',\n   '.']],\n [['<unk>',\n   '<unk>',\n   'president',\n   'joseph',\n   '<unk>',\n   'iii',\n   'convinced',\n   'richmond',\n   'to',\n   'make',\n   'the',\n   'change',\n   'to',\n   'stock',\n   'car',\n   'racing',\n   'on',\n   'the',\n   '<unk>',\n   'circuit'],\n  ['richmond',\n   'made',\n   'his',\n   'first',\n   '<unk>',\n   'start',\n   'two',\n   'months',\n   'after',\n   'winning',\n   'the',\n   'indianapolis',\n   '500',\n   'rookie',\n   'of',\n   'the',\n   'year',\n   'award'],\n  ['he',\n   'debuted',\n   'at',\n   'the',\n   'coca',\n   '@-@',\n   'cola',\n   '500',\n   'at',\n   '<unk>',\n   'on',\n   'july',\n   '27',\n   ',',\n   '1980',\n   ',',\n   'finishing',\n   '12th',\n   'in',\n   'a',\n   'd.',\n   'k.',\n   '<unk>',\n   '@-@',\n   'owned',\n   '<unk>'],\n  ['that',\n   'season',\n   ',',\n   'he',\n   'competed',\n   'in',\n   'five',\n   'events',\n   ',',\n   'with',\n   'two',\n   '<unk>',\n   '(',\n   'did',\n   'not',\n   'finish',\n   ')',\n   'and',\n   'three',\n   '12th',\n   '@-@',\n   'place',\n   'finishes'],\n  ['overall',\n   ',',\n   'he',\n   'finished',\n   'the',\n   '1980',\n   'season',\n   '41st',\n   'in',\n   'points',\n   '.']],\n [['the',\n   '<unk>',\n   'cap',\n   'sits',\n   'on',\n   'top',\n   'of',\n   'the',\n   'tower',\n   ',',\n   'giving',\n   'the',\n   'mill',\n   'an',\n   'overall',\n   'height',\n   'of',\n   '45',\n   'feet',\n   '(',\n   '13',\n   '@.@',\n   '72',\n   'm',\n   ')',\n   'to',\n   'the',\n   '<unk>'],\n  ['it',\n   'houses',\n   'the',\n   'cast',\n   '@-@',\n   'iron',\n   '<unk>',\n   'and',\n   '7',\n   'feet',\n   '2',\n   'inches',\n   '(',\n   '2',\n   '@.@',\n   '18',\n   'm',\n   ')',\n   'diameter',\n   'wooden',\n   'brake',\n   'wheel',\n   'internally'],\n  ['<unk>',\n   'the',\n   'four',\n   'double',\n   'patent',\n   'sails',\n   'span',\n   '64',\n   'feet',\n   '(',\n   '19',\n   '@.@',\n   '51',\n   'm',\n   ')'],\n  ['they',\n   'are',\n   '9',\n   'feet',\n   '(',\n   '2',\n   '@.@',\n   '74',\n   'm',\n   ')',\n   'wide',\n   'and',\n   'can',\n   'develop',\n   '30',\n   'horsepower',\n   '(',\n   '22',\n   'kw',\n   ')'],\n  ['the',\n   'eight',\n   '<unk>',\n   '<unk>',\n   'keeps',\n   'the',\n   'mill',\n   'turned',\n   'into',\n   'wind',\n   '.']],\n [['the',\n   'prevailing',\n   'winds',\n   'are',\n   'from',\n   'the',\n   'west',\n   '@-@',\n   'southwest',\n   ',',\n   'and',\n   'they',\n   'are',\n   'normally',\n   'strongest',\n   'in',\n   'march',\n   'and',\n   'april'],\n  ['<unk>',\n   ',',\n   'intense',\n   'thunderstorms',\n   'are',\n   'common',\n   'between',\n   'april',\n   'and',\n   'october'],\n  ['<unk>',\n   'in',\n   'the',\n   'summer',\n   'months',\n   'tend',\n   'to',\n   'be',\n   'more',\n   'isolated',\n   'and',\n   'often',\n   'produce',\n   'dry',\n   'lightning',\n   'strikes',\n   '.']]]"
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    \"\"\"Split text lines into word or character tokens\"\"\"\n",
    "    if token == 'word':\n",
    "        # 根据空格切分句子\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type: ' + token)\n",
    "\n",
    "\n",
    "# paragraphs_tokenize[0]表示一个段落\n",
    "# paragraphs_tokenize[0][0]表示该段落的第一句话\n",
    "# paragraphs_tokenize[0][0][0]表示该段落的第一句话的第一个单词\n",
    "test_tokenize = [tokenize(paragraph, token='word') for paragraph in test_read_wiki]\n",
    "test_tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "outputs": [],
   "source": [
    "def count_corpus(tokens):\n",
    "    \"\"\"Count token frequencies\"\"\"\n",
    "    # Here `tokens` is a 1D list or 2D list\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # Flatten a list of token lists into a list of tokens\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text\"\"\"\n",
    "\n",
    "    def __init__(self, tokens=None,\n",
    "                 # The minimum frequency needed to include a token in the vocabulary.\n",
    "                 min_freq=2,\n",
    "                 reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        counter = count_corpus(tokens)\n",
    "        # Sort according to frequencies\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {\n",
    "            token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        \"\"\"Index for the unknown token\"\"\"\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "outputs": [],
   "source": [
    "def _get_next_sentence(\n",
    "        # 句子(列表表示)\n",
    "        sentence,\n",
    "        # sentence的下一个句子(列表表示)\n",
    "        next_sentence,\n",
    "        # 段落集合\n",
    "        paragraphs):\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        is_next = False\n",
    "        while True:\n",
    "            # paragraphs是三重列表的嵌套(故进行两次random.choice)\n",
    "            temp = random.choice(random.choice(paragraphs))\n",
    "            if temp != next_sentence:\n",
    "                # 此时next_sentence不再为sentence的下一句\n",
    "                next_sentence = temp\n",
    "                break\n",
    "    return sentence, next_sentence, is_next"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "当输⼊为单个文本时,BERT输⼊序列是特殊类别词元\"<cls>\"、文本序列的标记、以及特殊分隔词元\"<sep>\"的连结.\n",
    "当输⼊为文本对时,BERT输⼊序列是\"<cls>\"、第一个文本序列的标记、\"<sep>\"、第二个文本序列标记、以及\"<sep>\"的连结\n",
    "\n",
    "BERT输⼊序列的嵌⼊是词元嵌⼊、片段嵌⼊和位置嵌⼊的和,如下图所示:\n",
    "\n",
    "<img src=\"../../../Other/img/Bert输入.jpg\"  style=\"width:1000px;height:300px;float:bottom\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<cls>', 'a', 'b', 'c', 'd', '<sep>', 'A', 'B', 'C', '<sep>']\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"获取输⼊序列的词元及其⽚段索引\"\"\"\n",
    "    # 使用0标记片段A\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "\n",
    "    # 使用1标记片段B\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments\n",
    "\n",
    "\n",
    "tokens_1 = ['a', 'b', 'c', 'd']\n",
    "tokens_2 = ['A', 'B', 'C']\n",
    "tokens, segments = get_tokens_and_segments(tokens_1, tokens_2)\n",
    "\n",
    "print(tokens)\n",
    "print(segments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "outputs": [
    {
     "data": {
      "text/plain": "[(['<cls>',\n   'shortly',\n   'before',\n   'the',\n   'third',\n   'leg',\n   'of',\n   'the',\n   '<unk>',\n   'tour',\n   ',',\n   'the',\n   'september',\n   '11',\n   'attacks',\n   'occurred',\n   'in',\n   'new',\n   'york',\n   'city',\n   'and',\n   'washington',\n   'd.c.',\n   'during',\n   'the',\n   'band',\n   \"'s\",\n   'first',\n   'show',\n   'in',\n   'new',\n   'york',\n   'city',\n   'following',\n   'the',\n   'attacks',\n   ',',\n   'the',\n   'band',\n   'performed',\n   '\"',\n   'where',\n   'the',\n   'streets',\n   'have',\n   'no',\n   'name',\n   '\"',\n   ',',\n   'and',\n   'when',\n   'the',\n   'stage',\n   'lights',\n   'illuminated',\n   'the',\n   'audience',\n   ',',\n   'the',\n   'band',\n   'saw',\n   'tears',\n   'streaming',\n   'down',\n   'the',\n   'faces',\n   'of',\n   'many',\n   'fans',\n   '<sep>',\n   'the',\n   'experience',\n   'was',\n   'one',\n   'inspiration',\n   'for',\n   'the',\n   'song',\n   '\"',\n   'city',\n   'of',\n   '<unk>',\n   'lights',\n   '\"',\n   '<sep>'],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n (['<cls>',\n   'the',\n   'experience',\n   'was',\n   'one',\n   'inspiration',\n   'for',\n   'the',\n   'song',\n   '\"',\n   'city',\n   'of',\n   '<unk>',\n   'lights',\n   '\"',\n   '<sep>',\n   'that',\n   'season',\n   ',',\n   'he',\n   'competed',\n   'in',\n   'five',\n   'events',\n   ',',\n   'with',\n   'two',\n   '<unk>',\n   '(',\n   'did',\n   'not',\n   'finish',\n   ')',\n   'and',\n   'three',\n   '12th',\n   '@-@',\n   'place',\n   'finishes',\n   '<sep>'],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n (['<cls>',\n   'the',\n   'band',\n   'paid',\n   'tribute',\n   'to',\n   'the',\n   '9',\n   '/',\n   '11',\n   'victims',\n   'during',\n   'their',\n   'performance',\n   'of',\n   'the',\n   'song',\n   'at',\n   'the',\n   'super',\n   'bowl',\n   '<unk>',\n   'halftime',\n   'show',\n   'on',\n   '3',\n   'february',\n   '2002',\n   '<sep>',\n   'for',\n   'the',\n   'following',\n   'event',\n   ',',\n   'richmond',\n   'was',\n   'hired',\n   'to',\n   'drive',\n   '<unk>',\n   'stacy',\n   \"'s\",\n   'no.',\n   '2',\n   'car',\n   '<sep>'],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n (['<cls>',\n   'the',\n   'performance',\n   'featured',\n   'the',\n   'names',\n   'of',\n   'the',\n   'september',\n   '11',\n   'victims',\n   'projected',\n   'onto',\n   'a',\n   'large',\n   'white',\n   'banner',\n   'behind',\n   'the',\n   'band',\n   '<sep>',\n   'u2',\n   \"'s\",\n   'appearance',\n   'was',\n   'later',\n   'ranked',\n   'number',\n   '1',\n   'on',\n   'sports',\n   'illustrated',\n   \"'s\",\n   'list',\n   'of',\n   '\"',\n   'top',\n   '10',\n   'super',\n   'bowl',\n   '<unk>',\n   '<unk>',\n   '\"',\n   '.',\n   '<sep>'],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True)]"
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_nsp_data_from_paragraph(\n",
    "        # 段落\n",
    "        paragraph,\n",
    "        # 段落集合\n",
    "        paragraphs,\n",
    "        # 指定预训练期间的BERT输⼊序列的最大长度\n",
    "        max_len):\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_a_next, is_next = _get_next_sentence(paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # 输入序列包含1个'<cls>'词元和2个'<sep>'词元\n",
    "        if len(tokens_a) + len(tokens_a_next) + 3 > max_len:\n",
    "            continue\n",
    "        tokens, segments = get_tokens_and_segments(tokens_a, tokens_a_next)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph\n",
    "\n",
    "\n",
    "test_get_nsp_data_from_paragraph = _get_nsp_data_from_paragraph(test_tokenize[0], test_tokenize, 1024)\n",
    "test_get_nsp_data_from_paragraph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "预训练任务中,将随机选择15%的词元作为预测的遮蔽词元.要预测⼀个遮蔽词元而不使⽤标签作弊,\n",
    "一个简单的方法是用一个特殊的\"<mask>\"替换输⼊序列中的词元.然而,人造特殊词元\"<mask>\"不会出现在微调中.\n",
    "为了避免预训练和微调之间的这种不匹配,如果为预测而屏蔽词元(例如,在\"this movie is great\"中选择遮蔽和预测\"great\"),则在输⼊中将其替换为:\n",
    "\n",
    "    * 80%时间为特殊的\"<mask>\"词元(例如,\"this movie is great\"变为\"this movie is <mask>\"\n",
    "    * 10%时间为随机词元(例如,\"this movie is great\"变为\"this movie is drink\")\n",
    "    * 10%时间内为不变的标签词元(例如,\"this movie is great\"变为\"this movie is great\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "outputs": [
    {
     "data": {
      "text/plain": "[([3,\n   0,\n   0,\n   5,\n   0,\n   0,\n   7,\n   5,\n   0,\n   0,\n   6,\n   7,\n   0,\n   0,\n   2,\n   0,\n   10,\n   30,\n   2,\n   0,\n   8,\n   2,\n   0,\n   0,\n   5,\n   35,\n   31,\n   26,\n   0,\n   10,\n   2,\n   0,\n   0,\n   0,\n   5,\n   2,\n   6,\n   5,\n   35,\n   0,\n   14,\n   0,\n   5,\n   0,\n   0,\n   0,\n   0,\n   14,\n   6,\n   20,\n   0,\n   5,\n   0,\n   0,\n   0,\n   5,\n   0,\n   2,\n   5,\n   35,\n   0,\n   0,\n   0,\n   0,\n   5,\n   0,\n   7,\n   0,\n   0,\n   4,\n   37,\n   38,\n   2,\n   22,\n   0,\n   10,\n   0,\n   0,\n   6,\n   33,\n   0,\n   0,\n   2,\n   0,\n   33,\n   0,\n   18,\n   8,\n   2,\n   0,\n   2,\n   0,\n   0,\n   4],\n  [11, 14, 18, 21, 30, 35, 49, 57, 64, 72, 82, 84, 88, 90],\n  [5, 0, 0, 0, 30, 0, 8, 6, 5, 6, 17, 0, 0, 21],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   2,\n   2,\n   11,\n   27,\n   0,\n   12,\n   5,\n   0,\n   14,\n   0,\n   2,\n   0,\n   0,\n   14,\n   4,\n   0,\n   5,\n   2,\n   0,\n   2,\n   0,\n   0,\n   0,\n   0,\n   17,\n   0,\n   0,\n   0,\n   0,\n   18,\n   4],\n  [1, 2, 11, 18, 20],\n  [5, 0, 7, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   5,\n   35,\n   0,\n   0,\n   9,\n   5,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   2,\n   5,\n   0,\n   15,\n   5,\n   0,\n   0,\n   0,\n   0,\n   0,\n   13,\n   2,\n   0,\n   0,\n   4,\n   10,\n   0,\n   0,\n   2,\n   5,\n   0,\n   0,\n   5,\n   0,\n   0,\n   2,\n   2,\n   12,\n   0,\n   2,\n   4],\n  [4, 14, 25, 32, 39, 40, 43],\n  [0, 7, 0, 6, 11, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   5,\n   0,\n   0,\n   5,\n   0,\n   17,\n   5,\n   0,\n   0,\n   0,\n   0,\n   2,\n   2,\n   0,\n   2,\n   0,\n   0,\n   5,\n   35,\n   4,\n   20,\n   2,\n   0,\n   0,\n   29,\n   5,\n   0,\n   21,\n   0,\n   6,\n   8,\n   0,\n   0,\n   0,\n   0,\n   10,\n   0,\n   8,\n   0,\n   4],\n  [6, 12, 13, 15, 21, 22],\n  [7, 0, 19, 0, 5, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   12,\n   2,\n   8,\n   30,\n   0,\n   5,\n   0,\n   0,\n   0,\n   9,\n   0,\n   0,\n   0,\n   0,\n   10,\n   5,\n   0,\n   7,\n   0,\n   0,\n   20,\n   0,\n   2,\n   13,\n   5,\n   2,\n   0,\n   8,\n   2,\n   0,\n   7,\n   19,\n   0,\n   7,\n   2,\n   0,\n   4,\n   5,\n   0,\n   2,\n   0,\n   29,\n   5,\n   0,\n   39,\n   6,\n   0,\n   11,\n   0,\n   0,\n   4],\n  [2, 23, 26, 28, 29, 30, 35, 40],\n  [0, 0, 0, 8, 5, 0, 0, 36],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   10,\n   0,\n   16,\n   5,\n   0,\n   0,\n   0,\n   0,\n   19,\n   0,\n   12,\n   0,\n   0,\n   0,\n   9,\n   0,\n   2,\n   8,\n   0,\n   0,\n   0,\n   4,\n   5,\n   0,\n   0,\n   0,\n   0,\n   8,\n   0,\n   12,\n   2,\n   30,\n   0,\n   4],\n  [3, 13, 17, 21, 31],\n  [0, 0, 0, 0, 5],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   5,\n   0,\n   0,\n   0,\n   0,\n   8,\n   0,\n   12,\n   5,\n   30,\n   0,\n   4,\n   10,\n   0,\n   2,\n   6,\n   5,\n   2,\n   2,\n   5,\n   0,\n   0,\n   11,\n   0,\n   2,\n   0,\n   0,\n   4],\n  [15, 18, 19, 25],\n  [0, 0, 0, 12],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   10,\n   0,\n   0,\n   6,\n   5,\n   0,\n   0,\n   5,\n   2,\n   2,\n   2,\n   2,\n   2,\n   0,\n   0,\n   4,\n   2,\n   0,\n   0,\n   5,\n   0,\n   7,\n   5,\n   0,\n   0,\n   0,\n   0,\n   0,\n   19,\n   0,\n   0,\n   0,\n   0,\n   5,\n   35,\n   4],\n  [9, 10, 11, 12, 13, 17],\n  [0, 0, 11, 0, 12, 5],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   2,\n   2,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   8,\n   0,\n   4,\n   12,\n   0,\n   8,\n   30,\n   0,\n   5,\n   0,\n   0,\n   0,\n   9,\n   0,\n   0,\n   0,\n   0,\n   10,\n   18,\n   0,\n   7,\n   2,\n   0,\n   7,\n   0,\n   0,\n   13,\n   5,\n   0,\n   0,\n   8,\n   5,\n   39,\n   7,\n   19,\n   0,\n   7,\n   0,\n   0,\n   4],\n  [1, 2, 24, 27, 30, 32, 41],\n  [5, 0, 0, 5, 0, 20, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   0,\n   0,\n   0,\n   0,\n   28,\n   2,\n   0,\n   9,\n   0,\n   0,\n   2,\n   0,\n   9,\n   0,\n   0,\n   4,\n   0,\n   6,\n   0,\n   0,\n   0,\n   0,\n   2,\n   0,\n   8,\n   0,\n   4],\n  [6, 7, 11, 23],\n  [29, 0, 8, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   0,\n   0,\n   0,\n   9,\n   2,\n   5,\n   0,\n   0,\n   0,\n   0,\n   28,\n   8,\n   5,\n   0,\n   2,\n   28,\n   8,\n   2,\n   2,\n   6,\n   0,\n   36,\n   0,\n   4,\n   23,\n   0,\n   24,\n   26,\n   0,\n   0,\n   0,\n   0,\n   2,\n   0,\n   2,\n   0,\n   0,\n   0,\n   2,\n   5,\n   0,\n   0,\n   4],\n  [5, 15, 18, 19, 33, 35, 39],\n  [0, 8, 30, 0, 0, 5, 7],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   0,\n   2,\n   28,\n   6,\n   2,\n   0,\n   2,\n   0,\n   7,\n   5,\n   32,\n   0,\n   17,\n   0,\n   18,\n   13,\n   0,\n   0,\n   0,\n   4,\n   37,\n   38,\n   6,\n   22,\n   0,\n   10,\n   0,\n   0,\n   6,\n   33,\n   0,\n   0,\n   17,\n   0,\n   0,\n   0,\n   2,\n   8,\n   0,\n   0,\n   21,\n   0,\n   0,\n   4],\n  [1, 2, 5, 7, 24, 25, 37],\n  [0, 9, 0, 0, 22, 0, 18],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   22,\n   11,\n   0,\n   19,\n   2,\n   7,\n   5,\n   2,\n   7,\n   5,\n   0,\n   17,\n   0,\n   18,\n   14,\n   10,\n   0,\n   7,\n   0,\n   0,\n   2,\n   2,\n   0,\n   0,\n   14,\n   13,\n   0,\n   0,\n   4,\n   24,\n   0,\n   20,\n   0,\n   0,\n   33,\n   2,\n   0,\n   0,\n   0,\n   0,\n   5,\n   32,\n   0,\n   2,\n   0,\n   0,\n   0,\n   0,\n   0,\n   6,\n   8,\n   33,\n   2,\n   2,\n   0,\n   29,\n   0,\n   2,\n   8,\n   2,\n   28,\n   31,\n   0,\n   0,\n   10,\n   5,\n   0,\n   7,\n   0,\n   0,\n   0,\n   4],\n  [5, 8, 21, 22, 36, 38, 44, 53, 54, 58, 60],\n  [0, 0, 10, 5, 5, 0, 25, 0, 0, 0, 29],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   24,\n   0,\n   20,\n   0,\n   0,\n   33,\n   5,\n   0,\n   0,\n   0,\n   0,\n   5,\n   32,\n   0,\n   25,\n   2,\n   0,\n   0,\n   0,\n   0,\n   6,\n   8,\n   2,\n   18,\n   0,\n   0,\n   2,\n   0,\n   0,\n   8,\n   29,\n   28,\n   31,\n   0,\n   0,\n   10,\n   5,\n   0,\n   7,\n   2,\n   0,\n   0,\n   4,\n   5,\n   26,\n   0,\n   2,\n   36,\n   0,\n   9,\n   5,\n   0,\n   0,\n   0,\n   5,\n   0,\n   7,\n   24,\n   0,\n   6,\n   8,\n   22,\n   0,\n   9,\n   0,\n   2,\n   5,\n   0,\n   7,\n   5,\n   0,\n   2,\n   6,\n   0,\n   0,\n   0,\n   0,\n   2,\n   2,\n   10,\n   0,\n   14,\n   9,\n   0,\n   5,\n   0,\n   4],\n  [1, 16, 23, 24, 27, 40, 47, 54, 66, 72, 78, 79, 82],\n  [24, 0, 33, 0, 29, 0, 0, 0, 33, 0, 0, 6, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   5,\n   26,\n   0,\n   0,\n   36,\n   0,\n   2,\n   5,\n   0,\n   0,\n   0,\n   5,\n   0,\n   7,\n   24,\n   0,\n   6,\n   2,\n   22,\n   0,\n   9,\n   0,\n   33,\n   5,\n   0,\n   7,\n   5,\n   2,\n   0,\n   2,\n   0,\n   0,\n   0,\n   0,\n   0,\n   6,\n   2,\n   0,\n   0,\n   9,\n   0,\n   5,\n   0,\n   4,\n   5,\n   0,\n   36,\n   0,\n   29,\n   5,\n   0,\n   39,\n   6,\n   0,\n   11,\n   2,\n   2,\n   4],\n  [7, 11, 18, 28, 30, 37, 48, 56, 57],\n  [9, 0, 8, 0, 6, 10, 0, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   5,\n   0,\n   0,\n   2,\n   0,\n   25,\n   0,\n   9,\n   32,\n   0,\n   0,\n   6,\n   5,\n   0,\n   7,\n   5,\n   32,\n   0,\n   6,\n   37,\n   0,\n   22,\n   0,\n   0,\n   2,\n   0,\n   12,\n   0,\n   0,\n   0,\n   0,\n   25,\n   0,\n   0,\n   6,\n   8,\n   5,\n   0,\n   2,\n   0,\n   11,\n   0,\n   2,\n   12,\n   2,\n   0,\n   0,\n   21,\n   0,\n   0,\n   2,\n   0,\n   2,\n   0,\n   0,\n   4,\n   0,\n   9,\n   0,\n   6,\n   22,\n   0,\n   0,\n   6,\n   0,\n   0,\n   24,\n   2,\n   34,\n   13,\n   32,\n   2,\n   0,\n   15,\n   0,\n   6,\n   0,\n   5,\n   0,\n   0,\n   4],\n  [4, 8, 22, 25, 39, 43, 45, 51, 53, 68, 71, 72],\n  [6, 9, 0, 9, 0, 0, 0, 0, 0, 26, 5, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   0,\n   0,\n   2,\n   0,\n   2,\n   0,\n   0,\n   20,\n   0,\n   0,\n   0,\n   6,\n   2,\n   6,\n   13,\n   0,\n   0,\n   2,\n   4,\n   0,\n   0,\n   25,\n   0,\n   0,\n   10,\n   0,\n   6,\n   0,\n   2,\n   5,\n   0,\n   31,\n   0,\n   0,\n   8,\n   0,\n   0,\n   4],\n  [3, 5, 13, 18, 29, 37],\n  [32, 0, 0, 0, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   0,\n   25,\n   2,\n   0,\n   10,\n   0,\n   6,\n   22,\n   0,\n   5,\n   0,\n   31,\n   0,\n   2,\n   8,\n   0,\n   0,\n   4,\n   0,\n   6,\n   22,\n   0,\n   5,\n   0,\n   38,\n   2,\n   10,\n   15,\n   16,\n   4],\n  [4, 9, 15, 27, 29],\n  [0, 0, 0, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   2,\n   0,\n   0,\n   0,\n   6,\n   0,\n   11,\n   0,\n   9,\n   0,\n   20,\n   0,\n   7,\n   5,\n   0,\n   0,\n   0,\n   2,\n   4,\n   0,\n   0,\n   0,\n   0,\n   32,\n   0,\n   0,\n   0,\n   0,\n   10,\n   0,\n   28,\n   0,\n   36,\n   0,\n   0,\n   9,\n   0,\n   5,\n   0,\n   0,\n   2,\n   0,\n   0,\n   2,\n   0,\n   10,\n   5,\n   0,\n   2,\n   0,\n   2,\n   0,\n   18,\n   16,\n   4],\n  [1, 18, 39, 41, 44, 49, 51, 54],\n  [13, 0, 0, 0, 0, 0, 17, 16],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   23,\n   0,\n   0,\n   2,\n   19,\n   0,\n   0,\n   0,\n   19,\n   27,\n   21,\n   34,\n   0,\n   2,\n   0,\n   12,\n   0,\n   0,\n   2,\n   16,\n   0,\n   0,\n   4,\n   23,\n   0,\n   2,\n   0,\n   7,\n   5,\n   0,\n   21,\n   14,\n   0,\n   9,\n   0,\n   0,\n   6,\n   0,\n   29,\n   5,\n   2,\n   33,\n   0,\n   0,\n   4],\n  [4, 14, 19, 20, 26, 32, 41],\n  [0, 9, 15, 5, 0, 0, 34],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   23,\n   2,\n   0,\n   0,\n   7,\n   5,\n   0,\n   21,\n   0,\n   0,\n   2,\n   0,\n   0,\n   6,\n   0,\n   29,\n   5,\n   34,\n   33,\n   0,\n   0,\n   4,\n   12,\n   5,\n   2,\n   0,\n   6,\n   23,\n   2,\n   0,\n   2,\n   0,\n   0,\n   0,\n   31,\n   0,\n   0,\n   0,\n   4],\n  [2, 11, 16, 25, 29, 31],\n  [0, 9, 29, 0, 11, 9],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   12,\n   5,\n   0,\n   0,\n   2,\n   23,\n   11,\n   2,\n   9,\n   0,\n   0,\n   0,\n   31,\n   0,\n   0,\n   23,\n   4,\n   10,\n   24,\n   26,\n   34,\n   12,\n   5,\n   2,\n   2,\n   23,\n   0,\n   24,\n   26,\n   0,\n   0,\n   0,\n   0,\n   0,\n   22,\n   0,\n   0,\n   15,\n   2,\n   0,\n   4],\n  [5, 8, 16, 24, 25, 39],\n  [6, 0, 0, 0, 6, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   2,\n   24,\n   26,\n   34,\n   12,\n   2,\n   0,\n   6,\n   23,\n   0,\n   24,\n   2,\n   0,\n   0,\n   0,\n   0,\n   0,\n   14,\n   0,\n   0,\n   2,\n   0,\n   0,\n   4,\n   0,\n   9,\n   0,\n   6,\n   22,\n   0,\n   0,\n   6,\n   0,\n   2,\n   24,\n   2,\n   34,\n   13,\n   5,\n   0,\n   0,\n   15,\n   0,\n   6,\n   2,\n   5,\n   0,\n   0,\n   4],\n  [1, 6, 12, 18, 21, 34, 36, 45],\n  [10, 5, 26, 22, 15, 0, 26, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   9,\n   0,\n   6,\n   22,\n   0,\n   0,\n   6,\n   0,\n   0,\n   24,\n   18,\n   34,\n   13,\n   5,\n   0,\n   0,\n   15,\n   0,\n   2,\n   0,\n   2,\n   0,\n   0,\n   4,\n   2,\n   37,\n   38,\n   6,\n   22,\n   2,\n   24,\n   26,\n   0,\n   0,\n   15,\n   0,\n   4],\n  [10, 12, 20, 22, 26, 31],\n  [0, 26, 6, 5, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   37,\n   38,\n   6,\n   22,\n   0,\n   24,\n   26,\n   0,\n   0,\n   2,\n   0,\n   4,\n   0,\n   6,\n   0,\n   0,\n   2,\n   0,\n   2,\n   2,\n   8,\n   0,\n   4],\n  [11, 18, 20, 21],\n  [15, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n  False),\n ([3,\n   5,\n   0,\n   2,\n   2,\n   2,\n   12,\n   5,\n   0,\n   34,\n   7,\n   5,\n   38,\n   2,\n   23,\n   0,\n   38,\n   0,\n   2,\n   6,\n   0,\n   0,\n   0,\n   15,\n   5,\n   0,\n   4,\n   5,\n   0,\n   0,\n   0,\n   13,\n   0,\n   7,\n   5,\n   0,\n   6,\n   0,\n   5,\n   0,\n   0,\n   0,\n   2,\n   7,\n   25,\n   0,\n   17,\n   0,\n   0,\n   0,\n   0,\n   18,\n   9,\n   5,\n   0,\n   4],\n  [3, 4, 5, 13, 16, 18, 42, 44],\n  [0, 9, 0, 0, 24, 34, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   0,\n   0,\n   0,\n   37,\n   14,\n   0,\n   23,\n   0,\n   0,\n   5,\n   0,\n   11,\n   0,\n   14,\n   4,\n   5,\n   0,\n   36,\n   0,\n   29,\n   5,\n   2,\n   2,\n   2,\n   0,\n   11,\n   2,\n   0,\n   4],\n  [22, 23, 24, 27],\n  [0, 39, 6, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   5,\n   0,\n   36,\n   0,\n   29,\n   2,\n   0,\n   39,\n   6,\n   0,\n   11,\n   0,\n   0,\n   4,\n   5,\n   0,\n   0,\n   11,\n   2,\n   0,\n   5,\n   0,\n   0,\n   2,\n   7,\n   2,\n   8,\n   2,\n   5,\n   0,\n   0,\n   0,\n   2,\n   0,\n   21,\n   0,\n   7,\n   0,\n   0,\n   16,\n   4],\n  [6, 19, 24, 26, 28, 33],\n  [5, 0, 0, 0, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   27,\n   7,\n   5,\n   0,\n   0,\n   11,\n   0,\n   9,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   6,\n   8,\n   2,\n   19,\n   0,\n   8,\n   5,\n   2,\n   0,\n   11,\n   0,\n   33,\n   0,\n   4,\n   2,\n   0,\n   27,\n   12,\n   5,\n   0,\n   0,\n   2,\n   0,\n   27,\n   15,\n   19,\n   0,\n   6,\n   20,\n   0,\n   2,\n   5,\n   0,\n   7,\n   2,\n   0,\n   7,\n   5,\n   15,\n   0,\n   4],\n  [18, 21, 23, 30, 32, 37, 46, 50, 54],\n  [0, 7, 0, 5, 0, 36, 0, 5, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   2,\n   0,\n   0,\n   12,\n   5,\n   2,\n   0,\n   36,\n   0,\n   27,\n   15,\n   9,\n   0,\n   6,\n   20,\n   0,\n   0,\n   5,\n   0,\n   7,\n   5,\n   0,\n   7,\n   5,\n   0,\n   0,\n   4,\n   27,\n   0,\n   0,\n   0,\n   4],\n  [1, 6, 12, 19, 24],\n  [5, 0, 19, 0, 5],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3, 27, 0, 0, 2, 4, 5, 0, 11, 27, 0, 2, 5, 0, 14, 2, 7, 0, 0, 14, 4],\n  [4, 11, 15],\n  [0, 12, 0],\n  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n  False),\n ([3,\n   0,\n   25,\n   0,\n   8,\n   5,\n   39,\n   2,\n   0,\n   2,\n   0,\n   0,\n   17,\n   0,\n   0,\n   18,\n   0,\n   5,\n   0,\n   0,\n   39,\n   0,\n   9,\n   0,\n   4,\n   0,\n   0,\n   0,\n   25,\n   2,\n   0,\n   6,\n   5,\n   0,\n   2,\n   11,\n   0,\n   6,\n   2,\n   5,\n   0,\n   0,\n   2,\n   2,\n   6,\n   0,\n   11,\n   0,\n   10,\n   27,\n   0,\n   0,\n   37,\n   2,\n   0,\n   0,\n   0,\n   9,\n   2,\n   0,\n   0,\n   37,\n   0,\n   0,\n   16,\n   4],\n  [7, 9, 29, 34, 38, 42, 43, 53, 58, 61],\n  [25, 0, 0, 39, 0, 39, 0, 5, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   0,\n   0,\n   0,\n   2,\n   0,\n   0,\n   2,\n   9,\n   0,\n   2,\n   0,\n   9,\n   0,\n   0,\n   0,\n   13,\n   5,\n   0,\n   0,\n   4,\n   2,\n   0,\n   24,\n   26,\n   2,\n   0,\n   0,\n   0,\n   0,\n   0,\n   5,\n   0,\n   0,\n   0,\n   7,\n   5,\n   0,\n   0,\n   4],\n  [5, 8, 11, 22, 26, 30],\n  [0, 23, 5, 23, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   23,\n   0,\n   24,\n   26,\n   0,\n   2,\n   0,\n   0,\n   0,\n   0,\n   5,\n   0,\n   2,\n   0,\n   7,\n   5,\n   0,\n   0,\n   4,\n   2,\n   0,\n   2,\n   5,\n   0,\n   21,\n   2,\n   0,\n   15,\n   0,\n   13,\n   31,\n   10,\n   6,\n   0,\n   6,\n   0,\n   0,\n   10,\n   19,\n   0,\n   0,\n   0,\n   21,\n   0,\n   0,\n   4],\n  [6, 13, 20, 22, 26, 31, 32],\n  [0, 0, 22, 15, 0, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   22,\n   0,\n   15,\n   5,\n   0,\n   21,\n   2,\n   2,\n   15,\n   0,\n   13,\n   0,\n   0,\n   6,\n   0,\n   6,\n   0,\n   0,\n   10,\n   19,\n   0,\n   0,\n   0,\n   21,\n   0,\n   0,\n   4,\n   37,\n   38,\n   6,\n   22,\n   0,\n   10,\n   2,\n   0,\n   6,\n   33,\n   0,\n   0,\n   17,\n   0,\n   2,\n   2,\n   18,\n   8,\n   0,\n   0,\n   21,\n   2,\n   2,\n   4],\n  [7, 8, 33, 34, 42, 43, 49, 50],\n  [0, 0, 10, 0, 0, 0, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   37,\n   38,\n   6,\n   36,\n   2,\n   10,\n   0,\n   0,\n   6,\n   33,\n   0,\n   0,\n   17,\n   0,\n   0,\n   0,\n   18,\n   8,\n   0,\n   0,\n   21,\n   0,\n   0,\n   4,\n   12,\n   5,\n   0,\n   2,\n   6,\n   23,\n   11,\n   0,\n   2,\n   0,\n   0,\n   0,\n   2,\n   0,\n   0,\n   2,\n   4],\n  [4, 5, 28, 33, 37, 40],\n  [22, 0, 0, 9, 31, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   5,\n   0,\n   0,\n   0,\n   13,\n   0,\n   7,\n   2,\n   0,\n   6,\n   2,\n   5,\n   0,\n   0,\n   2,\n   0,\n   2,\n   0,\n   0,\n   17,\n   0,\n   0,\n   0,\n   0,\n   18,\n   9,\n   5,\n   0,\n   4,\n   5,\n   0,\n   2,\n   5,\n   2,\n   7,\n   5,\n   0,\n   2,\n   0,\n   35,\n   0,\n   19,\n   0,\n   0,\n   0,\n   0,\n   5,\n   35,\n   4],\n  [8, 11, 15, 17, 32, 34, 38, 40],\n  [5, 0, 0, 7, 0, 0, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   0,\n   21,\n   5,\n   0,\n   21,\n   0,\n   0,\n   8,\n   0,\n   0,\n   0,\n   0,\n   17,\n   0,\n   0,\n   0,\n   0,\n   18,\n   0,\n   0,\n   12,\n   0,\n   0,\n   4,\n   5,\n   0,\n   0,\n   2,\n   0,\n   0,\n   2,\n   2,\n   8,\n   0,\n   4],\n  [2, 21, 28, 31, 32],\n  [0, 0, 0, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   0,\n   5,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   2,\n   17,\n   0,\n   0,\n   0,\n   0,\n   18,\n   4,\n   0,\n   31,\n   0,\n   11,\n   0,\n   0,\n   0,\n   2,\n   13,\n   0,\n   0,\n   31,\n   2,\n   7,\n   14,\n   2,\n   0,\n   0,\n   2,\n   0,\n   2,\n   14,\n   16,\n   4],\n  [9, 24, 29, 32, 35, 37],\n  [0, 0, 0, 0, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   2,\n   0,\n   2,\n   0,\n   17,\n   0,\n   0,\n   0,\n   0,\n   18,\n   0,\n   8,\n   0,\n   0,\n   15,\n   0,\n   17,\n   0,\n   0,\n   18,\n   4,\n   0,\n   31,\n   0,\n   11,\n   0,\n   2,\n   2,\n   0,\n   13,\n   0,\n   0,\n   31,\n   0,\n   7,\n   14,\n   0,\n   0,\n   2,\n   0,\n   0,\n   0,\n   14,\n   16,\n   4],\n  [1, 3, 9, 15, 27, 28, 39],\n  [0, 0, 0, 0, 0, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   5,\n   0,\n   0,\n   0,\n   29,\n   5,\n   0,\n   21,\n   2,\n   6,\n   8,\n   0,\n   0,\n   0,\n   0,\n   10,\n   2,\n   8,\n   2,\n   4,\n   0,\n   2,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   8,\n   2,\n   4],\n  [9, 17, 19, 22, 30],\n  [0, 0, 0, 6, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   6,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   8,\n   0,\n   4,\n   0,\n   10,\n   5,\n   0,\n   0,\n   2,\n   9,\n   0,\n   0,\n   0,\n   8,\n   2,\n   0,\n   0,\n   0,\n   2,\n   2,\n   4],\n  [17, 23, 27, 28],\n  [0, 0, 0, 16],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True)]"
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    # 为遮蔽语言模型的输⼊创建新的词元副本\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "\n",
    "    pred_positions_and_labels = []\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        # 80%的时间:将词替换为\"<mask>\"词元\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10%的时间:保持词不变\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10%的时间:用随机词替换该词\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token  # 遮蔽后新的词元(可能包含替换的\"<mask>\"或随机词元)\n",
    "        # mlm_pred_position:遮蔽词元的位置\n",
    "        # tokens[mlm_pred_position]:遮蔽词元处真实的词元\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input_tokens, pred_positions_and_labels\n",
    "\n",
    "\n",
    "def _get_mlm_data_from_tokens(\n",
    "        # 词元列表\n",
    "        tokens, vocab):\n",
    "    candidate_pred_positions = []\n",
    "    # tokens为⼀个字符串列表\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 在遮蔽语言模型任务中不会预测特殊词元\"<cls\">与\"<sep>\"\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    # 遮蔽语言模型任务中预测15%的随机词元\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens, candidate_pred_positions,\n",
    "                                                                      num_mlm_preds, vocab)\n",
    "\n",
    "    # 根据pred_positions_and_labels中的第一列排序(从小到大)\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    # vocab[mlm_input_tokens]:遮蔽后新的词元转换为数值矩阵\n",
    "    # vocab[mlm_pred_labels]:遮蔽词元处真实的词元转换为数值矩阵\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n",
    "\n",
    "\n",
    "# 不再区分段落\n",
    "# sentences[0]表示一句话\n",
    "# sentences[0][0]表示该句话的第一个单词\n",
    "sentences = [sentence for paragraph in test_tokenize for sentence in paragraph]\n",
    "\n",
    "vocab = Vocab(sentences, min_freq=5, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "\n",
    "examples = []\n",
    "for paragraph in test_tokenize:\n",
    "    examples.extend(_get_nsp_data_from_paragraph(paragraph, test_tokenize, 1024))\n",
    "\n",
    "test_get_mlm_data_from_tokens = [(_get_mlm_data_from_tokens(tokens, vocab) + (segments, is_next))\n",
    "                                 for tokens, segments, is_next in examples]\n",
    "test_get_mlm_data_from_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "outputs": [],
   "source": [
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    all_token_ids, all_segments, valid_lens, = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (max_len - len(token_ids)), dtype=torch.long))\n",
    "        all_segments.append(torch.tensor(segments + [0] * (max_len - len(segments)), dtype=torch.long))\n",
    "        # valid_lens不包括\"<pad>\"的计数\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)),\n",
    "                                               dtype=torch.long))\n",
    "        # 填充词元的预测将通过乘以0权重在损失中过滤掉\n",
    "        all_mlm_weights.append(\n",
    "            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (max_num_mlm_preds - len(pred_positions)),\n",
    "                         dtype=torch.float32))\n",
    "        all_mlm_labels.append(\n",
    "            torch.tensor(mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "    return (all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "outputs": [],
   "source": [
    "class _WikiTextDataset(Data.Dataset):  # 继承Dataset类\n",
    "    \"\"\"包装成pytorch Data.DataSet数据集\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # paragraphs表示段落集合\n",
    "                 paragraphs,\n",
    "                 # 指定预训练期间的BERT输⼊序列的最大长度\n",
    "                 max_len):\n",
    "        paragraphs = [tokenize(paragraph, token='word') for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs for sentence in paragraph]\n",
    "\n",
    "        # 现次数少于5次的不频繁词元将被过滤掉\n",
    "        self.vocab = Vocab(sentences, min_freq=5, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        # 获取下⼀句子预测任务的数据\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(paragraph, paragraphs, max_len))\n",
    "        # 获取遮蔽语言模型任务的数据\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next)) for tokens, segments, is_next\n",
    "                    in examples]\n",
    "\n",
    "        (self.all_token_ids,\n",
    "         self.all_segments,\n",
    "         self.valid_lens,\n",
    "         self.all_pred_positions,\n",
    "         self.all_mlm_weights,\n",
    "         self.all_mlm_labels,\n",
    "         self.nsp_labels) = _pad_bert_inputs(examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
    "                self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "outputs": [],
   "source": [
    "def load_data_wiki(file_name, batch_size, max_len):\n",
    "    \"\"\"加载WikiText-2数据集\"\"\"\n",
    "    paragraphs = _read_wiki(file_name)\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "    return train_iter, train_set.vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64])\n",
      "torch.Size([512, 64])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 10])\n",
      "torch.Size([512, 10])\n",
      "torch.Size([512, 10])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "file_name = \"WikiText2/wiki.valid.tokens\"\n",
    "train_iter, vocab = load_data_wiki(file_name, batch_size, max_len)\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y) in train_iter:\n",
    "    print(tokens_X.shape)\n",
    "    print(segments_X.shape)\n",
    "    print(valid_lens_x.shape)\n",
    "    print(pred_positions_X.shape)\n",
    "    print(mlm_weights_X.shape)\n",
    "    print(mlm_Y.shape)\n",
    "    print(nsp_y.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}