{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 从头训练分词器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Models\n",
    "\n",
    "Models are the core algorithms used to actually tokenize, and therefore, they are the only mandatory component of a Tokenizer.\n",
    "\n",
    "| Name      | Description                                                  |\n",
    "| --------- | ------------------------------------------------------------ |\n",
    "| WordLevel | This is the “classic” tokenization algorithm. It let’s you simply map words to IDs without anything fancy. This has the advantage of being really simple to use and understand, but it requires extremely large vocabularies for a good coverage. Using this `Model` requires the use of a `PreTokenizer`. No choice will be made by this model directly, it simply maps input tokens to IDs. |\n",
    "| BPE       | One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding works by starting with characters, while merging those that are the most frequently seen together, thus creating new tokens. It then works iteratively to build new tokens out of the most frequent pairs it sees in a corpus. BPE is able to build words it has never seen by using multiple subword tokens, and thus requires smaller vocabularies, with less chances of having “unk” (unknown) tokens. |\n",
    "| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don’t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word). |\n",
    "| Unigram   | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "<tokenizers.models.BPE at 0x1f2b8f5ad50>"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An implementation of the BPE (Byte-Pair Encoding) algorithm\n",
    "model_BPE = BPE(unk_token='[UNK]')\n",
    "model_BPE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "<tokenizers.Tokenizer at 0x1f2968a8a40>"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A :obj:`Tokenizer` works as a pipeline. It processes some raw text as input and outputs an :class:`~tokenizers.Encoding`.\n",
    "tokenizer = Tokenizer(\n",
    "    # The core algorithm that this Tokenizer should be using.\n",
    "    model=model_BPE)\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### normalizer(可选)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "# Takes care of normalizing raw text before giving it to a Bert model.\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-tokenizers\n",
    "\n",
    "The PreTokenizer takes care of splitting the input according to a set of rules. This pre-processing lets you ensure that the underlying Model does not build tokens across multiple “splits”. For example if you don’t want to have whitespaces inside a token, then you can have a PreTokenizer that splits on these whitespaces."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "# This pre-tokenizer simply splits using the following regex: `\\w+|[^\\w\\s]+`\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trainers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "<tokenizers.trainers.BpeTrainer at 0x1f2b8f5acd0>"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer capable of training a BPE model\n",
    "trainer = BpeTrainer(\n",
    "    # The size of the final vocabulary, including all tokens and alphabet.\n",
    "    vocab_size=30000,  # 默认:30000\n",
    "    # The minimum frequency a pair should have in order to be merged.\n",
    "    min_frequency=0,  # 默认:0\n",
    "    # A list of special tokens the model should know of.\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])  # 默认:[]\n",
    "trainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "['实战/PreTrained_Bert/wikitext-103-raw/wiki.test.raw',\n '实战/PreTrained_Bert/wikitext-103-raw/wiki.train.raw',\n '实战/PreTrained_Bert/wikitext-103-raw/wiki.valid.raw']"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [f\"实战/PreTrained_Bert/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "<tokenizers.Tokenizer at 0x1f2968a8a40>"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Tokenizer using the given files.\n",
    "tokenizer.train(\n",
    "    files=files,  # 文件路径或包含路径的列表\n",
    "    trainer=trainer)\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "data": {
      "text/plain": "30000"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "{'腎': 3240,\n 'catap': 22054,\n 'krupp': 26966,\n 'dez': 15997,\n 'myco': 17604,\n 'inaccessible': 29398,\n 'metr': 21459,\n 'pardon': 23265,\n 'maryland': 9047,\n 'constitutes': 27418,\n 'bastion': 23619,\n 'deposited': 18062,\n 'pocket': 13926,\n 'array': 14724,\n 'anderson': 8618,\n 'allocated': 14824,\n 'fundament': 12083,\n 'insul': 13206,\n 'besie': 14971,\n 'coordinated': 18121,\n 'seabird': 26446,\n 'restoring': 19232,\n '烟': 2776,\n 'mughal': 25175,\n 'theoretical': 15743,\n 'imaging': 20704,\n '覚': 3445,\n 'ฤ': 724,\n 'pas': 10747,\n 'aggregator': 24059,\n 'target': 8514,\n 'alike': 17366,\n 'greene': 17282,\n 'stee': 28671,\n 'panthers': 17217,\n '佳': 1501,\n 'alleviate': 26095,\n 'dreadnought': 15562,\n '春': 2391,\n 'medieval': 9252,\n 'thursday': 18030,\n 'retta': 27126,\n 'studios': 7447,\n 'fis': 10203,\n 'mayan': 29672,\n 'strata': 29555,\n 'oughton': 24343,\n '响': 1781,\n 'directing': 13192,\n 'composers': 13783,\n 'sink': 14327,\n '丞': 1400,\n 'ortho': 24737,\n 'fidelity': 27145,\n 'terminal': 12014,\n '絡': 3116,\n 'rene': 15543,\n 'princi': 11435,\n 'jamaican': 26812,\n 'coleridge': 25367,\n 'jesuits': 27712,\n '၇': 791,\n 'outspoken': 23472,\n 'booker': 17908,\n 'consc': 9510,\n 'organisation': 10340,\n 'interned': 24378,\n 'external': 11612,\n 'deeds': 21963,\n 'apologize': 28351,\n 'meyer': 13083,\n '书': 1427,\n 'eccentricity': 27746,\n 'bia': 7114,\n 'woodstock': 23840,\n 'mcclellan': 20521,\n 'groom': 23363,\n 'reclaim': 26229,\n '蟲': 3403,\n 'tech': 5282,\n 'ᆰ': 878,\n 'aspir': 16082,\n 'unt': 4667,\n 'iad': 27199,\n 'slalom': 26873,\n 'pennant': 19844,\n 'ϑ': 268,\n 'glasses': 19355,\n 'totall': 20251,\n 'extend': 11577,\n 'explosive': 14312,\n '担': 2292,\n 'sty': 5119,\n 'ornitho': 21337,\n 'elo': 10738,\n 'parachute': 12699,\n 'exciting': 15219,\n '15': 4598,\n 'iard': 24676,\n 'neptune': 16591,\n 'tie': 9373,\n '胡': 3227,\n 'starboard': 17787,\n 'climax': 15644,\n '★': 1174,\n 'heid': 17016,\n '網': 3130,\n 'pat': 4930,\n 'rober': 9476,\n 'genuinely': 23164,\n 'fierc': 29121,\n 'broker': 24191,\n 'µm': 16568,\n 'brushed': 27620,\n '港': 2714,\n 'portrayed': 9223,\n 'totaling': 20058,\n 'youths': 28610,\n 'luongo': 29271,\n '相': 2942,\n 'lev': 8049,\n 'enlist': 21528,\n 'burgoyne': 25957,\n 'treasures': 27317,\n 'thirteenth': 16506,\n 'aper': 6043,\n 'hanover': 19665,\n 'folklore': 18189,\n 'nes': 5738,\n 'pip': 9991,\n 'cule': 18405,\n 'georgian': 16619,\n 'collector': 15107,\n 'subl': 19980,\n 'permanent': 9100,\n 'bulls': 20757,\n 'kie': 12444,\n 'gence': 11676,\n 'rake': 29564,\n '沙': 2647,\n 'due': 4941,\n 'gret': 29317,\n 'reigning': 18317,\n 'scorp': 25902,\n 'ili': 9226,\n 'definitive': 18346,\n 'fair': 6503,\n 'flowed': 22248,\n 'mares': 20656,\n 'shay': 25635,\n 'capit': 19528,\n 'obligation': 22963,\n 'fixtures': 23284,\n 'justify': 19985,\n 'lyrics': 6791,\n 'essay': 13937,\n 'bru': 8000,\n 'discussions': 14537,\n 'offers': 9190,\n 'jana': 27993,\n 'protecting': 14784,\n 'petersburg': 16028,\n 'q': 59,\n 'appreciate': 22332,\n 'choice': 7735,\n 'metac': 13153,\n 'validity': 23450,\n 'khaz': 28611,\n '138': 18717,\n 'arkham': 22778,\n 'oland': 29223,\n '免': 1566,\n '到': 1628,\n 'priest': 9459,\n 'focused': 8600,\n 'digest': 15157,\n 'doom': 14571,\n 'bledon': 17640,\n 'winkle': 29061,\n 'higgins': 19827,\n 'traded': 12188,\n 'intervening': 27741,\n 'watershed': 11895,\n 'elig': 11269,\n '‖': 1040,\n 'frederick': 10482,\n 'hebrew': 16629,\n 'faul': 20128,\n 'covent': 17731,\n 'avionics': 29167,\n 'iodine': 29846,\n 'southampton': 15547,\n 'mccarthy': 14414,\n 'tents': 26676,\n 'feather': 11004,\n 'barbar': 10472,\n 'shooter': 16211,\n 'pharaoh': 23782,\n '戸': 2267,\n 'ieve': 12157,\n 'remain': 4775,\n 'batsmen': 16831,\n 'spectators': 14510,\n 'ค': 695,\n 'usual': 7849,\n 'media': 6103,\n 'smugg': 18303,\n 'ova': 18471,\n '770': 29093,\n 'casem': 16416,\n 'iness': 5558,\n 'cly': 14936,\n 'brotherhood': 23770,\n 'dixon': 17470,\n 'ruby': 19631,\n 'larry': 12786,\n 'explosion': 10968,\n 'mod': 7255,\n 'maker': 11338,\n 'twel': 7483,\n 'forbidden': 16613,\n 'warwick': 15442,\n 'trevor': 18274,\n 'singular': 19224,\n 'mor': 4607,\n 'baptism': 19637,\n 'edna': 26825,\n 'nin': 24952,\n 'speculates': 28729,\n 'oras': 28863,\n '遠': 3655,\n 'promin': 7163,\n 'himal': 21991,\n 'nixon': 11522,\n '1811': 21164,\n 'confluence': 21294,\n 'unle': 21964,\n 'shot': 5988,\n 'rer': 15109,\n 'jarvis': 27565,\n 'fertility': 24511,\n 'daleks': 25976,\n 'astronomer': 19847,\n '昭': 2392,\n 'classmates': 26530,\n 'auburn': 17713,\n 'power': 4958,\n 'piles': 28688,\n 'baron': 12347,\n '鑒': 3737,\n 'lesser': 11151,\n '判': 1624,\n 'humid': 16525,\n 'bogdan': 27658,\n 'engine': 5678,\n 'others': 5774,\n 'cocon': 22341,\n 'plume': 28369,\n 'mation': 19200,\n 'dose': 16769,\n 'monkeys': 19591,\n 'regist': 8492,\n 'precedented': 17291,\n 'devotion': 18999,\n 'raise': 9769,\n 'positions': 6743,\n 'marquis': 23778,\n 'strangers': 28701,\n 'prospe': 17540,\n 'kurt': 11534,\n 'psychiatrist': 25502,\n 'norwe': 9424,\n 'ghet': 25537,\n 'nutrient': 28570,\n 'canning': 18818,\n '129': 19434,\n 'sie': 7895,\n 'plausible': 28169,\n 'keller': 21291,\n 'lombard': 21935,\n 'ষ': 540,\n 'suit': 6516,\n 'aspirations': 27002,\n 'iple': 6106,\n 'endless': 23099,\n 'onda': 25245,\n 'clark': 8390,\n 'preval': 15302,\n '梏': 2507,\n '全': 1572,\n 'nancy': 13168,\n 'belonged': 14321,\n 'shua': 16091,\n 'suppress': 18357,\n 'emphasizes': 25904,\n 'dict': 9989,\n 'paralysis': 28624,\n 'surprise': 10629,\n 'civic': 15513,\n 'cac': 18725,\n 'sper': 10294,\n 'cian': 27531,\n 'thi': 16242,\n '1707': 29428,\n 'who': 4306,\n 'memphis': 18724,\n 'patience': 21842,\n 'nm': 23563,\n 'manip': 12623,\n 'ree': 4297,\n 'zai': 24979,\n 'korsakov': 27530,\n 'carter': 9957,\n 'veh': 6918,\n 'reve': 5950,\n 'straight': 8201,\n 'grounded': 18802,\n 'for': 4133,\n 'audio': 9240,\n 'headley': 28466,\n 'ⵖ': 1246,\n 'oul': 10625,\n 'realignment': 26838,\n 'aaron': 14342,\n '剛': 1638,\n 'sanction': 18480,\n 'buo': 19686,\n 'factory': 9402,\n 'penn': 7146,\n '妙': 1927,\n 'immin': 18433,\n 'primate': 29510,\n '偃': 1532,\n 'impacted': 17626,\n 'fortification': 20872,\n 'iroquois': 23505,\n 'coco': 21252,\n 'dev': 6308,\n 'majesty': 18103,\n 'attended': 7284,\n 'ano': 7458,\n 'outward': 20621,\n 'dependent': 13102,\n 'prestige': 18377,\n 'glenn': 15861,\n 'element': 7753,\n 'anos': 29442,\n 'underway': 18911,\n 'ড': 525,\n 'disciplines': 23934,\n '1951': 9795,\n 'dew': 24930,\n 'resulted': 7091,\n '業': 2538,\n 'including': 4678,\n '239': 23015,\n 'eruptions': 17807,\n 'ered': 4563,\n 'shakur': 24483,\n 'movies': 11017,\n 'environ': 6651,\n 'rehabil': 15546,\n 'contenders': 23967,\n 'wana': 21818,\n 'ƛ': 119,\n 'sugarcane': 28848,\n 'gathered': 11931,\n 'ears': 15580,\n 'livestock': 16310,\n 'blos': 22860,\n 'oakland': 17466,\n 'general': 4986,\n 'videos': 10137,\n 'authority': 7385,\n 'belongings': 29241,\n 'namara': 23607,\n 'phosp': 13769,\n 'psp': 22374,\n 'unpleasant': 25575,\n 'auster': 28632,\n '1838': 18137,\n '172': 24163,\n 'pieces': 7712,\n 'stepping': 23404,\n 'stress': 11757,\n 'elow': 28203,\n 'ogs': 23136,\n '𐏁': 4087,\n 'goldeneye': 28609,\n 'tired': 16102,\n 'titles': 8265,\n 'scuttled': 22681,\n 'tanker': 22962,\n 'activist': 16111,\n 'rouge': 20707,\n '1922': 10308,\n 'lers': 8342,\n 'idge': 5047,\n '241': 27309,\n 'offshore': 12788,\n 'cough': 24793,\n 'footsteps': 29188,\n '远': 3626,\n 'concludes': 15564,\n 'fortune': 12869,\n '秋': 3010,\n 'knock': 13244,\n 'faded': 23056,\n '虔': 3384,\n 'excellent': 11273,\n 'glas': 8236,\n 'modelled': 20469,\n 'performance': 5444,\n 'sions': 7545,\n 'profit': 9955,\n '総': 3135,\n 'favour': 7228,\n 'semi': 8694,\n 'accol': 14306,\n 'sinatra': 17455,\n '册': 1589,\n 'ded': 6289,\n 'sevent': 10373,\n 'atche': 19168,\n 'conceptual': 20639,\n 'joss': 27549,\n 'hierarch': 16881,\n '序': 2116,\n 'visible': 9304,\n 'cele': 6472,\n '告': 1764,\n '漏': 2737,\n 'interval': 21768,\n 'leary': 27797,\n 'lost': 5396,\n 'mea': 28121,\n 'prompt': 11013,\n 'staffel': 24821,\n 'salaries': 23199,\n 'tombs': 18721,\n 'songwriters': 21705,\n 'emblem': 17741,\n 'magna': 24084,\n 'ez': 9217,\n 'zur': 15840,\n 'logistics': 21802,\n '710': 28527,\n '畢': 2902,\n '權': 2579,\n 'restrictions': 13190,\n 'gaps': 20532,\n '敢': 2347,\n 'lag': 12667,\n 'paral': 27019,\n 'glomer': 24915,\n 'cougar': 23883,\n 'mark': 5575,\n 'ane': 4633,\n 'awes': 19657,\n 'sectors': 19977,\n 'ශ': 687,\n 'ĸ': 108,\n 'toure': 29980,\n 'massa': 21228,\n 'warhol': 29733,\n 'ท': 711,\n 'surrender': 8238,\n 'sanct': 13030,\n 'fare': 8040,\n 'precedence': 26806,\n 'rat': 7758,\n 'propriet': 22337,\n 'cylinder': 14368,\n 'regiment': 6605,\n 'formally': 9900,\n 'phyll': 23802,\n 'spot': 7226,\n 'ple': 4924,\n 'resign': 10306,\n 'saudi': 17012,\n 'turing': 23435,\n 'ance': 4384,\n 'gran': 10857,\n '歓': 2587,\n 'voluntary': 19120,\n 'microphone': 20996,\n 'segment': 9527,\n 'radial': 21481,\n 'aul': 14563,\n 'mul': 7198,\n 'donald': 8842,\n 'dioxide': 17065,\n 'mediterranean': 9582,\n 'weas': 25995,\n 'statut': 21857,\n 'ה': 366,\n 'specialist': 15154,\n 'amas': 23460,\n 'mould': 18167,\n 'steve': 8628,\n '34th': 23844,\n 'graduated': 11807,\n 'ive': 4237,\n 'rupting': 24228,\n 'inmates': 21598,\n 'leban': 14881,\n 'hest': 6084,\n 'argentina': 12085,\n 'lub': 28021,\n 'belt': 9371,\n 'pregnant': 12828,\n 'allegedly': 15119,\n 'hog': 14636,\n 'commemor': 9758,\n 'iolet': 15946,\n 'lafayette': 16717,\n '贺': 3563,\n 'seeks': 17829,\n 'yce': 23582,\n '③': 1159,\n 'niks': 22691,\n '⅝': 1097,\n 'craft': 5262,\n 'gio': 12900,\n 'zy': 7812,\n 'bend': 13904,\n 'incorpor': 7224,\n 'ញ': 967,\n 'germanic': 18740,\n 'java': 16675,\n 'saves': 16062,\n 'reper': 16766,\n 'dough': 21418,\n '俱': 1522,\n 'offerings': 22116,\n 'ih': 19718,\n 'interpol': 25383,\n 'yields': 24387,\n 'size': 6538,\n 'kee': 8092,\n 'и': 281,\n 'acclaim': 11490,\n 'aiming': 19146,\n '机': 2441,\n 'ಮ': 643,\n 'oi': 19513,\n 'handsome': 23323,\n 'partial': 11374,\n 'acton': 26785,\n 'tink': 21527,\n 'maine': 12365,\n 'gaseous': 28709,\n '1929': 10262,\n '瑣': 2871,\n '和': 1771,\n 'mes': 5903,\n 'xide': 14243,\n 'appearance': 6161,\n 'conson': 20409,\n 'clones': 22630,\n 'proposition': 22975,\n 'tide': 12845,\n 'commemorative': 19469,\n 'mores': 21161,\n 'geoff': 12421,\n 'erect': 20797,\n 'juan': 11375,\n 'liability': 24851,\n 'nitro': 13981,\n 'thar': 26581,\n 'limbs': 14883,\n 'seller': 16149,\n 'protest': 7126,\n 'oku': 24801,\n 'rever': 8749,\n '〜': 1269,\n 'natalie': 22818,\n 'alice': 11926,\n 'freeman': 18611,\n 'amura': 27631,\n '@,@': 4368,\n 'your': 6489,\n 'rotated': 25457,\n 'whil': 10634,\n 'surround': 6568,\n '雜': 3816,\n 'turn': 4541,\n 'layout': 15132,\n 'abouts': 23378,\n 'chil': 22398,\n 'artis': 21461,\n '巻': 2084,\n 'tournaments': 16208,\n '斗': 2358,\n 'ceiling': 14245,\n 'nica': 27294,\n 'undoubtedly': 24824,\n '弥': 2151,\n 'elling': 8103,\n 'recuper': 24925,\n 'quarrel': 23471,\n '痴': 2913,\n 'purchases': 23493,\n 'asis': 18017,\n 'ramirez': 18597,\n 'draws': 15103,\n 'commanding': 11281,\n 'aliens': 14666,\n 'gaza': 16418,\n 'tuning': 24860,\n 'oasis': 23074,\n 'gical': 15761,\n '©': 79,\n 'spectacular': 16465,\n 'murdo': 21972,\n 'alpine': 15080,\n 'colin': 13919,\n 'glee': 11579,\n 'fails': 15806,\n 'nob': 14140,\n 'ア': 1326,\n 'midway': 14221,\n '蘅': 3377,\n 'shouting': 24637,\n 'attraction': 13403,\n 'genetically': 20976,\n 'moons': 20362,\n 'shi': 7200,\n '矯': 2959,\n 'voyager': 17912,\n 'anny': 10500,\n 'ign': 4308,\n 'gotham': 24788,\n 'stubborn': 29383,\n 'var': 4810,\n 'aver': 5404,\n 'ished': 4592,\n '島': 2062,\n 'switzer': 11337,\n 'outdoors': 27806,\n 'townsend': 19439,\n 'portion': 6907,\n '－': 4045,\n 'igne': 17721,\n 'mons': 11877,\n 'ⵇ': 1238,\n 'ⴼ': 1235,\n 'ocytes': 28483,\n 'celebrating': 19174,\n 'plain': 10022,\n 'mcn': 22245,\n '宇': 1969,\n 'reads': 13716,\n 'compress': 29500,\n 'foundations': 15972,\n 'sources': 7574,\n 'cathedral': 9858,\n 'absorbing': 27380,\n 'supplies': 9049,\n 'sentinel': 25475,\n 'victory': 6041,\n 'undy': 22674,\n 'acid': 9035,\n 'severe': 7933,\n 'batman': 10028,\n 'annihil': 28109,\n 'deals': 14487,\n 'etr': 11320,\n 'exhibitions': 20010,\n '※': 1056,\n 'khmer': 20548,\n '廿': 2140,\n 'asian': 9109,\n 'seyd': 27452,\n 'lud': 13067,\n 'presidents': 18403,\n 'trot': 20495,\n '唐': 1787,\n '九': 1424,\n 'toll': 10814,\n 'clive': 22877,\n 'dusty': 28961,\n '195': 5097,\n 'jacqu': 12662,\n 'vene': 26835,\n 'neolithic': 22510,\n 'thood': 15429,\n '罰': 3178,\n 'darling': 24280,\n 'stip': 15714,\n 'trude': 27049,\n 'quag': 23060,\n 'detonation': 29963,\n 'tae': 20602,\n 'eid': 29855,\n 'vot': 8065,\n 'because': 4797,\n 'reefs': 23362,\n 'ctur': 18285,\n '109': 13477,\n 'cables': 20277,\n 'cake': 15960,\n 'moreno': 18255,\n 'upanis': 21442,\n 'patt': 11476,\n 'bav': 15726,\n 'trafficking': 19353,\n 'fritz': 19783,\n 'faults': 21559,\n 'enne': 26517,\n 'earl': 8709,\n 'marijuana': 20305,\n 'ು': 654,\n '录': 2162,\n 'tricky': 27917,\n 'upward': 21785,\n 'shoot': 6920,\n 'gad': 14787,\n 'intoxic': 22710,\n 'mobiles': 25868,\n 'attitude': 11829,\n 'ishing': 6677,\n 'liszt': 24863,\n 'roma': 22469,\n 'wait': 10674,\n 'kest': 21676,\n 'dall': 11246,\n 'drafting': 25422,\n '407': 28604,\n '思': 2203,\n 'earlier': 6151,\n 'bids': 21716,\n 'threads': 27583,\n '1980s': 7894,\n 'gra': 9873,\n 'investment': 11609,\n 'dollars': 10723,\n 'weakness': 14057,\n 'cats': 12426,\n 'equipped': 9258,\n 'vicinity': 13390,\n 'electoral': 13842,\n 'millennium': 12126,\n 'viral': 16078,\n 'northumbria': 20879,\n 'confucian': 25433,\n 'fundraising': 20577,\n 'jewellery': 24363,\n '童': 3042,\n 'continuity': 17661,\n 'tube': 11751,\n 'alc': 24678,\n 'indler': 27888,\n 'goodwill': 28680,\n 'coloration': 19115,\n 'gillian': 19116,\n 'reliance': 20447,\n 'tsar': 17991,\n 'reading': 8412,\n 'uses': 5831,\n 'accomplished': 13962,\n 'disregard': 29456,\n 'unrealistic': 29598,\n 'sentiments': 20846,\n 'performed': 5695,\n '怖': 2202,\n 'que': 4892,\n 'flying': 7352,\n 'matt': 8896,\n 'moscow': 12385,\n 'useless': 23359,\n 'forgive': 26394,\n 'geometry': 20045,\n 'candidacy': 20304,\n 'poo': 22650,\n 'holden': 16079,\n 'synthesizers': 21630,\n 'unsc': 26263,\n 'sportsman': 27613,\n 'bassist': 15134,\n '》': 1261,\n 'generating': 15992,\n 'stint': 19685,\n 'suspect': 11858,\n 'voy': 7444,\n 'courtesy': 25643,\n 'genre': 8852,\n 'arnhem': 29025,\n '藤': 3372,\n 'wholly': 19524,\n 'triad': 28194,\n 'vocalists': 27871,\n 'gh': 4225,\n 'gm': 17067,\n 'murderer': 21864,\n 'tutor': 22197,\n 'inflated': 27655,\n 'estim': 6014,\n 'fidel': 21395,\n 'bray': 23117,\n 'genetics': 20174,\n 'favorite': 9525,\n 'kali': 16915,\n 'intensifying': 23283,\n '動': 1661,\n 'pia': 24633,\n 'iversity': 25544,\n 'acquaint': 20379,\n 'lent': 16151,\n 'intact': 14548,\n 'toul': 19677,\n 'airfields': 19836,\n 'undergoes': 29115,\n 'estone': 14160,\n 'silky': 29946,\n 'ւ': 357,\n 'ர': 591,\n 'ya': 6704,\n 'seekers': 25355,\n 'damage': 5859,\n '心': 2190,\n 'astically': 18388,\n 'fawkes': 29851,\n 'grae': 29173,\n 'huffington': 26094,\n 'ા': 568,\n 'ჺ': 835,\n 'mm': 5636,\n 'vre': 14789,\n 'diet': 10073,\n 'matched': 14990,\n 'potassium': 17887,\n 'philosophers': 22231,\n 'ደ': 920,\n '芯': 3280,\n 'endorf': 27570,\n 'வ': 596,\n '1964': 8571,\n 'plead': 18408,\n 'brac': 19360,\n 'dignity': 21588,\n 'urgical': 22935,\n 'tonga': 28653,\n 'cho': 5117,\n 'caucasus': 26287,\n 'ク': 1337,\n 'manages': 14529,\n 'archer': 15888,\n '撃': 2330,\n '950': 22490,\n 'lenses': 25597,\n 'eon': 13200,\n '信': 1519,\n 'pamphlets': 29760,\n 'willamette': 22580,\n 'transmitted': 17926,\n 'vara': 24103,\n 'uf': 17879,\n 'controversy': 8895,\n 'roster': 13559,\n 'employers': 21283,\n 'legisl': 7766,\n 'դ': 327,\n 'replaces': 28161,\n 'rika': 25760,\n 'certified': 9410,\n 'wrestled': 24401,\n 'ice': 4354,\n 'elaborate': 13051,\n 'spans': 17110,\n '旅': 2370,\n 'finances': 19711,\n 'eps': 26370,\n 'oldest': 9471,\n '族': 2372,\n 'clients': 18909,\n 'ned': 5074,\n 'strum': 22470,\n 'scholar': 8498,\n 'finn': 10104,\n 'concern': 9845,\n 'certification': 17891,\n 'ᄉ': 846,\n '22nd': 17295,\n 'mell': 17588,\n '遼': 3660,\n '12': 4659,\n 'clou': 8293,\n 'capitals': 19968,\n 'recess': 23195,\n 'beneath': 12073,\n 'humanitarian': 18749,\n '浪': 2685,\n 'whites': 15202,\n 'wary': 23507,\n 'aleppo': 24825,\n 'breakout': 22479,\n 'pione': 10419,\n 'ᄈ': 845,\n 'worcester': 14755,\n 'panel': 11159,\n '☾': 1181,\n '○': 1170,\n 'divor': 10503,\n 'quinn': 15161,\n '299': 27998,\n 'holly': 7992,\n 'pasha': 21188,\n 'acle': 10907,\n 'prevention': 19091,\n 'zor': 24336,\n 'trac': 29602,\n 'swe': 6998,\n '284': 29512,\n 'acular': 13776,\n 'familiar': 10390,\n 'commissions': 19481,\n 'gunner': 23767,\n 'boundaries': 12890,\n 'chinook': 24840,\n 'meantime': 16596,\n 'achievements': 13802,\n '狀': 2825,\n 'ara': 6372,\n 'descends': 26315,\n 'ff': 5645,\n 'meet': 5703,\n 'impressive': 12903,\n 'sopho': 16559,\n 'pierce': 14934,\n 'melod': 11086,\n 'upbringing': 25715,\n 'yang': 14310,\n 'ifier': 17098,\n 'avon': 15781,\n 'collapsed': 11939,\n 'gangs': 28190,\n '220': 13155,\n 'regulation': 15038,\n 'except': 6416,\n 'secrecy': 25148,\n 'sier': 14544,\n 'strath': 26073,\n 'mach': 6198,\n 'flood': 6349,\n 'been': 4363,\n 'loss': 5805,\n '庫': 2126,\n 'geology': 17062,\n 'believing': 10859,\n 'botanist': 23386,\n 'staple': 21632,\n 'weiss': 24271,\n 'verge': 22911,\n 'trust': 8317,\n 'es': 4117,\n 'dens': 10437,\n 'mounted': 7745,\n 'preseason': 18448,\n 'acidic': 25143,\n 'explains': 11302,\n 'bud': 6426,\n 'err': 6632,\n 'turnpike': 12471,\n 'hun': 5256,\n '50': 4830,\n 'psycho': 9298,\n 'newspapers': 10477,\n 'infan': 6220,\n 'obsolete': 18805,\n 'tastes': 24820,\n 'goes': 8151,\n 'domain': 12858,\n 'bread': 14050,\n 'gird': 25371,\n 'compul': 16941,\n '毅': 2607,\n 'anta': 7931,\n 'approximately': 6596,\n 'itals': 12216,\n 'ᛠ': 959,\n 'จ': 698,\n '丹': 1407,\n 'fm': 13410,\n '昏': 2386,\n 'anity': 21080,\n 'relay': 15804,\n 'istol': 8577,\n 'yer': 7334,\n 'understand': 10253,\n 'favourable': 15338,\n 'offensives': 28811,\n '分': 1617,\n '潤': 2745,\n 'turtle': 17257,\n 'azz': 8576,\n 'after': 4293,\n '加': 1652,\n 'attende': 23221,\n 'correspond': 8569,\n ...}"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.token_to_id(\"[SEP]\"))\n",
    "print(tokenizer.id_to_token(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the given sequence and pair. This method can process raw text sequences as well as already pre-tokenized sequences.\n",
    "output = tokenizer.encode(\n",
    "    sequence=\"Hello, y'all! How are you 😁 ?\",  # 未分好词\n",
    "    is_pretokenized=False)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?']\n",
      "[22477, 16, 67, 11, 4190, 5, 5405, 4200, 4815, 0, 35]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(output.tokens)\n",
    "print(output.ids)\n",
    "print(output.type_ids)  # The generated type IDs\n",
    "print(output.attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'pre', 'token', 'ized', 'sequence', 'and', 'its', 'pair']\n",
      "[43, 4333, 26647, 4806, 7453, 4112, 4269, 5704]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_pair = tokenizer.encode(\n",
    "    sequence=[\"A\", \"pre\", \"tokenized\", \"sequence\"],  # 已经分好词\n",
    "    pair=[\"And\", \"its\", \"pair\"],\n",
    "    # Whether the input is already pre-tokenized\n",
    "    is_pretokenized=True\n",
    ")\n",
    "print(output_pair.tokens)\n",
    "print(output_pair.ids)\n",
    "print(output_pair.type_ids)\n",
    "print(output_pair.attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Post-Processors\n",
    "\n",
    "After the whole pipeline, we sometimes want to insert some special tokens before feed a tokenized string into a model like ”[CLS] My horse is amazing [SEP]”. The PostProcessor is the component doing just that."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "# Provides a way to specify templates in order to add the special tokens to each input sequence as relevant.\n",
    "# Let’s take BERT tokenizer as an example. It uses two special tokens, used to delimitate each sequence. [CLS] is always used at the beginning of the first sequence, and [SEP] is added at the end of both the first, and the pair sequences. The final result looks like this:\n",
    "# Then, we specify the template for sentence pairs, which should have the form \"[CLS] $A [SEP] $B [SEP]\" where $A represents the first sentence and $B the second one. The :1 added in the template represent the type IDs we want for each part of our input: it defaults to 0 for everything (which is why we don’t have $A:0) and here we set it to 1 for the tokens of the second sentence and the last \"[SEP]\" token.\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    # The template used for single sequences\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    # The template used when both sequences are specified\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    #  The list of special tokens used in each sequences\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n",
      "[1, 22477, 16, 67, 11, 4190, 5, 5405, 4200, 4815, 0, 35, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_temp = tokenizer.encode(\n",
    "    sequence=\"Hello, y'all! How are you 😁 ?\",  # 未分好词\n",
    "    is_pretokenized=False)\n",
    "\n",
    "print(output_temp.tokens)\n",
    "print(output_temp.ids)\n",
    "print(output_temp.type_ids)\n",
    "print(output_temp.attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'a', 'pre', 'token', 'ized', 'sequence', '[SEP]', 'and', 'its', 'pair', '[SEP]']\n",
      "[1, 43, 4333, 26647, 4806, 7453, 2, 4112, 4269, 5704, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_pair_temp = tokenizer.encode(\n",
    "    sequence=[\"A\", \"pre\", \"tokenized\", \"sequence\"],  # 已经分好词\n",
    "    pair=[\"And\", \"its\", \"pair\"],\n",
    "    # Whether the input is already pre-tokenized\n",
    "    is_pretokenized=True\n",
    ")\n",
    "\n",
    "print(output_pair_temp.tokens)\n",
    "print(output_pair_temp.ids)\n",
    "print(output_pair_temp.type_ids)\n",
    "print(output_pair_temp.attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
      "\n",
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', '[SEP]']\n",
      "[1, 22477, 16, 67, 11, 4190, 5, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "['[CLS]', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n",
      "[1, 5405, 4200, 4815, 0, 35, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_batch = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you 😁 ?\"])\n",
    "print(output_batch, end='\\n\\n')\n",
    "\n",
    "# 第一个句子\n",
    "print(output_batch[0].tokens)  # 长度为8\n",
    "print(output_batch[0].ids)\n",
    "print(output_batch[0].type_ids)\n",
    "print(output_batch[0].attention_mask, end='\\n\\n')\n",
    "\n",
    "# 第二个句子\n",
    "print(output_batch[1].tokens)  # 长度为7\n",
    "print(output_batch[1].ids)\n",
    "print(output_batch[1].type_ids)\n",
    "print(output_batch[1].attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.token_to_id('[PAD]'))\n",
    "\n",
    "# Enable the padding\n",
    "tokenizer.enable_padding(\n",
    "    pad_id=3,  # The id to be used when padding\n",
    "    pad_token=\"[PAD]\",  # The pad token to be used when padding\n",
    "    pad_type_id=0)  # The type id to be used when padding\n",
    "\n",
    "# Enable truncation\n",
    "tokenizer.enable_truncation(\n",
    "    max_length=512)  # 截断的最大长度"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
      "\n",
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', '[SEP]']\n",
      "[1, 22477, 16, 67, 11, 4190, 5, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "['[CLS]', 'how', 'are', 'you', '[UNK]', '?', '[SEP]', '[PAD]']\n",
      "[1, 5405, 4200, 4815, 0, 35, 2, 3]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "output_batch_padding = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you 😁 ?\"])\n",
    "print(output_batch_padding, end='\\n\\n')\n",
    "\n",
    "# 第一个句子\n",
    "print(output_batch_padding[0].tokens)\n",
    "print(output_batch_padding[0].ids)\n",
    "print(output_batch_padding[0].type_ids)\n",
    "print(output_batch_padding[0].attention_mask, end='\\n\\n')\n",
    "\n",
    "# 第二个句子(编码结果与第一个句子等长,通过'[PAD']填充)\n",
    "print(output_batch_padding[1].tokens)\n",
    "print(output_batch_padding[1].ids)\n",
    "print(output_batch_padding[1].type_ids)\n",
    "print(output_batch_padding[1].attention_mask)  # 填充部分为0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "# WordPiece Decoder\n",
    "tokenizer.decoder = decoders.WordPiece(prefix='##')  # 选择解码器(根据Pre-tokenizers来选择)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "\"hello, y'all!\""
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the given list of ids back to a string\n",
    "tokenizer.decode([1, 22477, 16, 67, 11, 4190, 5, 2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "# Save the :class:`~tokenizers.Tokenizer` to the file at the given path.\n",
    "tokenizer.save(\"tokenizer-wiki.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "<tokenizers.Tokenizer at 0x1f29ae737d0>"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a new :class:`~tokenizers.Tokenizer` from the file at the given path.\n",
    "tokenizer_load = Tokenizer.from_file(\"tokenizer-wiki.json\")\n",
    "tokenizer_load"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}