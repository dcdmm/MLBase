{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;对样本$ (\\mathbf{x}, y)$,传统回归模型通常直接基于模型输出$f(\\mathbf{x})$与真实输出$y$直接的差别来计算损失,\n",
    "当且仅当$f(\\mathbf{x})$与$y$完全相同时,损失才为零.于此不同,支持向量回归(Support Vector Regression,简称SVR)假设我们能够\n",
    "容忍$f(\\mathbf{x})$与$y$之间最多有$\\epsilon$的偏差,即仅当$f(\\mathbf{x})$与$y$的差别绝对值大于$\\epsilon$时才计算损失.\n",
    "如下图所示,这相当于以$f(\\mathbf{x})$为中心,构建了一个宽度为$2 \\epsilon$的间隔带,若训练样本落入此间隔带,则认为是被预测正确的.    \n",
    "\n",
    "<img src=\"../../../Other/img/svr1.png\" style=\"width:500px;height:400px;float:bottom\">\n",
    "\n",
    "&emsp;&emsp;于是SVR问题可以形式化为    \n",
    "$$ \\min_{\\mathbf{w}, b} \\quad \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^N l_{\\epsilon} (f(\\mathbf{x}_i) - y_i) $$       \n",
    "其中,$C$为正则化常数,$ l_{\\epsilon} $是下图所示的$\\epsilon$-不敏感损失函数       \n",
    "\n",
    "\\begin{equation}\n",
    "l_{\\epsilon}(z) = \\begin{cases}\n",
    "\t\t0, & \\text{if} \\quad  |z| \\leq \\epsilon \\\\\n",
    "        |z| - \\epsilon, & \\mathrm{otherwise}\n",
    "     \\end{cases}\n",
    "\\end{equation}   \n",
    "\n",
    "<img src=\"../../../Other/img/svr2.png\" style=\"width:500px;height:400px;float:bottom\">\n",
    "\n",
    "&emsp;&emsp;引入松弛变量$\\xi_i$和$\\hat{\\xi}_i$,可将SVR问题重写为       \n",
    "\n",
    "\\begin{align}\n",
    "& \\min_{\\mathbf{w}, b, \\xi_i \\hat{\\xi}_i} \\quad \\frac{1}{2} ||\\mathbf{w}||^2 + C\\sum_{i=1}^N (\\xi_i + \\hat{\\xi}_i) \\\\\n",
    "& s.t.  \\qquad f(\\mathbf{x}_i) -y_i \\leq \\epsilon + \\xi_i \\\\\n",
    "&\\qquad \\qquad y_i - f(\\mathbf{x}_i) \\leq \\epsilon + \\hat{\\xi}_i  \\\\\n",
    "&\\qquad \\qquad \\xi_i \\geq 0, \\hat{\\xi}_i \\geq 0, \\quad i=1,2,\\dots, N\n",
    "\\end{align}\n",
    "\n",
    "&emsp;&emsp;与求解线性支持向量机的对偶问题类似,SVR的对偶问题是\n",
    "\n",
    "\\begin{align}\n",
    "& \\max_{\\alpha, \\hat{\\alpha}} \\quad \\sum_{i=1}^N y_i(\\hat{\\alpha}_i - \\alpha_i) - \\epsilon (\\hat{\\alpha}_i + \\alpha_i) -\\frac{1}{2}  \\sum_{i=1}^N \\sum_{j=1}^N (\\hat{\\alpha}_i - \\alpha_i)(\\hat{\\alpha}_j - \\alpha_j) (\\mathbf{x}_i \\cdot \\mathbf{x}_j) \\\\\n",
    "& s.t. \\qquad \\sum_{i=1}^N (\\hat{\\alpha}_i - \\alpha_i)=0 \\\\\n",
    "&\\qquad \\qquad 0 \\leq \\alpha_i, \\hat{\\alpha}_i \\leq C  \\\\\n",
    "\\end{align}\n",
    "\n",
    "&emsp;&emsp;由KKT条件可解得SVR的解形如    \n",
    "$$ f(\\mathbf{x}) = \\sum_{i=1}^N (\\hat{\\alpha}_i - \\alpha_i) (\\mathbf{x} \\cdot \\mathbf{x}_i) + b $$     \n",
    "若$ 0 < \\alpha_i < C $,则必有$ \\xi_i=0 $,进而有      \n",
    "$$ b = y_i + \\epsilon - \\sum_{j=1}^N( \\hat{\\alpha}_j - \\alpha_j) (\\mathbf{x}_i \\cdot \\mathbf{x}_j) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}