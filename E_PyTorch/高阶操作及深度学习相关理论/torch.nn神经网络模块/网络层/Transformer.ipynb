{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "TransformerEncoderLayer(\n  (self_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n  )\n  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (dropout1): Dropout(p=0.1, inplace=False)\n  (dropout2): Dropout(p=0.1, inplace=False)\n)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 编码器Block\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512,  # 特征数目(E_q = E_k = E_v)\n",
    "                                           # 多头数\n",
    "                                           nhead=8,\n",
    "                                           # the dimension of the feedforward network model\n",
    "                                           dim_feedforward=2048,  # 默认dim_feedforward=2048\n",
    "                                           dropout=0.1,  # 默认dropout=0.1\n",
    "                                           activation='relu',  # 默认activattion='relu'\n",
    "                                           # the eps value in layer normalization components\n",
    "                                           layer_norm_eps=1e-05,  # 默认layer_norm_eps=1e05\n",
    "                                           # f True, layer norm is done prior to attention and feedforward operations, respectivaly. Otherwise it’s done after\n",
    "                                           norm_first=False,  # 默认norm_first=False\n",
    "                                           device=None)\n",
    "encoder_layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.8692, -0.1515,  1.3918,  ..., -1.7462, -0.8727, -0.8541],\n         [-0.9987, -0.4768,  0.4260,  ...,  1.2531, -0.9542,  0.3313],\n         [-2.1213, -0.7085,  0.4212,  ..., -0.1166, -0.3093,  1.9479],\n         ...,\n         [-0.3635, -1.0434,  1.0164,  ..., -0.6747, -1.4213,  1.6979],\n         [-1.8652,  0.2515,  1.6178,  ..., -0.7412,  0.5465,  1.4093],\n         [-0.3876,  0.7023,  1.2365,  ...,  1.1873, -0.3462,  0.9643]],\n\n        [[-0.0787, -1.0650, -1.4129,  ...,  1.6642, -0.3840, -0.4622],\n         [ 1.0155, -0.1173, -0.9439,  ...,  0.5986, -0.1919,  0.9009],\n         [-1.5084, -1.6509, -0.2305,  ...,  0.6838, -1.4779,  0.2462],\n         ...,\n         [-0.5524, -0.5299, -1.1652,  ...,  0.6802, -0.2298,  2.2992],\n         [-0.5457, -0.5121,  1.6386,  ..., -0.0680, -1.5193,  1.8945],\n         [-0.1030,  0.3906,  0.6759,  ..., -0.0294, -0.8602,  1.3786]],\n\n        [[-0.8895, -1.5635, -1.2232,  ...,  0.7131,  0.2084,  1.7777],\n         [-0.4362, -0.9548, -0.7247,  ..., -0.7186, -0.3510,  0.6049],\n         [-2.2871, -1.1118, -0.3742,  ...,  0.6686, -0.8116,  1.6705],\n         ...,\n         [-0.3035, -1.4338, -1.0569,  ...,  0.4052, -0.1116, -0.0506],\n         [-0.8258, -1.1028, -0.5787,  ...,  1.3320, -0.2172, -0.0295],\n         [-1.0733,  0.3608, -0.2667,  ...,  0.0362, -1.5052,  1.3287]],\n\n        ...,\n\n        [[-0.8333, -1.3868,  0.7016,  ...,  1.3189, -1.1155,  0.9422],\n         [-2.1265, -1.5269,  1.4582,  ..., -0.5629, -1.6107,  1.1221],\n         [-0.9398, -0.6286,  1.1621,  ..., -0.0720, -0.3643,  1.5331],\n         ...,\n         [-0.9509,  0.2993, -0.3503,  ..., -0.1135,  0.0736,  0.2322],\n         [-2.2094, -0.2741,  0.8955,  ...,  0.4286, -0.7145, -0.4387],\n         [-1.1319,  0.0730,  0.5312,  ...,  1.2775,  0.5306,  0.9276]],\n\n        [[-1.0618, -0.0505,  0.1500,  ...,  0.2006,  0.0738,  0.0809],\n         [-1.3185, -1.6536,  0.5260,  ...,  0.4643, -0.6251,  0.3693],\n         [-0.1955, -2.0901,  0.0542,  ..., -0.0283,  0.2258,  0.2920],\n         ...,\n         [ 0.1872,  0.3827,  0.4474,  ...,  1.1544, -0.1382, -0.6057],\n         [-0.1575,  0.1174,  0.5814,  ..., -0.7466, -1.4746,  1.0961],\n         [ 0.0337, -1.8883,  0.1394,  ..., -0.3216, -1.3149,  0.1085]],\n\n        [[-1.2829, -1.1551,  0.4250,  ...,  1.5707,  0.1149,  1.9818],\n         [-1.0970,  0.1878,  0.3345,  ...,  0.6387, -0.1440,  2.4662],\n         [-1.4418, -0.9856, -0.9159,  ...,  0.2166, -0.7241,  1.6989],\n         ...,\n         [-1.7267,  0.2130, -0.1551,  ..., -0.3368, -0.7345,  0.4625],\n         [-1.2708,  0.3426,  1.3429,  ...,  1.1331, -0.4728,  2.0423],\n         [-2.7480, -1.0236, -0.0581,  ..., -0.1081, -0.5533,  1.9875]]],\n       grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.rand(10, 32, 512)  # 输入形状: (batch, seq, feature)\n",
    "encoder_layer(\n",
    "    # the sequence to the encoder layer (required)\n",
    "    src=src,\n",
    "    # the mask for the src sequence (optional)\n",
    "    src_mask=None)  # 多头注意力中的attn_mask参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.0774,  0.8736,  0.5131,  ..., -1.3639, -1.7990, -0.7536],\n         [-0.6161,  0.2664,  0.8244,  ..., -0.5872, -1.8289, -0.5937],\n         [-0.7910,  0.8121,  1.0874,  ..., -0.7002, -0.6048, -1.0531],\n         ...,\n         [-1.5661,  0.4009,  1.5873,  ..., -2.1164, -1.6451,  0.5340],\n         [-2.4691, -0.1829,  0.6436,  ..., -1.9597, -1.5631,  1.0037],\n         [-0.7987,  0.4233,  0.4577,  ..., -1.4027, -0.9080, -0.2784]],\n\n        [[-0.2829,  0.3466,  0.0111,  ..., -0.2984, -1.8073, -0.3881],\n         [-0.6042,  0.2372,  0.5459,  ..., -0.0340, -1.2929, -0.1769],\n         [-0.5447, -0.1324, -0.3678,  ...,  0.7825, -1.5298, -0.6594],\n         ...,\n         [-0.8469, -0.0421, -0.2172,  ..., -1.7386, -1.6365, -0.1385],\n         [-1.0369, -0.2661,  1.2981,  ..., -0.3540, -2.0920,  0.5069],\n         [-0.6639, -0.2423,  0.3724,  ..., -1.4955, -1.0934,  0.1709]],\n\n        [[-0.4786,  0.5861, -0.9579,  ..., -0.7987, -2.2739,  0.5027],\n         [-0.4958, -0.1908,  0.5486,  ..., -1.3566, -2.0612, -0.4554],\n         [-0.8219, -0.0211,  0.2681,  ..., -0.2295, -1.8459, -0.6599],\n         ...,\n         [-1.4480,  0.3517,  0.4813,  ..., -1.5292, -1.1587, -0.1054],\n         [-0.6321, -0.1371,  1.0855,  ..., -0.5341, -1.8176,  0.7947],\n         [-1.5021,  0.7124,  0.6117,  ..., -0.8380, -1.5865, -0.4144]],\n\n        ...,\n\n        [[-0.9234, -0.0258, -0.2219,  ..., -1.1670, -1.7514,  0.5146],\n         [-1.0851,  0.1670,  0.9933,  ..., -0.5256, -1.9955,  0.4086],\n         [-0.9735,  0.5268,  1.4266,  ..., -1.2315, -1.8678, -1.3542],\n         ...,\n         [-2.0963,  0.5319,  0.2681,  ..., -1.3200, -1.2409, -0.5373],\n         [-1.0887, -0.4519,  0.7643,  ..., -1.3741, -2.0657,  0.4016],\n         [-1.4651,  0.2421,  0.5971,  ..., -0.9242, -0.7704,  0.4718]],\n\n        [[-0.9778,  0.6445, -0.0606,  ..., -0.3550, -2.2193, -0.1585],\n         [-1.1594, -0.6847,  0.1558,  ..., -1.7398, -1.8801,  0.0061],\n         [-0.3829,  0.1587,  0.7845,  ..., -0.9509, -0.6983, -1.5706],\n         ...,\n         [-1.5110,  0.8661,  0.5164,  ..., -1.5541, -1.3069, -0.2914],\n         [-0.6183, -0.0784,  0.3959,  ..., -1.0793, -1.6397,  0.2484],\n         [-0.9880, -0.3123,  0.5200,  ..., -1.4085, -1.3550, -0.4612]],\n\n        [[-1.1710,  0.6958,  0.4118,  ..., -0.7075, -1.3398, -0.1888],\n         [-0.6490,  0.3073,  0.3140,  ..., -0.5157, -1.5941, -0.0805],\n         [-0.4006, -0.0374,  0.1232,  ...,  0.1439, -1.4234, -0.7624],\n         ...,\n         [-1.7943,  0.5438,  0.1099,  ..., -1.9909, -1.6761,  0.0501],\n         [-1.5233,  0.8174,  1.5332,  ..., -0.3929, -2.1546,  0.9198],\n         [-2.4291, -0.1527,  0.9626,  ..., -1.5463, -1.1416, -0.2956]]],\n       grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 编码器\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layer,\n",
    "                                            num_layers=6)\n",
    "out_src_en = transformer_encoder(src, mask=None)\n",
    "out_src_en"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "TransformerDecoderLayer(\n  (self_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n  )\n  (multihead_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n  )\n  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (dropout1): Dropout(p=0.1, inplace=False)\n  (dropout2): Dropout(p=0.1, inplace=False)\n  (dropout3): Dropout(p=0.1, inplace=False)\n)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 解码器Block\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512,\n",
    "                                           nhead=8,\n",
    "                                           dim_feedforward=2048,  # 默认dim_feedforward=2048\n",
    "                                           dropout=0.1,  # 默认dropout=0.1\n",
    "                                           activation='relu',  # 默认activattion='relu'\n",
    "                                           layer_norm_eps=1e-05,  # 默认layer_norm_eps=1e05\n",
    "                                           norm_first=False,  # 默认norm_first=False\n",
    "                                           device=None)\n",
    "decoder_layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.7794,  0.3020,  0.8144,  ...,  0.2260,  0.7677, -1.4454],\n         [ 0.5657,  1.9052,  0.5356,  ...,  0.7191, -0.4774,  0.1977],\n         [ 0.7530, -0.5442, -0.6075,  ..., -1.3486,  0.1988, -2.2017],\n         ...,\n         [-1.1420,  0.1529, -1.4197,  ..., -1.2877,  1.3177, -1.4238],\n         [ 0.0899,  1.4249, -0.4623,  ...,  0.5833,  2.5259, -1.2417],\n         [-0.4235,  0.4794,  0.9448,  ..., -0.0151,  0.5411, -1.6328]],\n\n        [[-1.3327,  1.3460,  0.4856,  ...,  0.1713,  2.2537,  0.0601],\n         [-0.8601,  1.9882, -0.0268,  ..., -0.7024, -0.3792, -1.5450],\n         [-0.9184, -0.6669, -0.2834,  ..., -0.9357, -0.4182, -0.1712],\n         ...,\n         [ 0.7501,  1.5452, -0.7583,  ..., -0.2121, -0.3210, -0.7857],\n         [ 1.2439,  0.4849, -1.2655,  ..., -1.3022, -0.3788,  0.3491],\n         [ 0.2848,  1.2148,  1.1248,  ..., -0.4752,  1.3973, -0.3600]],\n\n        [[ 0.6340, -1.0626, -0.7325,  ...,  1.0751, -1.1455, -0.7567],\n         [ 1.4131,  0.0188,  1.3580,  ...,  0.9399, -0.2056, -1.4580],\n         [ 1.1005, -0.2118, -0.8366,  ..., -1.0426,  0.1888,  0.2645],\n         ...,\n         [ 1.3564, -0.7895,  0.4834,  ..., -0.2628,  0.3409, -0.9364],\n         [ 0.2025,  0.3898, -0.6329,  ..., -1.1642,  1.8376,  0.8228],\n         [ 1.1520,  1.7673,  0.5788,  ..., -0.0069,  0.2451,  0.9156]],\n\n        ...,\n\n        [[ 0.6143,  1.6612,  0.7604,  ..., -0.0557,  1.4296, -0.1533],\n         [ 1.9401,  0.2595, -1.5036,  ..., -1.0465,  0.9764,  0.5346],\n         [ 0.3954,  1.0828, -1.3320,  ..., -2.0064, -0.1637,  0.6210],\n         ...,\n         [ 1.0488,  1.4036, -0.1966,  ..., -0.4539,  0.4915,  0.4507],\n         [ 1.2723, -0.0884, -1.3319,  ...,  0.2292,  0.7627, -1.1988],\n         [ 0.0681,  1.3370, -0.0821,  ..., -0.0598,  1.9231,  0.4593]],\n\n        [[ 1.8754, -0.5329, -1.5453,  ...,  0.2982,  0.2509, -2.0237],\n         [ 1.9073,  0.2485, -1.4537,  ...,  0.5326,  2.3352,  0.6217],\n         [ 1.7224,  0.5980,  0.1460,  ...,  0.1179, -0.0102, -1.7274],\n         ...,\n         [-0.7510,  0.3779, -0.5649,  ..., -1.8995, -0.2003, -0.1927],\n         [-0.8759,  1.8934, -1.4067,  ...,  0.0558,  0.0227, -1.1141],\n         [-0.8251,  0.4681, -1.7269,  ...,  0.6654,  1.3975, -0.5762]],\n\n        [[-0.2196, -0.4134, -0.4087,  ..., -0.7155,  1.8043, -1.2050],\n         [ 0.6303,  1.3316,  0.7399,  ...,  0.8932, -0.0744, -1.2749],\n         [-0.4146, -0.3917,  0.5561,  ..., -0.3033,  0.5403, -0.2706],\n         ...,\n         [ 0.1375,  2.1372,  0.6374,  ...,  0.0618,  1.5824, -0.6235],\n         [ 0.6629, -0.5520,  0.0559,  ..., -1.5842, -0.2540,  0.2517],\n         [ 1.2346, -0.2439,  0.9284,  ...,  1.0078,  1.3775,  0.4451]]],\n       grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "decoder_layer(\n",
    "    # the sequence to the decoder layer (required).\n",
    "    tgt=tgt,\n",
    "    # the sequence from the last layer of the encoder (required).\n",
    "    memory=memory,\n",
    "    # the mask for the tgt sequence (optional)\n",
    "    tgt_mask=None,\n",
    "    # the mask for the memory sequence (optional)\n",
    "    memory_mask=None\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.2073,  0.4202,  0.8138,  ...,  0.0120,  1.7593,  1.1015],\n         [ 0.0341,  1.7794,  1.1330,  ...,  0.6530,  1.2724,  1.2094],\n         [ 0.5981, -0.3376,  0.2681,  ..., -0.9261,  1.7796,  0.2171],\n         ...,\n         [-0.5149,  0.8838,  0.4348,  ..., -0.5097,  0.9922,  0.8278],\n         [-0.6107,  0.7843,  0.3483,  ..., -0.1543,  1.8903,  0.2653],\n         [-0.1463,  1.5205,  1.4868,  ..., -0.3387,  2.2893,  0.6776]],\n\n        [[-0.8949,  0.9534,  0.7246,  ...,  0.3101,  2.2774,  1.5423],\n         [ 0.0929,  0.8505,  0.4263,  ...,  0.0185,  0.5956,  0.6751],\n         [-0.1499, -0.3266, -0.3626,  ..., -0.5831,  0.5289,  1.0730],\n         ...,\n         [-0.4351,  1.1652,  0.7969,  ..., -0.5113,  1.3693,  0.6590],\n         [ 0.2015,  1.1427,  0.3377,  ..., -0.0245,  0.6900,  0.3974],\n         [ 0.1684,  0.7656,  0.6523,  ..., -0.2814,  1.8408,  0.7199]],\n\n        [[-0.1547, -0.1658,  0.7835,  ...,  0.3473,  0.6477,  1.1922],\n         [ 0.5024,  0.3546,  1.0316,  ...,  0.2708,  0.8725,  0.8018],\n         [ 0.3262,  0.0388, -0.1803,  ..., -0.3848,  1.2376,  0.8188],\n         ...,\n         [-0.2102,  1.0163,  0.7562,  ..., -0.2878,  0.9032, -0.0475],\n         [ 0.4391,  1.1150, -0.0172,  ...,  0.0086,  1.4910,  0.7293],\n         [-0.2016,  1.6455,  0.7635,  ..., -0.3659,  1.2356,  1.3438]],\n\n        ...,\n\n        [[ 0.0338,  0.7637,  1.1833,  ..., -0.0075,  2.3794,  0.8697],\n         [ 0.5684,  1.1136,  0.3004,  ...,  0.1191,  1.0760,  1.0211],\n         [-0.1650,  0.2291,  0.2662,  ..., -0.8455,  1.1305,  0.6824],\n         ...,\n         [-0.1761,  1.4850,  0.6020,  ..., -0.3073,  1.5702,  1.3894],\n         [ 0.3291, -0.0045,  0.4724,  ..., -0.2248,  1.6540, -0.1567],\n         [-0.4023,  1.0279,  0.5448,  ...,  0.0279,  1.8846,  0.8711]],\n\n        [[ 0.1145, -0.2149,  0.4923,  ..., -0.1527,  2.1451,  0.4180],\n         [ 0.3045,  0.4123,  0.0596,  ...,  0.4195,  1.9727,  1.7055],\n         [ 0.3180, -0.1309, -0.1581,  ..., -0.0117,  1.6372,  0.5442],\n         ...,\n         [-0.1855,  0.7912,  0.2683,  ..., -0.9200,  1.4358, -0.1292],\n         [-0.5690,  0.9660,  0.5982,  ..., -0.3569,  1.0536,  0.2811],\n         [-0.1663,  1.2321, -0.3613,  ..., -0.3243,  1.5490,  0.8966]],\n\n        [[-0.5239,  0.5472,  1.2245,  ..., -0.3711,  2.1966,  0.4998],\n         [-0.1759,  0.6645,  0.8120,  ...,  0.0747,  0.7668, -0.0599],\n         [-0.5413,  0.2049,  0.4078,  ..., -0.2938,  1.2818,  0.3515],\n         ...,\n         [-0.9868,  1.5535,  0.9560,  ...,  0.2772,  1.1005,  0.9167],\n         [ 0.3298,  0.2851,  0.8965,  ..., -0.5243,  2.0247,  1.2785],\n         [ 0.2536,  0.7601,  0.7279,  ..., -0.0320,  1.1521,  0.6944]]],\n       grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 编码器\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer=decoder_layer,\n",
    "                                            num_layers=6)\n",
    "out_src_de = transformer_decoder(tgt=tgt, memory=memory,\n",
    "                                 tgt_mask=None, memory_mask=None)  # 多头注意力中的attn_mask参数\n",
    "out_src_de"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "Transformer(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (4): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (5): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (4): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (5): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformer默认参数如下\n",
    "transformer_model_default = nn.Transformer(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    activation='relu',\n",
    "    custom_encoder=None,\n",
    "    custom_decoder=None,\n",
    "    layer_norm_eps=1e-05,\n",
    "    norm_first=False,\n",
    "    device=None)\n",
    "transformer_model_default"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "Transformer(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (4): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (5): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (4): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (5): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n)"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model_obj = nn.Transformer(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    activation='relu',\n",
    "    custom_encoder=transformer_encoder,  # 自定义编码器\n",
    "    custom_decoder=transformer_decoder,  # 自定义解码器\n",
    "    layer_norm_eps=1e-05,\n",
    "    norm_first=False,\n",
    "    device=None)\n",
    "transformer_model_obj"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transformer_model_default(src=src,\n",
    "                          tgt=tgt,\n",
    "                          tgt_mask=None,\n",
    "                          memory_mask=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}