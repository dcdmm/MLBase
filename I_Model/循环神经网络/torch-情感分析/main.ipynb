{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "from torchtext import datasets\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torchtext.datasets.imdb.IMDB at 0x1b91fa47850>"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    \"\"\"定义分词操作\"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "TEXT = data.Field(tokenize=tokenizer, include_lengths=True)\n",
    "LABEL = data.Field(sequential=False, unk_token=None, dtype=torch.float32)\n",
    "\n",
    "# 创造情感分析数据集\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)  # load the IMDb dataset(电影评论数据集)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# 数据集的长度\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "print(len(train_data.examples))\n",
    "print(len(test_data.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'Teachers', '\"', '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'Teachers', '\"', '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!']\n",
      "165\n",
      "490\n",
      "177\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "print(train_data.examples[0].text)  # 每条数据的文本\n",
    "\n",
    "print(len(train_data.examples[0].text))\n",
    "print(len(train_data.examples[1].text))\n",
    "print(len(train_data.examples[2].text))\n",
    "print(len(train_data.examples[3].text))  # 每条数据的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'pos'"
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.examples[0].label  # 每条数据的标签,这里为积极的情感"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = train_data.split(random_state=np.random.seed(1))  # 划分训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_dataset)}')\n",
    "print(f'Number of validation examples: {len(valid_dataset)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立词表\n",
    "TEXT.build_vocab(train_dataset, max_size=25000, vectors=\"glove.6B.100d\",\n",
    "                 unk_init=torch.Tensor.normal_,\n",
    "                 vectors_cache='D:/PythonCode/F_PyTorch/高阶操作及深度学习相关理论/torchtext自然语言处理/vector_cache')\n",
    "LABEL.build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 204174), (',', 193571), ('.', 165997), ('a', 110027), ('and', 110009), ('of', 101501), ('to', 94178), ('is', 76668), ('in', 61632), ('I', 54381), ('it', 53573), ('that', 49552), ('\"', 45198), (\"'s\", 43656), ('this', 42324), ('-', 37404), ('/><br', 35836), ('was', 35335), ('as', 30840), ('with', 30243)]\n",
      "Counter({'pos': 8793, 'neg': 8707})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "print(LABEL.vocab.freqs)  # 数量上基本上是1:1的比例,所以不需要对样本做重采样来保持正负样本比例均衡"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    shuffle=False,  # 这里可以不打乱顺序\n",
    "    sort_within_batch=True)  # IMDB内置了sort_key方法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:('[torch.cuda.LongTensor of size 49x64 (GPU 0)]', '[torch.cuda.LongTensor of size 64 (GPU 0)]')\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "torch.Size([49, 64]) \n",
      " tensor([[   0, 1561,    0,  ...,    0,   66, 3033],\n",
      "        [ 382, 8684,   45,  ...,   16,    9,    3],\n",
      "        [  42, 2504, 2497,  ...,   23,    2,  395],\n",
      "        ...,\n",
      "        [   8,   83,   40,  ...,    1,    1,    1],\n",
      "        [ 128,   68,   40,  ...,    1,    1,    1],\n",
      "        [   4,    4,   40,  ...,    1,    1,    1]], device='cuda:0')\n",
      "torch.Size([64]) \n",
      " tensor([49, 49, 49, 48, 48, 48, 48, 48, 48, 48, 48, 47, 47, 47, 47, 46, 46, 46,\n",
      "        46, 46, 46, 46, 46, 45, 45, 44, 44, 43, 43, 43, 42, 42, 42, 42, 42, 42,\n",
      "        41, 41, 41, 41, 41, 40, 40, 40, 38, 38, 38, 37, 37, 36, 35, 35, 35, 34,\n",
      "        34, 33, 33, 31, 30, 29, 24, 24, 20, 18], device='cuda:0')\n",
      "torch.Size([64]) \n",
      " tensor([0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 0., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(valid_iterator))\n",
    "print(batch)\n",
    "\n",
    "print(batch.text[0].shape, '\\n', batch.text[0])  # 长度为49,批次为64;1表示填充\n",
    "print(batch.text[1].shape, '\\n', batch.text[1])  # 每个批次的长度\n",
    "\n",
    "print(batch.label.shape, '\\n', batch.label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25002 100\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors  # 预训练词向量\n",
    "vocal_size, embedding_size = pretrained_embeddings.shape\n",
    "hidden_size = 256\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "out_size = 1\n",
    "num_layers = 2\n",
    "lr = 0.001  # 学习率\n",
    "weight_decay = 1e-5\n",
    "print(vocal_size, embedding_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "from BRNNModel import BRNN\n",
    "% run BRNNModel.py\n",
    "\n",
    "net = BRNN(vocal_size=vocal_size,\n",
    "           embedding_size=embedding_size,\n",
    "           hidden_size=hidden_size,\n",
    "           num_layers=num_layers,\n",
    "           dropout=dropout,\n",
    "           bidirectional=True,\n",
    "           out_size=out_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,810,857 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"计算要训练的参数个数\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(net):,} trainable parameters')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.1846, -0.9289,  1.1936,  ...,  0.4853,  1.7517, -0.6086],\n        [ 0.1281, -0.0542,  0.6163,  ..., -0.9423,  1.7000, -0.6532],\n        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n        ...,\n        [ 0.0700,  0.6116,  0.2543,  ..., -0.4421,  2.2551, -0.4453],\n        [-0.8700, -0.9032,  0.3782,  ...,  0.4581,  1.0631, -0.1945],\n        [-0.1995,  0.6562, -0.8827,  ..., -0.0890, -0.6464,  0.0147]])"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.embed.weight.data.copy_(pretrained_embeddings)  # 也可以使用from_pretrained方法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<pad>\n",
      "0\n",
      "1\n",
      "tensor([ 1.1846e+00, -9.2890e-01,  1.1936e+00, -1.4279e+00,  9.1318e-01,\n",
      "         2.0662e+00,  2.3511e+00,  1.1339e+00, -8.3455e-03, -5.0807e-01,\n",
      "        -1.5593e-01,  4.7428e-01,  4.1383e-01, -1.7115e+00, -2.0870e+00,\n",
      "         1.2422e+00, -2.4315e-01,  1.9318e+00,  4.5387e-01,  4.9044e-01,\n",
      "         1.3784e+00, -3.0550e-01, -6.7882e-01,  1.7509e-03,  3.9291e-01,\n",
      "        -1.8198e+00, -6.2958e-01,  3.9505e-01, -3.0487e-01, -1.5252e+00,\n",
      "        -1.4177e+00,  4.7480e-01, -1.6798e-01,  4.6515e-01,  5.0571e-01,\n",
      "        -9.1175e-02,  3.0353e-01,  8.2069e-01,  7.0419e-01,  1.4249e+00,\n",
      "         4.4996e-02,  9.8268e-01, -8.8870e-01, -1.5350e+00,  5.9949e-01,\n",
      "        -3.7801e-01, -1.8546e+00, -7.2452e-02,  1.2489e+00, -8.8147e-01,\n",
      "        -1.1103e+00,  3.2287e+00,  4.3954e-01, -1.8650e-01, -3.2149e-01,\n",
      "        -2.2880e-01, -6.5669e-02, -1.6626e-01,  8.6614e-01,  4.8681e-01,\n",
      "        -1.8137e-03, -1.8987e+00,  1.9274e+00,  1.2491e+00,  1.0557e+00,\n",
      "         1.9068e+00, -1.2057e-01,  1.4715e+00, -1.1862e+00,  4.9174e-03,\n",
      "         4.6795e-01, -2.2115e+00, -1.3694e+00, -2.1484e-01, -7.5709e-01,\n",
      "         1.6442e+00, -7.9983e-01, -4.2661e-01, -5.4484e-01,  6.9651e-01,\n",
      "        -7.3908e-02, -2.1130e+00, -4.4753e-01,  3.9070e-01,  6.4103e-01,\n",
      "        -5.4329e-01, -8.8557e-01, -1.9657e-01,  1.8472e+00,  1.7068e+00,\n",
      "        -1.3306e-01,  1.8741e+00,  2.0196e+00, -6.4659e-02, -5.3362e-01,\n",
      "         1.7420e+00,  1.2925e+00,  4.8533e-01,  1.7517e+00, -6.0855e-01])\n",
      "tensor([ 0.1281, -0.0542,  0.6163, -0.2586, -1.3888, -0.4756, -1.1480,  0.9521,\n",
      "        -0.0979,  1.4499, -0.0397,  0.0843,  0.8404,  0.0872, -1.6120,  0.7032,\n",
      "        -0.4652, -0.1915, -0.0246, -0.4111,  0.3766, -0.2683, -2.3905, -0.2373,\n",
      "         0.0291,  0.5117, -0.9882,  0.1000, -0.6344,  1.1793, -0.7247,  0.8932,\n",
      "         0.9692, -0.0167,  0.1665, -3.0043, -0.9298,  0.3007, -0.3019, -1.5057,\n",
      "        -0.8078, -0.0910,  0.0884, -2.2718,  2.0624,  1.2872,  0.6853,  0.4235,\n",
      "         0.5093, -0.9626, -0.8128,  0.0919, -0.1805,  0.8742,  0.7589,  0.2843,\n",
      "         0.7534, -0.1301,  0.4518, -0.3101,  1.1586,  0.2915,  0.6709,  0.4398,\n",
      "         0.4800, -0.9944,  1.1001, -0.5986,  1.6748, -0.3326, -0.5504,  2.9894,\n",
      "        -1.2233, -0.4429,  0.6698,  1.1147,  0.1373,  1.4806,  0.6367,  2.5664,\n",
      "        -0.4061,  0.5652,  0.5271, -0.6449,  0.4947, -1.4264, -0.4753,  0.8893,\n",
      "         0.6426,  0.3030,  0.4895, -1.6038, -0.6224,  0.1424, -0.5535,  0.5691,\n",
      "        -2.0741, -0.9423,  1.7000, -0.6532])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.unk_token)\n",
    "print(TEXT.pad_token)\n",
    "print(TEXT.vocab.stoi['<unk>'])\n",
    "print(TEXT.vocab.stoi['<pad>'])\n",
    "print(TEXT.vocab.vectors[TEXT.vocab.stoi[TEXT.unk_token]])  # unk的预训练词向量(这个预训练的词向量没有设置为0)\n",
    "print(TEXT.vocab.vectors[TEXT.vocab.stoi[TEXT.pad_token]])  # pad的预训练词向量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 0.0700,  0.6116,  0.2543,  ..., -0.4421,  2.2551, -0.4453],\n",
      "        [-0.8700, -0.9032,  0.3782,  ...,  0.4581,  1.0631, -0.1945],\n",
      "        [-0.1995,  0.6562, -0.8827,  ..., -0.0890, -0.6464,  0.0147]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "UNK_INDEX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PAD_INDEX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "net.embed.weight.data[UNK_INDEX] = torch.zeros(embedding_size)\n",
    "net.embed.weight.data[PAD_INDEX] = torch.zeros(embedding_size)\n",
    "print(net.embed.weight)  # 此时前2行被初始化为0向量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = nn.BCEWithLogitsLoss()  # 二分类的损失函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "from Senti_train import Train_Evaluate\n",
    "% run Senti_train.py\n",
    "trainer = Train_Evaluate(model=net, optimizer=optimizer, criterion=criterion, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "def epoch_time(start, end):\n",
    "    \"\"\"计算运行时间\"\"\"\n",
    "    elapsed_time = end - start\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.677 | Train Acc: 57.46%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 55.09%\n",
      "Epoch: 02 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.656 | Train Acc: 61.17%\n",
      "\t Val. Loss: 0.681 |  Val. Acc: 54.22%\n",
      "Epoch: 03 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.589 | Train Acc: 68.69%\n",
      "\t Val. Loss: 0.507 |  Val. Acc: 76.89%\n",
      "Epoch: 04 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.528 | Train Acc: 73.71%\n",
      "\t Val. Loss: 0.376 |  Val. Acc: 83.85%\n",
      "Epoch: 05 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.376 | Train Acc: 83.55%\n",
      "\t Val. Loss: 0.320 |  Val. Acc: 86.21%\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = trainer.train(train_iterator)\n",
    "    valid_loss, valid_acc = trainer.evaluate(valid_iterator)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(trainer.model.state_dict(), 'Senti_model.pth')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "data": {
      "text/plain": "BRNN(\n  (embed): Embedding(25002, 100)\n  (rnn): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n  (linear): Linear(in_features=512, out_features=1, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)"
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = BRNN(vocal_size=vocal_size,\n",
    "                  embedding_size=embedding_size,\n",
    "                  hidden_size=hidden_size,\n",
    "                  num_layers=num_layers,\n",
    "                  dropout=dropout,\n",
    "                  bidirectional=True,\n",
    "                  out_size=out_size)\n",
    "best_model.load_state_dict(torch.load('Senti_model.pth'))  # 加载模型\n",
    "best_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.330 | Test Acc: 85.58%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = Train_Evaluate(model=best_model, criterion=criterion, device=device).evaluate(test_iterator)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "data": {
      "text/plain": "<spacy.lang.en.English at 0x1b8a3217df0>"
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_en"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(None, {'pos': 0, 'neg': 1})"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi  # pos:用0表示,neg用1表示"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    \"\"\"预测句子的评价\"\"\"\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp_en.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9640713930130005"
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(best_model, \"fuck, garbage\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [
    {
     "data": {
      "text/plain": "0.994071900844574"
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(best_model, \"This film is terrible\")  # 越接近与1,越能代表为负面评价"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 66],\n",
      "        [ 24],\n",
      "        [  9],\n",
      "        [103]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.053380392491817474"
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(best_model, \"This film is great\")  # 越接近与0,越能代表为正面评价"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}