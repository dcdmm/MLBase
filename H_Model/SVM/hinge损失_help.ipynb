{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;软间隔最大化允许某些样本不满足约束       \n",
    "$$ y_i(\\mathbf{w} \\cdot \\mathbf{x}_i) + b \\geq 1 $$        \n",
    "当然,在最大化间隔的同时,不满足约束的样本应尽可能少.于是,优化目标函数可写为        \n",
    "$$ \\min_{\\mathbf{w}, b} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^N l_{0/1}(y_i(\\mathbf{w} \\cdot \\mathbf{x} + b) -1) \\tag{1}$$          \n",
    "其中$C>0$是一个常数,$l_{0/1}$是\"0/1损失函数\"          \n",
    "\n",
    "\\begin{equation}\n",
    "l_{0/1}(z)=\\begin{cases}\n",
    "\t\t1, & \\text{if} \\quad  z < 0 \\\\\n",
    "        0, & \\text{if} \\quad \\mathrm{otherwise}\n",
    "     \\end{cases}\n",
    "\\end{equation}          \n",
    "\n",
    "显然,当$C$为无穷大时,优化目标函数迫使所有样本均满足约束,此时软间隔最大化等价于硬间隔最大化;当$C$取有限值\n",
    "时,优化目标函数允许一些样本不满足约束.             \n",
    "&emsp;&emsp;然而,$l_{0/1}$非凸,非连续,数学性质不太好,使式(1)不易直接求解.于是,\n",
    "人们通常用一些函数来代替$l_{0/1}$,称为\"替代损失\"(surrogate loss).替代损失函数一般具有较好的数学性质,\n",
    "如它们通常是凸的连续函数且是$l_{0/1}$的上界.下图给出了三种常用的替代损失函数$ z=yf(\\mathbf{x}) $.             \n",
    "\n",
    "\\begin{align}\n",
    "& \\mathrm{hinge} 损失(比0/1损失更严格): l_{\\mathrm{hinge}}(z) = \\max\\{0, 1-z\\} \\\\\n",
    "& 指数损失(\\mathrm{exponential \\quad loss}):l_{\\exp}(x) = \\exp(-z)  \\\\\n",
    "& 对率损失(\\mathrm{logistic loss}): l_{log}(z) = \\log(1+\\exp(-z))\n",
    "\\end{align}           \n",
    "\n",
    "若采用$\\mathrm{hinge}$损失(确信度足够高时损失才是0,也就是说,合页损失函数对学习有更高的要求),优化目标函数变成             \n",
    "$$ \\min_{w, b} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^N \\max(0, y_i(\\mathbf{w} \\cdot \\mathbf{x} + b) -1) $$        \n",
    "引入\"松弛变量\"(slack variables)\"$\\xi_i \\geq 0 $,上式可重写为                 \n",
    "\n",
    "\\begin{align}\n",
    "& \\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^N \\xi_i \\\\\n",
    "& s.t. y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1- \\xi_i, \\quad i=1,2,\\cdots,N \\\\\n",
    "& \\xi_i \\geq 0, \\quad i=1,2,\\cdots,N\n",
    "\\end{align}          \n",
    "\n",
    "这就是常见的\"软间隔支持向量机\".           \n",
    "\n",
    "<img src=\"../../Other/img/svm2.png\" style=\"width:500px;height:300px;float:bottom\">\n",
    "\n",
    "&emsp;&emsp;我们还可以把优化目标函数中的$l_{0/1}$换成别的替代函数以得到其他学习模型,\n",
    "这些模型的性质与所用的替代函数直接相关,但是它们具有一个共性:优化目标中的第一项用来描述分离超平面的\"间隔\"大小,\n",
    "另一项$\\sum_{i=1}^{N}l(f(\\mathbf{x}_i), y_i)$用来表述训练集上的误差,可写为更一般的形式              \n",
    "$$ \\min_{f} \\Omega(f) + C\\sum_{i=1}^{N}l(f(\\mathbf{x}_i), y_i) $$               \n",
    "其中,,$\\Omega(f)$称为\"结构风险\"(structural risk),用于描述模型$f$的某些性质;第二项$C\\sum_{i=1}^{N}l(f(\\mathbf{x}_i), y_i)$称为\n",
    "\"经验风险\"(empirical risk),用以描述模型与训练数据集的契合程度;$C$用于对二者进行折中.从经验风险最小化的角度来看,$\\Omega(f)$表述了\n",
    "我们希望获得具有何种性质的模型(例如希望获得复杂度小的模型),这为引入领域知识和用户意图提供了途径;另一方面,该信息有助于\n",
    "消减假设空间,从而降低了最小化训练误差的过拟合风险.从这个角度来看,上式称为\"正则化\"(regularization)问题,$\\Omega(f)$为正则化项,$ C $则称\n",
    "为正则化常数.$L_p$范数(norm)是常用的正则化项,其中$L_2$范数$||\\mathbf{w}||_2$倾向于$\\mathbf{w}$的分量取值尽量均衡,即非零分量个数尽量稠密,\n",
    "而$ L_0 $范数$ ||\\mathbf{w}||_0 $和$ L_1 $范数$ ||\\mathbf{w}||_1 $则倾向于$w$的分量尽量稀疏,即非零分量个数尽量少.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
