{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "\n",
    "**Input**: Values of $x$ over a mini-batch: $\\mathcal{B}=\\{x_1,\\cdots,x_m \\}$;\n",
    "\n",
    "&emsp;&emsp;Parameters to be learned:$\\gamma,\\beta$\n",
    "\n",
    "**output**:$\\{ y_i = BN_{\\gamma,\\beta}(x_i) \\}$\n",
    "\n",
    "$$ \\mu_{\\mathcal{B}} = \\frac{1}{m} \\sum_{i=1}^m x_i   \\qquad//mini-bath \\,\\, mean $$\n",
    "\n",
    "$$ \\sigma^2_{\\mathcal{B}} = \\sum_{m}^m (x_i - \\mu_{\\mathcal{B}}) \\qquad//mini-bath \\,\\, variance $$\n",
    "\n",
    "$$ \\widehat{x}_{i} = \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} \\qquad //normalize $$\n",
    "\n",
    "$$ y_{i} =\\gamma \\widehat{x}_{i}+\\beta \\equiv \\mathrm{B} \\mathrm{N}_{\\gamma, \\beta}\\left(x_{i}\\right)  \\qquad// scale\\,\\, and \\,\\, shift $$\n",
    "\n",
    "***\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E(\\widehat{x}_{i} ) &= E \\left( \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} \\right) \\\\\n",
    "\t&= \\frac{1}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} E(x_{i}-\\mu_{\\mathcal{B}}) \\\\\n",
    "\t&= 0\n",
    "\\tag{1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{var}(\\widehat{x}_{i}) &= E \\left([\\widehat{x}_{i} - E(\\widehat{x}_{i})]^2 \\right) \\\\\n",
    "&= E \\left( \\widehat{x}_{i}^2 - 2\\widehat{x}_{i} E(\\widehat{x}_{i})+ [E(\\widehat{x}_{i})]^2 \\right) \\\\\n",
    "&=E(\\widehat{x}_{i}^2 ) - [E(\\widehat{x}_{i})]^2 \\\\\n",
    "&= E \\left( \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} \\right)^2 \\\\\n",
    "&= \\frac{1}{\\mathcal{B}^{2}+\\epsilon} E([x_{i}-\\mu_{\\mathcal{B}}]^2) \\\\\n",
    "&= \\frac{\\mathcal{B}^{2}}{\\mathcal{B}^{2}+\\epsilon}  \\\\\n",
    "& \\approx 1  \n",
    "\\tag{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;由上可知,算法步骤三将一个mini-batch的输入数据规范化为**近似**标准正态分布;步骤四对数据进行了缩放和平移,在这一步,使用学习得到的参数$\\gamma,\\beta$,恢复数据的特征表达,得到的结果为**近似**满足均值为$\\beta$,方差为$\\gamma^2$的正态分布. \n",
    "\n",
    "\n",
    "**批量归一化的作用**\n",
    "\n",
    "1. 应用BN层后,传递给后一层的输出数据,以及反向传播给前一层的误差损失,都经过一次缩放平移调整,从而避免了梯度弥散(数据重新回到非饱和区)和梯度爆炸问题,故对学习率的参数初始化的敏感度降低\n",
    "2. 缓和过拟合的作用,对模型起到了正则化效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
