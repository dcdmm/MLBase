{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n",
      "['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "主要预训练模型:\n",
    "'roberta-base'\n",
    "'roberta-large'\n",
    "'''\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "print(tokenizer.model_input_names)  # 动态编码\n",
    "print(tokenizer.all_special_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaConfig {\n  \"_name_or_path\": \"roberta-base\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.17.0\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for name, para in model.named_parameters():\n",
    "    print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 19163, 22098,    12,   805,  3092,    32,  3276,     7,   609,\n",
      "           251, 26929,   528,     7,    49,  1403,    12,  2611, 19774,  2513,\n",
      "             6,    61, 21423, 15694,   338, 23050,    19,     5, 13931,  5933,\n",
      "             4, 50118,  3972,  1100,    42, 22830,     6,    52,  6581,     5,\n",
      "          2597, 22098,    19,    41,  1503,  9562,    14, 21423, 24248, 23099,\n",
      "            19, 13931,  5933,     6,   442,    24,  1365,     7,   609,  2339,\n",
      "             9,  1583,     9, 22121,    50,  1181,     4, 50118, 21001, 22098,\n",
      "            17,    27,    29,  1503,  9562,    16,    10,  1874,    12,   179,\n",
      "          5010,    13,     5,  2526,  1403,    12,  2611, 19774,     8, 15678,\n",
      "            10,   400,  2931,   196,  1503,    19,    10,  3685,  7958,   720,\n",
      "          1503,     4, 50118, 20839,  2052,   173,    15,   251,    12, 46665,\n",
      "          7891,   268,     6,    52, 10516,  2597, 22098,    15,  2048,    12,\n",
      "          4483,  2777, 19039,     8,  3042,   194,    12,  1116,    12,   627,\n",
      "            12,  2013,   775,    15,  2788,   398,     8,  1177, 39224,   398,\n",
      "             4, 50118,  1121,  5709,     7,   144,  2052,   173,     6,    52,\n",
      "            67, 11857,  9946,  2597, 22098,     8,  8746,   594,  4438,    24,\n",
      "            15,    10,  3143,     9, 18561,  8558,     4, 50118,  2522, 11857,\n",
      "         26492,  2597, 22098,  6566,  9980, 33334,  3830, 11126, 38495,    15,\n",
      "           251,  3780,  8558,     8,  3880,    92,   194,    12,  1116,    12,\n",
      "           627,    12,  2013,   775,    15, 45569,    12, 30158,     8,  6892,\n",
      "         11409,  1864,   250,     4, 19163, 22098,    12,   805,  3092,    32,\n",
      "          3276,     7,   609,   251, 26929,   528,     7,    49,  1403,    12,\n",
      "          2611, 19774,  2513,     6,    61, 21423, 15694,   338, 23050,    19,\n",
      "             5, 13931,  5933,     4, 50118,  3972,  1100,    42, 22830,     6,\n",
      "            52,  6581,     5,  2597, 22098,    19,    41,  1503,  9562,    14,\n",
      "         21423, 24248, 23099,    19, 13931,  5933,     6,   442,    24,  1365,\n",
      "             7,   609,  2339,     9,  1583,     9, 22121,    50,  1181,     4,\n",
      "         50118, 21001, 22098,    17,    27,    29,  1503,  9562,    16,    10,\n",
      "          1874,    12,   179,  5010,    13,     5,  2526,  1403,    12,  2611,\n",
      "         19774,     8, 15678,    10,   400,  2931,   196,  1503,    19,    10,\n",
      "          3685,  7958,   720,  1503,     4, 50118, 20839,  2052,   173,    15,\n",
      "           251,    12, 46665,  7891,   268,     6,    52, 10516,  2597, 22098,\n",
      "            15,  2048,    12,  4483,  2777, 19039,     8,  3042,   194,    12,\n",
      "          1116,    12,   627,    12,  2013,   775,    15,  2788,   398,     8,\n",
      "          1177, 39224,   398,     4, 50118,  1121,  5709,     7,   144,  2052,\n",
      "           173,     6,    52,    67, 11857,  9946,  2597, 22098,     8,  8746,\n",
      "           594,  4438,    24,    15,    10,  3143,     9, 18561,  8558,     4,\n",
      "         50118,  2522, 11857, 26492,  2597, 22098,  6566,  9980, 33334,  3830,\n",
      "         11126, 38495,    15,   251,  3780,  8558,     8,  3880,    92,   194,\n",
      "            12,  1116,    12,   627,    12,  2013,   775,    15, 45569,    12,\n",
      "         30158,     8,  6892, 11409,  1864,   250,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "torch.Size([1, 408])\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length.\n",
    "To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer.\n",
    "Longformer’s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention.\n",
    "Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8.\n",
    "In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks.\n",
    "Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on Wiki-Hop and TriviaQA.\"\"\"\n",
    "text = text * 2\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "print(encoded_input.keys())\n",
    "print(encoded_input['input_ids'].shape)  # [1, 408]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.7965e-02,  1.0580e-01,  8.5783e-04,  ..., -6.5853e-02,\n",
      "          -3.6090e-03, -2.2129e-02],\n",
      "         [-2.5865e-03,  1.6560e-01, -1.0407e-01,  ..., -9.0544e-02,\n",
      "          -4.3457e-02,  4.4587e-03],\n",
      "         [ 1.0382e-02, -8.6941e-02, -1.9525e-01,  ..., -1.0271e+00,\n",
      "           1.1050e-01,  4.4149e-01],\n",
      "         ...,\n",
      "         [-1.5290e-01, -3.3107e-01, -1.9680e-01,  ..., -2.4695e-01,\n",
      "           3.6188e-01,  1.6483e-01],\n",
      "         [-1.0169e-02,  1.0597e-01, -3.4931e-02,  ..., -1.3704e-01,\n",
      "          -2.3220e-03, -7.7600e-02],\n",
      "         [ 1.1030e-01,  7.8143e-02,  7.2378e-02,  ...,  9.1892e-02,\n",
      "           9.6042e-02,  4.4672e-02]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 408, 768])\n",
      "tensor([[-1.0994e-02, -1.9034e-01, -1.7558e-01, -5.5791e-02,  1.2444e-01,\n",
      "          1.9599e-01,  2.3791e-01, -1.0246e-01, -6.4833e-02, -1.4011e-01,\n",
      "          2.6034e-01,  2.8523e-02, -1.2916e-01,  8.8719e-02, -1.1078e-01,\n",
      "          4.4794e-01,  2.2751e-01, -4.6219e-01,  8.2148e-02, -2.2255e-03,\n",
      "         -2.2751e-01,  1.0433e-01,  4.5724e-01,  3.6334e-01,  1.0466e-01,\n",
      "          3.0131e-02, -1.5261e-01, -4.2597e-02,  1.3602e-01,  2.3867e-01,\n",
      "          2.7376e-01,  2.9228e-02,  1.0041e-01,  2.7978e-01, -2.3232e-01,\n",
      "          5.4016e-02, -2.9783e-01, -5.5564e-03,  2.7472e-01, -1.9868e-01,\n",
      "         -7.5016e-02,  1.7732e-01,  1.7710e-01, -1.5572e-01, -1.0521e-01,\n",
      "          3.9179e-01,  2.3094e-01,  8.8197e-03, -1.3270e-01, -4.8729e-02,\n",
      "         -3.1264e-01,  3.2978e-01,  2.8384e-01,  2.1433e-01, -2.9454e-02,\n",
      "          7.2633e-03, -1.0091e-01,  2.6324e-01, -5.3540e-02, -9.4403e-02,\n",
      "         -1.1059e-01, -2.1048e-01,  1.3554e-03, -6.3659e-02, -1.2247e-02,\n",
      "         -1.4640e-01,  9.7868e-02, -1.2631e-01, -1.4603e-01,  5.9679e-02,\n",
      "         -1.0646e-01,  1.3619e-01,  1.7248e-01, -2.8683e-01, -2.5688e-01,\n",
      "          6.3636e-02, -5.7732e-01, -7.1466e-02,  3.0351e-01,  4.1273e-01,\n",
      "         -7.1756e-02,  2.1555e-01,  2.4610e-02,  1.6424e-01, -2.0535e-02,\n",
      "         -4.9204e-02, -3.4548e-02, -1.3143e-01,  1.6077e-01,  2.4665e-01,\n",
      "         -2.0520e-01, -4.0473e-01,  8.5687e-02, -8.4065e-04, -7.8845e-02,\n",
      "          5.3744e-02, -2.9450e-02, -1.0520e-01, -1.7476e-01, -1.7038e-01,\n",
      "          1.0391e-01, -2.3091e-01, -1.4681e-01,  2.4665e-01,  4.0871e-02,\n",
      "         -1.7536e-01, -3.8194e-05,  3.1722e-01,  5.4762e-02, -8.1977e-02,\n",
      "         -1.6298e-01,  4.5066e-01,  2.9609e-01, -2.7043e-02,  1.6081e-02,\n",
      "          1.4899e-01,  1.4368e-01, -2.5767e-01,  4.3090e-01, -2.9099e-01,\n",
      "         -1.7911e-02, -8.6931e-02,  1.5205e-01,  1.6194e-01, -2.2069e-01,\n",
      "          2.4900e-01,  1.6146e-01,  2.6869e-01,  1.6807e-01,  1.1501e-01,\n",
      "         -5.0574e-02,  1.4558e-01, -1.5258e-01,  1.2455e-01,  2.3319e-01,\n",
      "          8.5818e-02,  3.8035e-02, -3.3805e-01, -2.3638e-01,  2.7197e-01,\n",
      "          3.0291e-01,  1.4835e-01, -2.6325e-02,  1.7918e-01,  1.3491e-01,\n",
      "          2.0233e-01,  1.2914e-01, -3.6035e-01,  6.9562e-02,  3.3219e-01,\n",
      "          1.1646e-01,  1.0395e-01, -1.1225e-01, -2.7800e-01, -2.7675e-01,\n",
      "         -6.4719e-02,  1.8911e-02, -3.4885e-01, -1.1522e-01,  3.4022e-01,\n",
      "          6.9710e-02, -6.0531e-02, -1.8903e-01, -2.0177e-01, -2.3815e-02,\n",
      "         -1.2835e-01,  2.6867e-02,  6.8578e-02, -3.7361e-02, -4.1684e-01,\n",
      "         -1.5577e-01, -5.2289e-01, -8.2935e-02,  2.0752e-01, -2.8772e-01,\n",
      "          2.8916e-01, -2.4034e-01,  8.8972e-02,  3.9320e-01,  6.3131e-02,\n",
      "         -2.0877e-02, -2.4113e-01, -7.2969e-02,  8.5289e-02,  3.0303e-01,\n",
      "          2.6541e-01, -3.9319e-01,  1.3862e-01,  1.4234e-01,  2.7909e-01,\n",
      "          9.7334e-02, -5.3496e-02, -1.2457e-01,  1.1467e-01, -2.2381e-01,\n",
      "          1.9659e-01, -1.9456e-01,  1.4112e-01, -2.3791e-01, -2.1013e-01,\n",
      "          3.1944e-01, -3.9788e-01, -6.8359e-02,  9.2595e-02,  2.3189e-01,\n",
      "         -1.6588e-04, -6.3721e-02, -1.1747e-01,  1.2637e-01,  1.8141e-01,\n",
      "          1.3287e-01, -4.0471e-01,  2.7973e-01, -1.2603e-02, -5.6257e-02,\n",
      "         -7.1583e-02,  1.6139e-01,  2.3964e-01,  9.7273e-02, -3.6111e-01,\n",
      "         -1.3926e-01,  1.4569e-01,  2.7835e-01, -2.2113e-01,  1.6099e-01,\n",
      "         -3.0845e-01, -4.1207e-01, -1.1053e-01,  2.1895e-01,  1.9785e-01,\n",
      "          1.6158e-01, -2.8074e-01,  1.8001e-01, -1.5277e-01, -4.1687e-01,\n",
      "         -3.5172e-01, -1.1223e-01,  2.4551e-01,  1.5447e-01,  1.7128e-01,\n",
      "          2.5347e-01,  2.2198e-02,  1.2946e-01,  1.4366e-01,  1.7166e-01,\n",
      "         -1.2294e-01,  1.5806e-01, -3.8866e-01, -5.6176e-02, -3.0466e-01,\n",
      "         -2.1200e-01, -1.8075e-01,  4.1050e-01, -2.4191e-01,  2.1915e-01,\n",
      "          4.1410e-01, -3.1590e-01, -1.1594e-01,  1.3711e-01,  1.2478e-01,\n",
      "          9.4899e-02, -1.2920e-01,  2.2191e-01,  1.7874e-01, -6.5980e-02,\n",
      "          2.4580e-01, -2.7286e-02,  2.5106e-01,  1.3790e-01,  5.4709e-02,\n",
      "          1.0356e-01,  1.5284e-01, -1.3840e-01,  8.7170e-02,  1.5816e-02,\n",
      "         -3.6532e-02, -2.6609e-01, -1.5678e-01,  1.9824e-01, -5.1680e-02,\n",
      "          2.2440e-02, -1.5632e-01, -2.6314e-02,  2.3182e-02,  3.9275e-01,\n",
      "         -3.6227e-01,  2.6456e-01,  6.4394e-02,  1.4616e-01, -2.1302e-01,\n",
      "         -2.1074e-01,  9.3911e-02,  2.2840e-01, -3.9072e-01,  3.4955e-02,\n",
      "          1.3624e-01,  1.2146e-01,  1.9449e-01,  2.7962e-01,  2.1819e-02,\n",
      "         -1.1837e-01,  5.2810e-01, -1.7104e-01, -1.0318e-01,  2.4914e-01,\n",
      "         -2.6623e-01, -2.3895e-01,  2.1874e-01, -2.8662e-03,  3.0731e-01,\n",
      "          1.3515e-01,  4.1092e-02,  5.3706e-02, -5.7323e-01,  5.6782e-02,\n",
      "         -4.1847e-01, -3.6581e-03,  4.0175e-02, -4.9546e-02, -2.0026e-01,\n",
      "          1.3821e-01,  2.7785e-01, -2.3765e-01, -8.2176e-03,  2.0651e-01,\n",
      "          9.1305e-02, -8.8238e-02,  4.4324e-01,  9.6136e-03,  2.1626e-01,\n",
      "         -6.8158e-02,  2.0948e-01, -2.1753e-01,  2.7476e-01, -2.6815e-01,\n",
      "         -1.2423e-01,  2.6585e-02,  9.0350e-02,  2.9713e-02, -8.0931e-02,\n",
      "         -3.4338e-01,  2.1608e-01,  1.1434e-02, -5.0881e-02, -7.0295e-02,\n",
      "          1.1055e-01, -6.1888e-02,  5.9433e-02,  6.3684e-02,  3.3719e-01,\n",
      "          2.3109e-01,  3.5661e-02, -3.7359e-01, -7.8694e-02, -1.4251e-01,\n",
      "          2.2803e-02,  4.0400e-02, -1.5352e-02,  4.4993e-01, -7.0610e-02,\n",
      "         -1.2209e-02, -1.2627e-01,  2.4736e-01,  2.2849e-01,  1.5709e-01,\n",
      "          1.7468e-01,  2.0727e-02,  1.4478e-01, -3.6898e-02, -1.7487e-02,\n",
      "         -1.6379e-01, -1.9888e-01, -2.9317e-01,  1.9563e-01, -2.2634e-01,\n",
      "         -2.0619e-01,  1.2134e-01,  1.8302e-01, -1.4774e-01,  1.5930e-01,\n",
      "          3.0154e-01,  1.1926e-01, -1.3293e-01,  2.8530e-01, -5.7422e-02,\n",
      "          6.7502e-02,  2.9028e-01, -5.1885e-02,  1.7809e-01,  4.9723e-01,\n",
      "          2.6180e-01, -3.7849e-01, -5.3460e-02, -2.1621e-01,  3.9440e-03,\n",
      "          2.1569e-01, -1.3337e-01,  1.6485e-01,  3.8192e-01,  3.2051e-01,\n",
      "          4.5469e-01, -4.6140e-03, -1.0681e-01,  1.2284e-01,  2.1188e-01,\n",
      "          3.1304e-02, -1.8013e-01, -1.6711e-01,  2.7657e-01,  8.9173e-02,\n",
      "         -1.4579e-01, -3.5438e-03, -1.7300e-01,  8.1863e-02, -1.6077e-01,\n",
      "         -3.7399e-01,  4.6067e-02,  1.7725e-01, -4.8648e-01,  1.4447e-01,\n",
      "         -2.7777e-01,  3.9249e-02, -2.5087e-01,  1.9681e-01, -2.1546e-01,\n",
      "         -1.0619e-01,  3.8715e-01, -4.4451e-02,  3.0718e-02, -1.8054e-01,\n",
      "         -1.3892e-01, -5.7935e-03, -1.9328e-03, -8.3097e-02,  3.2015e-03,\n",
      "          3.3897e-01, -1.4340e-01,  3.2577e-02, -2.5177e-03,  2.2274e-01,\n",
      "         -7.1567e-02,  1.6109e-01,  1.3644e-02, -1.2852e-01, -3.6273e-01,\n",
      "          1.2748e-01, -1.9210e-01, -4.2427e-01, -3.7559e-01,  3.9578e-01,\n",
      "         -1.4656e-01, -2.5633e-01, -1.8981e-01, -2.2057e-01,  5.9978e-02,\n",
      "          2.1762e-01,  4.3015e-01, -3.6705e-01, -5.9245e-02,  4.7655e-01,\n",
      "         -7.7295e-02, -1.7424e-01,  2.4230e-01,  2.1375e-01, -3.0431e-01,\n",
      "          3.3312e-01,  2.2206e-01, -2.1874e-02,  3.1070e-02,  4.7917e-01,\n",
      "          1.2846e-01,  1.7675e-01, -1.9293e-01,  4.6688e-01, -2.3043e-01,\n",
      "          2.9805e-01, -1.3977e-01, -1.9590e-01, -2.1113e-01, -4.1384e-02,\n",
      "          3.0495e-01,  1.8277e-01, -4.2950e-01, -1.0100e-01,  5.6922e-02,\n",
      "          3.2252e-01, -3.8414e-01, -5.3741e-02,  1.9545e-02, -3.4465e-01,\n",
      "          1.0260e-01,  1.2334e-01,  1.9888e-01, -4.1616e-01, -5.1801e-02,\n",
      "          4.0638e-01, -2.9832e-01,  1.3026e-01,  3.1272e-01,  9.0136e-02,\n",
      "          3.1917e-01, -7.4413e-02, -1.1528e-02,  3.7950e-02, -2.4780e-01,\n",
      "         -5.8527e-03,  1.3146e-01,  5.6413e-01,  1.1981e-01, -3.6878e-01,\n",
      "          7.9403e-02,  2.5339e-01, -1.6821e-01,  2.7952e-01, -1.0021e-01,\n",
      "         -4.0259e-02,  2.4000e-01, -7.5953e-02,  1.6221e-01, -1.0522e-01,\n",
      "         -2.3333e-01, -3.0429e-01,  3.6486e-01, -1.8493e-01, -9.1849e-02,\n",
      "         -1.5991e-01, -1.0928e-01, -1.7403e-01,  5.1935e-03, -3.9523e-01,\n",
      "          3.2704e-01,  1.3871e-01, -1.8016e-01, -5.7540e-02, -9.4332e-02,\n",
      "         -1.5599e-01, -2.0749e-01, -2.6611e-01,  4.1235e-01, -1.7268e-01,\n",
      "         -4.5330e-01,  2.1682e-01, -6.5992e-03,  3.6649e-01,  3.1304e-02,\n",
      "          9.9080e-02, -8.7545e-02,  1.0889e-01,  1.0280e-01, -1.0635e-01,\n",
      "          2.8471e-01,  7.5673e-02, -5.4592e-01, -1.1119e-01, -2.2519e-01,\n",
      "          1.0697e-01,  2.1211e-01, -3.4378e-01,  1.3294e-02,  2.5432e-02,\n",
      "          1.7571e-01,  2.2496e-02, -9.9184e-02, -6.2240e-02,  3.8091e-01,\n",
      "          2.1574e-01,  2.8248e-01,  1.2083e-01,  2.2748e-01, -4.9608e-03,\n",
      "         -3.1396e-01,  3.0480e-02,  9.7716e-02, -1.9314e-01,  4.0515e-01,\n",
      "         -8.2742e-02, -3.6844e-01, -6.6820e-02,  3.8425e-01,  7.8903e-02,\n",
      "         -2.7002e-02, -7.6601e-02,  2.1477e-01,  1.5510e-01, -1.3575e-01,\n",
      "          2.0666e-01, -3.0331e-02, -1.5269e-01, -1.0160e-01,  1.2101e-01,\n",
      "         -2.2242e-01,  2.7061e-02, -1.4874e-01, -2.2502e-02, -2.1382e-01,\n",
      "          1.7733e-02, -2.1486e-01,  2.7507e-01, -2.6922e-01,  1.0738e-01,\n",
      "          4.9441e-02,  2.9971e-01, -3.3708e-01, -1.6913e-01, -3.0616e-02,\n",
      "          1.8314e-01,  2.6151e-01,  3.6791e-01,  2.5632e-03,  3.9940e-03,\n",
      "         -1.8598e-01, -2.4415e-01,  4.3423e-02, -2.2556e-01,  1.6777e-01,\n",
      "          5.0727e-02,  2.3880e-01, -2.7082e-01, -1.8987e-01,  2.2603e-01,\n",
      "         -7.5662e-02, -1.2830e-01,  4.1914e-01,  2.5318e-01,  1.5698e-01,\n",
      "          1.5082e-02,  2.2142e-01,  4.4222e-02, -1.6348e-01, -1.1541e-01,\n",
      "         -2.7205e-01,  1.0168e-01, -1.2393e-01, -4.6372e-02, -7.4894e-02,\n",
      "         -9.3379e-02, -2.2381e-01, -1.6779e-01,  1.5432e-01,  8.3951e-02,\n",
      "          3.8077e-02, -5.1553e-02, -2.5946e-02, -2.7010e-01,  3.1797e-01,\n",
      "          1.5498e-02,  1.0590e-01, -7.1007e-02,  6.3646e-02, -1.5306e-01,\n",
      "          2.0211e-01,  2.1235e-01,  8.4972e-02, -1.9998e-01, -8.0311e-02,\n",
      "         -2.7869e-01, -3.3238e-01,  7.8680e-02,  1.3833e-01,  1.2885e-01,\n",
      "         -7.8042e-02, -2.2824e-01,  1.8777e-02, -8.5910e-02,  1.6450e-01,\n",
      "          3.5409e-02, -1.8078e-01, -1.1925e-01, -7.3663e-02, -3.0711e-02,\n",
      "          1.0722e-01, -1.6606e-01, -2.1162e-01, -1.2839e-01, -7.6469e-02,\n",
      "         -8.4960e-02,  3.3696e-01, -5.0386e-02,  2.6993e-01, -1.3082e-01,\n",
      "          2.1027e-02, -2.0916e-01,  1.3898e-01, -8.8179e-02,  6.3412e-02,\n",
      "          3.1132e-01, -4.1246e-01, -1.2311e-01, -4.3717e-02, -1.9277e-01,\n",
      "         -1.6082e-01, -7.4573e-02, -1.1499e-02,  2.3868e-01, -3.2334e-01,\n",
      "          1.8930e-01, -1.0201e-01,  2.0110e-01, -5.0777e-02, -2.0979e-01,\n",
      "         -1.6053e-01, -1.8362e-03,  2.7756e-01, -3.3797e-01, -2.2907e-01,\n",
      "         -2.4828e-01, -9.4268e-02, -1.0567e-01, -2.5181e-01,  4.1277e-01,\n",
      "         -6.5182e-02, -7.5316e-02,  4.3667e-02,  3.9607e-01,  2.0349e-01,\n",
      "          1.5822e-01,  1.9391e-01, -1.0567e-03,  2.2431e-02,  6.9236e-02,\n",
      "         -4.4571e-01,  2.4512e-01, -2.4191e-01, -1.4741e-01,  1.2963e-02,\n",
      "          1.2054e-01, -3.2141e-02,  2.2734e-02, -1.4177e-01, -1.2440e-01,\n",
      "          2.1284e-01, -3.6011e-01, -4.8716e-04,  2.4938e-01,  1.4468e-01,\n",
      "         -2.7648e-01,  3.6749e-02,  1.4324e-01,  3.7376e-01,  7.4253e-02,\n",
      "         -2.2780e-01,  1.5968e-01, -3.5286e-01, -2.1768e-02, -1.5675e-01,\n",
      "         -2.7748e-01,  1.7120e-01, -1.1559e-01,  6.3773e-02, -8.3253e-02,\n",
      "         -2.8841e-01,  2.0874e-01, -8.4390e-02, -7.2555e-02,  4.0779e-01,\n",
      "          5.3010e-02, -9.1379e-02,  1.8601e-01,  4.1889e-02,  1.3865e-02,\n",
      "         -8.4633e-02,  2.5142e-01,  1.8482e-01, -2.8148e-01,  8.6403e-02,\n",
      "         -1.7812e-01, -4.8606e-02, -1.3046e-01]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = model(**encoded_input)\n",
    "print(result.last_hidden_state)\n",
    "print(result.last_hidden_state.shape)  # [1, 408, 768](限制input_ids最大长度为514=512+2)\n",
    "print(result.pooler_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}