{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 模型及模型的假定\n",
    "&emsp;&emsp;多元线性模型的一般形式是         \n",
    "$$ y_i = \\beta_1 + \\beta_2x_{2i} + \\beta_3x_{3i} + \\dots + \\beta_k x_{ki} + \\epsilon_i, \\qquad i=1,2,\\dots,n $$\n",
    "\n",
    "其中,$ y_i$为被解释变量(因变量),$ x_{2i}, x_{3i}, \\dots, x_{ki}$为解释变量(自变量),$ \\epsilon_i $是随机误差项,$ \\beta_j,j=1,2,\\dots,k $为模型参数.\n",
    "\n",
    "&emsp;&emsp;这里应该注意到,由于$ \\epsilon_i $的影响使$y_i$变化偏离了$ E(y_i|x_{2i}, x_{3i}, \\dots, x_{ki}) = \\beta_1 + \\beta_2x_{2i}+ \\beta_3x_{3i}+\\dots + \\beta_k x_{ik}$决定的$ k-1$维空间向量.   \n",
    "\n",
    "&emsp;&emsp;用矩阵表示,上式变形为    \n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "y_1 &  \\\\\n",
    "y_2 &  \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}  =         \n",
    "\\begin{bmatrix}\n",
    "1 & x_{21}  & x_{31} & \\dots & x_{k1} \\\\\n",
    "1 & x_{22}  & x_{21} & \\dots & x_{k2} \\\\\n",
    "\\vdots & \\vdots & \\vdots &   & \\vdots \\\\\n",
    "1 & x_{2n}  & x_{3n} & \\dots & x_{kn} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\beta_1 &  \\\\\n",
    "\\beta_2 &  \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_k\n",
    "\\end{bmatrix}  +\n",
    "\\begin{bmatrix}\n",
    "\\epsilon_1 &  \\\\\n",
    "\\epsilon_2 &  \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_n\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "等价地,总体回归模型表示为  \n",
    "$$\\mathbf{y} = X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$$   \n",
    "总体回归方程为   \n",
    "$$ E(\\mathbf{y}|X) = X\\boldsymbol{\\beta} $$\n",
    "其中  \n",
    "$E(\\mathbf{y}|X)$ =\\begin{bmatrix}\n",
    "E(y_1|x_{21},x_{31},\\dots,x_{k1}) &  \\\\\n",
    "E(y_2|x_{22},x_{32},\\dots,x_{k2}) &  \\\\\n",
    "\\vdots \\\\\n",
    "E(y_n|x_{2n},x_{3n},\\dots,x_{kn}) &  \\\\\n",
    "\\end{bmatrix}   \n",
    "&emsp;&emsp;那么样本回归模型为    \n",
    "$$ \\mathbf{y} = X \\boldsymbol{\\hat{\\beta} + \\mathbf{e}} $$   \n",
    "&emsp;&emsp;样本回归方程为   \n",
    "$$ \\hat{\\mathbf{y}} = X \\boldsymbol{\\hat{\\beta}} $$   \n",
    "其中,     \n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{y}}=\n",
    "\\begin{bmatrix}\n",
    "\\hat{y_1} &  \\\\\n",
    "\\hat{y_2} &  \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y_n}\n",
    "\\end{bmatrix},\n",
    "\\hat{\\boldsymbol{\\beta}}=\n",
    "\\begin{bmatrix}\n",
    "\\hat{\\beta_1} &  \\\\\n",
    "\\hat{\\beta_2} &  \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{\\beta_k}\n",
    "\\end{bmatrix},\n",
    "\\mathbf{e}=\n",
    "\\begin{bmatrix}\n",
    "e_1 &  \\\\\n",
    "e_2 &  \\\\\n",
    "\\vdots \\\\\n",
    "e_n \n",
    "\\end{bmatrix}\n",
    "\\end{equation}  \n",
    "\n",
    "这里$ \\hat{\\mathbf{y}} $表示$\\mathbf{y} $的样本估计值向量,$ \\hat{\\boldsymbol{\\beta}} $表示回归系数$ \\boldsymbol{\\beta} $估计值向量,$\\mathbf{e} $表示残差向量.\n",
    "\n",
    "<font color='red' size=4>经典线性回归模型必须满足的假定条件</font>   \n",
    "\n",
    "1. 零均值假定 \n",
    "\n",
    "   假定随机干扰项$ \\boldsymbol{\\epsilon} $期望向量或均值向量为零,即         \n",
    "   \\begin{equation}\n",
    "   E(\\boldsymbol{\\epsilon})=\n",
    "   E\\begin{bmatrix}\n",
    "   \\epsilon_1 &  \\\\\n",
    "   \\epsilon_2 &  \\\\\n",
    "   \\vdots \\\\\n",
    "   \\epsilon_n\n",
    "   \\end{bmatrix}=\n",
    "   \\begin{bmatrix}\n",
    "   E\\epsilon_1 &  \\\\\n",
    "   E\\epsilon_2 &  \\\\\n",
    "   \\vdots \\\\\n",
    "   E\\epsilon_n\n",
    "   \\end{bmatrix}=\n",
    "   \\begin{bmatrix}\n",
    "   0 &  \\\\\n",
    "   0 &  \\\\\n",
    "   \\vdots \\\\\n",
    "   0\n",
    "   \\end{bmatrix}=\\mathbf{0}\n",
    "   \\end{equation}\n",
    "\n",
    "2. 同方差和无序列相关假定  \n",
    "\n",
    "   假定随机干扰项$ \\boldsymbol{\\epsilon} $不存在序列相关且方差相同,即    \n",
    "   $$ Var(\\epsilon) = \\sigma^2 I_n $$\n",
    "\n",
    "其中,$ I_n $为$ n$阶单位阵.\n",
    "\n",
    "3. 假定随机干扰项$ \\boldsymbol{\\epsilon} $与解释变量相互独立,即\n",
    "   $$  E(X'\\boldsymbol{\\epsilon}) = 0  $$\n",
    "   这里通常假定$X $中的元素$ x_{2i},x_{3i},\\dots,x_{ki} $为非随机变量.\n",
    "\n",
    "4. 无多重共线性的假定\n",
    "\n",
    "   假定各解释变量直接不存在线性关系,或者说各解释变量的观测值之间线性无关,在此条件下,数据矩阵$X$列满秩   \n",
    "   $$ \\mathrm{Rank}(X) = k $$\n",
    "\n",
    "此时,方阵$ X'X $满秩     \n",
    "$$ \\mathrm{Rank}(X'X)=k $$\n",
    "从而,$X'X $可逆,$(X'X)^{-1}存在$\n",
    "\n",
    "5. 正态性假定\n",
    "\n",
    "   假定随机干扰项$ \\boldsymbol{\\epsilon} $服从正态分布,即   \n",
    "   $$ \\boldsymbol{\\epsilon} \\sim N(0, \\sigma^2 I_n)  $$\n",
    "   \n",
    "\n",
    "### 最小二乘估计\n",
    "\n",
    "&emsp;&emsp;由样本回归模式   \n",
    "$$ \\mathbf{y} = X \\boldsymbol{\\hat{\\beta} + \\mathbf{e}}  $$   \n",
    "和样本回归方程   \n",
    "$$ \\hat{\\mathbf{y}} = X \\boldsymbol{\\hat{\\beta}}   $$   \n",
    "可得残差矩阵   \n",
    "$$  \\mathbf{e} = \\mathbf{y} -\\hat{\\mathbf{y}} = \\mathbf{y} - X \\boldsymbol{\\hat{\\beta}}  $$   \n",
    "那么,残差平方和为    \n",
    "\\begin{align}\n",
    "Q(\\boldsymbol{\\hat{\\beta}}) &= \\mathbf{e'}\\mathbf{e}\\\\\n",
    "&= (\\mathbf{y} - X \\boldsymbol{\\hat{\\beta}})'(\\mathbf{y} - X \\boldsymbol{\\hat{\\beta}})\n",
    "\\end{align}    \n",
    "对$ \\boldsymbol{\\hat{\\beta}} $求偏导,并令其为零,可以得到方程  \n",
    "$$ \\frac{\\partial Q(\\boldsymbol{\\hat{\\beta}})}{\\partial \\boldsymbol{\\hat{\\beta}}}=2X'(X\\hat{\\boldsymbol{\\beta}} - \\mathbf{y}) $$    \n",
    "称其为正规方程,因为$ X'X $是一个非退化矩阵,所以有\n",
    "$$ \\boldsymbol{\\hat{\\beta}} = (X'X)^{-1}X'y  $$   \n",
    "这就是线性回归模型参数的最小二乘估计量.\n",
    "\n",
    "\n",
    "\n",
    "###  特例(一元线性回归模型)\n",
    "\n",
    "&emsp;&emsp;假定影响被解释变量$ \\mathbf{y} $的因素只有一个,记为$\\mathbf{x}$.已知得到$ \\mathbf{y} $和$ \\mathbf{x} $的一组观测值$ (y_i, x_i) (i=1,2,\\dots,n)$,于是有    \n",
    "$$ y_i = \\alpha + \\beta x_i + \\epsilon_i  $$   \n",
    "这时,上面的正规方程式变为\n",
    "\\begin{bmatrix}\n",
    "n         &       \\sum x_i \\\\\n",
    "\\sum x_i  &       \\sum x_i^2  \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\alpha         \\\\\n",
    "\\beta \n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "\\sum y_i \\\\\n",
    "\\sum x_i y_i  \n",
    "\\end{bmatrix}   \n",
    "当$ x_i(i=1,2,\\dots,n) $不全相等时,$ \\sum(x_i-\\bar{x}) \\neq 0 $,这里$ \\bar{x} = (\\sum x_i)/n $.于是$ \\alpha $和$ \\beta $的最小二乘估计分别为    \n",
    "\\begin{align}\n",
    "\\alpha   &= \\frac{1}{m}\\sum_{i=1}^{n}(y_i - \\beta x_i)            \\\\\n",
    "\\beta    &= \\frac{\\sum_{i=1}^{n}y_i(x_i - \\bar{x})}{\\sum_{i=1}^{n}x_i^2 - \\frac{1}{n}(\\sum_{i=1}^{n}x_i)^2}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "### 线性模型的检验\n",
    "\n",
    "&emsp;&emsp; $ \\mathbf{y} $的总变差分解式为   \n",
    "$$ \\sum(y_i - \\bar{y})^2 = \\sum(y_i - \\hat{y_i})^2 + \\sum(\\hat{y_i} - \\bar{y})^2  $$    \n",
    "<font color='red' size=4>证明如下</font>    \n",
    "\\begin{align}\n",
    "\\sum(y_i - \\bar{y})^2 &= \\sum(y_i - \\hat{y_i} + \\hat{y_i} -  \\bar{y})^2    \\\\\n",
    "&= \\sum(y_i - \\hat{y_i})^2 + \\sum(\\hat{y_i} - \\bar{y})^2 + 2\\sum(y_i - \\hat{y_i})(\\hat{y_i}-\\bar{y})  \\\\\n",
    "&=\\sum(y_i - \\hat{y_i})^2 + \\sum(\\hat{y_i} - \\bar{y})^2 + 2\\sum(y_i\\hat{y_i}-y_i\\bar{y}-\\hat{y_i}\\hat{y_i}+\\hat{y_i}\\bar{y}) \\\\\n",
    "&=\\sum(y_i - \\hat{y_i})^2 + \\sum(\\hat{y_i} - \\bar{y})^2 + 2\\sum y_i\\hat{y_i} - 2\\sum y_i \\bar{y} -  2\\sum \\hat{y_i}\\hat{y_i} + 2\\sum \\hat{y_i}\\bar{y} \\\\\n",
    "&=\\sum(y_i - \\hat{y_i})^2 + \\sum(\\hat{y_i} - \\bar{y})^2 + 2\\sum y_i\\hat{y_i} -2n\\bar{y}^2 -2\\sum \\hat{y_i}\\hat{y_i} + 2\\sum(y_i-e_i)\\bar{y} \\\\\n",
    "&=\\sum(y_i - \\hat{y_i})^2 + \\sum(\\hat{y_i} - \\bar{y})^2 + 2\\sum y_i\\hat{y_i} -2n\\bar{y}^2 -2\\sum \\hat{y_i}\\hat{y_i} + 2n\\bar{y}^2 - 2\\bar{y}\\sum e_i \\\\ \n",
    "&=\\sum(y_i - \\hat{y_i})^2 + \\sum(\\hat{y_i} - \\bar{y})^2 + 2\\sum \\hat{y_i}e_i\\\\\n",
    "&=\\sum(y_i - \\hat{y_i})^2 + \\sum(\\hat{y_i} - \\bar{y})^2 + 2\\sum \\hat{y_i} \\sum e_i  \\qquad \\rightarrow干扰项与解释变量相互独立 \\\\\n",
    "&=\\sum(y_i - \\hat{y_i})^2 + \\sum(\\hat{y_i} - \\bar{y})^2\n",
    "\\end{align}   \n",
    "\n",
    "其中,$\\sum(y_i - \\bar{y})^2$称为总离差平方和,记为TSS,它反映了被解释变量观测值总变差的大小,其自由度为$ n-1 $;$\\sum(y_i - \\hat{y_i})^2$称为残差平方和,记为ESS,它反映了被解释变量观测值与估计值之间的变差,其自由度为$ n-k $;$\\sum(\\hat{y_i} - \\bar{y})^2$称为回归平方和,记为RSS,它反映了被解释变量回归估计值总变差的大小,其自由度为$ k-1 $.\n",
    "\n",
    "&emsp;&emsp;用向量表示为       \n",
    "\\begin{align}\n",
    "TSS &= \\mathbf{y}'\\mathbf{y} - n\\bar{y}^2\\\\\n",
    "RSS &= \\mathbf{\\hat{y}}'\\mathbf{\\hat{y}}- n\\bar{y}^2\\\\\n",
    "ESS &= \\mathbf{e}'\\mathbf{e}\n",
    "\\end{align}   \n",
    "易知    \n",
    "$$\\mathbf{y}'\\mathbf{y} - n\\bar{y}^2 = \\mathbf{e}'\\mathbf{e} +\\mathbf{\\hat{y}}'\\mathbf{\\hat{y}}- n\\bar{y}^2 $$   \n",
    "\n",
    "\n",
    "这里,回归平方和RSS越大,残差平方和ESS就越小,从而被解释变量观测值总变差中能由解释变量解释的那部分变差就越大,回归模型对观测值的拟合程度就越高.因此,可决系数$ R^2 $的定义为       \n",
    "$$ R^2 = \\frac{RSS}{TSS} = \\frac{\\mathbf{\\hat{y}}'\\mathbf{\\hat{y}}- n\\bar{y}^2}{\\mathbf{y}'\\mathbf{y} - n\\bar{y}^2}  $$     \n",
    "&emsp;&emsp;如果观测值$ y_i$ 不变,可决系数$ R_2 $随着解释变量数目的增大而增大.为了解决这一问题,定义修正可决系数为     \n",
    "$$ \\bar{R^2} = 1 - \\frac{ESS/(n-k)}{TSS/(n-1)}  $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}