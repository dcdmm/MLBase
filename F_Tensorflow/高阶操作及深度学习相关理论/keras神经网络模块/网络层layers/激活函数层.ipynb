{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### sigmoid激活函数\n",
    "### 参考torch.sigmoid/torch.nn.Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(-5, 5, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.00669285 0.00942264 0.01325097 0.0186055  0.02606656 0.03640861\n",
      " 0.05064057 0.07003141 0.09609567 0.13049925 0.17483739 0.23025063\n",
      " 0.29690726 0.37349752 0.45700301 0.54299699 0.62650248 0.70309274\n",
      " 0.76974937 0.82516261 0.86950075 0.90390433 0.92996859 0.94935943\n",
      " 0.96359139 0.97393344 0.9813945  0.98674903 0.99057736 0.99330715], shape=(30,), dtype=float64)\n",
      "\n",
      "tf.Tensor(\n",
      "[0.00669285 0.00942264 0.01325097 0.0186055  0.02606656 0.03640861\n",
      " 0.05064057 0.07003141 0.09609567 0.13049925 0.17483739 0.23025063\n",
      " 0.29690726 0.37349752 0.45700301 0.54299699 0.62650248 0.70309274\n",
      " 0.76974937 0.82516261 0.86950075 0.90390433 0.92996859 0.94935943\n",
      " 0.96359139 0.97393344 0.9813945  0.98674903 0.99057736 0.99330715], shape=(30,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# 函数\n",
    "print(tf.nn.sigmoid(x), end='\\n\\n')\n",
    "\n",
    "# 函数\n",
    "print(tf.math.sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid\n",
      "tf.Tensor(\n",
      "[0.00669285 0.00942264 0.01325097 0.0186055  0.02606656 0.03640861\n",
      " 0.05064056 0.07003141 0.09609567 0.13049924 0.17483738 0.23025063\n",
      " 0.29690725 0.37349752 0.457003   0.54299694 0.62650245 0.70309275\n",
      " 0.7697494  0.8251626  0.8695008  0.9039043  0.9299686  0.9493594\n",
      " 0.96359134 0.9739335  0.9813945  0.986749   0.99057734 0.9933072 ], shape=(30,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 类\n",
    "layer = tf.keras.layers.Activation(tf.nn.sigmoid, name='sigmoid')  # 可在网络层中简写为:'sigmoid'\n",
    "output = layer(x)\n",
    "\n",
    "print(layer.name)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### softmax激活函数\n",
    "### 参考tofch.nn.functional.softmax/torch.nn.Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(-5, 5, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(30,), dtype=float64, numpy=\narray([1.32416572e-05, 1.86938631e-05, 2.63909957e-05, 3.72573956e-05,\n       5.25979976e-05, 7.42550388e-05, 1.04829291e-04, 1.47992384e-04,\n       2.08927729e-04, 2.94952988e-04, 4.16398847e-04, 5.87849613e-04,\n       8.29894630e-04, 1.17160083e-03, 1.65400337e-03, 2.33503346e-03,\n       3.29647530e-03, 4.65378745e-03, 6.56996811e-03, 9.27512945e-03,\n       1.30941315e-02, 1.84855942e-02, 2.60969728e-02, 3.68423099e-02,\n       5.20120018e-02, 7.34277612e-02, 1.03661384e-01, 1.46343596e-01,\n       2.06600059e-01, 2.91666909e-01])>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 函数\n",
    "tf.nn.softmax(x)  # 等价与tf.math.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax\n",
      "tf.Tensor(\n",
      "[1.32416581e-05 1.86938669e-05 2.63910006e-05 3.72574068e-05\n",
      " 5.25980140e-05 7.42550692e-05 1.04829291e-04 1.47992381e-04\n",
      " 2.08927755e-04 2.94953032e-04 4.16398747e-04 5.87849587e-04\n",
      " 8.29894561e-04 1.17160077e-03 1.65400351e-03 2.33503385e-03\n",
      " 3.29647609e-03 4.65378864e-03 6.56996854e-03 9.27513093e-03\n",
      " 1.30941318e-02 1.84855945e-02 2.60969736e-02 3.68423164e-02\n",
      " 5.20120002e-02 7.34277666e-02 1.03661396e-01 1.46343589e-01\n",
      " 2.06600055e-01 2.91666925e-01], shape=(30,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "layer = tf.keras.layers.Activation(tf.nn.softmax, name='softmax')  # 可在网络层中简写为:'softmax'\n",
    "output = layer(x)\n",
    "\n",
    "print(layer.name)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 激活函数tanh\n",
    "### 参考torch.tanh/torch.nn.Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(-20, 20, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(50,), dtype=float64, numpy=\narray([-1.        , -1.        , -1.        , -1.        , -1.        ,\n       -1.        , -1.        , -1.        , -1.        , -1.        ,\n       -1.        , -1.        , -1.        , -0.99999999, -0.99999993,\n       -0.99999963, -0.99999812, -0.99999038, -0.99995078, -0.99974812,\n       -0.99871171, -0.99342468, -0.96680063, -0.8409736 , -0.38691202,\n        0.38691202,  0.8409736 ,  0.96680063,  0.99342468,  0.99871171,\n        0.99974812,  0.99995078,  0.99999038,  0.99999812,  0.99999963,\n        0.99999993,  0.99999999,  1.        ,  1.        ,  1.        ,\n        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n        1.        ,  1.        ,  1.        ,  1.        ,  1.        ])>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.tanh(x)  # 等价与tf.math.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(50,), dtype=float32, numpy=\narray([-1.        , -1.        , -1.        , -1.        , -1.        ,\n       -1.        , -1.        , -1.        , -1.        , -1.        ,\n       -1.        , -1.        , -1.        , -1.        , -1.        ,\n       -0.99999964, -0.9999983 , -0.9999904 , -0.99995077, -0.9997481 ,\n       -0.99871165, -0.99342465, -0.9668005 , -0.84097356, -0.38691205,\n        0.38691205,  0.84097356,  0.9668005 ,  0.99342465,  0.99871165,\n        0.9997481 ,  0.99995077,  0.9999904 ,  0.9999983 ,  0.99999964,\n        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n        1.        ,  1.        ,  1.        ,  1.        ,  1.        ],\n      dtype=float32)>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Activation(tf.nn.tanh)  # 可在网络层中简写为:'tanh'\n",
    "output = layer(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 激活函数Swish \n",
    "\n",
    "Swish activation function which returns $ x * sigmoid(x)$.\n",
    "It is a smooth, non-monotonic function that consistently matches or outperforms ReLU on deep networks, it is unbounded above and bounded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5,), dtype=float32, numpy=\narray([-4.1223068e-08, -2.6894143e-01,  0.0000000e+00,  7.3105854e-01,\n        2.0000000e+01], dtype=float32)>"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 函数\n",
    "tf.nn.swish(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5,), dtype=float32, numpy=\narray([-4.1223068e-08, -2.6894143e-01,  0.0000000e+00,  7.3105854e-01,\n        2.0000000e+01], dtype=float32)>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Activation(tf.nn.swish)\n",
    "output = layer(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 激活函数relu\n",
    "### 参考torch.functional.relu/torch.nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(-5, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(100,), dtype=float64, numpy=\narray([0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.05050505, 0.15151515, 0.25252525, 0.35353535, 0.45454545,\n       0.55555556, 0.65656566, 0.75757576, 0.85858586, 0.95959596,\n       1.06060606, 1.16161616, 1.26262626, 1.36363636, 1.46464646,\n       1.56565657, 1.66666667, 1.76767677, 1.86868687, 1.96969697,\n       2.07070707, 2.17171717, 2.27272727, 2.37373737, 2.47474747,\n       2.57575758, 2.67676768, 2.77777778, 2.87878788, 2.97979798,\n       3.08080808, 3.18181818, 3.28282828, 3.38383838, 3.48484848,\n       3.58585859, 3.68686869, 3.78787879, 3.88888889, 3.98989899,\n       4.09090909, 4.19191919, 4.29292929, 4.39393939, 4.49494949,\n       4.5959596 , 4.6969697 , 4.7979798 , 4.8989899 , 5.        ])>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 函数\n",
    "tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(100,), dtype=float32, numpy=\narray([0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.05050505, 0.15151516, 0.25252524, 0.35353535, 0.45454547,\n       0.5555556 , 0.65656567, 0.75757575, 0.85858583, 0.959596  ,\n       1.060606  , 1.1616162 , 1.2626263 , 1.3636364 , 1.4646465 ,\n       1.5656565 , 1.6666666 , 1.7676767 , 1.8686869 , 1.969697  ,\n       2.070707  , 2.1717172 , 2.2727273 , 2.3737373 , 2.4747474 ,\n       2.5757575 , 2.6767676 , 2.7777777 , 2.878788  , 2.979798  ,\n       3.0808082 , 3.1818182 , 3.2828283 , 3.3838384 , 3.4848485 ,\n       3.5858586 , 3.6868687 , 3.7878788 , 3.8888888 , 3.989899  ,\n       4.090909  , 4.1919193 , 4.292929  , 4.3939395 , 4.4949493 ,\n       4.5959597 , 4.6969695 , 4.79798   , 4.8989897 , 5.        ],\n      dtype=float32)>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Activation(tf.nn.relu)  # 可在网络层中简写为:'relu'\n",
    "output = layer(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 激活函数leaky_relu\n",
    "### 参考torch.nn.LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(-5, 5, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(50,), dtype=float64, numpy=\narray([-0.50000001, -0.47959184, -0.45918368, -0.43877552, -0.41836735,\n       -0.39795919, -0.37755103, -0.35714286, -0.3367347 , -0.31632654,\n       -0.29591837, -0.27551021, -0.25510204, -0.23469388, -0.21428572,\n       -0.19387755, -0.17346939, -0.15306123, -0.13265306, -0.1122449 ,\n       -0.09183674, -0.07142857, -0.05102041, -0.03061225, -0.01020408,\n        0.10204082,  0.30612245,  0.51020408,  0.71428571,  0.91836735,\n        1.12244898,  1.32653061,  1.53061224,  1.73469388,  1.93877551,\n        2.14285714,  2.34693878,  2.55102041,  2.75510204,  2.95918367,\n        3.16326531,  3.36734694,  3.57142857,  3.7755102 ,  3.97959184,\n        4.18367347,  4.3877551 ,  4.59183673,  4.79591837,  5.        ])>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 函数\n",
    "tf.nn.leaky_relu(x,\n",
    "                 alpha=0.1)  # alpha: Slope of the activation function at x < 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "class MyLeakyRelu(tf.keras.layers.Layer):  # This is the class from which all layers inherit.\n",
    "    \"\"\"自定义leaky_relu激活函数层\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.1, name='MyLeakyRelu'):\n",
    "        super(MyLeakyRelu, self).__init__(name=name)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.leaky_relu(inputs, self.alpha)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(50,), dtype=float32, numpy=\narray([-0.5       , -0.47959185, -0.4591837 , -0.4387755 , -0.41836736,\n       -0.3979592 , -0.37755105, -0.35714287, -0.3367347 , -0.31632653,\n       -0.29591838, -0.27551022, -0.25510204, -0.23469388, -0.21428572,\n       -0.19387756, -0.1734694 , -0.15306123, -0.13265306, -0.1122449 ,\n       -0.09183674, -0.07142857, -0.05102041, -0.03061225, -0.01020408,\n        0.10204082,  0.30612245,  0.5102041 ,  0.71428573,  0.9183673 ,\n        1.1224489 ,  1.3265306 ,  1.5306122 ,  1.7346939 ,  1.9387755 ,\n        2.142857  ,  2.3469388 ,  2.5510204 ,  2.7551022 ,  2.9591837 ,\n        3.1632652 ,  3.367347  ,  3.5714285 ,  3.7755103 ,  3.9795918 ,\n        4.1836734 ,  4.387755  ,  4.591837  ,  4.7959185 ,  5.        ],\n      dtype=float32)>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_layer = MyLeakyRelu(alpha=0.1)\n",
    "ouput = custom_layer(x)\n",
    "ouput"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}