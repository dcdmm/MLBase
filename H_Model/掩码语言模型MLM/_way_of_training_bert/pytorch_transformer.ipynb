{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from datasets import load_from_disk\n",
    "from transformers import BertTokenizer, BertModel, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 9600\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk('seamew_ChnSentiCorp/')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    epochs = 5\n",
    "    model_name = \"bert-base-chinese\"\n",
    "    batch_size = 64\n",
    "    lr = 5e-4\n",
    "    \n",
    "    num_warmup_steps = 50\n",
    "    num_training_steps = math.ceil(len(dataset['train']) / batch_size) * epochs  # 向上取整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般\n",
      "1\n",
      "('这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般',)\n"
     ]
    }
   ],
   "source": [
    "class Dataset(Data.Dataset):\n",
    "    \"\"\"定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, data, split):\n",
    "        self.split = split\n",
    "        self.dataset = data[split]\n",
    "\n",
    "    # 必须实现__len__魔法方法\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"定义索引方式\"\"\"\n",
    "        text = self.dataset[i]['text']\n",
    "        if self.split == 'test':\n",
    "            return text,  # 测试数据集不含标签\n",
    "        else:\n",
    "            label = self.dataset[i]['label']\n",
    "            return text, label\n",
    "\n",
    "\n",
    "dataset_train = Dataset(dataset, 'train')\n",
    "dataset_validation = Dataset(dataset, 'validation')\n",
    "dataset_test = Dataset(dataset, 'test')\n",
    "\n",
    "for text, label in dataset_train:\n",
    "    # 调用__getitem__方法\n",
    "    print(text)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "for text in dataset_test:\n",
    "    # 调用__getitem__方法\n",
    "    print(text)  # 元组\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask']\n",
      "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "102267648\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(CFG.model_name)\n",
    "print(tokenizer.model_input_names)\n",
    "print(tokenizer)\n",
    "\n",
    "pretrained = BertModel.from_pretrained(CFG.model_name)\n",
    "print(pretrained.num_parameters())\n",
    "\n",
    "# 冻结网络层参数(不进行梯度更新)\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_collate_fn(tokenizer, max_len=512):\n",
    "    \"\"\"返回collate_fun函数(通过闭包函数引入形参)\"\"\"\n",
    "\n",
    "    def collate_fn(data):\n",
    "        sents = [i[0] for i in data]\n",
    "\n",
    "        # 批量编码句子\n",
    "        text_token = tokenizer(text=sents,\n",
    "                               truncation=True,\n",
    "                               padding='max_length',\n",
    "                               max_length=max_len,\n",
    "                               return_token_type_ids=True,\n",
    "                               return_attention_mask=True,\n",
    "                               return_tensors='pt')\n",
    "\n",
    "        input_ids = text_token['input_ids']\n",
    "        attention_mask = text_token['attention_mask']\n",
    "        token_type_ids = text_token['token_type_ids']\n",
    "        # 返回值必须为字典(键与模型forward方法形参对应)\n",
    "        result = {'input_ids': input_ids,  # ★★★★★对应模型forward方法input_ids参数\n",
    "                  'attention_mask': attention_mask,  # ★★★★★对应模型forward方法attention_mask参数\n",
    "                  \"token_type_ids\": token_type_ids}  # ★★★★对应模型forward方法token_type_ids参数\n",
    "\n",
    "        if len(data[0]) == 1:\n",
    "            return result  # 测试数据集不含标签\n",
    "        else:\n",
    "            labels = [i[1] for i in data]\n",
    "            labels = torch.LongTensor(labels)\n",
    "            result['labels'] = labels  # ★★★★对应模型forward方法labels参数\n",
    "            return result\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 6821,  702,  ...,    0,    0,    0],\n",
      "        [ 101, 2577, 4708,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "dataLoader_test = Data.DataLoader(dataset=dataset_test, batch_size=2, collate_fn=get_collate_fn(tokenizer, max_len=512))\n",
    "for i in dataLoader_test:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \"\"\"下游训练任务模型\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)  # 二分类任务;768:模型hidden_size\n",
    "        self.pretrained = pretrained_model\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
    "        out = self.pretrained(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.fc(out.pooler_output)\n",
    "        out = out.softmax(dim=1)\n",
    "        loss = None\n",
    "        if labels is not None:  # 若包含标签\n",
    "            loss = self.criterion(out, labels)\n",
    "\n",
    "        if loss is not None:\n",
    "            return (loss, out)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "model = Model(pretrained)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"验证数据集评估函数\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions \n",
    "    preds_argmax = preds.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds_argmax)\n",
    "    return {\"accuracy\": acc} \n",
    "\n",
    "\n",
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n",
    "    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            # 学习率预热(线性增加)\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        # 学习率线性衰减(最小为0)\n",
    "        # num_training_steps后学习率恒为0\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "model_name = f\"{CFG.model_name}-finetuned-emotion\"\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CFG.lr)  # 优化器\n",
    "scheduler_lr = get_linear_schedule_with_warmup(optimizer, CFG.num_warmup_steps, CFG.num_training_steps)  # 学习率预热(必须为LambdaLR对象)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 03:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.605800</td>\n",
       "      <td>0.534635</td>\n",
       "      <td>0.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.519500</td>\n",
       "      <td>0.511587</td>\n",
       "      <td>0.815833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.504500</td>\n",
       "      <td>0.506941</td>\n",
       "      <td>0.814167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.495900</td>\n",
       "      <td>0.494394</td>\n",
       "      <td>0.830833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.491500</td>\n",
       "      <td>0.493230</td>\n",
       "      <td>0.831667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.5234297993977864, metrics={'train_runtime': 182.4005, 'train_samples_per_second': 263.157, 'train_steps_per_second': 4.112, 'total_flos': 0.0, 'train_loss': 0.5234297993977864, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    overwrite_output_dir=False,\n",
    "    save_total_limit=None, \n",
    "    seed=CFG.seed,\n",
    "    num_train_epochs=CFG.epochs,\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_accumulation_steps=1, \n",
    "    per_device_train_batch_size=CFG.batch_size, \n",
    "    per_device_eval_batch_size=CFG.batch_size, \n",
    "    dataloader_drop_last=False,  \n",
    "    eval_strategy=\"epoch\", \n",
    "    eval_steps=None, \n",
    "    logging_strategy='epoch', \n",
    "    save_strategy='epoch',\n",
    "    log_level='passive', \n",
    "    load_best_model_at_end=False, \n",
    "    metric_for_best_model=None,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_validation,\n",
    "    data_collator=get_collate_fn(tokenizer, max_len=512),  # 对应pytorch torch.utils.data.DataLoade 参数collate_fn\n",
    "    optimizers=(optimizer, scheduler_lr),  \n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()  # 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 0.0005\n",
       "    lr: 0.0\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer  # 初始化学习率0.0005,最终学习率归0(get_linear_schedule_with_warmup学习率预热归0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[0.28464532, 0.7153547 ],\n",
       "       [0.02458108, 0.975419  ],\n",
       "       [0.26128414, 0.7387159 ],\n",
       "       ...,\n",
       "       [0.51695514, 0.48304483],\n",
       "       [0.16963767, 0.8303623 ],\n",
       "       [0.9440778 , 0.05592221]], dtype=float32), label_ids=array([1, 1, 0, ..., 0, 1, 0]), metrics={'test_loss': 0.49323007464408875, 'test_accuracy': 0.8316666666666667, 'test_runtime': 3.8466, 'test_samples_per_second': 311.96, 'test_steps_per_second': 4.939})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output = trainer.predict(dataset_validation)  # 预测和评估包含标签的验证数据集\n",
    "preds_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28464532 0.7153547 ]\n",
      " [0.02458108 0.975419  ]\n",
      " [0.26128414 0.7387159 ]\n",
      " ...\n",
      " [0.51695514 0.48304483]\n",
      " [0.16963767 0.8303623 ]\n",
      " [0.9440778  0.05592221]]\n",
      "<class 'numpy.ndarray'>\n",
      "(1200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(preds_output.predictions)  # 预测结果\n",
    "print(type(preds_output.predictions))\n",
    "print(preds_output.predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.49323007464408875,\n",
       " 'test_accuracy': 0.8316666666666667,\n",
       " 'test_runtime': 3.8466,\n",
       " 'test_samples_per_second': 311.96,\n",
       " 'test_steps_per_second': 4.939}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output.metrics  # 评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 6821,  702,  ...,    0,    0,    0],\n",
      "        [ 101, 2577, 4708,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for i in dataLoader_test:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[0.72549987, 0.2745002 ],\n",
       "       [0.9635792 , 0.03642078],\n",
       "       [0.3198018 , 0.6801982 ],\n",
       "       ...,\n",
       "       [0.21934602, 0.780654  ],\n",
       "       [0.12182001, 0.87817997],\n",
       "       [0.9847793 , 0.01522069]], dtype=float32), label_ids=None, metrics={'test_runtime': 3.8365, 'test_samples_per_second': 312.787, 'test_steps_per_second': 4.952})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(dataset_test)  # 预测不含标签的测试数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7255, 0.2745],\n",
      "        [0.9636, 0.0364],\n",
      "        [0.3198, 0.6802],\n",
      "        ...,\n",
      "        [0.2193, 0.7807],\n",
      "        [0.1218, 0.8782],\n",
      "        [0.9848, 0.0152]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"预测不含标签的测试数据集(自定义)\"\"\"\n",
    "    model.eval()  # Sets the module in evaluation mode.\n",
    "    predict_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in data_loader:\n",
    "            input_ids = i['input_ids'].to(device)\n",
    "            attention_mask = i['attention_mask'].to(device)\n",
    "            token_type_ids = i['token_type_ids'].to(device)\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "            predict_list.append(output)\n",
    "    predict_all = torch.cat(predict_list, dim=0)\n",
    "    return predict_all\n",
    "\n",
    "\n",
    "result = predict(model, dataLoader_test)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
