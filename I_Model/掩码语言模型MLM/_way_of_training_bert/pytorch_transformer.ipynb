{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from datasets import load_from_disk\n",
    "from transformers import BertTokenizer, BertModel, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 9600\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk('seamew_ChnSentiCorp/')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    epochs = 5\n",
    "    model_name = \"bert-base-chinese\"\n",
    "    batch_size = 64\n",
    "    lr = 5e-4\n",
    "    \n",
    "    num_warmup_steps = 50\n",
    "    num_training_steps = math.ceil(len(dataset['train']) / batch_size) * epochs  # 向上取整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般\n",
      "1\n",
      "('这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般',)\n"
     ]
    }
   ],
   "source": [
    "class Dataset(Data.Dataset):\n",
    "    \"\"\"定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, data, split):\n",
    "        self.split = split\n",
    "        self.dataset = data[split]\n",
    "\n",
    "    # 必须实现__len__魔法方法\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"定义索引方式\"\"\"\n",
    "        text = self.dataset[i]['text']\n",
    "        if self.split == 'test':\n",
    "            return text,  # 测试数据集不含标签\n",
    "        else:\n",
    "            label = self.dataset[i]['label']\n",
    "            return text, label\n",
    "\n",
    "\n",
    "dataset_train = Dataset(dataset, 'train')\n",
    "dataset_validation = Dataset(dataset, 'validation')\n",
    "dataset_test = Dataset(dataset, 'test')\n",
    "\n",
    "for text, label in dataset_train:\n",
    "    # 调用__getitem__方法\n",
    "    print(text)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "for text in dataset_test:\n",
    "    # 调用__getitem__方法\n",
    "    print(text)  # 元组\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask']\n",
      "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102267648\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(CFG.model_name)\n",
    "print(tokenizer.model_input_names)\n",
    "print(tokenizer)\n",
    "\n",
    "pretrained = BertModel.from_pretrained(CFG.model_name)\n",
    "print(pretrained.num_parameters())\n",
    "\n",
    "# 冻结网络层参数(不进行梯度更新)\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_collate_fn(tokenizer, max_len=512):\n",
    "    \"\"\"返回collate_fun函数(通过闭包函数引入形参)\"\"\"\n",
    "\n",
    "    def collate_fn(data):\n",
    "        sents = [i[0] for i in data]\n",
    "\n",
    "        # 批量编码句子\n",
    "        text_token = tokenizer(text=sents,\n",
    "                               truncation=True,\n",
    "                               padding='max_length',\n",
    "                               max_length=max_len,\n",
    "                               return_token_type_ids=True,\n",
    "                               return_attention_mask=True,\n",
    "                               return_tensors='pt')\n",
    "\n",
    "        input_ids = text_token['input_ids']\n",
    "        attention_mask = text_token['attention_mask']\n",
    "        token_type_ids = text_token['token_type_ids']\n",
    "        # 返回值必须为字典(键与模型forward方法形参对应)\n",
    "        result = {'input_ids': input_ids,  # ★★★★★对应模型forward方法input_ids参数\n",
    "                  'attention_mask': attention_mask,  # ★★★★★对应模型forward方法attention_mask参数\n",
    "                  \"token_type_ids\": token_type_ids}  # ★★★★对应模型forward方法token_type_ids参数\n",
    "\n",
    "        if len(data[0]) == 1:\n",
    "            return result  # 测试数据集不含标签\n",
    "        else:\n",
    "            labels = [i[1] for i in data]\n",
    "            labels = torch.LongTensor(labels)\n",
    "            result['labels'] = labels  # ★★★★对应模型forward方法labels参数\n",
    "            return result\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 6821,  702,  ...,    0,    0,    0],\n",
      "        [ 101, 2577, 4708,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "dataLoader_test = Data.DataLoader(dataset=dataset_test, batch_size=2, collate_fn=get_collate_fn(tokenizer, max_len=512))\n",
    "for i in dataLoader_test:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \"\"\"下游训练任务模型\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)  # 二分类任务;768:模型hidden_size\n",
    "        self.pretrained = pretrained_model\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
    "        out = self.pretrained(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.fc(out.pooler_output)\n",
    "        out = out.softmax(dim=1)\n",
    "        loss = None\n",
    "        if labels is not None:  # 若包含标签\n",
    "            loss = self.criterion(out, labels)\n",
    "\n",
    "        # 训练与评估阶段\n",
    "        # ★★★★★\n",
    "        # 返回值为一个元组\n",
    "        # 元组的第一个元素必须为该批次数据的损失值\n",
    "        # 元组的第二个元素为该批次数据的预测值(可选)\n",
    "        # * 验证数据集评估函数指标的计算\n",
    "        # * predict方法预测结果(predictions)与评估结果(metrics)(结合输入labels)的计算\n",
    "        if loss is not None:\n",
    "            return (loss, out)\n",
    "        # 预测阶段\n",
    "        # ★★★★★\n",
    "        # 返回值为模型的预测结果\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "model = Model(pretrained)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"验证数据集评估函数\"\"\"\n",
    "    labels = pred.label_ids  # 对应自定义模型forward函数输入:labels\n",
    "    preds = pred.predictions  # 对应自定义模型forward函数返回值的第二个元素\n",
    "    preds_argmax = preds.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds_argmax)\n",
    "    return {\"accuracy\": acc}  # return a dictionary string to metric value\n",
    "\n",
    "\n",
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n",
    "    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            # 学习率预热(线性增加)\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        # 学习率线性衰减(最小为0)\n",
    "        # num_training_steps后学习率恒为0\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "model_name = f\"{CFG.model_name}-finetuned-emotion\"\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CFG.lr)  # 优化器\n",
    "scheduler_lr = get_linear_schedule_with_warmup(optimizer, CFG.num_warmup_steps, CFG.num_training_steps)  # 学习率预热(必须为LambdaLR对象)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 04:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.606000</td>\n",
       "      <td>0.543939</td>\n",
       "      <td>0.784167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.524400</td>\n",
       "      <td>0.512102</td>\n",
       "      <td>0.821667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.504700</td>\n",
       "      <td>0.500557</td>\n",
       "      <td>0.825833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.497011</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.493200</td>\n",
       "      <td>0.495462</td>\n",
       "      <td>0.829167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.5249578043619791, metrics={'train_runtime': 255.5864, 'train_samples_per_second': 187.803, 'train_steps_per_second': 2.934, 'total_flos': 0.0, 'train_loss': 0.5249578043619791, 'epoch': 5.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 主要调节的超参数\n",
    "training_args = TrainingArguments(\n",
    "    # The output directory where the model predictions and checkpoints will be written.\n",
    "    output_dir=model_name,\n",
    "    # If True, overwrite the content of the output directory. Use this to continue training if output_dir points to a checkpoint directory.\n",
    "    overwrite_output_dir=False,  # 默认:False\n",
    "    # save_total_limit (int, optional) —If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir.\n",
    "    save_total_limit=None,  # 默认:None\n",
    "\n",
    "    seed=CFG.seed,\n",
    "\n",
    "    # Total number of training epochs to perform\n",
    "    num_train_epochs=CFG.epochs,  # 默认:3.0\n",
    "    # If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs. I\n",
    "    # max_steps=100,  # 默认:-1\n",
    "\n",
    "    #  Maximum gradient norm (for gradient clipping).\n",
    "    max_grad_norm=1.0,  # 默认:1.0\n",
    "    # Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "    gradient_accumulation_steps=1,  # 默认:1\n",
    "    \n",
    "    # 对应pytorch DataLoader 参数batch_size\n",
    "    # The batch size per GPU/TPU core/CPU for training.\n",
    "    per_device_train_batch_size=CFG.batch_size,  # 默认:8\n",
    "    # The batch size per GPU/TPU core/CPU for evaluation.\n",
    "    # 对应pytorch DataLoader 参数batch_size\n",
    "    per_device_eval_batch_size=CFG.batch_size,  # 默认:8\n",
    "    # Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size) or not.\n",
    "    # 对应pytorch DataLoader 参数drop_last\n",
    "    dataloader_drop_last=False,  # 默认:False\n",
    "\n",
    "    # The evaluation strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No evaluation is done during training.\n",
    "    # \"steps\": Evaluation is done (and logged) every eval_steps.\n",
    "    # \"epoch\": Evaluation is done at the end of each epoch.\n",
    "    evaluation_strategy=\"epoch\",  # 默认:'no'\n",
    "    # Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set.\n",
    "    eval_steps=None,  # 默认None\n",
    "\n",
    "    # The logging strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No logging is done during training.\n",
    "    # \"epoch\": Logging is done at the end of each epoch.\n",
    "    # \"steps\": Logging is done every logging_steps.\n",
    "    logging_strategy='epoch',  # 默认:'steps'\n",
    "    # Number of update steps between two logs if logging_strategy=\"steps\".\n",
    "    # logging_steps=500,  # 默认:500\n",
    "\n",
    "    # The checkpoint save strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No save is done during training.\n",
    "    # \"epoch\": Save is done at the end of each epoch.\n",
    "    # \"steps\": Save is done every save_steps.\n",
    "    save_strategy='epoch',  # 默认:'steps'\n",
    "    #  Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "    save_steps=500,  # 默认:500\n",
    "\n",
    "    # Logger log level to use on the main process. Possible choices are the log levels as strings: ‘debug’, ‘info’, ‘warning’, ‘error’ and ‘critical’, plus a ‘passive’ level which doesn’t set anything and lets the application set the level.\n",
    "    log_level='passive',  # 默认:'passive'\n",
    "\n",
    "    # Whether or not to load the best model found during training at the end of training.\n",
    "    # When set to True, the parameters save_strategy needs to be the same as evaluation_strategy, and in the case it is “steps”, save_steps must be a round multiple of eval_steps.\n",
    "    load_best_model_at_end=False,  # 默认load_best_model_at_end=False\n",
    "    # Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models.\n",
    "    # Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\".\n",
    "    # Will default to \"loss\" if unspecified and load_best_model_at_end=True (to use the evaluation loss).\n",
    "    metric_for_best_model=None,\n",
    "\n",
    "    disable_tqdm=False,  # 是否使用tqdm显示进度(.py运行时设置disable_tqdm=True)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_validation,\n",
    "    data_collator=get_collate_fn(tokenizer, max_len=512),  # 对应pytorch torch.utils.data.DataLoade 参数collate_fn\n",
    "    optimizers=(optimizer, scheduler_lr),  # 自定义优化器与学习率预热\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()  # 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    initial_lr: 0.0005\n",
       "    lr: 0.0\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer  # 初始化学习率0.0005,最终学习率归0(get_linear_schedule_with_warmup学习率预热归0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[0.30090708, 0.699093  ],\n",
       "       [0.02519588, 0.97480416],\n",
       "       [0.22255814, 0.7774418 ],\n",
       "       ...,\n",
       "       [0.51666945, 0.48333058],\n",
       "       [0.18903433, 0.8109657 ],\n",
       "       [0.93123865, 0.06876133]], dtype=float32), label_ids=array([1, 1, 0, ..., 0, 1, 0]), metrics={'test_loss': 0.495461642742157, 'test_accuracy': 0.8291666666666667, 'test_runtime': 5.1215, 'test_samples_per_second': 234.308, 'test_steps_per_second': 3.71})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run prediction and returns predictions and potential metrics.\n",
    "# Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method will also return metrics, like in `evaluate()`.\n",
    "preds_output = trainer.predict(dataset_validation)  # 预测和评估包含标签的验证数据集\n",
    "preds_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30090708 0.699093  ]\n",
      " [0.02519588 0.97480416]\n",
      " [0.22255814 0.7774418 ]\n",
      " ...\n",
      " [0.51666945 0.48333058]\n",
      " [0.18903433 0.8109657 ]\n",
      " [0.93123865 0.06876133]]\n",
      "<class 'numpy.ndarray'>\n",
      "(1200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(preds_output.predictions)  # 预测结果\n",
    "print(type(preds_output.predictions))\n",
    "print(preds_output.predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.495461642742157,\n",
       " 'test_accuracy': 0.8291666666666667,\n",
       " 'test_runtime': 5.1215,\n",
       " 'test_samples_per_second': 234.308,\n",
       " 'test_steps_per_second': 3.71}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output.metrics  # 评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 6821,  702,  ...,    0,    0,    0],\n",
      "        [ 101, 2577, 4708,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for i in dataLoader_test:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[0.74712986, 0.25287017],\n",
       "       [0.9722216 , 0.02777843],\n",
       "       [0.40959132, 0.5904087 ],\n",
       "       ...,\n",
       "       [0.20984258, 0.7901574 ],\n",
       "       [0.09190209, 0.9080979 ],\n",
       "       [0.985579  , 0.01442092]], dtype=float32), label_ids=None, metrics={'test_runtime': 5.1017, 'test_samples_per_second': 235.214, 'test_steps_per_second': 3.724})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(dataset_test)  # 预测不含标签的测试数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7471, 0.2529],\n",
      "        [0.9722, 0.0278],\n",
      "        [0.4096, 0.5904],\n",
      "        ...,\n",
      "        [0.2098, 0.7902],\n",
      "        [0.0919, 0.9081],\n",
      "        [0.9856, 0.0144]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"预测不含标签的测试数据集(自定义)\"\"\"\n",
    "    model.eval()  # Sets the module in evaluation mode.\n",
    "    predict_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in data_loader:\n",
    "            input_ids = i['input_ids'].to(device)\n",
    "            attention_mask = i['attention_mask'].to(device)\n",
    "            token_type_ids = i['token_type_ids'].to(device)\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "            predict_list.append(output)\n",
    "    predict_all = torch.cat(predict_list, dim=0)\n",
    "    return predict_all\n",
    "\n",
    "\n",
    "result = predict(model, dataLoader_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch13",
   "language": "python",
   "name": "torch13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}