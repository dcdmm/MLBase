{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 思路1\n",
    "\n",
    "&emsp;&emsp;在BPTT算法中,将误差项不断进行展开有:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{\\delta}_{t, k} &=\\frac{\\partial \\mathcal{L}_{t}}{\\partial \\mathbf{z}_{k}} \\\\\n",
    "              &=\\frac{\\partial \\mathbf{h}_{k}}{\\partial \\mathbf{z}_{k}} \\frac{\\partial \\mathbf{z}_{k+1}}{\\partial \\mathbf{h}_{k}} \\frac{\\partial \\mathcal{L}_{t}}{\\partial \\mathbf{z}_{k+1}} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{ \\partial f([z_{k}]_1)}{\\partial [z_{k}]_1} & 0 & \\cdots & 0  \\\\\n",
    "0 & \\frac{ \\partial f([z_{k}]_2)}{\\partial [z_{k}]_2} & \\cdots & 0 \\\\\n",
    "\\vdots& \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\frac{ \\partial f([z_{k}]_{-1})}{\\partial [z_{k}]_{-1}}\n",
    "\\end{bmatrix}  W_{hh}^{T} \\boldsymbol{\\delta_{\\mathit{t, k+1}}} \\\\ \\\\\\\n",
    "&=A_k W_{hh}^{\\top} \\boldsymbol{\\delta_{\\mathit{t, k+1}}} \\\\ \\\\\\\n",
    "&=\\prod_{s=k}^{t-1} \\left( A_s W_{hh}^{\\top} \\right) \\boldsymbol{\\delta}_{t,t}\n",
    "\\end{align}\n",
    "\n",
    "如果定义$\\gamma = || A_s W_{hh}^{\\top} ||$,则\n",
    "\n",
    "$$ \\boldsymbol{\\delta}_{t,k} \\approx \\gamma^{t-k} \\boldsymbol{\\delta}_{t,t} $$\n",
    "\n",
    "&emsp;&emsp;若$\\gamma>1$,当$t-k \\rightarrow \\infty$时,$\\gamma^{t-k}  \\rightarrow  \\infty $.当间隔$t-k$比较大时,梯度也会变得很大,\n",
    "会造成系统不稳定,称为梯度爆炸问题.\n",
    "\n",
    "&emsp;&emsp;若$\\gamma<1$,当$t-k \\rightarrow \\infty$时,$\\gamma^{t-k}  \\rightarrow  0 $.当间隔$t-k$比较大时,梯度也会变得非常小,\n",
    "会出现和深层前馈神经网络类似的梯度消失问题.\n",
    "\n",
    "&emsp;&emsp;值得注意的是,在循环神经网络中的梯度消失不是说$\\frac{\\partial \\mathcal{L}}{\\partial W_{hh}}$的梯度消失了,而是$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{k}}$(原因:$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{k}} = \\frac{\\partial \\mathbf{z}_{k+1}}{\\partial \\mathbf{h}_{k}}  \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{k+1}} = W_{hh} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{k+1}} $)的梯度消失了(当间隔$t-k$比较大时).也就是说,参数$W_{hh}$的更新主要靠当前时刻$t$的几个相邻状态$\\mathbf{h}_{k}$(由循环神经网络更新公式易知)来更新,长距离的状态对参数$W_{hh}$没有影响\n",
    "\n",
    "### 思路2\n",
    "\n",
    "&emsp;&emsp;假设一个非常简单的,缺少非线性激活函数和输入$\\mathbf{x}$的循环神经网络为\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{h}_t &= W_{hh} \\mathbf{h}_{t-1} \\\\\n",
    "&=(W_{hh})^t \\mathbf{h}_0 \\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "若$W_{hh}$符合下列形式的特征分解\n",
    "\n",
    "$$ W_{hh} = Q \\Lambda Q^{-1}$$\n",
    "\n",
    "其中$Q$可逆,故式(1)可进一步写为\n",
    "\n",
    "$$ \\mathbf{h}_t = Q \\Lambda^t Q^{-1}\\mathbf{h}_0  $$\n",
    "\n",
    "特征值提升到$t$次后,导致幅值不到一的特征值衰减到零,而幅值大于一的就会激增.从而出现梯度小时或梯度爆炸问题.\n",
    "\n",
    "### 长期依赖问题\n",
    "&emsp;&emsp;由于循环神经网络经常使用非线性激活函数为Logistic函数或Tanh函数作为非线性激活函数,其导数值都小于$1$,\n",
    "并且权重矩阵$W_{hh}$也不会太大,因此如果时间间隔$t-k$过大,$\\mathbf{\\delta}_{t, k} $会趋向于0,因而经常会出现梯度消失问题.\n",
    "\n",
    "&emsp;&emsp;虽然简单循环网络理论上可以建立长时间间隔的状态之间的依赖关系,但是由于梯度爆炸或消失问题,实际上只能学习到短期的依赖关系.\n",
    "这样,如果时刻$t$的输出$y_t$依赖于时刻𝑘的输入$\\mathbf{x}_k$,当间隔$t-k$比较大时,简单神经网络很难建模这种长距离的依赖关系,\n",
    "称为长程依赖问题(Long-Term Dependencies Problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
