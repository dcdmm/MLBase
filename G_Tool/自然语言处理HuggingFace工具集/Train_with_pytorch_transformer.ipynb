{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt = \"bert-base-chinese\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般\n",
      "1\n",
      "('这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般',)\n"
     ]
    }
   ],
   "source": [
    "class Dataset(Data.Dataset):\n",
    "    \"\"\"定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, split):\n",
    "        self.split = split\n",
    "        self.dataset = load_dataset(path='seamew/ChnSentiCorp', split=self.split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"定义索引方式\"\"\"\n",
    "        text = self.dataset[i]['text']\n",
    "        if self.split == 'test':  # 测试数据集不含标签\n",
    "            return text,\n",
    "        else:\n",
    "            label = self.dataset[i]['label']\n",
    "            return text, label\n",
    "\n",
    "\n",
    "dataset_train = Dataset('train')\n",
    "dataset_validation = Dataset('validation')\n",
    "dataset_test = Dataset('test')\n",
    "\n",
    "for text, label in dataset_train:\n",
    "    # 调用__getitem__方法\n",
    "    print(text)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "for text in dataset_test:\n",
    "    # 调用__getitem__方法\n",
    "    print(text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\n",
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token = BertTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "\n",
    "    # 批量编码句子\n",
    "    text_token = token(text=sents,\n",
    "                       truncation=True,\n",
    "                       padding='max_length',\n",
    "                       max_length=512,\n",
    "                       return_token_type_ids=True,\n",
    "                       return_attention_mask=True,\n",
    "                       return_tensors='pt')\n",
    "\n",
    "    input_ids = text_token['input_ids']\n",
    "    attention_mask = text_token['attention_mask']\n",
    "    token_type_ids = text_token['token_type_ids']\n",
    "    # 返回值必须为字典(键与模型forward方法形参对应)\n",
    "    result = {'input_ids': input_ids,  # ★★★★★对应模型forward方法input_ids参数\n",
    "              'attention_mask': attention_mask,  # ★★★★★对应模型forward方法attention_mask参数\n",
    "              \"token_type_ids\": token_type_ids}  # ★★★★对应模型forward方法token_type_ids参数\n",
    "\n",
    "    if len(data[0]) == 1:  # 测试数据集不含标签\n",
    "        return result\n",
    "    else:\n",
    "        labels = [i[1] for i in data]\n",
    "        labels = torch.LongTensor(labels)\n",
    "        result['labels'] = labels  # ★★★★对应模型forward方法labels参数\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 6821,  702,  ...,    0,    0,    0],\n",
      "        [ 101, 2577, 4708,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "dataLoader_test = Data.DataLoader(dataset=dataset_test, batch_size=2, collate_fn=collate_fn)\n",
    "for i in dataLoader_test:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \"\"\"下游训练任务模型\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)  # 二分类任务\n",
    "        self.pretrained = pretrained_model\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
    "        out = self.pretrained(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.fc(out.pooler_output)\n",
    "        out = out.softmax(dim=1)\n",
    "        loss = None\n",
    "        if labels is not None:  # 若包含标签\n",
    "            loss = self.criterion(out, labels)\n",
    "\n",
    "        # ★★★★★\n",
    "        # 返回值为一个元组\n",
    "        # 元组的第一个元素必须为该批次训练数据的损失值\n",
    "        # 元素的第二个元素用于评估函数计算验证数据集(若有)的输出\n",
    "        return (loss, out)\n",
    "\n",
    "\n",
    "pretrained = BertModel.from_pretrained(model_ckpt)\n",
    "\n",
    "# 冻结网络层参数(不进行梯度更新)\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = Model(pretrained)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"验证数据集评估函数\"\"\"\n",
    "    labels = pred.label_ids  # 对应自定义模型forward函数输入:labels\n",
    "    preds = pred.predictions  # 对应自定义模型forward函数返回值的第二个元素\n",
    "    preds_argmax = preds.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds_argmax)\n",
    "    return {\"accuracy\": acc}  # return a dictionary string to metric value\n",
    "\n",
    "\n",
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n",
    "    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            # 学习率预热(线性增加)\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        # 学习率线性衰减(最小为0)\n",
    "        # num_training_steps后学习率恒为0\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "batch_size = 64  # 批次大小\n",
    "epochs = 2.0  # 训练轮数\n",
    "steps_all = int(len(dataset_train) / batch_size) * epochs  # 总学习步数\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4)  # 优化器\n",
    "scheduler_lr = get_linear_schedule_with_warmup(optimizer, 50, 0.9 * steps_all)  # 学习率预热(必须为LambdaLR对象)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 9600\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 02:32, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.607000</td>\n",
       "      <td>0.545359</td>\n",
       "      <td>0.804167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.533900</td>\n",
       "      <td>0.537090</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1200\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to bert-base-chinese-finetuned-emotion/checkpoint-150\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in bert-base-chinese-finetuned-emotion/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-finetuned-emotion/checkpoint-150/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1200\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to bert-base-chinese-finetuned-emotion/checkpoint-300\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in bert-base-chinese-finetuned-emotion/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-finetuned-emotion/checkpoint-300/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.5704058074951172, metrics={'train_runtime': 153.2937, 'train_samples_per_second': 125.25, 'train_steps_per_second': 1.957, 'total_flos': 0.0, 'train_loss': 0.5704058074951172, 'epoch': 2.0})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 主要调节的超参数\n",
    "training_args = TrainingArguments(\n",
    "    # The output directory where the model predictions and checkpoints will be written.\n",
    "    output_dir=model_name,\n",
    "    seed=42,\n",
    "\n",
    "    # Total number of training epochs to perform\n",
    "    num_train_epochs=epochs,  # 默认:3.0\n",
    "    # If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs. I\n",
    "    # max_steps=100,  # 默认:-1\n",
    "\n",
    "    #  Maximum gradient norm (for gradient clipping).\n",
    "    max_grad_norm=1.0,  # 默认:1.0\n",
    "\n",
    "    # 对应pytorch DataLoader 参数batch_size\n",
    "    # The batch size per GPU/TPU core/CPU for training.\n",
    "    per_device_train_batch_size=batch_size,  # 默认:8\n",
    "    # The batch size per GPU/TPU core/CPU for evaluation.\n",
    "    # 对应pytorch DataLoader 参数batch_size\n",
    "    per_device_eval_batch_size=batch_size,  # 默认:8\n",
    "    # Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size) or not.\n",
    "    # 对应pytorch DataLoader 参数drop_last\n",
    "    dataloader_drop_last=False,  # 默认:False\n",
    "\n",
    "    # The evaluation strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No evaluation is done during training.\n",
    "    # \"steps\": Evaluation is done (and logged) every eval_steps.\n",
    "    # \"epoch\": Evaluation is done at the end of each epoch.\n",
    "    evaluation_strategy=\"epoch\",  # 默认:'no'\n",
    "    # The logging strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No logging is done during training.\n",
    "    # \"epoch\": Logging is done at the end of each epoch.\n",
    "    # \"steps\": Logging is done every logging_steps.\n",
    "    logging_strategy='epoch',  # 默认:'steps'\n",
    "    # Number of update steps between two logs if logging_strategy=\"steps\".\n",
    "    # logging_steps=500,  # 默认:500\n",
    "    # The checkpoint save strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No save is done during training.\n",
    "    # \"epoch\": Save is done at the end of each epoch.\n",
    "    # \"steps\": Save is done every save_steps.\n",
    "    # Logger log level to use on the main process. Possible choices are the log levels as strings: ‘debug’, ‘info’, ‘warning’, ‘error’ and ‘critical’, plus a ‘passive’ level which doesn’t set anything and lets the application set the level.\n",
    "    log_level='passive',  # 默认'passive'\n",
    "    save_strategy='epoch',  # 默认:'steps'\n",
    "    # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "    # save_steps=500,  # 默认:500\n",
    "    disable_tqdm=False,  # 使用tqdm显示进度\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_validation,\n",
    "    data_collator=collate_fn,  # 对应pytorch DataLoader 参数collate_fn\n",
    "    optimizers=(optimizer, scheduler_lr),  # 自定义优化器与学习率预热\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=token)\n",
    "\n",
    "trainer.train()  # 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    initial_lr: 0.0005\n",
       "    lr: 0.0\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer  # 初始化学习率0.0005,最终学习率归0(get_linear_schedule_with_warmup学习率预热归0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1200\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[0.3600506 , 0.63994944],\n",
       "       [0.05834516, 0.94165486],\n",
       "       [0.22904089, 0.77095914],\n",
       "       ...,\n",
       "       [0.47691005, 0.52308995],\n",
       "       [0.13914941, 0.8608505 ],\n",
       "       [0.8262163 , 0.17378366]], dtype=float32), label_ids=array([1, 1, 0, ..., 0, 1, 0]), metrics={'test_loss': 0.5370903611183167, 'test_accuracy': 0.7875, 'test_runtime': 7.7982, 'test_samples_per_second': 153.881, 'test_steps_per_second': 2.436})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run prediction and returns predictions and potential metrics.\n",
    "preds_output = trainer.predict(dataset_validation)  # 包含标签\n",
    "preds_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3600506  0.63994944]\n",
      " [0.05834516 0.94165486]\n",
      " [0.22904089 0.77095914]\n",
      " ...\n",
      " [0.47691005 0.52308995]\n",
      " [0.13914941 0.8608505 ]\n",
      " [0.8262163  0.17378366]]\n",
      "<class 'numpy.ndarray'>\n",
      "(1200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(preds_output.predictions)  # 预测结果\n",
    "print(type(preds_output.predictions))\n",
    "print(preds_output.predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.5370903611183167,\n",
       " 'test_accuracy': 0.7875,\n",
       " 'test_runtime': 7.7982,\n",
       " 'test_samples_per_second': 153.881,\n",
       " 'test_steps_per_second': 2.436}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output.metrics  # 评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6240, 0.3760],\n",
      "        [0.8935, 0.1065],\n",
      "        [0.3765, 0.6235],\n",
      "        ...,\n",
      "        [0.2329, 0.7671],\n",
      "        [0.1845, 0.8155],\n",
      "        [0.9507, 0.0493]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"预测不含标签的测试数据集\"\"\"\n",
    "    model.eval()  # Sets the module in evaluation mode.\n",
    "    predict_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in data_loader:\n",
    "            input_ids = i['input_ids'].to(device)\n",
    "            attention_mask = i['attention_mask'].to(device)\n",
    "            token_type_ids = i['token_type_ids'].to(device)\n",
    "            output = model(input_ids, attention_mask, token_type_ids)[1]\n",
    "            predict_list.append(output)\n",
    "    predict_all = torch.cat(predict_list, dim=0)\n",
    "    return predict_all\n",
    "\n",
    "\n",
    "result = predict(model, dataLoader_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}