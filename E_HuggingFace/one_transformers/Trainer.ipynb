{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:02:33.902005Z",
     "start_time": "2024-11-13T12:02:27.970618Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:03:04.992715Z",
     "start_time": "2024-11-13T12:02:33.914181Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since dair-ai/emotion couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'split' at C:\\Users\\duanm\\.cache\\huggingface\\datasets\\dair-ai___emotion\\split\\0.0.0\\cab853a1dbdf4c42c2b3ef2173804746df8825fe (last modified on Wed Nov 13 16:08:02 2024).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = load_dataset(path='dair-ai/emotion')\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:03:05.337650Z",
     "start_time": "2024-11-13T12:03:05.322023Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    epochs = 4\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    batch_size = 4\n",
    "    lr = 4e-5\n",
    "    \n",
    "    num_warmup_steps = 50\n",
    "    num_training_steps = math.ceil(len(emotions['train']) / batch_size) * epochs  # 向上取整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:03:05.589447Z",
     "start_time": "2024-11-13T12:03:05.549683Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "seed = 2022\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:03:05.856297Z",
     "start_time": "2024-11-13T12:03:05.840268Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:03:26.416772Z",
     "start_time": "2024-11-13T12:03:06.012983Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109482240\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "print(tokenizer.model_input_names)\n",
    "\n",
    "pretrained = AutoModel.from_pretrained(CFG.model_name)\n",
    "print(pretrained.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:03:26.699200Z",
     "start_time": "2024-11-13T12:03:26.651939Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "\n",
    "# 批次处理,整个数据集同时进行处理\n",
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
    "emotions_encoded  # 原有数据与map函数新增数据('input_ids', 'token_type_ids', 'attention_mask')的联合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:03:26.871485Z",
     "start_time": "2024-11-13T12:03:26.796473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([0, 0, 3,  ..., 1, 3, 0]),\n",
       " 'input_ids': tensor([[  101,  1045,  2134,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2064,  ...,     0,     0,     0],\n",
       "         [  101, 10047,  9775,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1045,  2514,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2514,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2113,  ...,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded = emotions_encoded.remove_columns(['text'])  # 'text'列不参与训练(即不进入自定义模型forward函数)\n",
    "emotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
    "emotions_encoded['train'][:]  # 字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:03:27.572896Z",
     "start_time": "2024-11-13T12:03:26.950485Z"
    }
   },
   "outputs": [],
   "source": [
    "class Customize_Model(torch.nn.Module):\n",
    "    \"\"\"下游训练任务模型\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.classifier = torch.nn.Linear(768, num_labels)  # 多分类任务\n",
    "        self.pretrained = pretrained_model\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.loss_fct = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,  # ★★★★★训练阶段对应emotions_encoded['train']中的input_ids\n",
    "                attention_mask,  # ★★★★★训练阶段对应emotions_encoded['train']中的attention_mask\n",
    "                token_type_ids,  # ★★★★★训练阶段对应emotions_encoded['train']中的token_type_ids\n",
    "                labels=None):  # 标签;★★★★★训练阶段对应emotions_encoded['train']中的labels\n",
    "        outputs = self.pretrained(input_ids=input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:  # 若包含标签\n",
    "            loss = self.loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "\n",
    "        # 训练与评估阶段\n",
    "        # ★★★★★\n",
    "        # 返回值为一个元组\n",
    "        # 元组的第一个元素必须为该批次数据的损失值\n",
    "        # 元组的第二个元素为该批次数据的预测值(可选)\n",
    "        # * 验证数据集评估函数指标的计算\n",
    "        # * predict方法预测结果(predictions)与评估结果(metrics)(结合输入labels)的计算\n",
    "        if loss is not None:\n",
    "            return (loss, logits)\n",
    "        # 预测阶段\n",
    "        # ★★★★★\n",
    "        # 返回值为模型的预测结果\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "num_labels = 6\n",
    "\n",
    "model = Customize_Model(pretrained, num_labels)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:03:27.855324Z",
     "start_time": "2024-11-13T12:03:27.588522Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"验证数据集评估函数\"\"\"\n",
    "    labels = pred.label_ids  # 对应自定义模型forward函数输入:labels\n",
    "    preds = pred.predictions  # 对应自定义模型forward函数返回值的第二个元素\n",
    "    preds_argmax = preds.argmax(-1)\n",
    "    f1 = f1_score(labels, preds_argmax, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds_argmax)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}  # return a dictionary string to metric values\n",
    "\n",
    "\n",
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n",
    "    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            # 学习率预热(线性增加)\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        # 学习率线性衰减(最小为0)\n",
    "        # num_training_steps后学习率恒为0\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "model_name = f\"{CFG.model_name}-finetuned-emotion\"\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CFG.lr)  # 优化器\n",
    "scheduler_lr = get_linear_schedule_with_warmup(optimizer, CFG.num_warmup_steps, CFG.num_training_steps)  # 学习率预热(必须为LambdaLR对象)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:28:11.495191Z",
     "start_time": "2024-11-13T12:03:27.949475Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e476f0dfb9847e3a82f57149bf84080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01127777777777889, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\duanm\\Music\\GitHubProjects\\MLBase\\F_Tool\\自然语言处理HuggingFace工具集\\one_transformers\\wandb\\run-20241113_200335-2idh3xc1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dcdmmcomeon-google/huggingface/runs/2idh3xc1' target=\"_blank\">Trainer_log</a></strong> to <a href='https://wandb.ai/dcdmmcomeon-google/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dcdmmcomeon-google/huggingface' target=\"_blank\">https://wandb.ai/dcdmmcomeon-google/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dcdmmcomeon-google/huggingface/runs/2idh3xc1' target=\"_blank\">https://wandb.ai/dcdmmcomeon-google/huggingface/runs/2idh3xc1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16000' max='16000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16000/16000 24:26, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>0.248475</td>\n",
       "      <td>0.933000</td>\n",
       "      <td>0.933268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>0.266620</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.931252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.166400</td>\n",
       "      <td>0.267592</td>\n",
       "      <td>0.943500</td>\n",
       "      <td>0.943788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.326478</td>\n",
       "      <td>0.942000</td>\n",
       "      <td>0.942032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16000, training_loss=0.24299878311157228, metrics={'train_runtime': 1481.8381, 'train_samples_per_second': 43.19, 'train_steps_per_second': 10.797, 'total_flos': 0.0, 'train_loss': 0.24299878311157228, 'epoch': 4.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 主要调节的超参数\n",
    "training_args = TrainingArguments(\n",
    "    # The output directory where the model predictions and checkpoints will be written.\n",
    "    output_dir=model_name,\n",
    "    # If True, overwrite the content of the output directory. Use this to continue training if output_dir points to a checkpoint directory.\n",
    "    overwrite_output_dir=False,  # 默认:False\n",
    "    # save_total_limit (int, optional) —If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir.\n",
    "    save_total_limit=None,  # 默认:None\n",
    "\n",
    "    seed=42,\n",
    "\n",
    "    # Total number of training epochs to perform\n",
    "    num_train_epochs=CFG.epochs,  # 默认:3.0\n",
    "    # If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs.\n",
    "    # max_steps=2000,  # 默认:-1\n",
    "\n",
    "    # Maximum gradient norm (for gradient clipping).\n",
    "    max_grad_norm=1.0,  # 默认:1.0\n",
    "    # Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "    gradient_accumulation_steps=1,  # 默认:1\n",
    "\n",
    "    # 对应pytorch DataLoader 参数batch_size\n",
    "    # The batch size per GPU/TPU core/CPU for training.\n",
    "    per_device_train_batch_size=CFG.batch_size,  # 默认:8\n",
    "    # 对应pytorch DataLoader 参数batch_size\n",
    "    # The batch size per GPU/TPU core/CPU for evaluation.\n",
    "    per_device_eval_batch_size=CFG.batch_size,  # 默认:8\n",
    "    # Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size) or not.\n",
    "    # 对应pytorch DataLoader 参数drop_last\n",
    "    dataloader_drop_last=False,  # 默认:False\n",
    "\n",
    "    # The evaluation strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No evaluation is done during training.\n",
    "    # \"steps\": Evaluation is done (and logged) every eval_steps.\n",
    "    # \"epoch\": Evaluation is done at the end of each epoch.\n",
    "    eval_strategy=\"epoch\",  # 默认:'no'\n",
    "    # Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set.\n",
    "    eval_steps=None,  # 默认None\n",
    "\n",
    "    # The logging strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No logging is done during training.\n",
    "    # \"epoch\": Logging is done at the end of each epoch.\n",
    "    # \"steps\": Logging is done every logging_steps.\n",
    "    logging_strategy='epoch',  # 默认:'steps'\n",
    "    # Number of update steps between two logs if logging_strategy=\"steps\".\n",
    "    # logging_steps=500,  # 默认:500\n",
    "\n",
    "    # The checkpoint save strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No save is done during training.\n",
    "    # \"epoch\": Save is done at the end of each epoch.\n",
    "    # \"steps\": Save is done every save_steps.\n",
    "    save_strategy='epoch',  # 默认:'steps'\n",
    "    # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "    save_steps=500,  # 默认:500\n",
    "\n",
    "    # Logger log level to use on the main process. Possible choices are the log levels as strings: ‘debug’, ‘info’, ‘warning’, ‘error’ and ‘critical’, plus a ‘passive’ level which doesn’t set anything and lets the application set the level.\n",
    "    log_level='passive',  # 默认:'passive'\n",
    "\n",
    "    # Whether or not to load the best model found during training at the end of training.\n",
    "    # When set to True, the parameters save_strategy needs to be the same as evaluation_strategy, and in the case it is “steps”, save_steps must be a round multiple of eval_steps.\n",
    "    load_best_model_at_end=False,  # 默认load_best_model_at_end=False\n",
    "    # Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models.\n",
    "    # Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\".\n",
    "    # Will default to \"loss\" if unspecified and load_best_model_at_end=True (to use the evaluation loss).\n",
    "    metric_for_best_model=None,\n",
    "\n",
    "    # 原理: Activation checkpointing is a technique that trades compute for memory. Instead of keeping tensors needed for backward alive until they are used in gradient computation during backward, forward computation in checkpointed regions omits saving tensors for backward and recomputes them during the backward pass. Activation checkpointing can be applied to any part of a model.\n",
    "    # \n",
    "    # transformer.Trainer内部调用源码:\n",
    "    # ```python\n",
    "    # # Activate gradient checkpointing if needed\n",
    "    # if args.gradient_checkpointing:\n",
    "    #     if args.gradient_checkpointing_kwargs is None:\n",
    "    #         gradient_checkpointing_kwargs = {}\n",
    "    #     else:\n",
    "    #         gradient_checkpointing_kwargs = args.gradient_checkpointing_kwargs\n",
    "    #     self.model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n",
    "    # ````\n",
    "    # If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
    "    gradient_checkpointing=True,  # 默认gradient_checkpointing=False\n",
    "    # Key word arguments to be passed to the gradient_checkpointing_enable method.\n",
    "    gradient_checkpointing_kwargs=None,  # 默认gradient_checkpointing_kwargs=None\n",
    "\n",
    "    disable_tqdm=False,  # Whether or not to disable the tqdm progress bars and table of metrics produced by ~notebook.(.py运行时设置disable_tqdm=True)\n",
    "\n",
    "    # The list of integrations to report the results and logs to. Supported platforms are \"azure_ml\", \"clearml\", \"codecarbon\", \"comet_ml\", \"dagshub\", \"dvclive\", \"flyte\", \"mlflow\", \"neptune\", \"tensorboard\", and \"wandb\". Use \"all\" to report to all integrations installed, \"none\" for no integrations.\n",
    "    report_to=\"wandb\",  # 默认:'all'\n",
    "    # A descriptor for the run. Typically used for wandb, mlflow and comet logging. If not specified, will be the same as output_dir.\n",
    "    run_name=\"Trainer_log\"\n",
    ")\n",
    "\n",
    "# TrainingArguments优化器参数:\n",
    "# optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"`):\n",
    "#     The optimizer to use, such as \"adamw_torch\", \"adamw_torch_fused\", \"adamw_apex_fused\", \"adamw_anyprecision\",\n",
    "#     \"adafactor\". See `OptimizerNames` in [training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)\n",
    "#     for a full list of optimizers.\n",
    "# learning_rate (`float`, *optional*, defaults to 5e-5):\n",
    "#     The initial learning rate for [`AdamW`] optimizer.\n",
    "# weight_decay (`float`, *optional*, defaults to 0):\n",
    "#     The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`]\n",
    "#     optimizer.\n",
    "# adam_beta1 (`float`, *optional*, defaults to 0.9):\n",
    "#     The beta1 hyperparameter for the [`AdamW`] optimizer.\n",
    "# adam_beta2 (`float`, *optional*, defaults to 0.999):\n",
    "#     The beta2 hyperparameter for the [`AdamW`] optimizer.\n",
    "# adam_epsilon (`float`, *optional*, defaults to 1e-8):\n",
    "#     The epsilon hyperparameter for the [`AdamW`] optimizer.\n",
    "\n",
    "# TrainingArguments学习率调整参数:\n",
    "# lr_scheduler_type (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n",
    "#     The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n",
    "# lr_scheduler_kwargs ('dict', *optional*, defaults to {}):\n",
    "#     The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.\n",
    "# warmup_ratio (`float`, *optional*, defaults to 0.0):\n",
    "#     Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
    "# warmup_steps (`int`, *optional*, defaults to 0):\n",
    "#     Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.\n",
    "\n",
    "# optimizers (Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR], optional) — A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=emotions_encoded[\"train\"],  # 类型:datasets.arrow_dataset.Dataset\n",
    "    eval_dataset=emotions_encoded[\"validation\"],  # 类型:datasets.arrow_dataset.Dataset\n",
    "    optimizers=(optimizer, scheduler_lr),  # 自定义优化器与学习率预热\n",
    "    compute_metrics=compute_metrics,\n",
    "    # The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will default to [`default_data_collator`] if no `tokenizer` is provided, an instance of [`DataCollatorWithPadding`] otherwise\n",
    "    data_collator=None,  # 默认:None\n",
    "    # Processing class used to process the data. If provided, will be used to automatically process the inputs for the model, and it will be saved along the model to make it easier to rerun an interrupted training or reuse the fine-tuned model.\n",
    "    processing_class=tokenizer  # 默认:None\n",
    "    )\n",
    "trainer.train()  # 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:28:11.716456Z",
     "start_time": "2024-11-13T12:28:11.699904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 4e-05\n",
       "    lr: 0.0\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:28:18.726799Z",
     "start_time": "2024-11-13T12:28:12.067814Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[10.026421  , -1.2493951 , -2.0832083 , -1.463818  , -2.0060067 ,\n",
       "        -1.7284527 ],\n",
       "       [10.087032  , -1.3979279 , -1.9050596 , -1.516679  , -1.9069802 ,\n",
       "        -1.6398629 ],\n",
       "       [-2.3224888 ,  5.4730434 ,  4.8907237 , -2.767652  , -3.536753  ,\n",
       "        -2.8313003 ],\n",
       "       ...,\n",
       "       [-2.0526452 ,  9.663821  , -0.52668995, -2.7965734 , -2.6153255 ,\n",
       "        -2.043326  ],\n",
       "       [-1.9870696 ,  8.883894  ,  1.147463  , -2.7566857 , -3.1054263 ,\n",
       "        -2.5724087 ],\n",
       "       [-2.1567388 ,  9.649207  , -0.5594587 , -2.8418207 , -2.5324154 ,\n",
       "        -1.9988272 ]], dtype=float32), label_ids=array([0, 0, 2, ..., 1, 1, 1], dtype=int64), metrics={'test_loss': 0.3264780044555664, 'test_accuracy': 0.942, 'test_f1': 0.9420317738680476, 'test_runtime': 6.641, 'test_samples_per_second': 301.159, 'test_steps_per_second': 75.29})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run prediction and returns predictions and potential metrics.\n",
    "# Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method will also return metrics, like in `evaluate()`.\n",
    "preds_output = trainer.predict(emotions_encoded[\"validation\"])  # 预测和评估包含标签的验证数据集\n",
    "preds_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:28:18.882462Z",
     "start_time": "2024-11-13T12:28:18.869309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.026421   -1.2493951  -2.0832083  -1.463818   -2.0060067  -1.7284527 ]\n",
      " [10.087032   -1.3979279  -1.9050596  -1.516679   -1.9069802  -1.6398629 ]\n",
      " [-2.3224888   5.4730434   4.8907237  -2.767652   -3.536753   -2.8313003 ]\n",
      " ...\n",
      " [-2.0526452   9.663821   -0.52668995 -2.7965734  -2.6153255  -2.043326  ]\n",
      " [-1.9870696   8.883894    1.147463   -2.7566857  -3.1054263  -2.5724087 ]\n",
      " [-2.1567388   9.649207   -0.5594587  -2.8418207  -2.5324154  -1.9988272 ]]\n",
      "<class 'numpy.ndarray'>\n",
      "(2000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(preds_output.predictions)  # 预测结果\n",
    "print(type(preds_output.predictions))\n",
    "print(preds_output.predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:28:19.039510Z",
     "start_time": "2024-11-13T12:28:19.030801Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.3264780044555664,\n",
       " 'test_accuracy': 0.942,\n",
       " 'test_f1': 0.9420317738680476,\n",
       " 'test_runtime': 6.641,\n",
       " 'test_samples_per_second': 301.159,\n",
       " 'test_steps_per_second': 75.29}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output.metrics  # 评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:28:19.559985Z",
     "start_time": "2024-11-13T12:28:19.548338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = emotions_encoded[\"test\"].remove_columns(['label'])\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T12:28:26.559456Z",
     "start_time": "2024-11-13T12:28:19.902348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[10.029535  , -1.2211038 , -2.0303333 , -1.796847  , -1.8621632 ,\n",
       "        -1.6016061 ],\n",
       "       [10.07516   , -1.3358469 , -1.9799502 , -1.5594311 , -1.9193146 ,\n",
       "        -1.6537375 ],\n",
       "       [10.068274  , -1.3482922 , -1.9481524 , -1.6336232 , -1.8776803 ,\n",
       "        -1.6334101 ],\n",
       "       ...,\n",
       "       [-2.0575485 ,  9.643328  , -0.64553326, -2.8232603 , -2.5103939 ,\n",
       "        -1.9840453 ],\n",
       "       [-2.1385372 ,  9.623734  , -0.67554325, -2.8852339 , -2.3299308 ,\n",
       "        -1.9631996 ],\n",
       "       [-2.4684706 , -2.1929386 , -3.0233455 , -1.8496861 ,  5.593462  ,\n",
       "         3.6263864 ]], dtype=float32), label_ids=None, metrics={'test_runtime': 6.6473, 'test_samples_per_second': 300.872, 'test_steps_per_second': 75.218})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_dataset)  # 预测不含标签的测试数据集"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
