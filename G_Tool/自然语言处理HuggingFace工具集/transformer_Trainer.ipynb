{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "seed = 2022\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\dcdmm\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\emotion\\348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705 (last modified on Fri Apr  1 16:15:56 2022) since it couldn't be found locally at emotion., or remotely on the Hugging Face Hub.\n",
      "Using custom data configuration default\n",
      "Reusing dataset emotion (C:\\Users\\dcdmm\\.cache\\huggingface\\datasets\\emotion\\default\\0.0.0\\348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cedcce10d74fa388344a555f729311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = load_dataset(path='emotion')\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109482240\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "print(tokenizer.model_input_names)\n",
    "\n",
    "model_from_pretrained = AutoModel.from_pretrained(model_ckpt)\n",
    "print(model_from_pretrained.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf565c2c2d54091979bc55f659c24dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3551f81f5bf4e3f92fdc9737e14451f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51a0efee2da42ff97342f71d7b5a25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "\n",
    "# 批次处理,整个数据集同时进行处理\n",
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
    "emotions_encoded  # 原有数据与map函数新增数据('input_ids', 'token_type_ids', 'attention_mask')的联合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([0, 0, 3,  ..., 1, 3, 0]),\n",
       " 'input_ids': tensor([[  101,  1045,  2134,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2064,  ...,     0,     0,     0],\n",
       "         [  101, 10047,  9775,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1045,  2514,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2514,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2113,  ...,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded = emotions_encoded.remove_columns(['text'])  # 'text'列不参与训练(即不进入自定义模型forward函数)\n",
    "emotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
    "emotions_encoded['train'][:]  # 字典形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customize_Model(torch.nn.Module):\n",
    "    \"\"\"下游训练任务模型\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.classifier = torch.nn.Linear(768, num_labels)  # 多分类任务\n",
    "        self.pretrained = pretrained_model\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.loss_fct = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,  # ★★★★★训练阶段对应emotions_encoded['train']中的input_ids\n",
    "                attention_mask,  # ★★★★★训练阶段对应emotions_encoded['train']中的attention_mask\n",
    "                token_type_ids,  # ★★★★★训练阶段对应emotions_encoded['train']中的token_type_ids\n",
    "                labels=None):  # 标签;★★★★★训练阶段对应emotions_encoded['train']中的labels\n",
    "        outputs = self.pretrained(input_ids=input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:  # 若包含标签\n",
    "            loss = self.loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "\n",
    "        # 训练与评估阶段\n",
    "        # ★★★★★\n",
    "        # 返回值为一个元组\n",
    "        # 元组的第一个元素必须为该批次数据的损失值\n",
    "        # 元组的第二个元素为该批次数据的预测值(可选)\n",
    "        # * 验证数据集评估函数指标的计算\n",
    "        # * predict方法预测结果(predictions)与评估结果(metrics)(结合输入labels)的计算\n",
    "        if loss is not None:\n",
    "            return (loss, logits)\n",
    "        # 预测阶段\n",
    "        # ★★★★★\n",
    "        # 返回值为模型的预测结果\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "num_labels = 6\n",
    "\n",
    "model = Customize_Model(copy.deepcopy(model_from_pretrained),  # 必须进行深拷贝(pretrained(模型子网络结构)会参与梯度更新)\n",
    "                        num_labels)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"验证数据集评估函数\"\"\"\n",
    "    labels = pred.label_ids  # 对应自定义模型forward函数输入:labels\n",
    "    preds = pred.predictions  # 对应自定义模型forward函数返回值的第二个元素\n",
    "    preds_argmax = preds.argmax(-1)\n",
    "    f1 = f1_score(labels, preds_argmax, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds_argmax)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}  # return a dictionary string to metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcdmm\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8000' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8000/8000 39:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.458700</td>\n",
       "      <td>0.264718</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.934866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.217183</td>\n",
       "      <td>0.937000</td>\n",
       "      <td>0.937069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-emotion\\checkpoint-4000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-emotion\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-emotion\\checkpoint-4000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-emotion\\checkpoint-8000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-emotion\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-emotion\\checkpoint-8000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8000, training_loss=0.31951457214355466, metrics={'train_runtime': 2362.4992, 'train_samples_per_second': 13.545, 'train_steps_per_second': 3.386, 'total_flos': 0.0, 'train_loss': 0.31951457214355466, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 主要调节的超参数\n",
    "batch_size = 4\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "training_args = TrainingArguments(\n",
    "    # The output directory where the model predictions and checkpoints will be written.\n",
    "    output_dir=model_name,\n",
    "    # If True, overwrite the content of the output directory. Use this to continue training if output_dir points to a checkpoint directory.\n",
    "    overwrite_output_dir=False,  # 默认:False\n",
    "    # save_total_limit (int, optional) —If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir.\n",
    "    save_total_limit=None,  # 默认:None\n",
    "\n",
    "    seed=42,\n",
    "\n",
    "    # Total number of training epochs to perform\n",
    "    num_train_epochs=2,  # 默认:3.0\n",
    "    # If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs. I\n",
    "    # max_steps=2000,  # 默认:-1\n",
    "\n",
    "    #  Maximum gradient norm (for gradient clipping).\n",
    "    max_grad_norm=1.0,  # 默认:1.0\n",
    "\n",
    "    # The batch size per GPU/TPU core/CPU for training.\n",
    "    per_device_train_batch_size=batch_size,  # 默认:8\n",
    "    # The batch size per GPU/TPU core/CPU for evaluation.\n",
    "    per_device_eval_batch_size=batch_size,  # 默认:8\n",
    "\n",
    "    # Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "    gradient_accumulation_steps=1,  # 默认:1\n",
    "\n",
    "    # HuggingFace AdamW优化器超参数\n",
    "    # The initial learning rate for AdamW optimizer.\n",
    "    learning_rate=2e-5,  # 默认: 5e-5\n",
    "    # The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer.\n",
    "    weight_decay=0.01,  # 默认:0\n",
    "    # The beta1 hyperparameter for the AdamW optimizer.\n",
    "    adam_beta1=0.9,  # 默认:0.9\n",
    "    # The beta2 hyperparameter for the AdamW optimizer.\n",
    "    adam_beta2=0.999,  # 默认:0.999\n",
    "    # The epsilon hyperparameter for the AdamW optimizer.\n",
    "    adam_epsilon=1e-8,  # 默认:1e-8\n",
    "\n",
    "    # HuggingFace 不同学习率预热超参数\n",
    "    #  The scheduler type to use. See the documentation of SchedulerType for all possible values.\n",
    "    lr_scheduler_type='linear',  # 默认:'linear'\n",
    "    # Ratio of total training steps used for a linear warmup from 0 to learning_rate.\n",
    "    warmup_ratio=0.0,  # 默认:0.0\n",
    "    # Number of steps used for a linear warmup from 0 to learning_rate. Overrides any effect of warmup_ratio.\n",
    "    # warmup_steps=0,  # 默认0\n",
    "\n",
    "    # The evaluation strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No evaluation is done during training.\n",
    "    # \"steps\": Evaluation is done (and logged) every eval_steps.\n",
    "    # \"epoch\": Evaluation is done at the end of each epoch.\n",
    "    evaluation_strategy=\"epoch\",  # 默认:'no'\n",
    "    # Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set.\n",
    "    eval_steps=None,  # 默认None\n",
    "\n",
    "    # The logging strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No logging is done during training.\n",
    "    # \"epoch\": Logging is done at the end of each epoch.\n",
    "    # \"steps\": Logging is done every logging_steps.\n",
    "    logging_strategy='epoch',  # 默认:'steps'\n",
    "    # Number of update steps between two logs if logging_strategy=\"steps\".\n",
    "    # logging_steps=500,  # 默认:500\n",
    "\n",
    "    # The checkpoint save strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No save is done during training.\n",
    "    # \"epoch\": Save is done at the end of each epoch.\n",
    "    # \"steps\": Save is done every save_steps.\n",
    "    save_strategy='epoch',  # 默认:'steps'\n",
    "    #  Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "    save_steps=500,  # 默认:500\n",
    "\n",
    "    # Logger log level to use on the main process. Possible choices are the log levels as strings: ‘debug’, ‘info’, ‘warning’, ‘error’ and ‘critical’, plus a ‘passive’ level which doesn’t set anything and lets the application set the level.\n",
    "    log_level='passive',  # 默认:'passive'\n",
    "\n",
    "    # Whether or not to load the best model found during training at the end of training.\n",
    "    # When set to True, the parameters save_strategy needs to be the same as evaluation_strategy, and in the case it is “steps”, save_steps must be a round multiple of eval_steps.\n",
    "    load_best_model_at_end=False,  # 默认load_best_model_at_end=False\n",
    "    # Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models.\n",
    "    # Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\".\n",
    "    # Will default to \"loss\" if unspecified and load_best_model_at_end=True (to use the evaluation loss).\n",
    "    metric_for_best_model=None,\n",
    "\n",
    "    disable_tqdm=False,  # 是否使用tqdm显示进度(.py运行时设置disable_tqdm=True)\n",
    ")\n",
    "\n",
    "# 建议通过optimizers参数自定义优化器与学习率预热\n",
    "# optimizers (Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR], optional) — A tuple containing the optimizer and the scheduler to use.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=emotions_encoded[\"train\"],  # 类型:datasets.arrow_dataset.Dataset\n",
    "    eval_dataset=emotions_encoded[\"validation\"],  # 类型:datasets.arrow_dataset.Dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer)\n",
    "trainer.train()  # 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 8.74499   , -1.6454196 , -1.3407766 , -1.3644311 , -1.6235143 ,\n",
       "        -1.8594097 ],\n",
       "       [ 8.740201  , -1.5983361 , -1.753396  , -1.2184671 , -1.410719  ,\n",
       "        -1.6200553 ],\n",
       "       [-2.1097214 ,  2.4274385 ,  5.151494  , -1.6792418 , -2.2815318 ,\n",
       "        -2.322027  ],\n",
       "       ...,\n",
       "       [-2.0081527 ,  8.669637  , -0.66810536, -2.04628   , -2.075761  ,\n",
       "        -1.5750041 ],\n",
       "       [-2.1995275 ,  3.2017984 ,  4.738045  , -1.6810935 , -2.2954392 ,\n",
       "        -2.4768968 ],\n",
       "       [-2.0613208 ,  8.638503  , -1.208316  , -2.001938  , -1.7295588 ,\n",
       "        -1.0406128 ]], dtype=float32), label_ids=array([0, 0, 2, ..., 1, 1, 1], dtype=int64), metrics={'test_loss': 0.21718303859233856, 'test_accuracy': 0.937, 'test_f1': 0.9370687362877385, 'test_runtime': 33.3576, 'test_samples_per_second': 59.956, 'test_steps_per_second': 14.989})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run prediction and returns predictions and potential metrics.\n",
    "# Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method will also return metrics, like in `evaluate()`.\n",
    "preds_output = trainer.predict(emotions_encoded[\"validation\"])  # 类型:datasets.arrow_dataset.Dataset;验证数据集(包含标签)\n",
    "preds_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.74499    -1.6454196  -1.3407766  -1.3644311  -1.6235143  -1.8594097 ]\n",
      " [ 8.740201   -1.5983361  -1.753396   -1.2184671  -1.410719   -1.6200553 ]\n",
      " [-2.1097214   2.4274385   5.151494   -1.6792418  -2.2815318  -2.322027  ]\n",
      " ...\n",
      " [-2.0081527   8.669637   -0.66810536 -2.04628    -2.075761   -1.5750041 ]\n",
      " [-2.1995275   3.2017984   4.738045   -1.6810935  -2.2954392  -2.4768968 ]\n",
      " [-2.0613208   8.638503   -1.208316   -2.001938   -1.7295588  -1.0406128 ]]\n",
      "<class 'numpy.ndarray'>\n",
      "(2000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(preds_output.predictions)  # 预测结果\n",
    "print(type(preds_output.predictions))\n",
    "print(preds_output.predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.21718303859233856,\n",
       " 'test_accuracy': 0.937,\n",
       " 'test_f1': 0.9370687362877385,\n",
       " 'test_runtime': 33.3576,\n",
       " 'test_samples_per_second': 59.956,\n",
       " 'test_steps_per_second': 14.989}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = emotions_encoded[\"test\"].remove_columns(['label'])\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 8.547188 , -1.1954035, -1.6077578, -1.3239136, -1.8036917,\n",
       "        -1.7479213],\n",
       "       [ 8.737176 , -1.561123 , -1.5552757, -1.1923537, -1.7461095,\n",
       "        -1.7708706],\n",
       "       [ 8.780567 , -1.6004115, -1.5823025, -1.1673901, -1.728886 ,\n",
       "        -1.6798965],\n",
       "       ...,\n",
       "       [-1.7076166,  8.718234 , -1.158481 , -1.8851666, -1.949625 ,\n",
       "        -1.4237279],\n",
       "       [-1.8153396,  8.687426 , -1.2887311, -1.8366973, -1.5447539,\n",
       "        -1.5415577],\n",
       "       [-1.3077655, -2.1242712, -2.5342546, -1.3820418,  5.358166 ,\n",
       "         3.467203 ]], dtype=float32), label_ids=None, metrics={'test_runtime': 35.9695, 'test_samples_per_second': 55.603, 'test_steps_per_second': 13.901})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_dataset)  # 预测不含标签的测试数据集"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}