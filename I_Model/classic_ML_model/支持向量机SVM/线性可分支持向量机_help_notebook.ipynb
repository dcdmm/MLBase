{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red' size=4>定义(线性可分的支持向量机):</font>给定线性可分训练数据集,通过间隔最大化或等价地求解相应的凸二次规划问题学\n",
    "习得到的分离超平面为    \n",
    "$$  \\mathbf{w}^* \\cdot \\mathbf{x} + b^* =0 $$     \n",
    "以及相应的分类决策函数     \n",
    "$$  f(\\mathbf{x}) = \\mathrm{sign}(\\mathbf{w}^* \\cdot \\mathbf{x} + b^*) $$     \n",
    "称为线性可分支持向量机  \n",
    "\n",
    "***\n",
    "\n",
    "### 函数间隔   \n",
    "<font color='red' size=4>定义(函数间隔):</font>对于给定的训练数据集$T$和超平面$(\\mathbf{w},b)$,定义超平面$(\\mathbf{w},b)$关于样\n",
    "本点$(\\mathbf{x}_i, y_i)$的函数间隔为    \n",
    "$$ \\hat{\\gamma}_i = y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b)  $$           \n",
    "&emsp;&emsp;定义超平面$(\\mathbf{w},b)$关于训练数据集$T$的函数间隔为超平面$(\\mathbf{w}, b)$关于$T$中所有样本点$(\\mathbf{x}_i, y_i)$的函数间隔之最小值,即        \n",
    "$$ \\hat{\\gamma} = \\min_{i=1, \\dots, N} \\hat{\\gamma}_i  $$       \n",
    "&emsp;&emsp;函数间隔可以表示分类预测的正确度(即$|\\mathbf{w}\\cdot \\mathbf{x} + b|$)及确信度(即$\\mathbf{w} \\cdot \\mathbf{x} + b$的符号是否与类标记$y$的符号一致,所\n",
    "以可用向量$y(\\mathbf{w} \\cdot \\mathbf{x} + b)$来表示分类的正确性及准确性).但是选择分离超平面时,只有函数间隔还不够,因为只要成比例地改变$\\mathbf{w}$和$b$,例\n",
    "如将它们改为$2\\mathbf{w}$和$2b$,超平面并没有改变,但函数间隔却变为原来的2倍.这一事实启示我们,可以对分离超平面的法向量$\\mathbf{w}$加某些\n",
    "约束,如规范化,$||\\mathbf{w}||=1$,使函数间隔是确定的.这时函数间隔成为几何间隔(geometric margin).\n",
    "\n",
    "***\n",
    "\n",
    "### 几何间隔\n",
    "<font color='red' size=4>定义(几何间隔):</font>对于给定的训练数据集$T$和超平面$(\\mathbf{w},b)$,定义超平面$(\\mathbf{w},b)$关于\n",
    "样本点$(\\mathbf{x}_i,y_i)$的几何间隔为        \n",
    "$$ \\gamma_i = y_i(\\frac{\\mathbf{w}}{||\\mathbf{w}||} \\cdot \\mathbf{x}_i  + \\frac{b}{||\\mathbf{w}||})  $$     \n",
    "&emsp;&emsp;定义超平面$(\\mathbf{w},b)$关于训练数据集$T$的几何间隔为超平面$(\\mathbf{x},b)$关于$T$中所有样本点$(\\mathbf{x}_i, y_i)$的几何间隔之最小值,即         \n",
    "$$ \\gamma = \\min_{i=i,\\dots, N} \\gamma_i  $$              \n",
    "&emsp;&emsp;超平面$(\\mathbf{w},b)$关于样本点$(\\mathbf{x}_i, y_i)$的几何间隔一般是\n",
    "实例点到超平面的有符号距离(signed distance),当样本被超平面正确分类时就是实例点到超平面的距离.          \n",
    "&emsp;&emsp;从函数间隔和几何间隔的定义可知,函数间隔和几何间隔又下面的关系:        \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\gamma_i & = \\frac{\\hat{\\gamma}_i}{||\\mathbf{w}||} \\\\\n",
    "\\gamma   &= \\frac{\\hat{\\gamma}}{||\\mathbf{w}||}\n",
    "\\end{aligned}           \n",
    "$$\n",
    "\n",
    "如果$||\\mathbf{w}||=1$,那么函数间隔和几何间隔相等.如果超平面参数$\\mathbf{w}$和$b$成比例地改变(超平面没有改变),函数间隔也按此比例改变,而几何间隔不变.\n",
    "\n",
    "***\n",
    "\n",
    "### 间隔最大化\n",
    "&emsp;&emsp;下面考虑如何求一个几何间隔最大的分离超平面,即最大间隔分离超平面.具体地,这个问题可以表示为下面的约束最优化问题:         \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\max_{\\mathbf{w}, b} \\quad \\gamma \\\\\n",
    "& s.t  \\quad y_i(\\frac{\\mathbf{w}}{||\\mathbf{w}||} \\cdot \\mathbf{x}_i + \\frac{b}{||\\mathbf{w}||}) \\geq \\gamma, \\qquad i=1,2,\\dots, N\n",
    "\\end{aligned}\n",
    "$$       \n",
    "\n",
    "即我们希望最大化超平面$(\\mathbf{w},b)$关于训练数据集的几何间隔$\\gamma$,约束问题表示的是超\n",
    "平面$(\\mathbf{w},b)$关于每个训练样本点的几何间隔至少是$\\gamma$.          \n",
    "&emsp;&emsp;考虑几何间隔和函数间隔的关系式,可将这个问题改写为           \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\max_{\\mathbf{w}, b} \\quad \\frac{\\hat{\\gamma}}{||\\mathbf{w}||} \\\\\n",
    "& s.t  \\quad y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq \\hat{\\gamma}, \\qquad i=1,2,\\dots, N\n",
    "\\end{aligned}              \n",
    "$$\n",
    "\n",
    "&emsp;&emsp;函数间隔$\\hat{\\gamma}$的取值并不影响最优化问题的解.事实上,假设将$\\mathbf{w}$和$b$按比\n",
    "例改变为$\\lambda \\mathbf{w}$和$\\lambda b$,这时函数间隔为$\\lambda \\hat{\\gamma}$.函数间隔的这一改变对上面最\n",
    "优化不等式约束没有影响,对目标函数的优化也没\n",
    "有影响(即$\\frac{\\lambda \\hat{\\gamma}}{\\lambda ||\\mathbf{w}||} = \\frac{\\hat{\\gamma}}{||\\mathbf{w}||}$),也就是说,它产生一个等价的最优\n",
    "化问题.这样,就可以取$\\hat{\\gamma}=1$.将$\\hat{\\gamma}=1$代入上面的最优化问题,注意到最大化$\\frac{1}{||\\mathbf{w}||}$和最\n",
    "小化$\\frac{1}{2}||\\mathbf{w}||^2$是等价的,于是就得到下面的线性可分支持向量机学习的最优化问题:              \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\min_{\\mathbf{w}, b} \\quad \\frac{1}{2} ||\\mathbf{w}||^2 \\\\\n",
    "& s.t  \\quad y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) -1 \\geq 0, \\qquad i=1,2,\\dots, N\n",
    "\\end{aligned}\n",
    "$$             \n",
    "\n",
    "这是一个凸二次规划(convex quadratic programming)问题.\n",
    "\n",
    "***\n",
    "\n",
    "<font color='red' size=4>算法:</font>        \n",
    "输入:线性可分训练数据集$T=\\{ (\\mathbf{x}_1,y_1),(\\mathbf{x}_2,y_2),\\dots, (\\mathbf{x}_N,y _N) \\}$,其中,$\\mathbf{x}_i \\in \\mathcal{X}  = \\mathbb{R}^n, y_i \\in \\mathcal{Y} = \\{ -1, 1 \\}, i=1,2,\\dots,N$;      \n",
    "输出:最大间隔分离超平面和分类决策函数.         \n",
    "1. 构造并求解最优化问题:    \n",
    "\\begin{align}\n",
    "& \\min_{\\mathbf{w}, b} \\quad \\frac{1}{2} ||\\mathbf{w}||^2 \\\\\n",
    "& s.t  \\quad y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) -1 \\geq 0, \\qquad i=1,2,\\dots, N\n",
    "\\end{align}       \n",
    "\n",
    "求得最优解$\\mathbf{w}^*, b^*$     \n",
    "\n",
    "2. 由此得到分离超平面:      \n",
    "$$ \\mathbf{w}^* \\cdot \\mathbf{x} + b^* =0  $$\n",
    "3. 分类决策函数    \n",
    "$$ f(\\mathbf{x}) = \\mathrm{sign}(\\mathbf{w}^* \\cdot \\mathbf{x} + b^*) $$     \n",
    "\n",
    "线性可分训练数据集的最大间隔分离超平面是唯一存在的.\n",
    "\n",
    "***\n",
    "\n",
    "### 支持向量与间隔边界    \n",
    "&emsp;&emsp;在线性可分的情况下,训练数据集的样本点中与分离超平面最近的样本点的实例称为支持向量(support vector).支持\n",
    "向量是使约束式$y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) -1 \\geq 0$等号成立的点,即      \n",
    "$$ y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) -1 =0 $$     \n",
    "&emsp;&emsp;对$y_i = +1$的正例点,支持向量在超平面      \n",
    "$$  H_1: \\mathbf{w} \\cdot \\mathbf{x} + b =1  $$     \n",
    "上,对$y_i = -1$的负例点,支持向量在超平面    \n",
    "$$  H_1: \\mathbf{w} \\cdot \\mathbf{x} + b = -1  $$     \n",
    "上,如下面所示,在$H_1$和$H_2$上的点就是支持向量.     \n",
    "\n",
    "<img src=\"../../../Other/img/支持向量.png\" style=\"width:500px;height:300px;float:bottom\">\n",
    "\n",
    "&emsp;&emsp;注意到$H_1$与$H_2$平行,并且没有实例点落在它们中间.在$H_1$与$H_2$之间形成一条长带,分离超平面\n",
    "与它们平行且位于它们中央.长带的宽度,即$H_1$与$H_2$之间的距离称为间隔(margin).间隔依赖于分离超平面\n",
    "的法向量$\\mathbf{w}$,等于$\\frac{2}{||\\mathbf{w}||}$.$H_1$和$H_2$称为间隔边界.           \n",
    "&emsp;&emsp;在决定分离超平面时只有支持向量起作用,而其他实例点并不起作用.如果移动支持向量将改变所求的解;但是\n",
    "如果在间隔边界之外移动其他实例点,甚至去掉这些点,则解是不会改变的.由于支持向量在确定分离超平面中起着决定\n",
    "性作用,所有将这种分类模型称为支持向量机.支持向量的个数一般很少,所以支持向量机由很少的\"重要点\"训练样本确定.      \n",
    "\n",
    "***\n",
    "\n",
    "### 学习的对偶算法       \n",
    "优点:一是对偶问题往往更容易求解;二是自然引入核函数,进而推广到非线性分类问题.      \n",
    "&emsp;&emsp;首先构建拉格朗日函数(Lagrange function).为此,对每一个原始问题的不等\n",
    "式约束引入拉格朗日乘子(Lagrange multiplier)$\\alpha_i \\geq 0,i=1,2, \\cdots,N$,定义拉格朗日函数:         \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\mathbf{w}, b, \\alpha) &= \\frac{1}{2} ||\\mathbf{w}||^2- \\sum_{i=1}^{N}\\alpha_i(y_i(\\mathbf{w} \\cdot \\mathbf{x} + b) -1) \\\\\n",
    "                &= \\frac{1}{2} ||\\mathbf{w}||^2- \\sum_{i=1}^{N}\\alpha_i y_i(\\mathbf{w} \\cdot \\mathbf{x} + b) +  \\sum_{i=1}^N \\alpha_i\n",
    "\\end{aligned}\n",
    "$$     \n",
    "\n",
    "其中,$\\alpha=(\\alpha_1, \\alpha_2, \\cdots, \\alpha_N)^T$为拉格朗日乘子.       \n",
    "&emsp;&emsp;根据拉格朗日对偶性,原始问题的对偶问题是极大极小问题:      \n",
    "$$  \\max_{\\alpha} \\min_{\\mathbf{w}, b} L(\\mathbf{w}, b, \\alpha)  $$        \n",
    "所以,为了得到对偶问题的解,需要先求$L(\\mathbf{w}, b, \\alpha)$对$\\mathbf{w}, b$的极小,再求$\\alpha$的极大.            \n",
    "1. 求$\\min_{\\mathbf{w}, b} L(\\mathbf{w}, b,\\alpha)$         \n",
    "将拉格朗日函数$L(\\mathbf{w}, b, \\alpha)$分别对$\\mathbf{w}, b$求偏导数并令其等于0.  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\mathbf{w} L(\\mathbf{w}, b, \\alpha) & = \\mathbf{w} - \\sum_{i=1}^{N} \\alpha_i y_i \\mathbf{x}_i =0  \\qquad 注意:   \\frac{\\partial \\mathbf{w}^T \\mathbf{w}}{\\partial \\mathbf{w}} = \\frac{\\partial \\| \\mathbf{w} \\|^2}{\\partial \\mathbf{w}}  =  2\\mathbf{w}^T \\\\\n",
    "\\nabla_b L(\\mathbf{w}, b, \\alpha) & = - \\sum_{i=1}^{N} \\alpha_i y_i=0\n",
    "\\end{aligned}\n",
    "$$           \n",
    "\n",
    "得      \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i  \\tag{1}         \\\\\n",
    "& \\sum_{i=1}^N \\alpha_i y_i =0  \\tag{2}\n",
    "\\end{align}\n",
    "$$         \n",
    "\n",
    "将(1)式代入拉格朗日函数,并利用(2),即得\t \n",
    "    \n",
    "2. 求$ \\min_{\\mathbf{w}, b} L(\\mathbf{w}, b, \\alpha) $对$ \\alpha $的极大,即是对偶问题  \n",
    "\n",
    "$$ \n",
    "\\begin{aligned}      \n",
    "\\max_{\\alpha} \\quad &-\\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (\\mathbf{x}_i \\cdot \\mathbf{x}_j)  + \\sum_{i=1}^N \\alpha_i \\\\\n",
    "s.t.    \\quad      & \\sum_{i=1}^N \\alpha_i y_i =0 \\\\     \n",
    "                & \\alpha_i \\geq 0, \\quad i=1,2,\\cdots, N   \n",
    "\\end{aligned}\n",
    "$$           \n",
    "          \n",
    "将上式的目标函数由求极大转换成求极小,就得到下面与之等价的对偶问题最优化问题:    \n",
    "\n",
    "$$         \n",
    "\\begin{aligned}       \n",
    "  \\min_{\\alpha} \\quad &\\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (\\mathbf{x}_i \\cdot \\mathbf{x}_j)  - \\sum_{i=1}^N \\alpha_i \\\\\n",
    "  s.t.    \\quad      & \\sum_{i=1}^N \\alpha_i y_i =0 \\\\\n",
    "                     & \\alpha_i \\geq 0, \\quad i=1,2,\\cdots, N      \n",
    "\\end{aligned}\n",
    "$$       \n",
    "\n",
    "&emsp;&emsp;原始问题是一个凸二次规划问题,由拉格朗日对偶性知,存在$\\mathbf{w}^*, \\alpha^*, \\beta^*$,使$\\mathbf{w}^*$是原\n",
    "始问题的解,$\\alpha^*, \\beta^*$是对偶问题的解,这意味着求解原始问题可以等价地转换为求解其对偶问题.\n",
    "\n",
    "***\n",
    "\n",
    "<font size=4 color='red'>定理:</font>设$\\alpha^* = (\\alpha^*_1, \\alpha^*_2, \\cdots, \\alpha^*_l)^T$是对偶最优\n",
    "化问题的解,则存在下标$j$,使得$\\alpha_j^* >0$,并可按下式求得原始最优化问题的解$\\mathbf{w}^*, b^*$:        \n",
    "$$\\mathbf{w}^* = \\sum_{i=1}^N \\alpha^*_i y_i \\mathbf{x}_i     \\\\  b^* = y_j - \\sum_{i=1}^N \\alpha^*_i y_i (\\mathbf{x}_i \\cdot \\mathbf{x}_j)  $$      \n",
    "<font color='red' size=4>证明:</font>原始问题是一个凸二次规划问题,则KKT条件成立,即得      \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\nabla _\\mathbf{w} L(\\mathbf{w}^*, b^*, \\alpha^*) = \\mathbf{w}^* - \\sum_{i=1}^N \\alpha^*_i y_i \\mathbf{x}_i = 0 \\\\\n",
    "&  \\nabla _b L(\\mathbf{w}^*, b^*, \\alpha^*) = -\\sum_{i=1}^N \\alpha^*_i y_i =0 \\\\\n",
    "& \\alpha^*_i (y_i(\\mathbf{w}^* \\cdot \\mathbf{x}_i + b^*) -1) =0, \\quad i=1,2,\\cdots ,N  \\\\\n",
    "& y_i(\\mathbf{w}^* \\cdot \\mathbf{x}_i + b^*) -1 \\geq 0, \\quad i=1,2,\\cdots ,N \\\\\n",
    "& \\alpha_i^* \\geq 0, \\quad i=1,2,\\cdots ,N \n",
    "\\end{aligned}\n",
    "$$     \n",
    "\n",
    "由此得    \n",
    "$$ \\mathbf{w}^* =  \\sum_i \\alpha_i^* y_i \\mathbf{x}_i  $$     \n",
    "其中至少有一个$\\alpha_j^* >0$,对此$j$有      \n",
    "$$  y_j(\\mathbf{w}^* \\cdot \\mathbf{x}_j + b^* ) -1 =0  $$     \n",
    "将$\\mathbf{w}^* =  \\sum_i \\alpha_i^* y_i \\mathbf{x}_i$代入上式并注意到$y^2_j=1$,即得       \n",
    "$$  b^* = y_j - \\sum_{i=1}^N \\alpha^*_i y_i (\\mathbf{x}_i \\cdot \\mathbf{x}_j) $$     \n",
    "&emsp;&emsp;由此定义可知,分离超平面可以写成      \n",
    "$$   \\sum_{i=1}^{N} \\alpha^*_i y_i(\\mathbf{x} \\cdot \\mathbf{x}_i) + b^* =0 $$    \n",
    "&emsp;&emsp;分离决策函数可以写成        \n",
    "$$ f(\\mathbf{x}) = \\mathrm{sign} \\left(  \\sum_{i=1}^{N} \\alpha^*_i y_i(\\mathbf{x} \\cdot \\mathbf{x}_i) + b^*\\right) $$     \n",
    "这就是说,分类决策函数只依赖于输入$\\mathbf{x}$和训练样本输入的内积.上式称为线性可分支持向量机的对偶形式.    \n",
    "\n",
    "***\n",
    "\n",
    "<font color='red' size=4>算法:</font>         \n",
    "输入:线性可分训练数据集$T=\\{ (\\mathbf{x}_1,y_1),(\\mathbf{x}_2,y_2),\\dots, (\\mathbf{x}_N,y _N) \\}$,其中,$\\mathbf{x}_i \\in \\mathcal{X}  = \\mathbb{R}^n, y_i \\in \\mathcal{Y} = \\{ -1, 1 \\}, i=1,2,\\dots,N$;      \n",
    "输出:分离超平面和分类决策函数            \n",
    "\n",
    "1. 构建并求解最优化问题\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\min_{\\alpha} \\quad &\\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (\\mathbf{x}_i \\cdot \\mathbf{x}_j)  + \\sum_{i=1}^N \\alpha_i \\\\\n",
    "  s.t.    \\quad      & \\sum_{i=1}^N \\alpha_i y_i =0 \\\\\n",
    "                   & \\alpha_i \\geq 0, \\quad i=1,2,\\cdots, N\n",
    "\\end{aligned}\n",
    "$$\n",
    "  \n",
    "求得最优解$\\alpha^* = (\\alpha^*_1, \\alpha^*_2, \\cdots, \\alpha^*_l)^T$     \n",
    "\n",
    "2. 计算\n",
    "$$ \\mathbf{w}^* =  \\sum_i \\alpha_i^* y_i \\mathbf{x}_i  $$\n",
    "并选择$\\alpha^*_i$的一个正分量$\\alpha^*_j > 0$,计算     \n",
    "$$ b^* = y_j - \\sum_{i=1}^N \\alpha^*_i y_i (\\mathbf{x}_i \\cdot \\mathbf{x}_j) $$\n",
    "\n",
    "3. 求得分离超平面\n",
    "$$  \\mathbf{w}^* \\cdot \\mathbf{x} + b^* =0  $$\n",
    "分类决策函数    \n",
    "$$  f(\\mathbf{x}) = \\mathrm{sign} (\\mathbf{w}^* \\cdot \\mathbf{x} + b^* )  $$    \n",
    "&emsp;&emsp;在线性可分支持向量机中,易知$\\mathbf{w}^*$和$b^*$只依赖于训练数据中对应于$\\alpha^*_i > 0$的\n",
    "样本点$(\\mathbf{x}_i,y_i)$,而其他样本点对$\\mathbf{w}^*$和$b^*$没有影响.我们将训练数据中对应于$\\alpha^*_i > 0$的实例点$\\mathbf{x}_i \\in \\mathbb{R}^n$称为支持向量.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
