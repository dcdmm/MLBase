{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "723d1884-b7b7-44f5-ba37-5ef3b560b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import (DataLoader, Dataset)\n",
    "from transformers import (BertTokenizerFast, BertModel)\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from model import CustomRelation\n",
    "from utils import multilabel_categorical_crossentropy, MetricsCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f100a72-e25a-4a8e-ab27-1d866c6bb46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "seed = 2022\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce00bc6-9109-4338-89ad-b3effa15fb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5dd7453-7b3c-4d1a-ba52-9394bb4ccd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3557\n",
      "396\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/train_rel.json', 'r', encoding='utf-8') as f:\n",
    "    train = json.load(f)  # 列表\n",
    "\n",
    "# 划分训练/验证数据集\n",
    "train_data, valid_data = train_test_split(train, test_size=0.1, random_state=seed)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3898cc-5135-4d1c-83de-eacff9304dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, sentences):\n",
    "        self._sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self._sentences[index]\n",
    "\n",
    "        return {'text': sentence['sentence_text'],\n",
    "                'position_ids': sentence['position_ids'],\n",
    "                'relation_label': sentence['relation_label'],\n",
    "                'relation_idx': sentence['relation_idx']}\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(sentences=train_data)\n",
    "valid_dataset = CustomDataset(sentences=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70bb0a7a-20e6-44bb-9239-20f115722621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='save_tokenizer/', vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325559296\n"
     ]
    }
   ],
   "source": [
    "tokenizer_fast = BertTokenizerFast.from_pretrained('save_tokenizer/')\n",
    "print(tokenizer_fast)\n",
    "\n",
    "pretrained = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext-large')\n",
    "pretrained.resize_token_embeddings(len(tokenizer_fast))\n",
    "print(pretrained.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af1456d-4e3a-4a5a-989b-293396cb65f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids.shape: torch.Size([4, 247])\n",
      "attention_mask.shape: torch.Size([4, 247])\n",
      "token_type_ids.shape: torch.Size([4, 247])\n",
      "position_ids.shape.shape: torch.Size([4, 247])\n",
      "label.shape: torch.Size([4, 1482])\n",
      "labels_mask.shape: torch.Size([4, 1482])\n",
      "relations_idx.shape: (4, 1482, 4)\n"
     ]
    }
   ],
   "source": [
    "def get_collate_fn(tokenizer, max_len=512):\n",
    "    \"\"\"返回collate_fun函数(通过闭包函数引入形参)\"\"\"\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        sentences_list = [sentence['text'] for sentence in batch]\n",
    "        # 设置add_special_tokens=False(数据处理中已经加入了'[CLS]','[SEP]'等特殊token)\n",
    "        outputs = tokenizer(sentences_list, truncation=True, max_length=max_len, padding=True,\n",
    "                            add_special_tokens=False, return_tensors='pt')\n",
    "        input_ids, attention_mask, token_type_ids = outputs.input_ids, outputs.attention_mask, outputs.token_type_ids\n",
    "\n",
    "        relation_max_len = 0  # 该batch内最长的relation_label\n",
    "        position_ids = []\n",
    "        for sentence in batch:\n",
    "            if len(sentence['relation_label']) >= relation_max_len:\n",
    "                relation_max_len = len(sentence['relation_label'])\n",
    "            pos_ids = sentence['position_ids'].copy()\n",
    "            pad = [0] * (input_ids.shape[1] - len(pos_ids))\n",
    "            position_ids.append(pos_ids + pad)\n",
    "        position_ids = torch.tensor(position_ids, dtype=torch.long)  # 自定义位置嵌入\n",
    "\n",
    "        labels, labels_mask, relations_idx = [], [], []\n",
    "        for sentence in batch:\n",
    "            r_label = []\n",
    "            for label in sentence['relation_label']:\n",
    "                r_label.append(int(label == '属性'))  # 1表示存在关系,0表示不存在关系\n",
    "            pad = [0] * (relation_max_len - len(r_label))\n",
    "            labels.append(r_label + pad)\n",
    "            labels_mask.append([1] * len(r_label) + pad)\n",
    "\n",
    "            relation_idx = sentence['relation_idx'].copy()\n",
    "            for _ in pad:\n",
    "                relation_idx.append([0, 0, 0, 0])\n",
    "            relations_idx.append(relation_idx)\n",
    "        # labels.shape=[batch_size, relation_max_len] \n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        # labels_mask.shape=[batch_size, relation_max_len]\n",
    "        labels_mask = torch.tensor(labels_mask, dtype=torch.long)\n",
    "        # relations_idx.shape=[batch_size, relation_max_len, 4]\n",
    "        relations_idx = np.array(relations_idx)\n",
    "\n",
    "        return input_ids, attention_mask, token_type_ids, position_ids, labels, labels_mask, relations_idx\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=4,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=get_collate_fn(tokenizer_fast))\n",
    "dataloader_valid = DataLoader(dataset=valid_dataset,\n",
    "                              batch_size=4,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=get_collate_fn(tokenizer_fast))\n",
    "\n",
    "for i in dataloader_train:\n",
    "    print('input_ids.shape:', i[0].shape)\n",
    "    print('attention_mask.shape:', i[1].shape)\n",
    "    print('token_type_ids.shape:', i[2].shape)\n",
    "    print('position_ids.shape.shape:', i[3].shape)\n",
    "    print('label.shape:', i[4].shape)  # 本赛题只有一种关系(若有n_type种关系,则label.shape=[batch_size, n_type, relation_max_len])\n",
    "    print('labels_mask.shape:', i[5].shape)\n",
    "    print('relations_idx.shape:',\n",
    "          i[6].shape)  # 若有n_type种关系,则relations_idx.shape=[batch_size, n_type, relation_max_len, 4]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5cfa364-b784-447e-9e77-98492f108580",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomRelation(copy.deepcopy(pretrained), 64, True).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "237ccc71-aef5-4de5-8bfc-c71915e7664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    for idx, (input_ids, attention_mask, token_type_ids, position_ids, labels, labels_mask, relations_idx) in enumerate(\n",
    "            dataloader):\n",
    "        # 数据设备切换\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels_mask = labels_mask.to(device)\n",
    "        # labels.shape=[batch_size, relation_max_len]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # logits.shape=[batch_size, relation_max_len]\n",
    "        logits = model(input_ids, attention_mask, token_type_ids, position_ids, relations_idx, labels_mask)\n",
    "        loss = multilabel_categorical_crossentropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0 and idx > 0:\n",
    "            mc = MetricsCalculator()  # 计算实体关系的查准率、查全率、F1 score\n",
    "            mc.calc_confusion_matrix_rel(logits, labels)\n",
    "            print('| step {:5d} | loss {:8.5f} | precision {:8.5f} | recall {:8.5f} | f1 {:8.5f} |'.format(idx,\n",
    "                                                                                                           loss.item(),\n",
    "                                                                                                           mc.precision,\n",
    "                                                                                                           mc.recall,\n",
    "                                                                                                           mc.f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cde64ba-d2dd-48f5-b8a5-a65e2c3d6fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型验证\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    mc = MetricsCalculator()\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, token_type_ids, position_ids, labels, labels_mask, relations_idx in dataloader:\n",
    "            # 数据设备切换\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            labels_mask = labels_mask.to(device)\n",
    "            # logits.shape=[batch_size, relation_max_len]\n",
    "            logits = model(input_ids, attention_mask, token_type_ids, position_ids, relations_idx, labels_mask)\n",
    "\n",
    "            mc.calc_confusion_matrix_rel(logits, labels)\n",
    "    return mc.precision, mc.recall, mc.f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6606985-eae0-4466-8ae9-5c99dd4800f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step   100 | loss  3.81342 | precision  0.93407 | recall  0.94444 | f1  0.93923 |\n",
      "| step   200 | loss  2.71993 | precision  0.98750 | recall  0.96341 | f1  0.97531 |\n",
      "| step   300 | loss  3.84497 | precision  0.96040 | recall  0.98980 | f1  0.97487 |\n",
      "| step   400 | loss  3.19924 | precision  0.95312 | recall  0.84722 | f1  0.89706 |\n",
      "| step   500 | loss  2.29865 | precision  0.94231 | recall  0.98990 | f1  0.96552 |\n",
      "| step   600 | loss  3.48252 | precision  0.96296 | recall  0.94891 | f1  0.95588 |\n",
      "| step   700 | loss  2.94238 | precision  0.93269 | recall  0.98980 | f1  0.96040 |\n",
      "| step   800 | loss  2.43509 | precision  0.96341 | recall  0.96341 | f1  0.96341 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch:     0 | time: 245.81s | valid precision  0.96787 | valid recall  0.96549 | valid f1  0.96668 | train f1  0.97087 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss  1.03164 | precision  0.97826 | recall  1.00000 | f1  0.98901 |\n",
      "| step   200 | loss  3.00651 | precision  0.97744 | recall  0.89655 | f1  0.93525 |\n",
      "| step   300 | loss  1.08332 | precision  1.00000 | recall  0.98361 | f1  0.99174 |\n",
      "| step   400 | loss  3.55086 | precision  0.96622 | recall  0.94079 | f1  0.95333 |\n",
      "| step   500 | loss  0.85915 | precision  1.00000 | recall  0.96471 | f1  0.98204 |\n",
      "| step   600 | loss  2.33957 | precision  0.90741 | recall  0.96078 | f1  0.93333 |\n",
      "| step   700 | loss  1.41861 | precision  0.89796 | recall  1.00000 | f1  0.94624 |\n",
      "| step   800 | loss  1.98259 | precision  1.00000 | recall  0.98734 | f1  0.99363 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch:     1 | time: 242.07s | valid precision  0.96598 | valid recall  0.96695 | valid f1  0.96646 | train f1  0.97584 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss  2.31096 | precision  0.93137 | recall  0.96939 | f1  0.95000 |\n",
      "| step   200 | loss  1.04791 | precision  0.98913 | recall  0.98913 | f1  0.98913 |\n",
      "| step   300 | loss  0.25979 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   400 | loss  2.60745 | precision  0.96324 | recall  0.97761 | f1  0.97037 |\n",
      "| step   500 | loss  1.44582 | precision  0.95161 | recall  1.00000 | f1  0.97521 |\n",
      "| step   600 | loss  1.12556 | precision  0.98734 | recall  1.00000 | f1  0.99363 |\n",
      "| step   700 | loss  2.77454 | precision  0.96296 | recall  0.96296 | f1  0.96296 |\n",
      "| step   800 | loss  1.55244 | precision  1.00000 | recall  0.97753 | f1  0.98864 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch:     2 | time: 242.22s | valid precision  0.95779 | valid recall  0.97800 | valid f1  0.96779 | train f1  0.97827 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, dataloader_train, optimizer, device)\n",
    "    _, _, train_f1 = evaluate(model, dataloader_train, device)\n",
    "    valid_precision, valid_recall, valid_f1 = evaluate(model, dataloader_valid, device)\n",
    "    print('-' * 123)\n",
    "    print('| epoch: {:5d} | time: {:5.2f}s '\n",
    "          '| valid precision {:8.5f} '\n",
    "          '| valid recall {:8.5f} '\n",
    "          '| valid f1 {:8.5f} | train f1 {:8.5f} |'.format(epoch,\n",
    "                                                           time.time() - epoch_start_time,\n",
    "                                                           valid_precision,\n",
    "                                                           valid_recall,\n",
    "                                                           valid_f1,\n",
    "                                                           train_f1))\n",
    "    print('-' * 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf0eb80d-457b-4938-a9bb-659cc0eeea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rel_predict = []\n",
    "\n",
    "with open('my_data/ner_rel_predict.json', 'r', encoding='utf-8') as f:\n",
    "    for ners_sentence in json.load(f):  # 每次预测一条数据\n",
    "        if not ners_sentence['relation']:  # relation可能为空列表\n",
    "            all_rel_predict.append([])  # 此时关系预测为:[]\n",
    "        else:\n",
    "            # 设置add_special_tokens=False(数据处理中已经加入了'[CLS]','[SEP]'等特殊token)\n",
    "            outputs = tokenizer_fast(ners_sentence['sentence_text'], add_special_tokens=False, return_tensors='pt')\n",
    "            input_ids, attention_mask, token_type_ids = outputs.input_ids.to(device), outputs.attention_mask.to(\n",
    "                device), outputs.token_type_ids.to(device)\n",
    "            # position_ids.shape=[1, seq_len]\n",
    "            position_ids = torch.tensor(ners_sentence['position_ids'])[None, :]\n",
    "            # relation_idx.shape=[1, seq_len, 4]\n",
    "            relations_idx = np.array(ners_sentence['relation_idx'])[None, :]\n",
    "            relation = np.array(ners_sentence['relation'])\n",
    "            logits = torch.squeeze(model(input_ids, attention_mask, token_type_ids, position_ids, relations_idx)).cpu()\n",
    "            predict = torch.where(logits > 0)[0].tolist()  # 阈值(threshold)设置为0.0\n",
    "            last_result = relation[predict]\n",
    "            last_result = [i + '\\t' + '属性' for i in last_result]\n",
    "            all_rel_predict.append(last_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5cdc54d-253d-4f90-9e67-496c161e6a45",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[6, 14, '器官组织', '右肺下叶内基底段']\\t[14, 17, '异常现象', '小结节']\\t属性\",\n",
       " \"[37, 45, '器官组织', '左肺下叶前基底段']\\t[47, 50, '异常现象', '钙化灶']\\t属性\",\n",
       " \"[45, 47, '修饰描述', '点状']\\t[47, 50, '异常现象', '钙化灶']\\t属性\",\n",
       " \"[64, 66, '器官组织', '胆囊']\\t[70, 72, '异常现象', '结石']\\t属性\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rel_predict[0]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3812",
   "language": "python",
   "name": "env_3812"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}