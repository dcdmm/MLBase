{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AutoTokenizer, AutoModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BertTokenizer(可通过`__init__`实例化): Construct a BERT tokenizer. Based on WordPiece.\n",
    "# from_pretrained(类方法): Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',\n",
    ")\n",
    "tokenizer  # 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21128\n",
      "21128\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer))\n",
    "print(len(tokenizer.get_vocab()))  # 与上等价"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizerFast(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library\n",
    "#   when created with the [`AutoTokenizer.from_pretrained`] class method.\n",
    "#   This class cannot be instantiated directly using `__init__()` (throws an error).\n",
    "# 与上等价\n",
    "tokenizer_auto = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='bert-base-chinese')\n",
    "tokenizer_auto"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP]\n",
      "选 择 珠 江 花 园 的 原 因 就 是 方 便 。\n"
     ]
    }
   ],
   "source": [
    "list_of_token = [101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102]\n",
    "\n",
    "# Convert a list of lists of token ids into a list of strings by calling decode.\n",
    "print(tokenizer.decode(list_of_token))\n",
    "\n",
    "# skip_special_tokens:Whether or not to remove special tokens in the decoding.\n",
    "# If these tokens are already part of the vocabulary, it just let the Tokenizer know about them. If they don’t exist, the Tokenizer creates them, giving them a new id.\n",
    "# These special tokens will never be processed by the model (ie won’t be split into multiple tokens), and they can be removed from the output when decoding.\n",
    "print(tokenizer.decode(list_of_token, skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, 100, 100]\n",
      "['[UNK]', '[UNK]', '[UNK]', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "# Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the vocabulary.\n",
    "print(tokenizer.convert_tokens_to_ids(['月光', '希望', '[EOS', '<eop>']))\n",
    "\n",
    "# Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and added tokens.\n",
    "print(tokenizer.convert_ids_to_tokens([21128, 21128, 21130, 21131]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [3299, 1045, 4638, 100, 0, 101, 3173, 2361, 3307, 138, 100, 140, 133, 147, 9133, 135], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['月', '光', '的', '[UNK]', '[PAD]', '[CLS]', '新', '希', '望', '[', '[UNK]', ']', '<', 'e', '##op', '>']\n",
      "月 光 的 [UNK] [PAD] [CLS] 新 希 望 [ [UNK] ] < eop >\n",
      "['月', '光', '的', '[UNK]', '[PAD]', '[CLS]', '新', '希', '望', '[', '[UNK]', ']', '<', 'e', '##op', '>']\n"
     ]
    }
   ],
   "source": [
    "text = '月光的[UNK][PAD][CLS]新希望[EOS]<eop>'\n",
    "result = tokenizer(text=text, add_special_tokens=False)\n",
    "\n",
    "# 未添加新tokens前的编码效果\n",
    "print(result)\n",
    "\n",
    "print(tokenizer.tokenize(text))\n",
    "print(tokenizer.decode(result['input_ids']))\n",
    "print(tokenizer.convert_ids_to_tokens(result['input_ids']))  # 只是ids到tokens的转换"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21133\n",
      "['<eop>', '<eod>']\n",
      "[21131, 21132]\n"
     ]
    }
   ],
   "source": [
    "# Add a list of new tokens to the tokenizer class.\n",
    "tokenizer.add_tokens(new_tokens=['月光', '希望'])\n",
    "\n",
    "#  Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes.\n",
    "#  If special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the current vocabulary).\n",
    "\n",
    "# Keys should be in the list of predefined special attributes:\n",
    "#   [`bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].\n",
    "tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '[EOS]',\n",
    "                                                  # Additional special tokens used by the tokenizer.\n",
    "                                                  'additional_special_tokens': [\"<eop>\", \"<eod>\"]})\n",
    "print(len(tokenizer.get_vocab()))\n",
    "print(tokenizer.additional_special_tokens)\n",
    "print(tokenizer.additional_special_tokens_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [21128, 4638, 100, 0, 101, 3173, 21129, 21130, 21131], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['月光', '的', '[UNK]', '[PAD]', '[CLS]', '新', '希望', '[EOS]', '<eop>']\n",
      "月光 的 [UNK] [PAD] [CLS] 新 希望 [EOS] <eop>\n",
      "['月', '光', '的', '[UNK]', '[PAD]', '[CLS]', '新', '希', '望', '[', '[UNK]', ']', '<', 'e', '##op', '>']\n"
     ]
    }
   ],
   "source": [
    "result1 = tokenizer(text=text, add_special_tokens=False)\n",
    "\n",
    "# 添加新tokens后的编码效果\n",
    "print(result1)\n",
    "\n",
    "print(tokenizer.tokenize(text))\n",
    "print(tokenizer.decode(result1['input_ids']))  # 分词效果达到预期\n",
    "print(tokenizer.convert_ids_to_tokens(result['input_ids']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21133\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.get_vocab()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [
    {
     "data": {
      "text/plain": "('save_tokenizer/tokenizer_config.json',\n 'save_tokenizer/special_tokens_map.json',\n 'save_tokenizer/vocab.txt',\n 'save_tokenizer/added_tokens.json')"
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存分词器(包括新添加的tokens)\n",
    "tokenizer.save_pretrained(\"../extra_dataset/save_tokenizer/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='save_tokenizer/', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<eop>', '<eod>']})"
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地重新加载\n",
    "tokenizer1 = BertTokenizer.from_pretrained(\"../extra_dataset/save_tokenizer/\")\n",
    "tokenizer1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [21128, 4638, 100, 0, 101, 3173, 21129, 21130, 21131], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['月光', '的', '[UNK]', '[PAD]', '[CLS]', '新', '希望', '[EOS]', '<eop>']\n",
      "月光 的 [UNK] [PAD] [CLS] 新 希望 [EOS] <eop>\n"
     ]
    }
   ],
   "source": [
    "result2 = tokenizer1(text=text, add_special_tokens=False)\n",
    "\n",
    "print(result2)\n",
    "print(tokenizer1.tokenize(text))\n",
    "print(tokenizer1.decode(result2['input_ids']))  # 分词效果与上等价"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('bert-base-chinese')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [254]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:989\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    982\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[0;32m    986\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[0;32m    987\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m--> 989\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    990\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    991\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    992\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    993\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    994\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    995\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    996\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[0;32m    997\u001B[0m     embedding_output,\n\u001B[0;32m    998\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mextended_attention_mask,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1006\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[0;32m   1007\u001B[0m )\n\u001B[0;32m   1008\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:214\u001B[0m, in \u001B[0;36mBertEmbeddings.forward\u001B[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001B[0m\n\u001B[0;32m    211\u001B[0m         token_type_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(input_shape, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 214\u001B[0m     inputs_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    215\u001B[0m token_type_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken_type_embeddings(token_type_ids)\n\u001B[0;32m    217\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m inputs_embeds \u001B[38;5;241m+\u001B[39m token_type_embeddings\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\functional.py:2183\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2177\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2178\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2179\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2180\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2181\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2182\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2183\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model(**tokenizer(text, return_tensors='pt'))  # 报错"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [
    {
     "data": {
      "text/plain": "BertEmbeddings(\n  (word_embeddings): Embedding(21128, 768, padding_idx=0)\n  (position_embeddings): Embedding(512, 768)\n  (token_type_embeddings): Embedding(2, 768)\n  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(21128, 768, padding_idx=0)"
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_embedding维度为:21128 * 768(but此时tokenizer大小为:21133)\n",
    "model.get_input_embeddings()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(21133, 768)"
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [
    {
     "data": {
      "text/plain": "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.5206,  0.5423,  0.2691,  ...,  0.8091, -0.4797, -0.0561],\n         [-0.3456,  0.3025, -0.1301,  ..., -0.0090, -0.1680, -0.5165],\n         [-0.3690,  0.3482,  0.5202,  ..., -0.0231,  0.2632, -0.2211],\n         ...,\n         [-0.4824,  0.6202,  0.4320,  ...,  0.4577,  0.1774, -0.5592],\n         [-0.3892,  0.6652,  0.1083,  ...,  0.2178, -0.0246, -0.4248],\n         [-0.3988,  0.4876,  0.2308,  ...,  0.7583, -0.6936,  0.0380]]],\n       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.9999,  0.9977,  1.0000,  0.9492,  0.6740, -0.2831, -0.7346, -0.8150,\n          0.9780, -1.0000,  1.0000,  1.0000, -0.9065, -0.8526,  0.9999, -0.9999,\n         -0.2017,  0.8878,  0.9975, -0.2366,  0.9831, -1.0000, -0.1171, -0.9902,\n         -0.9017,  0.9994,  0.9341, -0.8814, -0.9968,  1.0000,  0.9962,  1.0000,\n          0.9858, -0.9944, -1.0000,  0.0295,  0.6967,  0.9986,  0.4683, -0.4691,\n         -0.9862, -0.5577, -0.8545, -0.9997, -0.8699,  0.8690, -1.0000, -1.0000,\n         -0.9774,  0.9948, -0.8861, -0.9999,  0.9993, -0.9985, -0.3254,  0.9971,\n         -0.9999,  0.9797,  1.0000,  0.4701,  1.0000, -0.9140, -0.0313, -0.9999,\n          1.0000, -0.9999, -0.9993, -0.8227,  0.9983,  1.0000, -0.9997,  0.8984,\n          1.0000, -0.6763, -0.2907,  1.0000, -0.9988,  0.8659, -1.0000,  0.7351,\n          1.0000,  0.9992, -0.9955,  0.9233, -0.9985, -1.0000, -0.9887,  1.0000,\n         -0.8269,  0.9962,  0.9290, -0.9999, -1.0000,  0.9999, -0.9999, -0.9982,\n         -0.8532,  0.9999, -0.7218, -0.9989, -0.6547, -0.2930, -1.0000, -1.0000,\n          0.9954,  0.9998,  0.8712, -0.9999,  1.0000, -0.5998, -1.0000, -0.9916,\n         -1.0000, -0.6179, -0.9975,  1.0000, -0.4368,  0.6886,  1.0000, -1.0000,\n          0.9907, -0.9999, -0.7372,  0.9328,  0.9998,  1.0000,  1.0000, -0.9751,\n          1.0000,  1.0000,  0.9957,  0.9945, -1.0000,  0.9966,  0.8108, -0.9733,\n         -0.5876, -0.9288,  1.0000,  0.9993,  0.9938, -0.7343,  0.9645, -0.9995,\n          1.0000, -1.0000,  0.9999, -1.0000, -0.9688,  0.9898,  0.5516,  1.0000,\n          0.0708,  0.9975, -0.8429, -0.9999,  0.9092, -0.9834,  0.9962, -1.0000,\n          0.5942, -0.8705, -0.2344, -0.2442, -1.0000,  1.0000, -0.9873,  1.0000,\n          0.9999, -0.9816, -0.9992, -0.9998, -0.3415, -1.0000,  0.5515,  0.9993,\n         -0.8767,  1.0000, -0.1083, -0.9942,  0.8133,  0.9220, -0.9985,  0.9991,\n         -0.8578,  0.7141, -0.6377,  0.9978,  0.9983,  0.7699, -0.9756,  1.0000,\n          0.7739,  0.9645,  0.9997,  0.4645,  0.8800, -0.9628, -1.0000, -0.7636,\n          0.9974, -0.9023, -1.0000,  0.0626, -1.0000,  0.9721,  0.5736, -0.6151,\n         -0.9911, -1.0000,  0.9992, -0.9953, -1.0000,  0.7572, -0.1954,  0.3005,\n         -0.9999, -0.2741,  0.9988, -0.9840,  0.8903, -0.9179, -0.9899,  0.9949,\n         -0.9922,  0.9998,  0.4943,  1.0000,  0.9897, -0.9971, -0.9967,  1.0000,\n         -0.8522, -1.0000,  0.7117, -0.9729,  0.2681,  1.0000, -0.9996,  0.7393,\n          1.0000,  0.1062,  1.0000,  0.5119, -0.9994, -0.9995,  1.0000,  0.9990,\n          1.0000, -0.9994, -0.9596,  0.2060,  0.5610, -1.0000, -0.9901, -0.1943,\n          0.9995,  1.0000, -0.1420, -0.9989, -0.4065, -1.0000,  1.0000,  0.1167,\n          0.9998,  0.9994, -0.8844, -0.7268,  0.8159,  0.5912, -0.9999,  0.6204,\n         -0.9950, -0.9978, -1.0000,  0.6537, -1.0000, -1.0000,  0.1102,  1.0000,\n         -0.0332, -0.9971,  1.0000,  0.9746, -0.6742, -0.7413,  0.9928, -1.0000,\n          1.0000, -0.9998,  0.9134, -0.9924, -0.9999, -0.5872,  0.9966,  0.9990,\n         -1.0000,  0.5814, -0.9987, -0.6704, -0.9008,  0.6714, -0.7949,  0.8884,\n         -0.9998, -0.6460,  0.3583, -0.7641, -1.0000,  0.2204,  1.0000, -0.9132,\n          1.0000,  0.8923,  1.0000, -0.9972, -0.9997,  0.9987, -0.4956, -0.6824,\n          0.6428,  0.0204,  0.9844,  0.2678, -0.7560, -1.0000,  0.9987,  0.4748,\n          0.3982,  0.8973, -0.5576, -0.7912,  0.9986, -0.9984,  0.9998, -1.0000,\n         -0.4948,  0.9997,  1.0000,  0.9998,  0.5364, -0.9694,  0.9728, -1.0000,\n          0.9999, -1.0000,  0.9999, -0.2936,  0.5509, -0.9899, -0.9984,  1.0000,\n          0.9827,  0.9588,  1.0000, -0.7347,  0.9684,  0.6133,  0.9680,  0.9959,\n          0.9432,  1.0000, -1.0000, -0.9993,  0.3304, -0.9990, -0.9999, -1.0000,\n         -0.7111, -0.9997, -0.9947,  0.5718, -0.5463,  0.3243, -0.9258, -0.9951,\n          0.5628,  0.6810,  0.4623,  0.8338,  0.5658, -0.9998, -0.9988, -1.0000,\n         -0.9999, -0.2328,  0.9961, -0.9960,  1.0000, -1.0000, -0.3740, -0.4969,\n         -0.2454,  0.1864,  0.9996, -0.9991,  0.9999,  0.9999,  1.0000,  1.0000,\n          0.9974, -0.9998, -0.9998, -0.9894, -0.9955, -1.0000, -0.9998,  0.8307,\n          0.0390, -1.0000,  0.5889,  0.9982,  1.0000,  0.9994, -0.9999, -0.5356,\n         -1.0000, -0.9960,  1.0000, -0.4170, -0.9999,  0.8674, -0.5279,  1.0000,\n         -0.1023, -0.1241,  0.7638, -0.2195,  0.9850, -1.0000,  0.9283,  1.0000,\n          0.9332, -1.0000, -0.2226, -0.5415, -1.0000, -0.5236,  0.9925,  0.9999,\n         -0.9968, -0.3107, -0.9999,  0.9417,  0.9764,  0.9999,  0.9999,  0.8733,\n          0.9510,  0.8576, -0.1810,  0.9993,  0.7115, -0.9999,  1.0000, -0.9375,\n          0.5031, -1.0000,  1.0000,  0.8154,  0.9983,  0.7920, -0.3443, -0.9582,\n         -0.9176,  0.9999,  1.0000, -0.4753, -0.6668, -1.0000, -0.9999, -0.9999,\n         -0.8076, -0.9798, -0.9954, -0.9999,  0.1291,  0.1711,  1.0000,  0.9976,\n          0.9765, -0.9145, -0.9823,  0.9989,  0.5127,  0.9969,  0.0097, -1.0000,\n         -0.9995, -0.9995,  0.9943, -0.3656,  0.5722, -0.9942,  0.9414,  0.9906,\n         -1.0000, -0.8749, -0.9284,  0.9282,  1.0000, -0.9974,  0.9997, -0.9999,\n         -0.2718,  0.9525,  0.9745,  0.9990, -0.3527,  0.5299, -0.8392,  0.9472,\n          0.6965,  0.9998, -0.7258,  0.8860,  1.0000, -0.9373,  0.9997,  0.0409,\n          0.8507,  0.9972,  1.0000,  0.6160,  0.9409,  0.9284,  0.9996,  1.0000,\n         -0.7940,  0.8157,  0.4396, -0.9933,  0.0916,  0.6194,  1.0000,  0.9594,\n         -0.4668, -1.0000,  0.8387,  0.9896,  1.0000, -0.8500,  0.9988,  0.3991,\n          0.2494,  0.9817,  0.9838,  0.8415,  0.3893,  0.9777,  1.0000, -1.0000,\n         -1.0000, -1.0000,  1.0000,  0.9993, -0.6264, -1.0000,  1.0000, -0.3336,\n          0.9736,  0.9999, -0.9546, -0.8150,  0.1632, -1.0000,  0.3719, -0.0671,\n          0.9905, -0.3525,  1.0000, -1.0000, -0.1550,  1.0000,  0.6016,  0.9980,\n          0.5139, -0.9991,  0.9956, -0.9974, -1.0000, -0.8914,  0.9903,  0.9999,\n         -0.9678,  0.8573,  0.9998, -0.9911,  1.0000, -1.0000,  0.9991, -0.9988,\n          1.0000, -0.9905, -0.9998, -0.5123,  0.4061,  0.4946, -0.6760,  1.0000,\n          0.5857, -0.4064,  0.9142, -0.9148, -0.9996, -0.9980,  0.9836, -0.9996,\n          0.9536,  0.8086, -0.1016, -0.8430, -0.9986,  0.9999,  0.4785, -0.9977,\n          1.0000, -0.9053, -1.0000,  0.8081, -0.9997, -0.5617,  0.9941,  0.9044,\n          0.5253, -1.0000,  0.5527,  0.9934, -1.0000,  0.6845, -0.9883, -0.8995,\n          0.8588,  0.9990,  0.4131, -0.5446,  0.5533,  0.5505,  0.9910, -0.2776,\n         -0.7653, -0.9994, -0.9831, -0.7989, -1.0000, -0.9866, -1.0000,  1.0000,\n          0.9980,  0.9975, -0.1173, -0.5262,  0.8880,  0.9976, -1.0000, -0.9472,\n         -0.4162,  0.9579, -0.9927, -1.0000,  0.4331, -1.0000, -0.2528,  0.6366,\n         -0.7505, -0.9838,  1.0000,  1.0000, -1.0000, -0.9992, -1.0000, -0.9965,\n          1.0000,  0.9999,  1.0000, -0.0108, -0.9699,  0.9994, -0.8399,  0.6171,\n         -0.9999, -1.0000, -0.9990,  0.9018, -0.9988, -0.9931,  0.9997,  0.9969,\n         -0.6617, -1.0000, -0.9396,  0.9958,  1.0000,  1.0000,  0.8159,  1.0000,\n         -0.9986,  0.9992, -0.9996,  1.0000, -1.0000,  1.0000,  1.0000,  0.9976,\n          0.9999, -0.9999,  0.9708, -0.6275, -0.9680,  0.9891, -0.5756, -0.9987,\n         -0.5883,  1.0000, -0.8716,  1.0000,  0.6740,  0.9172,  0.8281, -0.7856,\n          0.9508, -0.1427, -1.0000,  0.7390,  0.9929,  0.9996,  1.0000,  0.9907,\n          0.9995, -0.9984, -0.9999,  0.0821, -0.8719, -0.0216, -0.9980,  1.0000,\n          1.0000, -0.9973, -0.8949,  0.8161,  0.9114,  0.9993,  0.9997,  0.0730,\n          0.5621, -0.3435,  0.9999, -1.0000,  0.5581, -0.9564, -0.9692,  0.9985,\n         -0.8718,  0.9999, -0.9998,  1.0000, -0.9711,  0.5829,  1.0000,  0.9989,\n         -0.9994,  1.0000,  0.7567, -0.9999, -0.9154, -0.9999, -0.9996,  0.7728]],\n       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tokenizer(text, return_tensors='pt'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}