{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from heamy.dataset import Dataset\n",
    "from heamy.estimator import Classifier\n",
    "from heamy.pipeline import ModelsPipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 54)\n",
      "(9000,)\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "data = fetch_covtype()\n",
    "\n",
    "X, y = data['data'][:10000], data['target'][:10000]\n",
    "\n",
    "ord = OrdinalEncoder()\n",
    "y_enc = ord.fit_transform(y.reshape(-1, 1))\n",
    "y_enc = y_enc.reshape(-1, )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.1, random_state=1)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(eff47fde69d1e04cb6dc241bb4c1d9b5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建数据集\n",
    "'''\n",
    "use_cache : bool, default True\n",
    "    If use_cache=True then preprocessing step will be cached until function code is changed.\n",
    "'''\n",
    "dataset = Dataset(X_train=X_train, y_train=y_train, X_test=X_test, y_test=None, use_cache=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2167.  129.   26. ...    0.    0.    0.]\n",
      " [2813.  117.   13. ...    0.    0.    0.]\n",
      " [2993.  286.   14. ...    0.    0.    0.]\n",
      " ...\n",
      " [2929.   75.   15. ...    0.    0.    0.]\n",
      " [2208.  317.   33. ...    0.    0.    0.]\n",
      " [2606.  356.   18. ...    0.    0.    0.]]\n",
      "\n",
      "[3. 1. 0. ... 4. 5. 1.]\n",
      "\n",
      "[[2979.   89.   18. ...    0.    0.    0.]\n",
      " [2083.   21.   28. ...    0.    0.    0.]\n",
      " [2322.  281.   17. ...    0.    0.    0.]\n",
      " ...\n",
      " [2306.  224.   25. ...    0.    0.    0.]\n",
      " [3029.  113.   14. ...    0.    0.    0.]\n",
      " [2882.   37.   10. ...    0.    0.    0.]]\n",
      "\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset.X_train, end='\\n\\n')\n",
    "print(dataset.y_train, end='\\n\\n')\n",
    "print(dataset.X_test, end='\\n\\n')\n",
    "print(dataset.y_test, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def xgb_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"参数必须为X_train,y_train,X_test,y_test\"\"\"\n",
    "    params = {'objective': 'multi:softprob',\n",
    "              \"eval_metric\": 'mlogloss',\n",
    "              \"verbosity\": 0,\n",
    "              'num_class': 7,\n",
    "              'nthread': -1}\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    model = xgb.train(params, dtrain, num_boost_round=300)\n",
    "    predict = model.predict(dtest)\n",
    "\n",
    "    return predict  # 返回值必须为X_test的预测\n",
    "\n",
    "\n",
    "def lgb_model(X_train, y_train, X_test, y_test,\n",
    "              **parameters):  # Classifier处对字典进行了解包,此处需要重新打包\n",
    "    if parameters is None:\n",
    "        parameters = {}\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "    model = lgb.train(params=parameters, train_set=lgb_train, num_boost_round=300)\n",
    "    predict = model.predict(X_test)\n",
    "\n",
    "    return predict\n",
    "\n",
    "\n",
    "def rf_model(X_train, y_train, X_test, y_test):\n",
    "    params = {\"n_estimators\": 100, \"n_jobs\": -1}\n",
    "    model = RandomForestClassifier(**params).fit(X_train, y_train)\n",
    "    predict = model.predict_proba(X_test)\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\"objective\": \"multiclass\",\n",
    "          \"num_class\": 7,\n",
    "          \"n_jobs\": -1,\n",
    "          \"verbose\": -4, \"metric\": (\"multi_logloss\",)}\n",
    "'''\n",
    "name : str, optional\n",
    "    The unique name of Estimator object.\n",
    "\n",
    "parameters : dict, optional\n",
    "    Arguments for estimator object.\n",
    "\n",
    "use_cache : bool, optional\n",
    "    if True then validate/predict/stack/blend results will be cached.\n",
    "'''\n",
    "model_xgb = Classifier(dataset=dataset, estimator=xgb_model, name='xgb', use_cache=False)\n",
    "model_lgb = Classifier(dataset=dataset, estimator=lgb_model, name='lgb',\n",
    "                       parameters=params,\n",
    "                       use_cache=False)\n",
    "model_rf = Classifier(dataset=dataset, estimator=rf_model,\n",
    "                      name='rf',  # 默认parameters=None\n",
    "                      use_cache=False)  # 默认use_cache=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<heamy.pipeline.ModelsPipeline at 0x210301ab100>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = ModelsPipeline(model_xgb, model_lgb, model_rf)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscorer : function\\n    Scikit-learn like metric.\\n\\ntest_size : float, default 0.2\\n\\nmethod : str\\n    Type of solver. Should be one of:\\n        ‘Nelder-Mead’\\n        ‘Powell’\\n        ‘CG’\\n        ‘BFGS’\\n        ‘Newton-CG’\\n        ‘L-BFGS-B’\\n        ‘TNC’\\n        ‘COBYLA’\\n        ‘SLSQP’\\n        ‘dogleg’\\n        ‘trust-ncg’\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Finds optimal weights for weighted average of models.\n",
    "'''\n",
    "scorer : function\n",
    "    Scikit-learn like metric.\n",
    "\n",
    "test_size : float, default 0.2\n",
    "\n",
    "method : str\n",
    "    Type of solver. Should be one of:\n",
    "        ‘Nelder-Mead’\n",
    "        ‘Powell’\n",
    "        ‘CG’\n",
    "        ‘BFGS’\n",
    "        ‘Newton-CG’\n",
    "        ‘L-BFGS-B’\n",
    "        ‘TNC’\n",
    "        ‘COBYLA’\n",
    "        ‘SLSQP’\n",
    "        ‘dogleg’\n",
    "        ‘trust-ncg’\n",
    "'''\n",
    "pipeline.find_weights(scorer=log_loss, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Applies weighted mean to models.\n",
    "# 线性加权\n",
    "# pipeline.weight([0.5, 0.3, 0.2])  # 这里指定xgb模型权重0.5,lgb权重为0.3,rf权重为0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(894e9485140106e6e5186f237932eba0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "k : int, default 5\n",
    "    Number of folds.\n",
    "\n",
    "stratify : bool, default False\n",
    "\n",
    "shuffle : bool, default True\n",
    "\n",
    "seed : int, default 100\n",
    "\n",
    "full_test : bool, default True\n",
    "    If True then evaluate test dataset on the full data otherwise take the mean of every fold.\n",
    "'''\n",
    "stack_ds = pipeline.stack(k=5,\n",
    "                          stratify=False,  # 是否为分层k折\n",
    "                          shuffle=True,\n",
    "                          seed=1,\n",
    "                          full_test=False)\n",
    "stack_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             xgb_0         xgb_1         xgb_2         xgb_3         xgb_4  \\\n",
      "0     4.534841e-08  2.541179e-07  2.733032e-03  9.972505e-01  5.520512e-07   \n",
      "1     1.026551e-03  5.745058e-01  6.193155e-05  6.089087e-06  4.243897e-01   \n",
      "2     9.931676e-01  6.827588e-03  6.073082e-08  9.379712e-08  2.986425e-06   \n",
      "3     1.024370e-05  1.215085e-03  2.224298e-06  2.022765e-06  9.987329e-01   \n",
      "4     2.070961e-06  2.478734e-04  4.268239e-05  1.892980e-08  2.222941e-05   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "8995  1.498728e-06  7.729919e-06  9.997123e-01  1.818914e-05  3.111986e-07   \n",
      "8996  2.466629e-03  8.747428e-07  1.366907e-06  9.943136e-08  2.284424e-07   \n",
      "8997  4.480495e-03  2.889213e-01  6.571934e-05  1.598638e-06  7.065006e-01   \n",
      "8998  6.482577e-06  8.352361e-05  3.192859e-01  5.541099e-06  3.409723e-07   \n",
      "8999  4.172878e-04  9.854028e-01  1.783861e-03  1.912048e-06  1.223199e-02   \n",
      "\n",
      "             xgb_5         xgb_6         lgb_0         lgb_1         lgb_2  \\\n",
      "0     1.523519e-05  3.317873e-07  2.948738e-10  2.393523e-09  1.306743e-05   \n",
      "1     2.771559e-06  7.163896e-06  1.031326e-03  3.418498e-01  8.436671e-07   \n",
      "2     1.725809e-07  1.553244e-06  9.997157e-01  2.832818e-04  1.757258e-09   \n",
      "3     3.500483e-05  2.496738e-06  8.328621e-08  6.372429e-05  3.064596e-09   \n",
      "4     9.996850e-01  6.317720e-08  8.054011e-06  9.340819e-04  2.180221e-05   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "8995  2.597058e-04  2.188967e-07  2.071008e-08  1.433841e-07  9.999876e-01   \n",
      "8996  3.413513e-06  9.975274e-01  1.022808e-04  1.213109e-09  8.477269e-09   \n",
      "8997  4.880789e-06  2.533228e-05  5.982391e-04  1.041582e-01  5.031231e-08   \n",
      "8998  6.805968e-01  2.132009e-05  1.093692e-06  2.759543e-05  1.746375e-01   \n",
      "8999  1.520641e-04  1.003229e-05  2.147179e-04  9.994839e-01  2.836797e-05   \n",
      "\n",
      "      ...         lgb_4         lgb_5         lgb_6  rf_0  rf_1  rf_2  rf_3  \\\n",
      "0     ...  2.585061e-10  9.617795e-08  9.365659e-11  0.00  0.00  0.04  0.96   \n",
      "1     ...  6.571168e-01  1.157059e-06  1.693615e-08  0.15  0.54  0.00  0.00   \n",
      "2     ...  9.633757e-07  1.990608e-09  8.793855e-09  0.68  0.24  0.00  0.00   \n",
      "3     ...  9.999361e-01  4.413642e-08  4.536169e-11  0.00  0.01  0.00  0.00   \n",
      "4     ...  1.264324e-04  9.989096e-01  3.544767e-10  0.02  0.11  0.07  0.00   \n",
      "...   ...           ...           ...           ...   ...   ...   ...   ...   \n",
      "8995  ...  2.399200e-10  1.188213e-05  3.038512e-11  0.00  0.00  0.77  0.04   \n",
      "8996  ...  1.581365e-09  1.050151e-07  9.998976e-01  0.07  0.01  0.00  0.00   \n",
      "8997  ...  8.952435e-01  2.654109e-08  2.518098e-09  0.20  0.43  0.00  0.00   \n",
      "8998  ...  8.177757e-08  8.253336e-01  2.274948e-08  0.00  0.00  0.47  0.00   \n",
      "8999  ...  1.755359e-04  9.748730e-05  1.099679e-09  0.13  0.66  0.01  0.00   \n",
      "\n",
      "      rf_4  rf_5  rf_6  \n",
      "0     0.00  0.00  0.00  \n",
      "1     0.30  0.01  0.00  \n",
      "2     0.05  0.00  0.03  \n",
      "3     0.99  0.00  0.00  \n",
      "4     0.08  0.72  0.00  \n",
      "...    ...   ...   ...  \n",
      "8995  0.00  0.19  0.00  \n",
      "8996  0.00  0.00  0.92  \n",
      "8997  0.36  0.00  0.01  \n",
      "8998  0.00  0.53  0.00  \n",
      "8999  0.18  0.01  0.01  \n",
      "\n",
      "[9000 rows x 21 columns]\n",
      "[3. 1. 0. ... 4. 5. 1.]\n",
      "            xgb_0     xgb_1         xgb_2         xgb_3         xgb_4  \\\n",
      "0    8.131402e-01  0.186717  8.656114e-06  1.311125e-05  4.694143e-05   \n",
      "1    4.976726e-06  0.001448  2.397805e-01  1.328249e-05  1.544654e-06   \n",
      "2    3.379331e-06  0.000006  2.938978e-04  9.287729e-04  2.148000e-05   \n",
      "3    1.037145e-02  0.000046  3.112013e-07  4.553492e-07  2.564838e-07   \n",
      "4    2.181071e-07  0.000002  7.791304e-04  9.991627e-01  1.831632e-05   \n",
      "..            ...       ...           ...           ...           ...   \n",
      "995  4.977564e-02  0.006236  1.543476e-04  4.074186e-06  1.623728e-05   \n",
      "996  8.161975e-04  0.014963  1.404793e-03  3.925740e-05  9.823974e-01   \n",
      "997  1.373955e-05  0.000444  7.141369e-01  2.380274e-01  7.023184e-06   \n",
      "998  1.182706e-01  0.880993  1.057344e-04  3.358099e-05  2.914155e-05   \n",
      "999  3.397970e-01  0.621999  5.494626e-05  3.789779e-05  3.810450e-02   \n",
      "\n",
      "            xgb_5         xgb_6         lgb_0         lgb_1         lgb_2  \\\n",
      "0    2.258938e-05  5.158303e-05  6.793326e-01  3.206142e-01  8.976305e-07   \n",
      "1    7.587358e-01  1.555999e-05  5.330249e-07  3.127667e-03  2.471813e-01   \n",
      "2    9.987456e-01  5.056456e-07  3.502078e-07  2.790583e-06  1.512028e-04   \n",
      "3    1.548684e-07  9.895816e-01  2.184471e-03  2.945862e-07  5.022028e-11   \n",
      "4    3.705848e-05  5.048751e-07  6.331697e-10  2.353739e-08  1.858618e-05   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "995  1.812329e-06  9.438123e-01  1.685392e-03  2.121832e-04  4.413776e-08   \n",
      "996  3.107066e-04  6.863884e-05  1.919656e-04  4.189111e-03  1.316186e-04   \n",
      "997  4.736711e-02  4.135077e-06  4.233668e-06  1.456287e-04  8.666346e-01   \n",
      "998  2.421723e-05  5.439048e-04  6.575667e-02  9.342311e-01  1.623353e-06   \n",
      "999  2.714654e-06  4.100544e-06  2.221416e-01  7.692582e-01  4.453007e-07   \n",
      "\n",
      "     ...         lgb_4         lgb_5         lgb_6   rf_0   rf_1   rf_2  \\\n",
      "0    ...  4.586979e-05  5.253863e-06  1.066365e-06  0.392  0.360  0.014   \n",
      "1    ...  9.312075e-08  7.496903e-01  1.400495e-08  0.000  0.016  0.334   \n",
      "2    ...  2.151144e-07  9.997792e-01  9.454383e-10  0.000  0.004  0.074   \n",
      "3    ...  1.408569e-10  5.376641e-11  9.978152e-01  0.206  0.016  0.000   \n",
      "4    ...  2.206449e-09  2.466963e-07  1.125244e-10  0.000  0.000  0.026   \n",
      "..   ...           ...           ...           ...    ...    ...    ...   \n",
      "995  ...  7.820248e-09  7.597262e-10  9.981024e-01  0.150  0.072  0.000   \n",
      "996  ...  9.954639e-01  2.310140e-05  2.772402e-08  0.064  0.110  0.060   \n",
      "997  ...  8.199096e-07  5.385289e-02  7.729523e-09  0.000  0.002  0.358   \n",
      "998  ...  4.755855e-06  8.631815e-07  4.834077e-06  0.296  0.616  0.000   \n",
      "999  ...  8.599377e-03  7.574753e-08  3.929602e-10  0.286  0.348  0.000   \n",
      "\n",
      "      rf_3   rf_4   rf_5   rf_6  \n",
      "0    0.000  0.138  0.032  0.064  \n",
      "1    0.010  0.000  0.640  0.000  \n",
      "2    0.048  0.004  0.870  0.000  \n",
      "3    0.000  0.006  0.000  0.772  \n",
      "4    0.974  0.000  0.000  0.000  \n",
      "..     ...    ...    ...    ...  \n",
      "995  0.000  0.002  0.000  0.776  \n",
      "996  0.000  0.684  0.064  0.018  \n",
      "997  0.486  0.000  0.154  0.000  \n",
      "998  0.002  0.056  0.000  0.030  \n",
      "999  0.000  0.356  0.000  0.010  \n",
      "\n",
      "[1000 rows x 21 columns]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 第一层模型训练第二层模型的输出;这里xgb模型输出为概率,故有xgb_0, xgb_1,...,xgb_6\n",
    "print(stack_ds.X_train)\n",
    "print(stack_ds.y_train)\n",
    "print(stack_ds.X_test)  # 第一层模型测试第二层模型的输出\n",
    "print(stack_ds.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.19585658e-01, 3.57850412e-01, 9.88951577e-04, ...,\n",
       "        4.45500115e-03, 1.41537568e-03, 1.50464593e-02],\n",
       "       [1.47018948e-03, 9.98038236e-03, 1.78409531e-01, ...,\n",
       "        3.90134622e-03, 7.98681490e-01, 5.65184064e-04],\n",
       "       [7.66735912e-04, 3.92972120e-03, 2.50181821e-02, ...,\n",
       "        1.72667527e-03, 9.65578562e-01, 2.25347797e-04],\n",
       "       ...,\n",
       "       [2.17455457e-03, 1.14008118e-02, 7.58327317e-01, ...,\n",
       "        6.83067757e-03, 9.90292963e-02, 1.32156993e-03],\n",
       "       [1.80305552e-01, 8.05291060e-01, 1.08184254e-03, ...,\n",
       "        8.33464949e-03, 2.03318738e-03, 2.56379364e-03],\n",
       "       [3.43754708e-01, 6.01910105e-01, 1.94602633e-03, ...,\n",
       "        4.23618642e-02, 2.54976663e-03, 6.78415286e-03]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacker = Classifier(dataset=stack_ds, estimator=LogisticRegression, parameters={\"solver\": 'lbfgs', \"max_iter\": 1000},\n",
    "                     use_cache=False)\n",
    "# stack_ds.X_test的预测结果\n",
    "predict_stack = stacker.predict()\n",
    "predict_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.858\n",
      "0.865\n",
      "0.86\n",
      "0.87\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(np.argmax(stack_ds.X_test.iloc[:, :7].values, axis=1), y_test))\n",
    "print(accuracy_score(np.argmax(stack_ds.X_test.iloc[:, 7:14].values, axis=1), y_test))\n",
    "print(accuracy_score(np.argmax(stack_ds.X_test.iloc[:, 14:].values, axis=1), y_test))\n",
    "\n",
    "# 通过stacking模型融合,准确率得到了提升\n",
    "print(accuracy_score(np.argmax(predict_stack, axis=1), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
