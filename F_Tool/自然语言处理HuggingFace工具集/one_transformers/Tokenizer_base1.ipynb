{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BertTokenizer(可通过`__init__`实例化): Construct a BERT tokenizer. Based on WordPiece.\n",
    "# from_pretrained(类方法): Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',\n",
    ")\n",
    "tokenizer  # 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.\n",
    "# This class cannot be instantiated directly using `__init__()` (throws an error).\n",
    "# 与上等价\n",
    "tokenizer_auto = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='bert-base-chinese')\n",
    "tokenizer_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, 100, 100]\n",
      "['月', '光', '的', '[UNK]', '[PAD]', '[CLS]', '新', '希', '望', '[', '[UNK]', ']', '<', 'e', '##op', '>']\n",
      "21128\n"
     ]
    }
   ],
   "source": [
    "text = '月光的[UNK][PAD][CLS]新希望[EOS]<eop>'\n",
    "\n",
    "# 未添加新tokens前的编码效果\n",
    "print(tokenizer.convert_tokens_to_ids(['月光', '希望', '[EOS]', '<eop>']))\n",
    "print(tokenizer.tokenize(text))\n",
    "print(len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21133\n",
      "['<eop>', '<eod>']\n",
      "[21131, 21132]\n"
     ]
    }
   ],
   "source": [
    "# Add a list of new tokens to the tokenizer class.\n",
    "tokenizer.add_tokens(new_tokens=['月光', '希望'])\n",
    "\n",
    "#  Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes.\n",
    "#  If special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the current vocabulary).\n",
    "\n",
    "# Keys should be in the list of predefined special attributes:\n",
    "#   [`bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].\n",
    "tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '[EOS]',\n",
    "                                                  # Additional special tokens used by the tokenizer.\n",
    "                                                  'additional_special_tokens': [\"<eop>\", \"<eod>\"]})\n",
    "print(len(tokenizer.get_vocab()))\n",
    "print(tokenizer.additional_special_tokens)\n",
    "print(tokenizer.additional_special_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21128, 21129, 21130, 21131]\n",
      "['月光', '的', '[UNK]', '[PAD]', '[CLS]', '新', '希望', '[EOS]', '<eop>']\n",
      "21133\n"
     ]
    }
   ],
   "source": [
    "# 添加新tokens后的编码效果\n",
    "print(tokenizer.convert_tokens_to_ids(['月光', '希望', '[EOS]', '<eop>']))\n",
    "print(tokenizer.tokenize(text))\n",
    "print(len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../extra_dataset/save_tokenizer/tokenizer_config.json',\n",
       " '../extra_dataset/save_tokenizer/special_tokens_map.json',\n",
       " '../extra_dataset/save_tokenizer/vocab.txt',\n",
       " '../extra_dataset/save_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存分词器(包括新添加的tokens)\n",
    "tokenizer.save_pretrained(\"../extra_dataset/save_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='../extra_dataset/save_tokenizer/', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<eop>', '<eod>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21128: AddedToken(\"月光\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t21129: AddedToken(\"希望\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t21130: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21131: AddedToken(\"<eop>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21132: AddedToken(\"<eod>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地重新加载\n",
    "tokenizer1 = BertTokenizer.from_pretrained(\"../extra_dataset/save_tokenizer/\")\n",
    "tokenizer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21128, 21129, 21130, 21131]\n",
      "['月光', '的', '[UNK]', '[PAD]', '[CLS]', '新', '希望', '[EOS]', '<eop>']\n",
      "21133\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer1.convert_tokens_to_ids(['月光', '希望', '[EOS]', '<eop>']))\n",
    "print(tokenizer1.tokenize(text))\n",
    "print(len(tokenizer1.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[198], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1077\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m-> 1077\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1086\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:210\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    207\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 210\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    213\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\torch\\nn\\functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model(**tokenizer(text, return_tensors='pt'))  # 报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(21128, 768, padding_idx=0)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_embedding维度为:21128 * 768(but此时tokenizer大小为:21133)\n",
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(21133, 768, padding_idx=0)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.6175,  0.4736, -0.2744,  ...,  0.8804, -0.2183, -0.0349],\n",
       "         [-0.2959,  0.1713, -0.5264,  ...,  0.2661, -0.2603, -0.3372],\n",
       "         [-0.2881,  0.1268,  0.3013,  ..., -0.0535,  0.0557, -0.2193],\n",
       "         ...,\n",
       "         [-0.5904,  0.3490, -0.2575,  ...,  0.6640, -0.1979, -0.2463],\n",
       "         [-0.2524,  0.5001, -0.0464,  ...,  0.0849, -0.2772,  0.0111],\n",
       "         [-0.3726,  0.4881, -0.2034,  ...,  0.9245, -0.5306,  0.1590]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.9986,  0.9999,  0.9985,  0.9916,  0.8206,  0.1148, -0.8462, -0.7568,\n",
       "          0.9837, -0.9997,  1.0000,  0.9999, -0.8701, -0.8928,  0.9999, -0.9994,\n",
       "         -0.6200,  0.9927,  0.9970,  0.0879,  0.9934, -1.0000, -0.8927, -0.8095,\n",
       "         -0.7121,  0.9974,  0.9422, -0.8321, -0.9999,  0.9995,  0.9840,  0.9997,\n",
       "          0.9379, -0.9998, -0.9996,  0.5568,  0.2912,  0.9910, -0.5576, -0.7741,\n",
       "         -0.9592, -0.3574, -0.2776, -0.9953, -0.9840,  0.6313, -1.0000, -0.9999,\n",
       "         -0.9486,  0.9994, -0.8931, -0.9999,  0.9546, -0.9474, -0.3094,  0.9850,\n",
       "         -0.9993,  0.9819,  1.0000,  0.9021,  0.9999, -0.9125,  0.6013, -0.9998,\n",
       "          1.0000, -0.9996, -0.9904, -0.3291,  0.9998,  1.0000, -0.9825,  0.9800,\n",
       "          1.0000,  0.1536, -0.1741,  0.9999, -0.9963,  0.7774, -1.0000,  0.5795,\n",
       "          1.0000,  0.9984, -0.9871,  0.9795, -0.9813, -1.0000, -0.9993,  0.9998,\n",
       "         -0.9415,  0.9995,  0.9929, -0.9994, -1.0000,  0.9998, -0.9993, -0.9967,\n",
       "         -0.9448,  0.9987, -0.9010, -0.9140, -0.5088,  0.5844, -0.9997, -0.9991,\n",
       "          0.9955,  0.9990,  0.6906, -0.9997,  0.9998,  0.0777, -1.0000, -0.9598,\n",
       "         -1.0000, -0.6046, -0.9959,  1.0000, -0.3765, -0.4069,  0.9999, -0.9996,\n",
       "          0.9549, -0.9997, -0.9229,  0.7584,  0.9995,  1.0000,  0.9994, -0.9973,\n",
       "          0.9998,  1.0000,  0.9924,  0.9734, -0.9979,  0.9805,  0.9006, -0.9979,\n",
       "         -0.0326, -0.7147,  1.0000,  0.9965,  0.9864, -0.9430,  0.9980, -0.9986,\n",
       "          1.0000, -1.0000,  0.9998, -1.0000, -0.9980,  0.9959,  0.8201,  1.0000,\n",
       "         -0.7532,  0.9999, -0.9828, -0.9996,  0.9899, -0.7520,  0.9859, -1.0000,\n",
       "          0.9706, -0.9664,  0.2224, -0.3575, -1.0000,  1.0000, -0.9391,  1.0000,\n",
       "          0.9992, -0.9922, -0.9966, -0.9985,  0.4257, -0.9985, -0.4790,  0.9973,\n",
       "         -0.5234,  0.9996, -0.6677, -0.9914,  0.9130,  0.2426, -0.9998,  0.9975,\n",
       "         -0.8304,  0.8499, -0.1122,  0.9532,  0.9960,  0.9395, -0.8835,  1.0000,\n",
       "          0.4756,  0.9921,  0.9963,  0.0403, -0.1323, -0.9940, -1.0000, -0.3150,\n",
       "          0.9999, -0.7082, -0.9996,  0.8795, -1.0000,  0.9251,  0.0586, -0.7064,\n",
       "         -0.9979, -1.0000,  0.9996, -0.9905, -0.9996,  0.9341, -0.7938, -0.1840,\n",
       "         -0.9997,  0.5000,  0.9976, -0.8685,  0.3317, -0.7733, -0.9990,  0.9937,\n",
       "         -0.9965,  0.9795,  0.9360,  1.0000,  0.9617, -0.9477, -0.9736,  1.0000,\n",
       "         -0.3250, -1.0000,  0.8695, -0.9908, -0.6651,  1.0000, -0.9989,  0.9742,\n",
       "          1.0000,  0.8020,  1.0000,  0.0228, -0.9993, -0.9988,  1.0000,  0.9968,\n",
       "          1.0000, -0.9974, -0.9877,  0.4801, -0.0167, -1.0000, -0.9981, -0.4301,\n",
       "          0.9991,  0.9999,  0.7594, -0.9976, -0.7967, -0.9998,  1.0000, -0.3922,\n",
       "          1.0000,  0.9990, -0.9741, -0.9245,  0.8284, -0.0357, -0.9998, -0.1087,\n",
       "         -0.9998, -0.9825, -1.0000,  0.9375, -0.9997, -1.0000,  0.7445,  1.0000,\n",
       "          0.0993, -0.9999,  0.9996,  0.9989, -0.9522, -0.9327,  0.9847, -1.0000,\n",
       "          1.0000, -0.9992,  0.8019, -0.9678, -0.9991, -0.8769,  0.9997,  0.9942,\n",
       "         -0.9976, -0.1176, -0.9951, -0.9701, -0.7924,  0.9358, -0.8923,  0.9793,\n",
       "         -0.9878, -0.9371,  0.7965, -0.9712, -0.9999, -0.2373,  1.0000, -0.4514,\n",
       "          1.0000,  0.9402,  1.0000, -0.9595, -0.9974,  0.9961, -0.6743, -0.7759,\n",
       "          0.3423, -0.6359,  0.9461, -0.1518, -0.6867, -0.9998,  0.9999,  0.9154,\n",
       "          0.9262,  0.9856, -0.8360, -0.5613,  0.9846, -0.9950,  0.9998, -0.9998,\n",
       "         -0.9040,  0.9997,  1.0000,  0.9998,  0.6592, -0.9035,  0.9923, -0.9995,\n",
       "          0.9997, -0.9995,  0.9995, -0.6513,  0.8350, -0.8723, -0.9905,  1.0000,\n",
       "          0.8975,  0.9525,  0.9999, -0.9545,  0.9952,  0.9744,  0.9892,  0.9888,\n",
       "          0.8075,  1.0000, -0.9985, -0.9968, -0.4748, -0.9981, -0.9996, -1.0000,\n",
       "         -0.0443, -0.9980, -0.9891, -0.0327,  0.1368,  0.7648, -0.9955, -0.9858,\n",
       "         -0.4228,  0.6816, -0.0280,  0.7412,  0.8897, -0.9995, -0.9925, -1.0000,\n",
       "         -0.9992,  0.6329,  0.9998, -0.9999,  0.9996, -1.0000, -0.6531,  0.3378,\n",
       "         -0.8015, -0.6187,  0.9999, -1.0000,  0.9843,  0.9999,  1.0000,  0.9999,\n",
       "          0.9998, -0.9909, -0.9997, -0.9995, -0.9999, -1.0000, -0.9984,  0.9570,\n",
       "          0.6036, -1.0000, -0.5019,  0.9747,  1.0000,  0.9973, -0.9998, -0.7123,\n",
       "         -0.9997, -0.9996,  0.9993, -0.8624, -0.9992,  0.9915, -0.6326,  1.0000,\n",
       "         -0.5208,  0.9233,  0.9883,  0.5158,  0.9449, -0.9999,  0.9839,  1.0000,\n",
       "          0.9801, -1.0000, -0.5431, -0.8977, -0.9999, -0.5134,  0.9629,  0.9996,\n",
       "         -0.9999, -0.3368, -0.9995,  0.7914,  0.9197,  0.9998,  0.9999,  0.9632,\n",
       "          0.9840,  0.9521,  0.1580,  0.9999,  0.5031, -0.9995,  0.9998, -0.6108,\n",
       "          0.7257, -1.0000,  0.9991,  0.8917,  0.9999,  0.9357, -0.2113, -0.9948,\n",
       "         -0.9803,  0.9979,  1.0000, -0.6859, -0.5936, -0.9999, -0.9994, -0.9987,\n",
       "         -0.7612, -0.8417, -0.9915, -0.9998,  0.1585,  0.6535,  1.0000,  1.0000,\n",
       "          0.9992, -0.9457, -0.9136,  0.9947,  0.8222,  0.9941, -0.4341, -1.0000,\n",
       "         -0.9939, -0.9990,  0.9992, -0.3297,  0.1279, -0.9537,  0.6442,  0.9746,\n",
       "         -0.9999, -0.9932, -0.9827,  0.5085,  1.0000, -0.9990,  0.9996, -0.9995,\n",
       "         -0.2099,  0.8464,  0.8359,  0.9972, -0.8942,  0.0503, -0.7994,  0.7652,\n",
       "          0.9772,  0.9990, -0.8563,  0.7277,  0.9996, -0.9532,  0.9996,  0.2456,\n",
       "          0.9336,  0.9913,  1.0000,  0.6012,  0.9871,  0.9906,  0.9999,  1.0000,\n",
       "         -0.9918,  0.4817,  0.6568, -0.9626, -0.3854,  0.8638,  1.0000,  0.4201,\n",
       "         -0.8662, -0.9999,  0.9524,  0.9946,  1.0000, -0.9886,  0.9989, -0.6132,\n",
       "          0.8985,  0.9361,  0.8074,  0.6261,  0.4933,  0.9951,  0.9997, -0.9999,\n",
       "         -1.0000, -1.0000,  1.0000,  0.9992, -0.6880, -1.0000,  0.9997, -0.8952,\n",
       "          0.9966,  0.9991, -0.6772, -0.9132,  0.7030, -0.9998,  0.1913, -0.3630,\n",
       "          0.9265,  0.0852,  0.9996, -0.9999,  0.2466,  1.0000, -0.2875,  0.9999,\n",
       "          0.5355, -0.9999,  0.9995, -0.9992, -0.9999, -0.7518,  0.9998,  0.9997,\n",
       "         -0.6031,  0.5641,  0.9998, -0.9991,  1.0000, -1.0000,  0.9380, -0.9980,\n",
       "          1.0000, -0.9797, -0.9988, -0.7086,  0.5541,  0.8886, -0.6253,  1.0000,\n",
       "          0.2652, -0.8292,  0.0269, -0.9861, -0.9984, -0.9960,  0.8513, -1.0000,\n",
       "          0.9534,  0.8628, -0.4643, -0.9152, -0.9999,  0.9999, -0.0559, -0.9844,\n",
       "          1.0000, -0.9936, -1.0000,  0.9651, -0.9954,  0.2709,  0.9927,  0.9169,\n",
       "          0.1736, -1.0000,  0.7535,  0.9998, -0.9997, -0.2948, -0.9733, -0.9788,\n",
       "          0.9329,  0.9962,  0.8759, -0.2757,  0.5591,  0.4039,  0.8928, -0.7247,\n",
       "          0.0338, -0.9998, -0.9962, -0.9550, -0.9995, -0.9996, -1.0000,  1.0000,\n",
       "          0.9999,  1.0000, -0.2200, -0.6156,  0.7154,  0.9888, -0.9998, -0.9659,\n",
       "          0.4427,  0.9008, -0.8989, -0.9997, -0.3698, -1.0000, -0.7063,  0.5314,\n",
       "         -0.9580, -0.6932,  1.0000,  0.9999, -0.9997, -0.9918, -0.9997, -0.9994,\n",
       "          1.0000,  0.9994,  0.9995, -0.4236, -0.9039,  0.9877, -0.8605, -0.3696,\n",
       "         -0.9994, -0.9986, -0.9999,  0.9245, -0.9986, -0.9999,  0.9977,  0.9999,\n",
       "          0.1144, -1.0000, -0.8250,  0.9999,  0.9996,  1.0000,  0.8678,  1.0000,\n",
       "         -0.9981,  0.9960, -0.9999,  1.0000, -1.0000,  1.0000,  1.0000,  0.9932,\n",
       "          0.9993, -0.9998,  0.9733, -0.1924, -0.7034,  0.9837, -0.6679, -0.9980,\n",
       "          0.1510,  0.9986, -0.9747,  1.0000,  0.7559,  0.3522,  0.8344, -0.2227,\n",
       "          0.9809, -0.3156, -0.9998,  0.9782,  0.9994,  0.9986,  1.0000,  0.8861,\n",
       "          1.0000, -0.9849, -0.9988,  0.6677, -0.6641, -0.6461, -0.9998,  1.0000,\n",
       "          1.0000, -0.9999, -0.9350,  0.4165,  0.7575,  0.9999,  0.9989,  0.7216,\n",
       "          0.9171,  0.1111,  0.9995, -0.9998,  0.7423, -0.9754, -0.9381,  0.9999,\n",
       "         -0.9629,  0.9995, -0.9987,  0.9999, -0.9968,  0.4672,  0.9994,  0.9946,\n",
       "         -0.9978,  1.0000,  0.9092, -0.9992, -0.9900, -0.9999, -0.9956,  0.7481]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tokenizer(text, return_tensors='pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
