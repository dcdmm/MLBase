{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae14fd6-db34-4972-94b1-ca48aa63bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import random\n",
    "from colorama import Fore, Style\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639d2a2b-1992-4162-8052-2ca7111edefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d93f040-1c1b-401b-91cc-288055e601bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAXLEN = 64\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c23786-a1c1-4d97-bae8-db4e14954b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('一个女人正走在街对面吃香蕉，而一个男人正紧跟在他的公文包后面。', '那个女人在吃香蕉。', '一个女人走在人行道上吃冰淇淋，还有一个女人拿着钱包在她面前。')\n",
      "('一个戴着安全帽的男人在跳舞。', '一个戴着安全帽的男人在跳舞。', '5\\n')\n"
     ]
    }
   ],
   "source": [
    "def load_snli_data(filename):\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        return [(line['origin'], line['entailment'], line['contradiction']) for line in jsonlines.Reader(f)]\n",
    "\n",
    "\n",
    "def load_sts_data(filename):\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        return [(line.split(\"||\")[1], line.split(\"||\")[2], line.split(\"||\")[3]) for line in f]\n",
    "\n",
    "\n",
    "train_data = load_snli_data('cnsd-snli-process/train.jsonl')\n",
    "dev_data = load_sts_data('STS-B/cnsd-sts-dev.txt')\n",
    "test_data = load_sts_data('STS-B/cnsd-sts-test.txt')\n",
    "\n",
    "print(train_data[0])\n",
    "print(dev_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3df369a-7fdc-4a29-86a8-189006502ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask']\n",
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "print(TOKENIZER.model_input_names)\n",
    "print(TOKENIZER.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608160e6-6aca-40bf-8344-db177432bf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \"\"\"定义训练数据集\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def text_2_id(self, text):\n",
    "        return TOKENIZER([text[0], text[1], text[2]], max_length=MAXLEN,\n",
    "                         truncation=True, padding='max_length', return_tensors='pt')\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.text_2_id(self.data[index])\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(TrainDataset(train_data), batch_size=BATCH_SIZE, shuffle=True)\n",
    "for i in train_dataloader:\n",
    "    # i['input_ids'].shape=[batch_size, 3, MAXLEN]\n",
    "    print(i['input_ids'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66017155-b928-4f11-9891-150fb68e5d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 64])\n",
      "torch.Size([32, 1, 64])\n",
      "tensor([5, 4, 2, 2, 2, 5, 2, 5, 3, 1, 5, 4, 0, 2, 5, 4, 3, 1, 3, 2, 1, 1, 4, 1,\n",
      "        2, 5, 5, 4, 3, 5, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"定义测试数据集\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def text_2_id(self, text):\n",
    "        return TOKENIZER(text, max_length=MAXLEN, truncation=True, padding='max_length', return_tensors='pt')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.data[index]\n",
    "        return self.text_2_id([line[0]]), self.text_2_id([line[1]]), int(line[2])\n",
    "\n",
    "\n",
    "dev_dataloader = DataLoader(TestDataset(dev_data), batch_size=BATCH_SIZE)\n",
    "for i, j, k in dev_dataloader:\n",
    "    print(i['input_ids'].shape)\n",
    "    print(j['input_ids'].shape)\n",
    "    print(k)  # k.shape=[batch_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d37fe39-8a6e-448c-abea-c0b14be33dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimcseModel(nn.Module):\n",
    "    \"\"\"Simcse有监督模型\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model, pooling):\n",
    "        super(SimcseModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model)  # 有监督不需要修改dropout\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        out = self.bert(input_ids, attention_mask, token_type_ids, output_hidden_states=True)\n",
    "        # type(out.hidden_states):元组类型\n",
    "        # out.last_hidden_state.shape=[batch_size, seq_len, hidden_size]\n",
    "\n",
    "        if self.pooling == 'cls':\n",
    "            return out.last_hidden_state[:, 0]  # shape=[batch_size, hidden_size]\n",
    "\n",
    "        if self.pooling == 'pooler':\n",
    "            return out.pooler_output  # shape=[batch_size, hidden_size]\n",
    "\n",
    "        if self.pooling == 'last-avg':\n",
    "            last = out.last_hidden_state.transpose(1, 2)  # shape=[batch_size, hidden_size, seq_len]\n",
    "            return torch.avg_pool1d(last, kernel_size=last.shape[-1]).squeeze(-1)  # shape=[batch, hidden_size]\n",
    "\n",
    "        if self.pooling == 'first-last-avg':\n",
    "            first = out.hidden_states[1].transpose(1, 2)  # shape=[batch_size, hidden_size, seq_len]\n",
    "            last = out.hidden_states[-1].transpose(1, 2)  # shape=[batch_size, hidden_size, seq_len]                   \n",
    "            first_avg = torch.avg_pool1d(first, kernel_size=last.shape[-1]).squeeze(\n",
    "                -1)  # shape=[batch_size, hidden_size]\n",
    "            last_avg = torch.avg_pool1d(last, kernel_size=last.shape[-1]).squeeze(-1)  # shape=[batch_size, hidden_size]\n",
    "            avg = torch.cat((first_avg.unsqueeze(1), last_avg.unsqueeze(1)),\n",
    "                            dim=1)  # shape=[batch_size, 2, hidden_size]\n",
    "            return torch.avg_pool1d(avg.transpose(1, 2), kernel_size=2).squeeze(\n",
    "                -1)  # shape=[batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82e85b14-b3a7-4c1d-a967-9822372474de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simcse_sup_loss(y_pred, temperature=0.05):\n",
    "    \"\"\"损失函数(有监督)\"\"\"\n",
    "    # y_pred.shape=[BATCH_SIZE * 3, hidden_size]\n",
    "    y_true = torch.arange(y_pred.shape[0], device=DEVICE)\n",
    "    # example:[0,1, 3,4, 6,7, 9,10, xxxxxx]\n",
    "    use_row = torch.where((y_true + 1) % 3 != 0)[0]\n",
    "    # example:[1,0, 4,3, 7,6, 10,9, xxxxxx]\n",
    "    y_true = (use_row - use_row % 3 * 2) + 1\n",
    "    # sim.shape=[BATCH_SIZE * 3, BATCH_SIZE * 3]\n",
    "    sim = F.cosine_similarity(y_pred.unsqueeze(1), y_pred.unsqueeze(0), dim=-1)\n",
    "    # 相似度矩阵对角线处元素设置为很小的值(消除自身影响)\n",
    "    sim = sim - torch.eye(y_pred.shape[0], device=DEVICE) * 1e12\n",
    "    # 选取相似度矩阵的[0,1, 3,4, 6,7, 9,10, xxxxxx]行\n",
    "    sim = torch.index_select(sim, 0, use_row)\n",
    "    sim = sim / temperature\n",
    "    # 交叉熵损失的的target为:[1,0, 4,3, 7,6, 10,9, xxxxxx]\n",
    "    # 理解:x(本代码数据中的'origin')将x^+(本代码数据中的'entailment')作为正样本,将x^-(本代码数据中的'contradiction')与其他句子的x^+与x^-作为负样本\n",
    "    loss = F.cross_entropy(sim, y_true)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13127bd2-b109-4442-a8ce-30b265e7eddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SimcseModel(pretrained_model='hfl/chinese-roberta-wwm-ext', pooling='cls')\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89285cde-26a4-4047-aa4f-50f3e0bc4e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型验证\n",
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    sim_tensor = torch.tensor([], device=DEVICE)\n",
    "    label_array = np.array([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for source, target, label in dataloader:\n",
    "            # label.shape=[batch_size]\n",
    "            # source_input_ids.shape=[batch_size, seq_len]\n",
    "            source_input_ids = source['input_ids'].squeeze(1).to(DEVICE)\n",
    "            source_attention_mask = source['attention_mask'].squeeze(1).to(DEVICE)\n",
    "            source_token_type_ids = source['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "            # source_pred.shape=[batch_size, hidden_size]\n",
    "            source_pred = model(source_input_ids, source_attention_mask, source_token_type_ids)\n",
    "\n",
    "            # target_input_ids.shape=[batch_size, seq_len]\n",
    "            target_input_ids = target['input_ids'].squeeze(1).to(DEVICE)\n",
    "            target_attention_mask = target['attention_mask'].squeeze(1).to(DEVICE)\n",
    "            target_token_type_ids = target['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "            # target_pred.shape=[batch_size, hidden_size]\n",
    "            target_pred = model(target_input_ids, target_attention_mask, target_token_type_ids)\n",
    "            # sim.shape=[batch_size]\n",
    "            sim = F.cosine_similarity(source_pred, target_pred, dim=-1)  # result:是否相似\n",
    "            sim_tensor = torch.cat((sim_tensor, sim), dim=0)\n",
    "            label_array = np.append(label_array, np.array(label))\n",
    "\n",
    "    return spearmanr(label_array, sim_tensor.cpu().numpy()).correlation  # 斯皮尔曼相关系数(无序)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfee2059-622f-453e-969c-15d559cf24ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练与评估\n",
    "def train(model, train_dl, dev_dl, optimizer, best):\n",
    "    model.train()\n",
    "\n",
    "    early_stop_batch = 0\n",
    "    for batch_idx, source in enumerate(train_dl, start=1):\n",
    "        real_batch_num = source['input_ids'].shape[0]\n",
    "        input_ids = source['input_ids'].view(real_batch_num * 3, -1).to(DEVICE)\n",
    "        attention_mask = source['attention_mask'].view(real_batch_num * 3, -1).to(DEVICE)\n",
    "        token_type_ids = source['token_type_ids'].view(real_batch_num * 3, -1).to(DEVICE)\n",
    "\n",
    "        out = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = simcse_sup_loss(out)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每训练10个step进行一次模型验证\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('| step {:5d} | loss {:8.5f} |'.format(batch_idx, loss.item()))\n",
    "            corrcoef = eval(model, dev_dl)\n",
    "            model.train()\n",
    "            if best[0] < corrcoef:\n",
    "                best.clear()\n",
    "                best.append(corrcoef)\n",
    "                best.append(copy.deepcopy(model.state_dict()))\n",
    "                early_stop_batch = 0\n",
    "                continue\n",
    "\n",
    "            early_stop_batch += 1\n",
    "            if early_stop_batch == 20:  # 早停step步数为20 * 10\n",
    "                print(Fore.RED + f\"corrcoef doesn't improve for {early_stop_batch} batch, early stop!\")\n",
    "                print(Style.RESET_ALL, end='')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72c88092-2f57-4036-b57f-3f4780322ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************0********************\n",
      "| step    10 | loss  1.80268 |\n",
      "| step    20 | loss  0.95212 |\n",
      "| step    30 | loss  1.56213 |\n",
      "| step    40 | loss  1.04055 |\n",
      "| step    50 | loss  1.71093 |\n",
      "| step    60 | loss  1.46353 |\n",
      "| step    70 | loss  0.81570 |\n",
      "| step    80 | loss  1.18023 |\n",
      "| step    90 | loss  0.94789 |\n",
      "| step   100 | loss  0.69612 |\n",
      "| step   110 | loss  0.94707 |\n",
      "| step   120 | loss  1.37303 |\n",
      "| step   130 | loss  1.32752 |\n",
      "| step   140 | loss  0.98692 |\n",
      "| step   150 | loss  0.84953 |\n",
      "| step   160 | loss  1.46934 |\n",
      "| step   170 | loss  1.24137 |\n",
      "| step   180 | loss  1.28791 |\n",
      "| step   190 | loss  1.34628 |\n",
      "| step   200 | loss  0.84407 |\n",
      "| step   210 | loss  1.12489 |\n",
      "| step   220 | loss  0.68530 |\n",
      "| step   230 | loss  0.67318 |\n",
      "| step   240 | loss  1.23049 |\n",
      "| step   250 | loss  1.17653 |\n",
      "| step   260 | loss  0.97337 |\n",
      "| step   270 | loss  1.24604 |\n",
      "| step   280 | loss  1.13769 |\n",
      "| step   290 | loss  0.58476 |\n",
      "| step   300 | loss  1.01274 |\n",
      "| step   310 | loss  0.60883 |\n",
      "| step   320 | loss  0.77709 |\n",
      "| step   330 | loss  1.58936 |\n",
      "| step   340 | loss  1.26375 |\n",
      "| step   350 | loss  1.35079 |\n",
      "| step   360 | loss  0.56550 |\n",
      "| step   370 | loss  1.00448 |\n",
      "| step   380 | loss  1.29673 |\n",
      "| step   390 | loss  1.08516 |\n",
      "| step   400 | loss  0.64087 |\n",
      "| step   410 | loss  0.88816 |\n",
      "| step   420 | loss  1.12431 |\n",
      "| step   430 | loss  1.17656 |\n",
      "| step   440 | loss  0.68460 |\n",
      "| step   450 | loss  1.36449 |\n",
      "| step   460 | loss  1.08257 |\n",
      "| step   470 | loss  0.99961 |\n",
      "| step   480 | loss  1.52769 |\n",
      "| step   490 | loss  1.03927 |\n",
      "| step   500 | loss  0.67698 |\n",
      "| step   510 | loss  0.59850 |\n",
      "| step   520 | loss  0.92627 |\n",
      "| step   530 | loss  1.26291 |\n",
      "| step   540 | loss  0.88335 |\n",
      "| step   550 | loss  0.73419 |\n",
      "| step   560 | loss  1.17870 |\n",
      "| step   570 | loss  0.46543 |\n",
      "| step   580 | loss  0.77542 |\n",
      "| step   590 | loss  1.24796 |\n",
      "| step   600 | loss  0.75425 |\n",
      "| step   610 | loss  0.58449 |\n",
      "| step   620 | loss  0.68801 |\n",
      "| step   630 | loss  1.30206 |\n",
      "| step   640 | loss  0.95314 |\n",
      "| step   650 | loss  0.81043 |\n",
      "| step   660 | loss  1.50483 |\n",
      "\u001B[31mcorrcoef doesn't improve for 20 batch, early stop!\n",
      "\u001B[0m********************1********************\n",
      "| step    10 | loss  0.91148 |\n",
      "| step    20 | loss  0.78112 |\n",
      "| step    30 | loss  0.82832 |\n",
      "| step    40 | loss  1.32619 |\n",
      "| step    50 | loss  0.88490 |\n",
      "| step    60 | loss  0.89786 |\n",
      "| step    70 | loss  0.75764 |\n",
      "| step    80 | loss  1.23600 |\n",
      "| step    90 | loss  0.76117 |\n",
      "| step   100 | loss  0.74088 |\n",
      "| step   110 | loss  1.33353 |\n",
      "| step   120 | loss  0.90251 |\n",
      "| step   130 | loss  0.61262 |\n",
      "| step   140 | loss  0.52404 |\n",
      "| step   150 | loss  0.73822 |\n",
      "| step   160 | loss  1.00110 |\n",
      "| step   170 | loss  1.00383 |\n",
      "| step   180 | loss  0.81528 |\n",
      "| step   190 | loss  0.56727 |\n",
      "| step   200 | loss  0.72557 |\n",
      "| step   210 | loss  0.80812 |\n",
      "| step   220 | loss  1.06182 |\n",
      "| step   230 | loss  1.08981 |\n",
      "| step   240 | loss  0.83484 |\n",
      "| step   250 | loss  0.82033 |\n",
      "| step   260 | loss  0.64109 |\n",
      "| step   270 | loss  0.46419 |\n",
      "| step   280 | loss  0.93944 |\n",
      "| step   290 | loss  0.75207 |\n",
      "| step   300 | loss  0.82437 |\n",
      "| step   310 | loss  1.18280 |\n",
      "| step   320 | loss  0.88143 |\n",
      "| step   330 | loss  0.72205 |\n",
      "| step   340 | loss  0.98939 |\n",
      "| step   350 | loss  1.00790 |\n",
      "| step   360 | loss  1.02219 |\n",
      "| step   370 | loss  0.72763 |\n",
      "| step   380 | loss  1.52982 |\n",
      "| step   390 | loss  0.98315 |\n",
      "| step   400 | loss  0.40549 |\n",
      "\u001B[31mcorrcoef doesn't improve for 20 batch, early stop!\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "best = [0.0, None]  # 验证数据集最优相关系数与对应模型的状态字典\n",
    "for epoch in range(2):\n",
    "    print('*' * 20 + str(epoch) + '*' * 20)\n",
    "    train(model, train_dataloader, dev_dataloader, optimizer, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7242d838-062a-47f9-9241-e571abd2022f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8154346648730026"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "269b93ea-56cb-4bf8-814d-7b689fe8a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(TestDataset(test_data), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad49e686-2cc4-454e-9bbd-14c7c3e50501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_corrcoef: 0.81543466\n",
      "test_corrcoef: 0.78066004\n"
     ]
    }
   ],
   "source": [
    "best_model = SimcseModel(pretrained_model='hfl/chinese-roberta-wwm-ext', pooling='cls')\n",
    "best_model.load_state_dict(best[1])\n",
    "best_model.to(DEVICE)\n",
    "\n",
    "# 最优\n",
    "dev_corrcoef_best = eval(best_model, dev_dataloader)\n",
    "test_corrcoef_best = eval(best_model, test_dataloader)\n",
    "print(f'dev_corrcoef: {dev_corrcoef_best:.8f}')\n",
    "print(f'test_corrcoef: {test_corrcoef_best:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efbe8bbb-fe7e-4bb8-b899-ffd7b7d41066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_corrcoef: 0.80799295\n",
      "test_corrcoef: 0.77897552\n"
     ]
    }
   ],
   "source": [
    "dev_corrcoef_last = eval(model, dev_dataloader)\n",
    "test_corrcoef_last = eval(model, test_dataloader)\n",
    "\n",
    "# 未早停\n",
    "print(f'dev_corrcoef: {dev_corrcoef_last:.8f}')\n",
    "print(f'test_corrcoef: {test_corrcoef_last:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0db14265-7eda-4e3d-afbb-8ffaee2b31fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_corrcoef: 0.71675599\n",
      "test_corrcoef: 0.68252043\n"
     ]
    }
   ],
   "source": [
    "model_init = SimcseModel(pretrained_model='hfl/chinese-roberta-wwm-ext', pooling='cls')\n",
    "model_init.to(DEVICE)\n",
    "dev_corrcoef_init = eval(model_init, dev_dataloader)\n",
    "test_corrcoef_init = eval(model_init, test_dataloader)\n",
    "\n",
    "# 未训练\n",
    "print(f'dev_corrcoef: {dev_corrcoef_init:.8f}')\n",
    "print(f'test_corrcoef: {test_corrcoef_init:.8f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3812",
   "language": "python",
   "name": "env_3812"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}