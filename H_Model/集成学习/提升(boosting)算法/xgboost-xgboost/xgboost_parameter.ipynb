{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## xgboost主要调节参数\n",
    "\n",
    "1. 其他参数\n",
    "    * booster\n",
    "    * n_jobs\n",
    "    * objective\n",
    "    * verbosity\n",
    "\n",
    "2. 树调节参数\n",
    "    * n_estimators\n",
    "    * max_depth(重要程度高)\n",
    "    * min_child_weight(重要程度高,与max_depth一起进行网格搜索)\n",
    "\n",
    "2. 防止过拟合参数\n",
    "    * gamma\n",
    "    * learning_rate(Learning rate shrinks the contribution of each tree by learning_rate)\n",
    "    * subsample\n",
    "    * colsample_bytree\n",
    "    * reg_alpha\n",
    "    * reg_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier  # XGBRegressor 同理\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = datasets.fetch_covtype().data[:3000]\n",
    "y = datasets.fetch_covtype().target[:3000]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(2250, 54)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape  # 数据集有54个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 2, 3, 4, 5, 6, 7])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)  # 7分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 1., 2., 3., 4., 5., 6.])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "y_train_new = enc.fit_transform(y_train.reshape(-1, 1))\n",
    "np.unique(y_train_new)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "y_test_new = enc.transform(y_test.reshape(-1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booster=gbtree,  score= 0.8333333333333334\n",
      "coef_ [[ 3.28349e-04 -1.06196e-04 -8.55410e-06 -1.95018e-04 -1.29448e-04\n",
      "  -2.13075e-04  4.81626e-04 -2.03699e-03 -1.46347e-03  7.26749e-03\n",
      "   5.12693e-03 -1.00380e-04  2.08064e-03 -5.39129e-04 -5.03684e-02\n",
      "  -2.83966e-02  5.56726e-02 -6.23052e-02  1.17344e-02  5.47597e-02\n",
      "  -6.96490e-02  1.67561e-03  2.12958e-03 -2.52775e-03 -3.34843e-02\n",
      "  -4.34513e-03 -1.75680e-04 -2.24105e-03 -7.32984e-03 -8.69964e-04\n",
      "   7.61959e-03  3.12086e-02  2.95582e-03  2.48401e-03  3.27052e-04\n",
      "   2.97445e-04  1.32954e-04 -1.44724e-03  4.58231e-03 -5.42429e-04\n",
      "  -7.81192e-04 -2.77627e-04 -2.93538e-03  5.19595e-05 -7.57307e-04\n",
      "   8.70424e-03  4.81848e-03 -4.12414e-03  3.15295e-03 -1.58512e-03\n",
      "  -8.39636e-05  2.97580e-03  2.66030e-03  2.01438e-03]\n",
      " [-3.29196e-03  4.62823e-03  1.22340e-03  3.17006e-03  3.04714e-03\n",
      "  -2.17249e-02 -3.35930e-03  2.97621e-03 -1.95565e-03  9.86783e-05\n",
      "   1.35306e-04 -4.28894e-03 -3.07738e-03 -1.52381e-04 -8.51760e-04\n",
      "   2.04544e-04 -3.67355e-02  2.89859e-01 -4.76455e+00 -1.88291e+01\n",
      "   4.77968e-01 -1.31311e+01 -3.19671e-01  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "  -1.39984e+00 -1.27947e+00 -1.01179e+00 -1.57775e+01  1.54190e+00\n",
      "   1.42953e+00 -1.28394e+01 -1.03296e+01 -4.07334e+00  6.92185e-01\n",
      "   1.79043e+00 -1.42359e+01  7.41324e-01 -1.11309e+01 -4.82787e+00\n",
      "  -1.12250e+01  1.21533e+00  2.61465e-01 -5.60692e+00  6.59417e-01\n",
      "  -4.80223e+00 -4.57808e+00 -1.07570e+01  1.21485e+00]\n",
      " [-1.18111e+01 -5.16462e+00  1.24401e+00 -4.49657e+00 -6.26862e+00\n",
      "  -9.93684e+00  1.20316e+00  2.17190e+00 -7.27488e+00 -1.36183e+01\n",
      "  -6.23339e+00 -1.08719e+01  5.45119e-01 -1.22487e+01  3.54749e+00\n",
      "  -1.50394e+01  9.38910e-01 -4.83563e+00 -4.88629e+00 -1.05117e+01\n",
      "   1.08393e+00 -1.08187e+01 -5.66604e+00  1.04681e+00 -4.62629e+00\n",
      "  -4.14851e+00 -9.73065e+00  2.55570e+00 -1.19274e+01 -4.78156e+00\n",
      "   3.89467e-01 -4.09384e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00 -5.63848e+00\n",
      "   6.99452e+00 -1.92324e+00 -2.24373e+00 -3.86210e+00 -2.86015e+00\n",
      "  -4.29450e+00 -6.38850e+00  8.63892e+00 -3.99210e+00 -3.26977e+00\n",
      "  -6.74245e+00 -4.13019e+00 -5.88206e+00 -1.06262e+01]\n",
      " [ 8.87639e-01  6.19833e-01  1.47494e+00 -2.10604e+00  5.94271e-01\n",
      "  -5.23718e+00 -4.68926e+00 -5.42568e+00 -4.01951e+00 -1.84465e+00\n",
      "   7.52737e+00 -5.63493e+00 -2.68992e+00  1.63566e-01  1.60122e+00\n",
      "  -6.54967e+00 -7.27533e+00 -1.38655e+01 -6.42303e+00 -1.44746e+01\n",
      "  -1.27080e+01  3.18651e-01  4.10382e+00 -3.77721e+00 -2.02896e+00\n",
      "   2.33673e+00 -4.46848e+00 -3.40714e+00 -4.29297e+00 -5.92888e+00\n",
      "   9.84012e+00 -3.84570e+00 -5.51801e+00 -3.30790e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  2.23821e-01  2.23168e-01 -4.71255e+00 -8.90486e+00\n",
      "   1.10841e+00 -5.32523e+00 -1.36132e+01 -6.54907e+00 -9.77028e+00\n",
      "   5.36258e-01  2.74748e+00 -7.24283e+00 -1.33360e+01]\n",
      " [-6.34023e+00 -1.38037e+01 -2.05505e-01 -3.53122e+00 -3.87455e+00\n",
      "   2.68778e+00 -6.05853e+00 -1.44088e+01 -3.06425e-01  7.73277e-01\n",
      "  -6.74123e+00 -1.46207e+01  1.15730e+00 -1.13397e+01 -1.19607e+01\n",
      "   3.01206e-01  5.16543e-01 -9.09298e+00 -1.50106e+01 -2.38959e+00\n",
      "   2.46947e+00 -1.44247e+01  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  1.01263e+00\n",
      "  -1.12013e-01 -3.56718e+00 -1.41754e+01 -1.25084e+01 -5.72334e+00\n",
      "  -1.35224e+01  1.42355e-01 -2.38123e-01 -8.51964e+00 -1.46010e+01\n",
      "   1.61493e+00 -1.25318e+01 -6.73568e-01  8.28248e-01  1.34022e+00\n",
      "  -9.89872e+00 -1.01846e+01 -2.53662e+00  2.97633e-01 -1.17949e+01\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00]\n",
      " [ 0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00 -4.61655e+00 -5.61456e+00 -3.36011e+00\n",
      "  -1.28660e+00  7.16070e+00 -5.76530e+00 -2.08600e+00  2.70742e-01\n",
      "   3.26174e-01 -8.54064e+00 -1.46823e+01  1.11950e+00 -8.04612e+00\n",
      "  -2.71524e-01 -5.30294e-02  1.83404e-01 -9.13682e+00 -1.47033e+01\n",
      "   1.72163e+00 -7.88624e+00 -3.00837e+00  5.31957e-01  1.62655e+00\n",
      "  -9.31440e+00 -5.55783e+00  3.01033e-02  7.01818e-01 -4.58233e+00\n",
      "  -1.20405e+01  2.71454e+00 -9.00410e+00 -5.17731e+00 -7.23854e-01\n",
      "  -1.23992e+01 -5.68369e+00 -8.88920e-01  1.55626e+00]\n",
      " [-1.26161e+01 -5.24662e+00 -1.32527e+00  8.82219e-01 -5.13527e+00\n",
      "  -1.08106e+01  6.71642e-01 -5.89637e+00 -1.34424e+00 -1.28561e+01\n",
      "   2.54493e+00 -3.64456e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  1.00016e+00 -2.12420e+00\n",
      "  -5.11562e+00 -1.47240e+01 -1.20655e+01 -6.23677e+00  2.35647e+00\n",
      "   2.85640e-01 -1.52711e+01 -5.01881e+00 -1.27206e+01 -1.31304e+01\n",
      "  -6.36554e+00  2.19053e+00  2.34028e-01 -1.40119e+01 -2.61019e+00\n",
      "  -4.27495e+00 -1.04564e+01 -4.88317e+00  2.44001e+00]]\n",
      "intercept_ [ 0.267357   0.271501  -0.645045  -0.315983   0.646604   0.0985648\n",
      " -0.983543 ]\n",
      "booster=gblinear,  score= 0.676\n",
      "booster=dart,  score= 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# gbtree and dart use tree based models while gblinear uses linear functions.\n",
    "booster = ['gbtree', 'gblinear', 'dart']\n",
    "\n",
    "for i in booster:\n",
    "    xg = XGBClassifier(n_jobs=-1,  # xgboost虽属于boosting算法,但也可进行并行计算\n",
    "                       booster=i, use_label_encoder=False)  #  设置use_label_encoder=Fasle,和对标签进行标签编码避免警告\n",
    "\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    if i == 'gblinear':  # 此时没有feature_importances_属性\n",
    "        print(\"coef_\", xg.coef_)  # Coefficients property(当前仅当booster='gblinear')\n",
    "        print(\"intercept_\", xg.intercept_)  # Intercept (bias) property(当前仅当booster='gblinear')\n",
    "    print('booster=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duanm\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\duanm\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# Verbosity of printing messages. Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug)\n",
    "xg = XGBClassifier(n_jobs=-1, use_label_encoder=False)  # 默认verbosity=1\n",
    "xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "print(xg.score(X_test, y_test_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective=reg:squarederror, score= 0.8333333333333334\n",
      "objective=binary:logistic, score= 0.8333333333333334\n",
      "objective=multi:softmax, score= 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# Specify the learning task and the corresponding learning objective\n",
    "# 可自定义\n",
    "objective = [\"reg:squarederror\",  # L2损失\n",
    "             \"binary:logistic\",  # logistic regression for binary classification, output probability\n",
    "             \"multi:softmax\"]  #  set XGBoost to do multiclass classification using the softmax objective\n",
    "\n",
    "for i in objective:\n",
    "    '''\n",
    "    objective : string, callable or None, optional (default=None)\n",
    "            Specify the learning task and the corresponding learning objective or\n",
    "            a custom objective function to be used (see note below).\n",
    "            Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
    "    '''\n",
    "    xgb = XGBClassifier(n_jobs=-1, use_label_encoder=False, objective=i)\n",
    "    xgb.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print(\"objective=\" + str(i) + ', score=', xgb.score(X_test, y_test_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10,  score= 0.812\n",
      "n_estimators=20,  score= 0.8146666666666667\n",
      "n_estimators=50,  score= 0.8346666666666667\n",
      "n_estimators=100,  score= 0.8333333333333334\n",
      "n_estimators=200,  score= 0.8413333333333334\n",
      "n_estimators=500,  score= 0.84\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [10, 20, 50, 100, 200, 500]  # 默认n_estimators=100\n",
    "\n",
    "for i in n_estimators:\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False,\n",
    "                       n_estimators=i)  # Number of gradient boosted trees\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('n_estimators=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=1,  score= 0.7746666666666666\n",
      "max_depth=3,  score= 0.8133333333333334\n",
      "max_depth=6,  score= 0.8333333333333334\n",
      "max_depth=9,  score= 0.8413333333333334\n",
      "max_depth=12,  score= 0.832\n",
      "max_depth=15,  score= 0.8466666666666667\n",
      "max_depth=18,  score= 0.8413333333333334\n",
      "max_depth=21,  score= 0.8373333333333334\n"
     ]
    }
   ],
   "source": [
    "max_depth = [1, 3, 6, 9, 12, 15, 18, 21]  # 默认max_depth=6\n",
    "for i in max_depth:\n",
    "    # Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, max_depth=i)  #\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('max_depth=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_child_weight=0,  score= 0.8333333333333334\n",
      "min_child_weight=5,  score= 0.8266666666666667\n",
      "min_child_weight=10,  score= 0.816\n",
      "min_child_weight=50,  score= 0.7866666666666666\n",
      "min_child_weight=100,  score= 0.7506666666666667\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = [0, 5, 10, 50, 100]  # 默认min_child_weight=1\n",
    "for i in min_child_weight:\n",
    "    \"\"\"\n",
    "    Minimum sum of instance weight (hessian) needed in a child. \n",
    "    If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, \n",
    "    then the building process will give up further partitioning. \n",
    "    In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. \n",
    "    The larger min_child_weight is, the more conservative the algorithm will be.\n",
    "    \"\"\"\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, min_child_weight=i)\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('min_child_weight=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma=0,  score= 0.8333333333333334\n",
      "gamma=0.3,  score= 0.836\n",
      "gamma=0.9,  score= 0.8186666666666667\n",
      "gamma=2.7,  score= 0.8066666666666666\n",
      "gamma=8.1,  score= 0.7706666666666667\n",
      "gamma=27.3,  score= 0.716\n",
      "gamma=81.9,  score= 0.6213333333333333\n"
     ]
    }
   ],
   "source": [
    "gamma = [0, 0.3, 0.9, 2.7, 8.1, 27.3, 81.9]  # 默认gamma=0\n",
    "for i in gamma:\n",
    "    # Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, gamma=i)\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('gamma=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learing_rate=0.01,  score= 0.7786666666666666\n",
      "learing_rate=0.02,  score= 0.7973333333333333\n",
      "learing_rate=0.05,  score= 0.8213333333333334\n",
      "learing_rate=0.1,  score= 0.828\n",
      "learing_rate=0.15,  score= 0.832\n",
      "learing_rate=0.3,  score= 0.8333333333333334\n",
      "learing_rate=0.5,  score= 0.8386666666666667\n",
      "learing_rate=0.7,  score= 0.836\n",
      "learing_rate=0.9,  score= 0.8426666666666667\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.01, 0.02, 0.05, 0.1, 0.15, 0.3, 0.5, 0.7, 0.9]  # 默认learning_rate=0.3\n",
    "for i in learning_rate:\n",
    "    # Step size shrinkage used in update to prevents overfitting\n",
    "    # This is learning_rate parameters in the GBDT\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, learning_rate=i)  #\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('learing_rate=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample=0.1,  score= 0.8066666666666666\n",
      "subsample=0.3,  score= 0.8253333333333334\n",
      "subsample=0.4,  score= 0.8346666666666667\n",
      "subsample=0.6,  score= 0.832\n",
      "subsample=0.7,  score= 0.8293333333333334\n",
      "subsample=0.8,  score= 0.8373333333333334\n",
      "subsample=0.85,  score= 0.8426666666666667\n",
      "subsample=0.95,  score= 0.8346666666666667\n",
      "subsample=1,  score= 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "subsample = [0.1, 0.3, 0.4, 0.6, 0.7, 0.8, 0.85, 0.95, 1]  # 默认subsample=1\n",
    "for i in subsample:\n",
    "    # Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting.\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, subsample=i)\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('subsample=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colsample_bytree=0.1,  score= 0.772\n",
      "colsample_bytree=0.3,  score= 0.8333333333333334\n",
      "colsample_bytree=0.4,  score= 0.8373333333333334\n",
      "colsample_bytree=0.6,  score= 0.8386666666666667\n",
      "colsample_bytree=0.7,  score= 0.8346666666666667\n",
      "colsample_bytree=0.8,  score= 0.8466666666666667\n",
      "colsample_bytree=0.85,  score= 0.832\n",
      "colsample_bytree=0.95,  score= 0.844\n",
      "colsample_bytree=1,  score= 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = [0.1, 0.3, 0.4, 0.6, 0.7, 0.8, 0.85, 0.95, 1]  # 默认colsample_bytree=1\n",
    "for i in subsample:\n",
    "    # colsample_bytree is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, colsample_bytree=i)  # column (feature) subsampling,即列采样比率\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('colsample_bytree=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_alpha=0,  score= 0.8333333333333334\n",
      "reg_alpha=0.25,  score= 0.8346666666666667\n",
      "reg_alpha=0.5,  score= 0.8493333333333334\n",
      "reg_alpha=0.75,  score= 0.832\n",
      "reg_alpha=1,  score= 0.8333333333333334\n",
      "reg_alpha=3,  score= 0.8293333333333334\n",
      "reg_alpha=9,  score= 0.7986666666666666\n"
     ]
    }
   ],
   "source": [
    "reg_alpha = [0, 0.25, 0.5, 0.75, 1, 3, 9]  # 默认reg_alpha=0\n",
    "for i in reg_alpha:\n",
    "    # L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, reg_alpha=i)\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('reg_alpha=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_lambda=0,  score= 0.8293333333333334\n",
      "reg_lambda=1,  score= 0.8333333333333334\n",
      "reg_lambda=3,  score= 0.836\n",
      "reg_lambda=9,  score= 0.8453333333333334\n",
      "reg_lambda=27,  score= 0.8226666666666667\n",
      "reg_lambda=81,  score= 0.8213333333333334\n"
     ]
    }
   ],
   "source": [
    "reg_lambda = [0, 1, 3, 9, 27, 81]  # 默认reg_lambda=1\n",
    "for i in reg_lambda:\n",
    "    # L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, reg_lambda=i)\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('reg_lambda=' + str(i) + ',  score=', xg.score(X_test, y_test_new))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}