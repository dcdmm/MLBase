{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "seed = 2022\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般\n",
      "1\n",
      "('这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般',)\n"
     ]
    }
   ],
   "source": [
    "class Dataset(Data.Dataset):\n",
    "    \"\"\"定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, split):\n",
    "        self.split = split\n",
    "        self.dataset = load_dataset(path='seamew/ChnSentiCorp', split=self.split)\n",
    "\n",
    "    # 必须实现__len__魔法方法\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"定义索引方式\"\"\"\n",
    "        text = self.dataset[i]['text']\n",
    "        if self.split == 'test':\n",
    "            return text,  # 测试数据集不含标签\n",
    "        else:\n",
    "            label = self.dataset[i]['label']\n",
    "            return text, label\n",
    "\n",
    "\n",
    "dataset_train = Dataset('train')\n",
    "dataset_validation = Dataset('validation')\n",
    "dataset_test = Dataset('test')\n",
    "\n",
    "for text, label in dataset_train:\n",
    "    # 调用__getitem__方法\n",
    "    print(text)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "for text in dataset_test:\n",
    "    # 调用__getitem__方法\n",
    "    print(text)  # 元组\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102267648\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"bert-base-chinese\"\n",
    "token = BertTokenizer.from_pretrained(model_ckpt)\n",
    "print(token.model_input_names)\n",
    "pretrained = BertModel.from_pretrained(model_ckpt)\n",
    "print(pretrained.num_parameters())\n",
    "\n",
    "# 冻结网络层参数(不进行梯度更新)\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_collate_fn(tokenizer, max_len=512):\n",
    "    \"\"\"返回collate_fun函数(通过闭包函数引入形参)\"\"\"\n",
    "\n",
    "    def collate_fn(data):\n",
    "        sents = [i[0] for i in data]\n",
    "\n",
    "        # 批量编码句子\n",
    "        text_token = tokenizer(text=sents,\n",
    "                               truncation=True,\n",
    "                               padding='max_length',\n",
    "                               max_length=max_len,\n",
    "                               return_token_type_ids=True,\n",
    "                               return_attention_mask=True,\n",
    "                               return_tensors='pt')\n",
    "\n",
    "        input_ids = text_token['input_ids']\n",
    "        attention_mask = text_token['attention_mask']\n",
    "        token_type_ids = text_token['token_type_ids']\n",
    "        # 返回值必须为字典(键与模型forward方法形参对应)\n",
    "        result = {'input_ids': input_ids,  # ★★★★★对应模型forward方法input_ids参数\n",
    "                  'attention_mask': attention_mask,  # ★★★★★对应模型forward方法attention_mask参数\n",
    "                  \"token_type_ids\": token_type_ids}  # ★★★★对应模型forward方法token_type_ids参数\n",
    "\n",
    "        if len(data[0]) == 1:\n",
    "            return result  # 测试数据集不含标签\n",
    "        else:\n",
    "            labels = [i[1] for i in data]\n",
    "            labels = torch.LongTensor(labels)\n",
    "            result['labels'] = labels  # ★★★★对应模型forward方法labels参数\n",
    "            return result\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 6821,  702,  ...,    0,    0,    0],\n",
      "        [ 101, 2577, 4708,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "dataLoader_test = Data.DataLoader(dataset=dataset_test, batch_size=2, collate_fn=get_collate_fn(token, max_len=512))\n",
    "for i in dataLoader_test:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \"\"\"下游训练任务模型\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)  # 二分类任务\n",
    "        self.pretrained = pretrained_model\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
    "        out = self.pretrained(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.fc(out.pooler_output)\n",
    "        out = out.softmax(dim=1)\n",
    "        loss = None\n",
    "        if labels is not None:  # 若包含标签\n",
    "            loss = self.criterion(out, labels)\n",
    "\n",
    "        # 训练与评估阶段\n",
    "        # ★★★★★\n",
    "        # 返回值为一个元组\n",
    "        # 元组的第一个元素必须为该批次数据的损失值\n",
    "        # 元组的第二个元素为该批次数据的预测值(可选)\n",
    "        # * 验证数据集评估函数指标的计算\n",
    "        # * predict方法预测结果(predictions)与评估结果(metrics)(结合输入labels)的计算\n",
    "        if loss is not None:\n",
    "            return (loss, out)\n",
    "        # 预测阶段\n",
    "        # ★★★★★\n",
    "        # 返回值为模型的预测结果\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "model = Model(pretrained)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"验证数据集评估函数\"\"\"\n",
    "    labels = pred.label_ids  # 对应自定义模型forward函数输入:labels\n",
    "    preds = pred.predictions  # 对应自定义模型forward函数返回值的第二个元素\n",
    "    preds_argmax = preds.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds_argmax)\n",
    "    return {\"accuracy\": acc}  # return a dictionary string to metric value\n",
    "\n",
    "\n",
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n",
    "    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            # 学习率预热(线性增加)\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        # 学习率线性衰减(最小为0)\n",
    "        # num_training_steps后学习率恒为0\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "batch_size = 64  # 批次大小\n",
    "epochs = 5.0  # 训练轮数\n",
    "steps_all = int(len(dataset_train) / batch_size) * epochs  # 总学习步数\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4)  # 优化器\n",
    "scheduler_lr = get_linear_schedule_with_warmup(optimizer, 50, 0.9 * steps_all)  # 学习率预热(必须为LambdaLR对象)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 9600\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='689' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [689/750 05:33 < 00:29, 2.06 it/s, Epoch 4.59/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.596900</td>\n",
       "      <td>0.538197</td>\n",
       "      <td>0.797500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.521100</td>\n",
       "      <td>0.510306</td>\n",
       "      <td>0.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.499335</td>\n",
       "      <td>0.828333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.497134</td>\n",
       "      <td>0.829167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1200\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to bert-base-chinese-finetuned-emotion/checkpoint-150\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in bert-base-chinese-finetuned-emotion/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-finetuned-emotion/checkpoint-150/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1200\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to bert-base-chinese-finetuned-emotion/checkpoint-300\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in bert-base-chinese-finetuned-emotion/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-finetuned-emotion/checkpoint-300/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1200\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to bert-base-chinese-finetuned-emotion/checkpoint-450\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in bert-base-chinese-finetuned-emotion/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-finetuned-emotion/checkpoint-450/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1200\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to bert-base-chinese-finetuned-emotion/checkpoint-600\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in bert-base-chinese-finetuned-emotion/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-chinese-finetuned-emotion/checkpoint-600/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# 主要调节的超参数\n",
    "training_args = TrainingArguments(\n",
    "    # The output directory where the model predictions and checkpoints will be written.\n",
    "    output_dir=model_name,\n",
    "    seed=42,\n",
    "\n",
    "    # Total number of training epochs to perform\n",
    "    num_train_epochs=epochs,  # 默认:3.0\n",
    "    # If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs. I\n",
    "    # max_steps=100,  # 默认:-1\n",
    "\n",
    "    #  Maximum gradient norm (for gradient clipping).\n",
    "    max_grad_norm=1.0,  # 默认:1.0\n",
    "\n",
    "    # 对应pytorch DataLoader 参数batch_size\n",
    "    # The batch size per GPU/TPU core/CPU for training.\n",
    "    per_device_train_batch_size=batch_size,  # 默认:8\n",
    "    # The batch size per GPU/TPU core/CPU for evaluation.\n",
    "    # 对应pytorch DataLoader 参数batch_size\n",
    "    per_device_eval_batch_size=batch_size,  # 默认:8\n",
    "    # Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size) or not.\n",
    "    # 对应pytorch DataLoader 参数drop_last\n",
    "    dataloader_drop_last=False,  # 默认:False\n",
    "\n",
    "    # The evaluation strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No evaluation is done during training.\n",
    "    # \"steps\": Evaluation is done (and logged) every eval_steps.\n",
    "    # \"epoch\": Evaluation is done at the end of each epoch.\n",
    "    evaluation_strategy=\"epoch\",  # 默认:'no'\n",
    "    # The logging strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No logging is done during training.\n",
    "    # \"epoch\": Logging is done at the end of each epoch.\n",
    "    # \"steps\": Logging is done every logging_steps.\n",
    "    logging_strategy='epoch',  # 默认:'steps'\n",
    "    # Number of update steps between two logs if logging_strategy=\"steps\".\n",
    "    # logging_steps=500,  # 默认:500\n",
    "    # The checkpoint save strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No save is done during training.\n",
    "    # \"epoch\": Save is done at the end of each epoch.\n",
    "    # \"steps\": Save is done every save_steps.\n",
    "    # Logger log level to use on the main process. Possible choices are the log levels as strings: ‘debug’, ‘info’, ‘warning’, ‘error’ and ‘critical’, plus a ‘passive’ level which doesn’t set anything and lets the application set the level.\n",
    "    log_level='passive',  # 默认'passive'\n",
    "    save_strategy='epoch',  # 默认:'steps'\n",
    "    # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "    # save_steps=500,  # 默认:500\n",
    "    disable_tqdm=False,  # 使用tqdm显示进度\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_validation,\n",
    "    data_collator=get_collate_fn(token, max_len=512),  # 对应pytorch DataLoader 参数collate_fn\n",
    "    optimizers=(optimizer, scheduler_lr),  # 自定义优化器与学习率预热\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=token)\n",
    "\n",
    "trainer.train()  # 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer  # 初始化学习率0.0005,最终学习率归0(get_linear_schedule_with_warmup学习率预热归0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction and returns predictions and potential metrics.\n",
    "# Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method will also return metrics, like in `evaluate()`.\n",
    "preds_output = trainer.predict(dataset_validation)  # 预测和评估包含标签的验证数据集\n",
    "preds_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_output.predictions)  # 预测结果\n",
    "print(type(preds_output.predictions))\n",
    "print(preds_output.predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output.metrics  # 评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataLoader_test:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(dataset_test)  # 预测不含标签的测试数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"预测不含标签的测试数据集(自定义)\"\"\"\n",
    "    model.eval()  # Sets the module in evaluation mode.\n",
    "    predict_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in data_loader:\n",
    "            input_ids = i['input_ids'].to(device)\n",
    "            attention_mask = i['attention_mask'].to(device)\n",
    "            token_type_ids = i['token_type_ids'].to(device)\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "            predict_list.append(output)\n",
    "    predict_all = torch.cat(predict_list, dim=0)\n",
    "    return predict_all\n",
    "\n",
    "\n",
    "result = predict(model, dataLoader_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}