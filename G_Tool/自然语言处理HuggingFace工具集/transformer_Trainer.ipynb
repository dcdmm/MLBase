{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = load_dataset(\"dataset\")\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt = \"bert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at dataset/train/cache-9aa6937aa7a0509c.arrow\n",
      "Loading cached processed dataset at dataset/validation/cache-72fd4e085f395d06.arrow\n",
      "Loading cached processed dataset at dataset/test/cache-81fab418ff2a6a26.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "\n",
    "# 批次处理,整个数据集同时进行处理\n",
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
    "emotions_encoded  # 原有数据与map函数新增数据('input_ids', 'token_type_ids', 'attention_mask')的联合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([0, 0, 3,  ..., 1, 3, 0]),\n",
       " 'input_ids': tensor([[  101,  1045,  2134,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2064,  ...,     0,     0,     0],\n",
       "         [  101, 10047,  9775,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1045,  2514,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2514,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2113,  ...,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded = emotions_encoded.remove_columns(['text'])  # 'text'列不参与训练(即不进入自定义模型forward函数)\n",
    "emotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
    "emotions_encoded['train'][:]  # 字典形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class Customize_Model(torch.nn.Module):\n",
    "    \"\"\"下游训练任务模型\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.classifier = torch.nn.Linear(768, num_labels)  # 多分类任务\n",
    "        self.pretrained = pretrained_model\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.loss_fct = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,  # ★★★★★训练阶段对应emotions_encoded['train']中的input_ids\n",
    "                attention_mask,  # ★★★★★训练阶段对应emotions_encoded['train']中的attention_mask\n",
    "                token_type_ids,  # ★★★★★训练阶段对应emotions_encoded['train']中的token_type_ids\n",
    "                labels=None):  # 标签;★★★★★训练阶段对应emotions_encoded['train']中的labels\n",
    "        outputs = self.pretrained(input_ids=input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:  # 若包含标签\n",
    "            loss = self.loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "\n",
    "        # 训练与评估阶段\n",
    "        # ★★★★★\n",
    "        # 返回值为一个元组\n",
    "        # 元组的第一个元素必须为该批次数据的损失值\n",
    "        # 元组的第二个元素为该批次数据的预测值(可选)\n",
    "        # * 验证数据集评估函数指标的计算\n",
    "        # * predict方法预测结果(predictions)与评估结果(metrics)(结合输入labels)的计算\n",
    "        if loss is not None:\n",
    "            return (loss, logits)\n",
    "        # 预测阶段\n",
    "        # ★★★★★\n",
    "        # 返回值为模型的预测结果\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "num_labels = 6\n",
    "model_from_pretrained = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "model = Customize_Model(model_from_pretrained, num_labels)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"验证数据集评估函数\"\"\"\n",
    "    labels = pred.label_ids  # 对应自定义模型forward函数输入:labels\n",
    "    preds = pred.predictions  # 对应自定义模型forward函数返回值的第二个元素\n",
    "    preds_argmax = preds.argmax(-1)\n",
    "    f1 = f1_score(labels, preds_argmax, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds_argmax)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}  # return a dictionary string to metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.346318</td>\n",
       "      <td>0.894500</td>\n",
       "      <td>0.891665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>0.215337</td>\n",
       "      <td>0.918500</td>\n",
       "      <td>0.918275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-emotion/checkpoint-250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-emotion/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-emotion/checkpoint-250/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-emotion/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-emotion/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-emotion/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.6085659484863282, metrics={'train_runtime': 116.2753, 'train_samples_per_second': 275.209, 'train_steps_per_second': 4.3, 'total_flos': 0.0, 'train_loss': 0.6085659484863282, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 主要调节的超参数\n",
    "batch_size = 64\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "training_args = TrainingArguments(\n",
    "    # The output directory where the model predictions and checkpoints will be written.\n",
    "    output_dir=model_name,\n",
    "    # If True, overwrite the content of the output directory. Use this to continue training if output_dir points to a checkpoint directory.\n",
    "    overwrite_output_dir=False,  # 默认:False\n",
    "    seed=42,\n",
    "\n",
    "    # Total number of training epochs to perform\n",
    "    num_train_epochs=2,  # 默认:3.0\n",
    "    # If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs. I\n",
    "    # max_steps=2000,  # 默认:-1\n",
    "\n",
    "    #  Maximum gradient norm (for gradient clipping).\n",
    "    max_grad_norm=1.0,  # 默认:1.0\n",
    "\n",
    "    # The batch size per GPU/TPU core/CPU for training.\n",
    "    per_device_train_batch_size=batch_size,  # 默认:8\n",
    "    # The batch size per GPU/TPU core/CPU for evaluation.\n",
    "    per_device_eval_batch_size=batch_size,  # 默认:8\n",
    "\n",
    "    # Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "    gradient_accumulation_steps=1,  # 默认:1\n",
    "\n",
    "    # HuggingFace AdamW优化器超参数\n",
    "    # The initial learning rate for AdamW optimizer.\n",
    "    learning_rate=2e-5,  # 默认: 5e-5\n",
    "    # The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer.\n",
    "    weight_decay=0.01,  # 默认:0\n",
    "    # The beta1 hyperparameter for the AdamW optimizer.\n",
    "    adam_beta1=0.9,  # 默认:0.9\n",
    "    # The beta2 hyperparameter for the AdamW optimizer.\n",
    "    adam_beta2=0.999,  # 默认:0.999\n",
    "    # The epsilon hyperparameter for the AdamW optimizer.\n",
    "    adam_epsilon=1e-8,  # 默认:1e-8\n",
    "\n",
    "    # HuggingFace 不同学习率预热超参数\n",
    "    #  The scheduler type to use. See the documentation of SchedulerType for all possible values.\n",
    "    lr_scheduler_type='linear',  # 默认:'linear'\n",
    "    # Ratio of total training steps used for a linear warmup from 0 to learning_rate.\n",
    "    warmup_ratio=0.0,  # 默认:0.0\n",
    "    # Number of steps used for a linear warmup from 0 to learning_rate. Overrides any effect of warmup_ratio.\n",
    "    warmup_steps=0,  # 默认0\n",
    "\n",
    "    # The evaluation strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No evaluation is done during training.\n",
    "    # \"steps\": Evaluation is done (and logged) every eval_steps.\n",
    "    # \"epoch\": Evaluation is done at the end of each epoch.\n",
    "    evaluation_strategy=\"epoch\",  # 默认:'no'\n",
    "    # Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set.\n",
    "    eval_steps=None,  # 默认None\n",
    "    # The logging strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No logging is done during training.\n",
    "    # \"epoch\": Logging is done at the end of each epoch.\n",
    "    # \"steps\": Logging is done every logging_steps.\n",
    "    logging_strategy='epoch',  # 默认:'steps'\n",
    "    # Number of update steps between two logs if logging_strategy=\"steps\".\n",
    "    # logging_steps=500,  # 默认:500\n",
    "    # #################################################################\n",
    "    # The checkpoint save strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No save is done during training.\n",
    "    # \"epoch\": Save is done at the end of each epoch.\n",
    "    # \"steps\": Save is done every save_steps.\n",
    "    save_strategy='epoch',  # 默认:'steps'\n",
    "    #  Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "    save_steps=500,  # 默认:500\n",
    "    # #################################################################\n",
    "    # save_total_limit (int, optional) —If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir.\n",
    "    save_total_limit=None,  # 默认:None\n",
    "    # Logger log level to use on the main process. Possible choices are the log levels as strings: ‘debug’, ‘info’, ‘warning’, ‘error’ and ‘critical’, plus a ‘passive’ level which doesn’t set anything and lets the application set the level.\n",
    "    log_level='passive',  # 默认:'passive'\n",
    "    # #################################################################\n",
    "    # Whether or not to load the best model found during training at the end of training.\n",
    "    # When set to True, the parameters save_strategy needs to be the same as evaluation_strategy, and in the case it is “steps”, save_steps must be a round multiple of eval_steps.\n",
    "    load_best_model_at_end=False,  # 默认load_best_model_at_end=False\n",
    "    # Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models.\n",
    "    # Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\".\n",
    "    # Will default to \"loss\" if unspecified and load_best_model_at_end=True (to use the evaluation loss).\n",
    "    metric_for_best_model=None,\n",
    "    disable_tqdm=False,  # 是否使用tqdm显示进度\n",
    ")\n",
    "\n",
    "# 建议通过optimizers参数自定义优化器与学习率预热\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=emotions_encoded[\"train\"],  # 类型:datasets.arrow_dataset.Dataset\n",
    "    eval_dataset=emotions_encoded[\"validation\"],  # 类型:datasets.arrow_dataset.Dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer)\n",
    "trainer.train()  # 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 4.8580265 , -0.59786206, -1.1437703 , -0.9777483 , -0.80289173,\n",
       "        -1.6467062 ],\n",
       "       [ 4.945601  , -0.5794875 , -1.6377996 , -0.05106468, -0.8756255 ,\n",
       "        -1.6242746 ],\n",
       "       [-1.5682827 ,  2.4421551 ,  3.1642022 , -1.0934749 , -1.7419844 ,\n",
       "        -0.8062323 ],\n",
       "       ...,\n",
       "       [-0.95017964,  5.242554  ,  0.7574893 , -1.4055537 , -1.5912442 ,\n",
       "        -0.9436278 ],\n",
       "       [-2.3446176 ,  2.9856758 ,  2.889302  , -1.2757487 , -1.4608138 ,\n",
       "        -0.16156055],\n",
       "       [-1.424035  ,  5.2089243 ,  0.7045717 , -1.7643484 , -0.9928238 ,\n",
       "        -0.48339003]], dtype=float32), label_ids=array([0, 0, 2, ..., 1, 1, 1]), metrics={'test_loss': 0.21533739566802979, 'test_accuracy': 0.9185, 'test_f1': 0.9182753687347617, 'test_runtime': 2.0455, 'test_samples_per_second': 977.753, 'test_steps_per_second': 15.644})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run prediction and returns predictions and potential metrics.\n",
    "# Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method will also return metrics, like in `evaluate()`.\n",
    "preds_output = trainer.predict(emotions_encoded[\"validation\"])  # 类型:datasets.arrow_dataset.Dataset;验证数据集(包含标签)\n",
    "preds_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8580265  -0.59786206 -1.1437703  -0.9777483  -0.80289173 -1.6467062 ]\n",
      " [ 4.945601   -0.5794875  -1.6377996  -0.05106468 -0.8756255  -1.6242746 ]\n",
      " [-1.5682827   2.4421551   3.1642022  -1.0934749  -1.7419844  -0.8062323 ]\n",
      " ...\n",
      " [-0.95017964  5.242554    0.7574893  -1.4055537  -1.5912442  -0.9436278 ]\n",
      " [-2.3446176   2.9856758   2.889302   -1.2757487  -1.4608138  -0.16156055]\n",
      " [-1.424035    5.2089243   0.7045717  -1.7643484  -0.9928238  -0.48339003]]\n",
      "<class 'numpy.ndarray'>\n",
      "(2000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(preds_output.predictions)  # 预测结果\n",
    "print(type(preds_output.predictions))\n",
    "print(preds_output.predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.21533739566802979,\n",
       " 'test_accuracy': 0.9185,\n",
       " 'test_f1': 0.9182753687347617,\n",
       " 'test_runtime': 2.0455,\n",
       " 'test_samples_per_second': 977.753,\n",
       " 'test_steps_per_second': 15.644}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = emotions_encoded[\"test\"].remove_columns(['label'])\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 4.62717   ,  0.20967638, -1.2161316 ,  0.26625907, -1.5343432 ,\n",
       "        -1.7819738 ],\n",
       "       [ 5.02729   , -0.6014791 , -1.3574317 , -0.636112  , -0.9252229 ,\n",
       "        -1.6474454 ],\n",
       "       [ 4.92349   , -0.79665595, -1.6364161 , -0.63495547, -0.5856782 ,\n",
       "        -1.5237921 ],\n",
       "       ...,\n",
       "       [-1.0013064 ,  5.374679  ,  0.56565815, -1.4822636 , -1.3467855 ,\n",
       "        -0.92645323],\n",
       "       [-0.9053741 ,  5.1751623 ,  0.05427762, -1.5159779 , -0.80188656,\n",
       "        -0.87655663],\n",
       "       [-1.2898622 , -1.5045867 , -0.49642894, -1.3738636 ,  2.2142951 ,\n",
       "         2.386966  ]], dtype=float32), label_ids=None, metrics={'test_runtime': 1.8663, 'test_samples_per_second': 1071.612, 'test_steps_per_second': 17.146})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_dataset)  # 预测不含标签的测试数据集"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}