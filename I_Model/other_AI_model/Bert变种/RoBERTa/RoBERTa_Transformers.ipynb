{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n",
      "['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "主要预训练模型:\n",
    "'roberta-base'\n",
    "'roberta-large'\n",
    "'''\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "print(tokenizer.model_input_names)  # 动态编码\n",
    "print(tokenizer.all_special_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for name, para in model.named_parameters():\n",
    "    print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  3972,    47,   328,  1437,  1437, 50140,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 8])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "<s>To you!  \n",
      "\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "text = \"To you!  \\n\\n\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "print(encoded_input['input_ids'].shape)\n",
    "print(encoded_input.keys())\n",
    "print(tokenizer.decode(encoded_input['input_ids'].tolist()[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-5.8525e-02,  4.0718e-02,  1.3159e-02,  ..., -7.6262e-02,\n",
      "          -7.5294e-02,  1.1257e-02],\n",
      "         [-2.2028e-01,  6.7612e-02, -1.3074e-01,  ..., -1.3768e-01,\n",
      "          -4.7456e-02,  2.2498e-01],\n",
      "         [-1.2109e-01,  1.1744e-01,  7.9263e-02,  ..., -6.1696e-02,\n",
      "           9.2428e-02,  1.5653e-01],\n",
      "         ...,\n",
      "         [-3.0999e-02, -4.3408e-01,  2.0371e-03,  ...,  1.1959e-01,\n",
      "          -4.9194e-02,  4.6534e-01],\n",
      "         [ 9.3677e-02, -2.7647e-01, -4.0717e-02,  ...,  1.5561e-01,\n",
      "           3.0102e-02,  2.6382e-01],\n",
      "         [-5.1293e-02,  1.5703e-02, -6.2538e-03,  ..., -1.2526e-01,\n",
      "          -7.8549e-02, -1.4675e-04]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.0101, -0.2043, -0.2160, -0.0751,  0.1278,  0.1964,  0.2640, -0.0837,\n",
      "         -0.0720, -0.1720,  0.2218, -0.0080, -0.0790,  0.0928, -0.1275,  0.4980,\n",
      "          0.2221, -0.4540,  0.0594, -0.0435, -0.2422,  0.0752,  0.4521,  0.3605,\n",
      "          0.1228,  0.0608, -0.1532, -0.0233,  0.1426,  0.2497,  0.3042,  0.0540,\n",
      "          0.0874,  0.2410, -0.2346,  0.0565, -0.2918,  0.0229,  0.2768, -0.1871,\n",
      "         -0.0822,  0.1566,  0.2123, -0.1489, -0.1090,  0.4163,  0.2606,  0.0102,\n",
      "         -0.1244, -0.0767, -0.3661,  0.3469,  0.2756,  0.1981, -0.0129,  0.0400,\n",
      "         -0.1640,  0.2483, -0.0986, -0.1069, -0.0938, -0.2030, -0.0312, -0.0552,\n",
      "          0.0223, -0.1540,  0.0992, -0.1528, -0.1198,  0.0683, -0.0886,  0.1329,\n",
      "          0.1541, -0.3018, -0.2600,  0.0381, -0.5822, -0.1167,  0.3141,  0.4268,\n",
      "         -0.1265,  0.1580,  0.0231,  0.2278, -0.0065, -0.0744, -0.0208, -0.1046,\n",
      "          0.1986,  0.2601, -0.1810, -0.3935,  0.0536,  0.0006, -0.1264,  0.0158,\n",
      "         -0.0339, -0.0735, -0.1665, -0.2290,  0.0840, -0.2603, -0.1578,  0.2726,\n",
      "         -0.0352, -0.1960, -0.0055,  0.3071,  0.0788, -0.1302, -0.1719,  0.4384,\n",
      "          0.3224,  0.0085,  0.0149,  0.1722,  0.1081, -0.2960,  0.4320, -0.3314,\n",
      "         -0.0120, -0.1015,  0.1128,  0.1589, -0.2253,  0.2933,  0.1580,  0.2571,\n",
      "          0.1892,  0.0993, -0.0416,  0.1504, -0.1078,  0.1624,  0.2120,  0.1274,\n",
      "          0.0060, -0.3378, -0.2329,  0.2814,  0.3440,  0.1708, -0.0527,  0.1954,\n",
      "          0.1284,  0.2244,  0.1587, -0.4117,  0.0451,  0.3555,  0.1098,  0.1837,\n",
      "         -0.0818, -0.2681, -0.2658, -0.0868,  0.0256, -0.3069, -0.1350,  0.3578,\n",
      "          0.0214,  0.0010, -0.1559, -0.2365, -0.0398, -0.1114,  0.0041,  0.0842,\n",
      "         -0.0775, -0.4238, -0.0957, -0.5593, -0.1244,  0.1994, -0.3192,  0.2462,\n",
      "         -0.3092,  0.1104,  0.3819,  0.0397,  0.0028, -0.1931,  0.0060,  0.0988,\n",
      "          0.3072,  0.2525, -0.4128,  0.0991,  0.1702,  0.2384,  0.1416, -0.0624,\n",
      "         -0.1326,  0.1425, -0.1981,  0.1808, -0.2339,  0.1861, -0.2467, -0.2210,\n",
      "          0.2884, -0.4157, -0.0314,  0.0764,  0.2790,  0.0007, -0.0397, -0.0824,\n",
      "          0.1096,  0.1948,  0.1539, -0.3924,  0.2970, -0.0464, -0.0278, -0.0427,\n",
      "          0.1651,  0.2604,  0.0894, -0.3820, -0.1518,  0.1222,  0.2956, -0.2294,\n",
      "          0.1596, -0.3021, -0.4197, -0.1302,  0.2285,  0.2193,  0.1701, -0.2762,\n",
      "          0.1777, -0.1121, -0.4150, -0.3715, -0.1164,  0.2491,  0.1949,  0.1877,\n",
      "          0.2821,  0.0340,  0.1176,  0.1441,  0.1687, -0.1282,  0.1944, -0.3629,\n",
      "         -0.0736, -0.2511, -0.1906, -0.2267,  0.4071, -0.2153,  0.2312,  0.3903,\n",
      "         -0.3036, -0.1078,  0.1547,  0.0887,  0.0689, -0.1174,  0.2026,  0.1562,\n",
      "         -0.1361,  0.2313,  0.0201,  0.2482,  0.1742,  0.1177,  0.1366,  0.1324,\n",
      "         -0.1600,  0.0378,  0.0122, -0.0247, -0.2679, -0.1192,  0.2268, -0.0495,\n",
      "          0.0423, -0.1983, -0.1126,  0.0245,  0.4051, -0.3669,  0.2527,  0.1044,\n",
      "          0.1485, -0.2245, -0.2407,  0.0964,  0.1900, -0.4053,  0.0013,  0.1744,\n",
      "          0.0779,  0.2127,  0.2739,  0.0095, -0.1077,  0.4946, -0.1431, -0.1251,\n",
      "          0.2532, -0.2472, -0.3080,  0.2566, -0.0182,  0.3058,  0.1369,  0.0418,\n",
      "          0.0475, -0.6094,  0.0715, -0.4594, -0.0120,  0.0175, -0.0876, -0.2166,\n",
      "          0.1523,  0.2952, -0.2644, -0.0218,  0.1984,  0.0889, -0.1323,  0.4995,\n",
      "         -0.0302,  0.2299, -0.0786,  0.2558, -0.2081,  0.2647, -0.2695, -0.1077,\n",
      "          0.0307,  0.0999,  0.0468, -0.0528, -0.3386,  0.2232, -0.0167, -0.0399,\n",
      "         -0.0403,  0.1043, -0.0048,  0.0631,  0.0356,  0.3425,  0.2097, -0.0499,\n",
      "         -0.4080, -0.0202, -0.0912,  0.0489,  0.0337, -0.0355,  0.4333, -0.1063,\n",
      "          0.0171, -0.1571,  0.2457,  0.1994,  0.1263,  0.1121,  0.0591,  0.1580,\n",
      "         -0.0507, -0.0246, -0.1375, -0.2265, -0.2918,  0.1898, -0.2308, -0.1857,\n",
      "          0.1513,  0.1971, -0.1292,  0.1347,  0.3168,  0.0938, -0.1466,  0.2855,\n",
      "         -0.1320,  0.0925,  0.3094, -0.0155,  0.1787,  0.5156,  0.1941, -0.3661,\n",
      "         -0.0274, -0.2243,  0.0051,  0.2644, -0.1332,  0.1764,  0.3943,  0.3416,\n",
      "          0.4517,  0.0091, -0.1502,  0.0851,  0.2090,  0.0343, -0.1537, -0.1767,\n",
      "          0.2659,  0.0505, -0.1469, -0.0286, -0.1232,  0.0369, -0.1384, -0.3948,\n",
      "          0.0588,  0.2199, -0.4786,  0.1050, -0.3154,  0.0697, -0.2125,  0.2438,\n",
      "         -0.2181, -0.1171,  0.3970, -0.0930,  0.0412, -0.1950, -0.1574,  0.0206,\n",
      "          0.0130, -0.0278, -0.0266,  0.3506, -0.1388,  0.0408,  0.0163,  0.2271,\n",
      "         -0.0671,  0.2076,  0.0228, -0.1259, -0.3803,  0.1520, -0.1867, -0.4346,\n",
      "         -0.3837,  0.3612, -0.1349, -0.2481, -0.2081, -0.2477,  0.0635,  0.1871,\n",
      "          0.4584, -0.3986, -0.0835,  0.4801, -0.0607, -0.1599,  0.2931,  0.1930,\n",
      "         -0.3281,  0.3301,  0.2802, -0.0395,  0.0076,  0.5113,  0.1301,  0.1900,\n",
      "         -0.2176,  0.4360, -0.1997,  0.3307, -0.1549, -0.2017, -0.2088, -0.0151,\n",
      "          0.3279,  0.1953, -0.4240, -0.1180,  0.0245,  0.3715, -0.3953, -0.0784,\n",
      "          0.0148, -0.3367,  0.1307,  0.1173,  0.2140, -0.3976,  0.0042,  0.3983,\n",
      "         -0.3184,  0.1280,  0.3063,  0.0812,  0.3455, -0.0058, -0.0056,  0.0539,\n",
      "         -0.2389, -0.0556,  0.1518,  0.5639,  0.1627, -0.3820,  0.1165,  0.2580,\n",
      "         -0.1699,  0.3220, -0.1024, -0.0437,  0.2516, -0.0349,  0.1392, -0.1127,\n",
      "         -0.2232, -0.3209,  0.3795, -0.1885, -0.1058, -0.1511, -0.1144, -0.1519,\n",
      "          0.0452, -0.3816,  0.3465,  0.1258, -0.1791, -0.1000, -0.0577, -0.1575,\n",
      "         -0.2193, -0.2565,  0.4336, -0.1588, -0.4518,  0.2551,  0.0236,  0.3759,\n",
      "          0.0235,  0.0970, -0.0584,  0.1313,  0.1020, -0.1298,  0.2949,  0.0631,\n",
      "         -0.5532, -0.1305, -0.2097,  0.0946,  0.1848, -0.3429,  0.0242,  0.0206,\n",
      "          0.1348,  0.0384, -0.1288, -0.0661,  0.4017,  0.2206,  0.2895,  0.1034,\n",
      "          0.2305, -0.0205, -0.3218,  0.0502,  0.0889, -0.1958,  0.4522, -0.0940,\n",
      "         -0.4114, -0.0680,  0.4123,  0.0725, -0.0122, -0.0236,  0.2188,  0.1349,\n",
      "         -0.1186,  0.1717, -0.0070, -0.1300, -0.0895,  0.0985, -0.2387,  0.0480,\n",
      "         -0.1594, -0.0356, -0.1998, -0.0020, -0.2163,  0.2677, -0.3370,  0.1100,\n",
      "          0.0640,  0.2930, -0.3423, -0.1813, -0.0449,  0.1597,  0.2655,  0.3449,\n",
      "          0.0304,  0.0488, -0.1703, -0.2647,  0.0653, -0.2177,  0.1577,  0.0656,\n",
      "          0.2562, -0.3205, -0.2081,  0.2211, -0.0966, -0.1131,  0.4332,  0.2428,\n",
      "          0.2071,  0.0260,  0.2560,  0.0309, -0.1933, -0.1391, -0.2506,  0.0905,\n",
      "         -0.1152, -0.0555, -0.0684, -0.1226, -0.2011, -0.1704,  0.1621,  0.1310,\n",
      "          0.0100, -0.0543, -0.0305, -0.2834,  0.3233,  0.0205,  0.0762, -0.0485,\n",
      "          0.0306, -0.1537,  0.2315,  0.2198,  0.0869, -0.1940, -0.0575, -0.3034,\n",
      "         -0.3664,  0.0665,  0.1198,  0.1244, -0.1034, -0.2857, -0.0138, -0.1370,\n",
      "          0.1767,  0.0167, -0.1462, -0.1004, -0.0608, -0.0527,  0.0802, -0.1983,\n",
      "         -0.1953, -0.1161, -0.0836, -0.0832,  0.3644, -0.0748,  0.2999, -0.1626,\n",
      "          0.0244, -0.1632,  0.1167, -0.0717,  0.0766,  0.2598, -0.4543, -0.1610,\n",
      "         -0.0302, -0.2035, -0.1650, -0.0430, -0.0368,  0.2376, -0.3661,  0.2344,\n",
      "         -0.0880,  0.1821, -0.0814, -0.2761, -0.1646,  0.0050,  0.2515, -0.3507,\n",
      "         -0.2551, -0.2710, -0.1237, -0.0785, -0.2689,  0.4337, -0.1231, -0.0807,\n",
      "          0.0016,  0.4524,  0.2074,  0.1441,  0.2271, -0.0340,  0.0354,  0.1189,\n",
      "         -0.4879,  0.2231, -0.2652, -0.1001,  0.0089,  0.1047, -0.0260,  0.0129,\n",
      "         -0.1401, -0.0939,  0.2016, -0.3833, -0.0409,  0.2909,  0.1638, -0.2402,\n",
      "          0.0262,  0.1201,  0.3902,  0.1124, -0.2156,  0.1155, -0.3636, -0.0529,\n",
      "         -0.1948, -0.3049,  0.1733, -0.0776,  0.0863, -0.1062, -0.2931,  0.2022,\n",
      "         -0.0504, -0.0636,  0.4182,  0.0197, -0.1180,  0.1191,  0.0215,  0.0177,\n",
      "         -0.1121,  0.2689,  0.2023, -0.2934,  0.1200, -0.1406, -0.0602, -0.1013]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = model(**encoded_input)\n",
    "print(result.last_hidden_state)\n",
    "print(result.pooler_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}