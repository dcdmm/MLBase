{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['i didnt feel humiliated',\n",
       "  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       "  'im grabbing a minute to post i feel greedy wrong',\n",
       "  'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n",
       "  'i am feeling grouchy',\n",
       "  'ive been feeling a little burdened lately wasnt sure why that was',\n",
       "  'ive been taking or milligrams or times recommended amount and ive fallen asleep a lot faster but i also feel like so funny',\n",
       "  'i feel as confused about life as a teenager or as jaded as a year old man',\n",
       "  'i have been with petronas for years i feel that petronas has performed well and made a huge profit',\n",
       "  'i feel romantic too'],\n",
       " 'label': [0, 0, 3, 2, 3, 0, 5, 4, 1, 2]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_dataset(path='dair-ai/emotion')['train']\n",
    "train_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('E:\\huggingface_models\\Qwen2.5-0.5B-Instruct')\n",
    "train_dataset = train_dataset.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'attention_mask', 'yy'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.remove_columns('text')\n",
    "train_dataset = train_dataset.add_column('yy', list(range(16000)))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['label', 'input_ids', 'attention_mask', 'yy', 'labels'])\n",
      "yy:  tensor([0, 1])\n",
      "label:  tensor([0, 0])\n",
      "labels:  tensor([[   72, 47607,  2666,  2784, 53773,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100],\n",
      "        [   72,   646,   728,   504,  8266,   773, 74223,   311,   773, 67365,\n",
      "         37550,  1101,   504,  1660,  2163,  4325,   879, 33572,   323,   374,\n",
      "         34347]])\n",
      "labels.shape:  torch.Size([2, 21])\n",
      "input_ids:  tensor([[    72,  47607,   2666,   2784,  53773, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643],\n",
      "        [    72,    646,    728,    504,   8266,    773,  74223,    311,    773,\n",
      "          67365,  37550,   1101,    504,   1660,   2163,   4325,    879,  33572,\n",
      "            323,    374,  34347]])\n",
      "input_ids.shape:  torch.Size([2, 21])\n",
      "attention_mask.shape:  torch.Size([2, 21])\n"
     ]
    }
   ],
   "source": [
    "dac = Data.DataLoader(train_dataset, \n",
    "                      # 保留所有列(所有列必须为数值类型列)\n",
    "                      # Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they are not all of the same length.\n",
    "                      collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False), \n",
    "                      batch_size=2)\n",
    "\n",
    "for i in dac:\n",
    "    print(i.keys())\n",
    "    print(\"yy: \", i[\"yy\"])\n",
    "    print(\"label: \", i['label'])\n",
    "\n",
    "    # # labels内部计算实现(mlm=False时):\n",
    "    # labels = batch[\"input_ids\"].clone()\n",
    "    # if self.tokenizer.pad_token_id is not None:\n",
    "    #     labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "    # batch[\"labels\"] = labels\n",
    "    print(\"labels: \", i['labels'])\n",
    "    print(\"labels.shape: \", i['labels'].shape)\n",
    "    print('input_ids: ', i['input_ids'])\n",
    "    print(\"input_ids.shape: \", i['input_ids'].shape)\n",
    "    print(\"attention_mask.shape: \", i['attention_mask'].shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
