{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BartModel, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = BartModel.from_pretrained(\"facebook/bart-base\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "# Sequence of hidden-states at the output of the last layer of the decoder of the model.\n",
    "print(last_hidden_states.shape)  # batch_size, sequence_length, hidden_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "'PG&E stated it scheduled the blackouts in response to forecasts for high winds amid'"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cg = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "ARTICLE_TO_SUMMARIZE = (\n",
    "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    ")\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model_cg.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "torch.Size([1, 13, 50265])\n"
     ]
    }
   ],
   "source": [
    "TXT = \"My friends are <mask> but they eat too many carbs.\"\n",
    "input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n",
    "# Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax\n",
    "model_result = model_cg(input_ids)  # labels为None\n",
    "print(model_result.loss)  # None(labels为None)\n",
    "logits = model_result.logits\n",
    "print(logits.shape)  # batch_size, sequence_length, config.vocab_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,  2387,   964,    32, 50264,    53,    51,  3529,   350,   171,\n",
      "        33237,     4,     2])\n",
      "50264\n",
      "4\n",
      "tensor([  45,  205, 2245,  372,  182])\n"
     ]
    },
    {
     "data": {
      "text/plain": "['not', 'good', 'healthy', 'great', 'very']"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "print(input_ids[0])\n",
    "print(tokenizer.mask_token_id)\n",
    "print(masked_index)  # 4\n",
    "probs = logits[0, masked_index].softmax(dim=0)\n",
    "values, predictions = probs.topk(5)\n",
    "print(predictions)  # 最大的的5个元素的索引\n",
    "\n",
    "tokenizer.decode(predictions).split()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}