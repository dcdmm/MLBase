{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Models\n",
    "\n",
    "Models are the core algorithms used to actually tokenize, and therefore, they are the only mandatory component of a Tokenizer.\n",
    "\n",
    "| Name      | Description                                                  |\n",
    "| --------- | ------------------------------------------------------------ |\n",
    "| WordLevel | This is the “classic” tokenization algorithm. It let’s you simply map words to IDs without anything fancy. This has the advantage of being really simple to use and understand, but it requires extremely large vocabularies for a good coverage. Using this `Model` requires the use of a `PreTokenizer`. No choice will be made by this model directly, it simply maps input tokens to IDs. |\n",
    "| BPE       | One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding works by starting with characters, while merging those that are the most frequently seen together, thus creating new tokens. It then works iteratively to build new tokens out of the most frequent pairs it sees in a corpus. BPE is able to build words it has never seen by using multiple subword tokens, and thus requires smaller vocabularies, with less chances of having “unk” (unknown) tokens. |\n",
    "| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don’t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word). |\n",
    "| Unigram   | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.models.BPE at 0x1c4aa8a5c30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An implementation of the BPE (Byte-Pair Encoding) algorithm\n",
    "model_BPE = BPE(unk_token='[UNK]')\n",
    "model_BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x1c4aa85b830>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A :obj:`Tokenizer` works as a pipeline. It processes some raw text as input and outputs an :class:`~tokenizers.Encoding`.\n",
    "tokenizer = Tokenizer(\n",
    "    # The core algorithm that this Tokenizer should be using.\n",
    "    model=model_BPE)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### normalizer(可选)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Takes care of normalizing raw text before giving it to a Bert model.\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pre-tokenizers\n",
    "\n",
    "The PreTokenizer takes care of splitting the input according to a set of rules. This pre-processing lets you ensure that the underlying Model does not build tokens across multiple “splits”. For example if you don’t want to have whitespaces inside a token, then you can have a PreTokenizer that splits on these whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This pre-tokenizer simply splits using the following regex: `\\w+|[^\\w\\s]+`\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.trainers.BpeTrainer at 0x1c4ac51dc10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer capable of training a BPE model\n",
    "trainer = BpeTrainer(\n",
    "    # The size of the final vocabulary, including all tokens and alphabet.\n",
    "    vocab_size=30000,  # 默认:30000\n",
    "    # The minimum frequency a pair should have in order to be merged.\n",
    "    min_frequency=0,  # 默认:0\n",
    "    # A list of special tokens the model should know of.\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])  # 默认:[]\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../extra_dataset/wikitext-103-raw/wiki.test.raw',\n",
       " '../extra_dataset/wikitext-103-raw/wiki.train.raw',\n",
       " '../extra_dataset/wikitext-103-raw/wiki.valid.raw']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [f\"../extra_dataset/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x1c4aa85b830>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Tokenizer using the given files.\n",
    "tokenizer.train(\n",
    "    files=files,  # 文件路径或包含路径的列表\n",
    "    trainer=trainer)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'з': 280,\n",
       " '肩': 3208,\n",
       " 'monitor': 9985,\n",
       " 'zac': 12975,\n",
       " 'drey': 18905,\n",
       " 'ikovsky': 20040,\n",
       " '慧': 2237,\n",
       " 'atar': 14291,\n",
       " 'tumor': 23986,\n",
       " 'mendelssohn': 25731,\n",
       " 'machin': 12953,\n",
       " 'stole': 18752,\n",
       " 'de': 4108,\n",
       " '雨': 3797,\n",
       " 'ാ': 671,\n",
       " 'fined': 10601,\n",
       " 'microw': 26621,\n",
       " 'iki': 14946,\n",
       " 'ು': 654,\n",
       " 'woods': 10437,\n",
       " 'timed': 24839,\n",
       " 'over': 4319,\n",
       " 'ceremon': 7297,\n",
       " 'management': 7512,\n",
       " 'lutz': 21545,\n",
       " 'cim': 29562,\n",
       " 'battleship': 7527,\n",
       " 'wheels': 15899,\n",
       " 'isations': 28961,\n",
       " 'ⲫ': 1215,\n",
       " 'propos': 7438,\n",
       " 'iconic': 16228,\n",
       " 'tell': 5939,\n",
       " '静': 3822,\n",
       " 'resign': 10272,\n",
       " 'norwood': 29282,\n",
       " '天': 1893,\n",
       " '1772': 27027,\n",
       " 'brunswick': 16786,\n",
       " 'liszt': 24801,\n",
       " '筐': 3047,\n",
       " 'tempor': 5990,\n",
       " 'astronomy': 16702,\n",
       " 'config': 13083,\n",
       " 'aneous': 13435,\n",
       " 'accidentally': 13209,\n",
       " 'walters': 24549,\n",
       " 'ж': 279,\n",
       " 'comedian': 16470,\n",
       " 'gibbs': 18734,\n",
       " 'myself': 12925,\n",
       " 'monopol': 27731,\n",
       " '⁊': 1053,\n",
       " '宫': 1978,\n",
       " 'skating': 18935,\n",
       " 'def': 4849,\n",
       " 'freud': 27326,\n",
       " 'buenos': 15974,\n",
       " 'rehearsal': 19112,\n",
       " 'allmusic': 11177,\n",
       " '⁄': 1052,\n",
       " 'ships': 5048,\n",
       " 'deserve': 27221,\n",
       " 'seating': 17090,\n",
       " 'ranking': 10404,\n",
       " 'peacetime': 25332,\n",
       " 'diabet': 21398,\n",
       " 'ruz': 21747,\n",
       " '对': 2004,\n",
       " 'marketplace': 27872,\n",
       " '鵩': 3934,\n",
       " 'cey': 20310,\n",
       " 'embassy': 14913,\n",
       " 'denotes': 28098,\n",
       " 'ശ': 668,\n",
       " 'territor': 8270,\n",
       " 'inflicting': 26228,\n",
       " 'both': 4584,\n",
       " 'accol': 14280,\n",
       " 'knicks': 29769,\n",
       " 'nude': 18789,\n",
       " 'submitted': 10909,\n",
       " 'gear': 11430,\n",
       " 'assembling': 27574,\n",
       " 'ecosystems': 22430,\n",
       " 'madagas': 11426,\n",
       " 'ᶢ': 1022,\n",
       " 'ieval': 9095,\n",
       " 'dend': 26169,\n",
       " '।': 508,\n",
       " '1935': 9757,\n",
       " 'catholics': 15782,\n",
       " 'complex': 6653,\n",
       " 'grapes': 24945,\n",
       " '閃': 3732,\n",
       " 'cult': 5892,\n",
       " 'pora': 25881,\n",
       " 'escaping': 19327,\n",
       " 'amtrak': 23775,\n",
       " 'tries': 6188,\n",
       " 'ظ': 410,\n",
       " 'awaiting': 22632,\n",
       " 'codif': 28333,\n",
       " 'cryst': 17023,\n",
       " 'fall': 5059,\n",
       " 'cochr': 18866,\n",
       " 'mun': 6016,\n",
       " 'mushrooms': 17292,\n",
       " 'arte': 13307,\n",
       " 'ܗ': 444,\n",
       " 'pola': 21726,\n",
       " 'blan': 13736,\n",
       " 'scy': 28135,\n",
       " 'aff': 5384,\n",
       " 'unincorporated': 27655,\n",
       " 'practices': 11108,\n",
       " 'routine': 14046,\n",
       " 'depths': 20015,\n",
       " '続': 3112,\n",
       " 'ube': 10295,\n",
       " 'lohan': 25508,\n",
       " 'normally': 10055,\n",
       " 'ancies': 21768,\n",
       " 'denies': 25854,\n",
       " 'divorced': 16937,\n",
       " '数': 2342,\n",
       " 'consonant': 29406,\n",
       " 'bart': 7820,\n",
       " 'pembroke': 25129,\n",
       " 'investigates': 29611,\n",
       " 'raises': 22623,\n",
       " 'distinction': 13806,\n",
       " 'livingston': 27533,\n",
       " 'betrayed': 22280,\n",
       " 'member': 5351,\n",
       " 'unsigned': 21278,\n",
       " 'spons': 9233,\n",
       " 'commands': 15950,\n",
       " '育': 3209,\n",
       " 'steamship': 26651,\n",
       " 'separates': 24741,\n",
       " 'sexual': 7113,\n",
       " '兵': 1573,\n",
       " 'dedicated': 8503,\n",
       " 'ferdin': 14179,\n",
       " '块': 1830,\n",
       " 'anarch': 12843,\n",
       " 'extract': 19894,\n",
       " 'centric': 13618,\n",
       " 'existent': 26759,\n",
       " 'fullback': 26221,\n",
       " 'corresponds': 23676,\n",
       " 'detail': 6606,\n",
       " 'daddy': 23932,\n",
       " 'macaul': 28477,\n",
       " 'oked': 11849,\n",
       " 'asylum': 15605,\n",
       " 'bicy': 15503,\n",
       " 'bangladesh': 16472,\n",
       " 'lisle': 19241,\n",
       " 'drastic': 28002,\n",
       " 'austral': 4885,\n",
       " 'fledgling': 29047,\n",
       " '聡': 3197,\n",
       " '历': 1699,\n",
       " 'risen': 19910,\n",
       " 'r': 60,\n",
       " 'rho': 7747,\n",
       " 'pumping': 21786,\n",
       " 'taxa': 22273,\n",
       " 'sargent': 26080,\n",
       " 'tuber': 17383,\n",
       " 'contrasted': 20773,\n",
       " 'suspected': 13084,\n",
       " '閣': 3738,\n",
       " 'expenditures': 29211,\n",
       " '誕': 3453,\n",
       " 'landslide': 23700,\n",
       " 'ulting': 24052,\n",
       " 'quest': 5742,\n",
       " 'outlined': 20977,\n",
       " 'anity': 21123,\n",
       " 'jk': 26113,\n",
       " 'link': 8555,\n",
       " '駿': 3888,\n",
       " 'ailand': 16403,\n",
       " 'really': 6779,\n",
       " 'mg': 11732,\n",
       " 'ww': 12177,\n",
       " '伐': 1471,\n",
       " 'uncredited': 22895,\n",
       " 'mater': 5782,\n",
       " 'prestig': 13252,\n",
       " 'sque': 19776,\n",
       " 'periodic': 17474,\n",
       " 'iring': 8328,\n",
       " 'dna': 9929,\n",
       " 'participation': 12405,\n",
       " 'luor': 24522,\n",
       " 'effe': 28750,\n",
       " 'sync': 29753,\n",
       " 'exceeding': 18245,\n",
       " 'maple': 13774,\n",
       " '弹': 2149,\n",
       " 'litter': 22164,\n",
       " '218': 27896,\n",
       " 'restricting': 27216,\n",
       " 'sensu': 22990,\n",
       " 'fossil': 11095,\n",
       " 'vegetarian': 25176,\n",
       " '添': 2693,\n",
       " 'fairbanks': 22653,\n",
       " 'arjona': 25249,\n",
       " 'sex': 5507,\n",
       " 'س': 405,\n",
       " 'skate': 21921,\n",
       " 'membrane': 17364,\n",
       " 'aesthetic': 16415,\n",
       " 'shooto': 22362,\n",
       " 'lucas': 10997,\n",
       " '袈': 3402,\n",
       " 'recomm': 7456,\n",
       " 'entry': 8471,\n",
       " '厂': 1697,\n",
       " 'vall': 20051,\n",
       " 'forge': 20286,\n",
       " 'boron': 22122,\n",
       " 'president': 5196,\n",
       " 'examiner': 28892,\n",
       " 'installments': 22705,\n",
       " 'differed': 18116,\n",
       " '没': 2640,\n",
       " '獣': 2836,\n",
       " 'outnumbered': 19996,\n",
       " 'rearguard': 27876,\n",
       " '樛': 2551,\n",
       " 'sharing': 13190,\n",
       " 'lexus': 29202,\n",
       " 'notations': 27074,\n",
       " 'beside': 16371,\n",
       " 'reactivated': 29726,\n",
       " 'chill': 19011,\n",
       " 'preservation': 13729,\n",
       " 'ille': 9286,\n",
       " 'observations': 11225,\n",
       " '劉': 1640,\n",
       " 'staircase': 19924,\n",
       " '瑞': 2860,\n",
       " 'bute': 24456,\n",
       " 'odic': 26506,\n",
       " 'れ': 1312,\n",
       " 'sediments': 25408,\n",
       " 'sampling': 23342,\n",
       " 'assembl': 20044,\n",
       " 'mony': 27772,\n",
       " '契': 1906,\n",
       " 'taj': 27285,\n",
       " 'trademark': 15581,\n",
       " 'nicolas': 20673,\n",
       " 'kyo': 10537,\n",
       " 'æthelstan': 25973,\n",
       " 'columbia': 8485,\n",
       " 'blamed': 13740,\n",
       " 'accur': 8788,\n",
       " 'refuel': 22460,\n",
       " 'usch': 29322,\n",
       " 'flam': 18369,\n",
       " 'borne': 8532,\n",
       " 'outside': 6187,\n",
       " 'demonic': 27970,\n",
       " 'brack': 25921,\n",
       " 'saying': 6140,\n",
       " 'contr': 4923,\n",
       " 'abstr': 15996,\n",
       " 'worms': 18750,\n",
       " 'monsoon': 16775,\n",
       " 'nil': 23539,\n",
       " 'carac': 27698,\n",
       " 'puts': 15148,\n",
       " 'clough': 28647,\n",
       " '祠': 2983,\n",
       " 'भ': 489,\n",
       " 'harder': 16122,\n",
       " 'fully': 6646,\n",
       " 'clo': 4683,\n",
       " 'thayer': 29013,\n",
       " 'shone': 28172,\n",
       " 'excitement': 19851,\n",
       " 'liberties': 20142,\n",
       " 'haze': 28405,\n",
       " 'g': 49,\n",
       " 'instruct': 16523,\n",
       " '骸': 3901,\n",
       " '747': 20009,\n",
       " 'hands': 7863,\n",
       " 'site': 5785,\n",
       " 'ev': 4328,\n",
       " 'vig': 10627,\n",
       " 'toul': 19622,\n",
       " 'ϵ': 272,\n",
       " '毬': 2607,\n",
       " '⊢': 1141,\n",
       " '鉢': 3686,\n",
       " '作': 1492,\n",
       " 'ennial': 14263,\n",
       " 'caa': 11645,\n",
       " '繋': 3135,\n",
       " '門': 3731,\n",
       " 'dean': 9091,\n",
       " '臚': 3234,\n",
       " 'prisoners': 8947,\n",
       " 'reporting': 13310,\n",
       " 'x': 66,\n",
       " 'proportion': 11989,\n",
       " 'isenau': 25284,\n",
       " 'ec': 8084,\n",
       " 'transitioned': 20614,\n",
       " '銘': 3695,\n",
       " 'aldr': 28960,\n",
       " 'akest': 26862,\n",
       " 'happened': 10207,\n",
       " 'hir': 11688,\n",
       " 'rak': 16920,\n",
       " '56': 8092,\n",
       " 'pcs': 25657,\n",
       " 'chronicler': 22995,\n",
       " 'illumin': 16529,\n",
       " 'techn': 8281,\n",
       " 'wax': 17201,\n",
       " 'cartoon': 10989,\n",
       " 'sympath': 19858,\n",
       " '1796': 20170,\n",
       " 'disagreements': 22556,\n",
       " 'iy': 16307,\n",
       " 'katy': 23047,\n",
       " 'cook': 8332,\n",
       " 'goof': 26443,\n",
       " 'raaf': 10177,\n",
       " 'gene': 8496,\n",
       " 'bram': 19442,\n",
       " '峰': 2053,\n",
       " '-@': 4115,\n",
       " 'co': 4104,\n",
       " 'particle': 16849,\n",
       " 'lew': 20380,\n",
       " 'commandos': 25475,\n",
       " 'sof': 6941,\n",
       " '鎮': 3709,\n",
       " 'rating': 6373,\n",
       " 'clu': 10699,\n",
       " 'avery': 20877,\n",
       " 'ammunition': 10896,\n",
       " 'uton': 12498,\n",
       " 'ton': 4410,\n",
       " 'farmland': 16888,\n",
       " 'knew': 8718,\n",
       " 'bite': 17355,\n",
       " 'him': 4388,\n",
       " '狸': 2826,\n",
       " 'arguably': 19519,\n",
       " 'ingu': 7941,\n",
       " 'terminate': 23271,\n",
       " 'longfellow': 28265,\n",
       " 'ment': 4187,\n",
       " 'concern': 9834,\n",
       " 'fry': 18919,\n",
       " 'graphically': 21225,\n",
       " 'final': 4862,\n",
       " 'revolutionary': 10244,\n",
       " 'burnley': 23553,\n",
       " 'moat': 25535,\n",
       " 'anes': 10019,\n",
       " 'xx': 10934,\n",
       " 'mercedes': 13873,\n",
       " 'sang': 10638,\n",
       " 'aine': 9614,\n",
       " '183': 22641,\n",
       " 'recur': 10892,\n",
       " 'uncomfortable': 19439,\n",
       " 'manage': 12578,\n",
       " 'hind': 9844,\n",
       " 'unhappy': 15715,\n",
       " 'constrained': 28863,\n",
       " 'blake': 14763,\n",
       " 'conditioning': 24038,\n",
       " 'advance': 7022,\n",
       " 'elias': 21743,\n",
       " 'applicants': 25662,\n",
       " '+': 15,\n",
       " 'tombs': 18695,\n",
       " 'newsletter': 27441,\n",
       " '167': 21239,\n",
       " 'files': 10010,\n",
       " 'thief': 18375,\n",
       " 'glasses': 19315,\n",
       " 'pitch': 8566,\n",
       " '壱': 1875,\n",
       " 'བ': 756,\n",
       " 'stel': 20972,\n",
       " '祿': 2988,\n",
       " 'ⵕ': 1238,\n",
       " '爵': 2796,\n",
       " 'bachelor': 15067,\n",
       " 'criminal': 9647,\n",
       " 'strategies': 18051,\n",
       " 'uaries': 25317,\n",
       " 'agger': 11527,\n",
       " 'ledo': 22557,\n",
       " 'bounce': 25124,\n",
       " 'furniture': 17244,\n",
       " 'breakaway': 17699,\n",
       " '遥': 3634,\n",
       " 'atmo': 8254,\n",
       " 'sounding': 17562,\n",
       " 'fluids': 28696,\n",
       " '狡': 2821,\n",
       " 'limitations': 15594,\n",
       " 'text': 6102,\n",
       " 'organize': 15549,\n",
       " 'hybrids': 22287,\n",
       " 'phill': 9657,\n",
       " 'intervened': 22709,\n",
       " 'retic': 24842,\n",
       " 'gunmen': 21409,\n",
       " '郁': 3649,\n",
       " 'newspaper': 8014,\n",
       " 'hostages': 23506,\n",
       " 'disco': 7837,\n",
       " 'toxins': 29055,\n",
       " 'tells': 7816,\n",
       " '1885': 13035,\n",
       " 'java': 16649,\n",
       " 'undy': 22622,\n",
       " 'ⱱ': 1209,\n",
       " 'catching': 18497,\n",
       " 'reno': 12489,\n",
       " 'trit': 18999,\n",
       " 'throws': 16985,\n",
       " 'buffy': 19066,\n",
       " 'volat': 21586,\n",
       " 'cause': 6719,\n",
       " 'convoy': 9961,\n",
       " 'plac': 4861,\n",
       " 'banker': 25521,\n",
       " 'mediocre': 23193,\n",
       " 'aim': 4890,\n",
       " 'canadian': 6938,\n",
       " 'vocals': 7151,\n",
       " 'minaj': 19321,\n",
       " 'olar': 22293,\n",
       " 'lifespan': 22317,\n",
       " 'commentator': 19265,\n",
       " '事': 1431,\n",
       " 'bom': 9773,\n",
       " 'codex': 20203,\n",
       " 'recognize': 15088,\n",
       " 'recordings': 9869,\n",
       " 'communists': 16638,\n",
       " 'semitic': 25941,\n",
       " 'physio': 23799,\n",
       " '耆': 3185,\n",
       " '1902': 11392,\n",
       " 'odor': 14860,\n",
       " '献': 2832,\n",
       " '棺': 2517,\n",
       " 'hans': 9738,\n",
       " 'notion': 15277,\n",
       " '1866': 14193,\n",
       " 'repaired': 14679,\n",
       " 'justin': 11766,\n",
       " 'cruelty': 25286,\n",
       " 'nautical': 12331,\n",
       " 'rud': 9721,\n",
       " 'firm': 8277,\n",
       " 'jewish': 8530,\n",
       " 'yorker': 21872,\n",
       " 'vas': 11074,\n",
       " '槤': 2543,\n",
       " 'somme': 26522,\n",
       " 'rite': 21215,\n",
       " 'madison': 13635,\n",
       " '晶': 2393,\n",
       " 'speak': 8232,\n",
       " 'manganese': 26916,\n",
       " '組': 3100,\n",
       " 'libya': 20570,\n",
       " '门': 3745,\n",
       " 'beyonce': 8727,\n",
       " 'deput': 9508,\n",
       " 'cust': 13324,\n",
       " 'hangar': 19912,\n",
       " 'volta': 21644,\n",
       " 'illa': 8674,\n",
       " 'admired': 15908,\n",
       " 'schemes': 17013,\n",
       " 'active': 6120,\n",
       " 'rush': 7421,\n",
       " 'abra': 13447,\n",
       " 'scandinavian': 21273,\n",
       " 'requiem': 22197,\n",
       " 'parts': 6084,\n",
       " '模': 2552,\n",
       " '錄': 3700,\n",
       " 'disney': 8576,\n",
       " 'participant': 22652,\n",
       " 'mortars': 23583,\n",
       " 'repetition': 23965,\n",
       " 'emerald': 23449,\n",
       " 'gamespot': 14636,\n",
       " 'ordained': 24256,\n",
       " 'glowing': 26425,\n",
       " 'aspirations': 26976,\n",
       " 'favourable': 15291,\n",
       " 'dh': 15810,\n",
       " 'unil': 28828,\n",
       " 'strata': 29530,\n",
       " 'consists': 8609,\n",
       " 'initiation': 25198,\n",
       " 'cutler': 26411,\n",
       " 'rugby': 10768,\n",
       " 'levin': 21140,\n",
       " 'doctr': 11679,\n",
       " 'ಚ': 633,\n",
       " 'incentives': 26734,\n",
       " '凱': 1604,\n",
       " 'relatives': 12414,\n",
       " 'assass': 9922,\n",
       " '远': 3604,\n",
       " 'ixon': 10113,\n",
       " 'steal': 16094,\n",
       " 'ameters': 20744,\n",
       " 'iley': 10092,\n",
       " 'pele': 29786,\n",
       " 'ƙ': 118,\n",
       " 'directly': 7245,\n",
       " 'cinem': 8037,\n",
       " 'dietary': 24236,\n",
       " '失': 1897,\n",
       " 'describing': 9170,\n",
       " 'sizable': 27597,\n",
       " 'americ': 4525,\n",
       " '給': 3105,\n",
       " 'gunnery': 17670,\n",
       " 'brahma': 29454,\n",
       " 'consulting': 18529,\n",
       " 'densities': 28198,\n",
       " 'happen': 8088,\n",
       " 'skeptical': 21119,\n",
       " 'byzantines': 17580,\n",
       " 'manga': 11445,\n",
       " '尉': 2013,\n",
       " 'termine': 9916,\n",
       " 'tellur': 25328,\n",
       " 'pecul': 21404,\n",
       " 'shattered': 22510,\n",
       " 'yuri': 26173,\n",
       " 'gase': 28118,\n",
       " 'foreign': 6885,\n",
       " 'civilization': 15511,\n",
       " 'servant': 16571,\n",
       " 'affirm': 27317,\n",
       " 'telescope': 14197,\n",
       " 'comic': 7988,\n",
       " 'deceased': 15780,\n",
       " 'waited': 18002,\n",
       " 'sos': 20960,\n",
       " 'phones': 14980,\n",
       " 'intellig': 7638,\n",
       " 'producers': 7989,\n",
       " 'aded': 8163,\n",
       " 'fortune': 12845,\n",
       " 'beard': 17155,\n",
       " 'nicaragua': 21472,\n",
       " 'fern': 24760,\n",
       " '歳': 2587,\n",
       " '加': 1645,\n",
       " 'sidel': 23274,\n",
       " 'thinner': 28081,\n",
       " 'cab': 7057,\n",
       " 'urban': 7239,\n",
       " 'lem': 12496,\n",
       " 'freshwater': 19301,\n",
       " 'cameras': 15181,\n",
       " '劳': 1649,\n",
       " 'muster': 28180,\n",
       " '213': 22137,\n",
       " 'morph': 18380,\n",
       " '鄧': 3660,\n",
       " 'explosives': 18592,\n",
       " 'စ': 770,\n",
       " 'jai': 27313,\n",
       " 'persons': 11470,\n",
       " 'conform': 17100,\n",
       " 'adject': 27196,\n",
       " 'generic': 16615,\n",
       " 'hicks': 27721,\n",
       " '傭': 1538,\n",
       " 'maniac': 29161,\n",
       " 'diam': 8845,\n",
       " 'ateg': 5902,\n",
       " 'approximate': 23757,\n",
       " 'sec': 7100,\n",
       " 'eighth': 8548,\n",
       " 'uter': 7171,\n",
       " '帰': 2090,\n",
       " 'tight': 10670,\n",
       " 'ា': 984,\n",
       " '指': 2300,\n",
       " 'lude': 15109,\n",
       " 'kun': 15160,\n",
       " 'militar': 24713,\n",
       " 'recounts': 25186,\n",
       " 'urine': 18672,\n",
       " 'parking': 13481,\n",
       " 'etty': 20836,\n",
       " 'autonomous': 18386,\n",
       " '観': 3428,\n",
       " 'ethic': 27570,\n",
       " 'palette': 28143,\n",
       " 'pierce': 14940,\n",
       " 'distinguishing': 23453,\n",
       " 'midday': 25853,\n",
       " '亦': 1442,\n",
       " 'ially': 4796,\n",
       " 'els': 5344,\n",
       " '葦': 3324,\n",
       " 'arm': 5501,\n",
       " 'snyder': 24878,\n",
       " 'ills': 16580,\n",
       " '1689': 29743,\n",
       " '脩': 3222,\n",
       " 'glorious': 18945,\n",
       " 'lette': 17728,\n",
       " 'rist': 19207,\n",
       " 'phenomena': 19280,\n",
       " 'mina': 23261,\n",
       " 'socks': 29136,\n",
       " 'cany': 23293,\n",
       " 'turks': 14887,\n",
       " 'evolution': 8740,\n",
       " 'undo': 27604,\n",
       " 'offering': 11010,\n",
       " '镜': 3728,\n",
       " 'sect': 12012,\n",
       " 'chev': 11634,\n",
       " 'ution': 5117,\n",
       " 'generally': 6023,\n",
       " 'harvard': 12308,\n",
       " '7': 27,\n",
       " 'illegal': 11190,\n",
       " 'semifinal': 23169,\n",
       " 'pect': 5146,\n",
       " 'wo': 4236,\n",
       " 'bethan': 24226,\n",
       " 'crabs': 26441,\n",
       " 'tires': 27177,\n",
       " 'airspace': 29793,\n",
       " 'laun': 5901,\n",
       " 'realizes': 15156,\n",
       " 'creeks': 27784,\n",
       " 'pur': 5319,\n",
       " 'hawai': 13607,\n",
       " 'altitude': 12745,\n",
       " 'robbery': 20859,\n",
       " 'indianapolis': 16521,\n",
       " 'angry': 12584,\n",
       " 'frozen': 15313,\n",
       " 'urus': 19714,\n",
       " 'regulars': 21923,\n",
       " 'ento': 9334,\n",
       " 'boliv': 16830,\n",
       " '入': 1563,\n",
       " 'fee': 9714,\n",
       " 'freddy': 26652,\n",
       " 'afterward': 13938,\n",
       " 'staffel': 24776,\n",
       " 'dining': 16280,\n",
       " 'intern': 15172,\n",
       " 'codes': 17170,\n",
       " 'europa': 18381,\n",
       " 'ital': 4780,\n",
       " 'cdo': 21890,\n",
       " 'ᇂ': 877,\n",
       " 'northeastward': 16304,\n",
       " 'usur': 23215,\n",
       " 'wedding': 10403,\n",
       " 'wether': 25163,\n",
       " 'fest': 6203,\n",
       " 'ressed': 7550,\n",
       " 'uity': 20602,\n",
       " 'sherlock': 25373,\n",
       " 'crystall': 18784,\n",
       " 'shima': 19101,\n",
       " 'chin': 10438,\n",
       " 'combines': 19749,\n",
       " 'glen': 11417,\n",
       " 'reput': 8393,\n",
       " 'grimsby': 29209,\n",
       " 'constant': 7066,\n",
       " 'underway': 18920,\n",
       " 'oversight': 24886,\n",
       " 'sovie': 14573,\n",
       " 'middle': 6063,\n",
       " 'amazing': 12471,\n",
       " 'nons': 20328,\n",
       " 'scopy': 21941,\n",
       " 'caldwell': 23616,\n",
       " 'tles': 18959,\n",
       " 'olia': 13779,\n",
       " 'chi': 7595,\n",
       " 'illustrates': 27878,\n",
       " 'operas': 14937,\n",
       " 'incentive': 25663,\n",
       " 'kneale': 26374,\n",
       " 'helmet': 18495,\n",
       " 'treat': 5702,\n",
       " 'answering': 27224,\n",
       " 'prey': 9711,\n",
       " 'flie': 28973,\n",
       " 'latex': 27165,\n",
       " 'dumb': 25546,\n",
       " 'eject': 24808,\n",
       " 'kesha': 21380,\n",
       " 'minefield': 26766,\n",
       " 'offence': 20978,\n",
       " 'avo': 6406,\n",
       " 'showers': 25114,\n",
       " 'asleep': 22286,\n",
       " 'monter': 24116,\n",
       " 'tyres': 19341,\n",
       " 'uka': 16369,\n",
       " 'safe': 6980,\n",
       " 'flooded': 12039,\n",
       " 'interfere': 19998,\n",
       " 'amanda': 21419,\n",
       " 'myst': 14484,\n",
       " 'olds': 10950,\n",
       " 'clara': 22266,\n",
       " 'pistol': 15207,\n",
       " 'acknowledge': 19067,\n",
       " 'cs': 10547,\n",
       " 'cam': 6185,\n",
       " 'joseph': 7499,\n",
       " 'revelation': 16944,\n",
       " '恋': 2203,\n",
       " 'supplies': 9049,\n",
       " 'frid': 20746,\n",
       " 'mutation': 20590,\n",
       " 'stirling': 21211,\n",
       " 'violate': 28850,\n",
       " 'repealed': 27258,\n",
       " 'forage': 23938,\n",
       " 'leaf': 9185,\n",
       " 'scen': 5168,\n",
       " 'convince': 13821,\n",
       " 'bant': 29119,\n",
       " 'ȷ': 130,\n",
       " 'friar': 28474,\n",
       " 'hersh': 29790,\n",
       " '‒': 1029,\n",
       " 'lik': 6151,\n",
       " 'prosper': 12679,\n",
       " 'hydraulic': 22450,\n",
       " 'amp': 4409,\n",
       " 'atig': 28681,\n",
       " '蒲': 3329,\n",
       " 'hamasaki': 22414,\n",
       " 'artis': 21438,\n",
       " 'loves': 14069,\n",
       " 'lau': 8521,\n",
       " 'dirty': 14941,\n",
       " '་': 743,\n",
       " 'dante': 21265,\n",
       " 'itza': 22456,\n",
       " 'whist': 15053,\n",
       " 'dig': 6204,\n",
       " 'case': 5664,\n",
       " 'lg': 15626,\n",
       " 'laguna': 24499,\n",
       " 'lucia': 21990,\n",
       " 'moreau': 28601,\n",
       " 'patience': 21782,\n",
       " 'bowie': 14879,\n",
       " 'idis': 21998,\n",
       " 'precise': 15340,\n",
       " 'gran': 10848,\n",
       " '1768': 28138,\n",
       " 'thomp': 9883,\n",
       " 'mayor': 8739,\n",
       " 'logos': 26625,\n",
       " 'insistence': 20690,\n",
       " 'bott': 12372,\n",
       " 'pennsylvania': 8161,\n",
       " '逢': 3617,\n",
       " 'katherine': 19768,\n",
       " 'versat': 22265,\n",
       " 'prominence': 16310,\n",
       " 'prostitution': 23032,\n",
       " 'basing': 24891,\n",
       " 'ธ': 712,\n",
       " 'caught': 8294,\n",
       " 'easter': 12958,\n",
       " 'acquire': 14787,\n",
       " 'monarchy': 14979,\n",
       " 'reich': 12063,\n",
       " 'stint': 19651,\n",
       " 'erect': 20762,\n",
       " 'grasses': 22658,\n",
       " 'shaw': 9512,\n",
       " 'amine': 17438,\n",
       " '❆': 1200,\n",
       " 'tong': 11005,\n",
       " 'saved': 11059,\n",
       " 'nol': 13900,\n",
       " 'mobiles': 25829,\n",
       " 'division': 5181,\n",
       " 'blin': 11258,\n",
       " 'eb': 14257,\n",
       " '仰': 1464,\n",
       " 'province': 8269,\n",
       " 'indler': 27804,\n",
       " 'ส': 729,\n",
       " 'divisional': 16407,\n",
       " 'halo': 10311,\n",
       " 'շ': 346,\n",
       " 'turnpike': 12430,\n",
       " 'rehears': 18917,\n",
       " 'buoy': 28428,\n",
       " 'renowned': 16400,\n",
       " 'enlightenment': 23194,\n",
       " 'cooper': 9004,\n",
       " 'destruction': 9373,\n",
       " 'spic': 16145,\n",
       " 'conviction': 15618,\n",
       " 'princeton': 13717,\n",
       " 'iani': 28222,\n",
       " 'frightening': 28626,\n",
       " 'aghan': 23239,\n",
       " 'luxembourg': 23332,\n",
       " 'marceline': 23459,\n",
       " 'cret': 5762,\n",
       " 'repair': 7452,\n",
       " 'mess': 6826,\n",
       " 'alamos': 19645,\n",
       " 'lich': 13224,\n",
       " '＜': 4025,\n",
       " 'soil': 10453,\n",
       " 'formby': 27522,\n",
       " 'congrat': 18974,\n",
       " 'assic': 17193,\n",
       " 'inflammation': 26139,\n",
       " 'sperm': 19164,\n",
       " 'd4': 27593,\n",
       " 'secondly': 28431,\n",
       " 'dos': 13596,\n",
       " '∂': 1108,\n",
       " 'miocene': 27944,\n",
       " 'kend': 15729,\n",
       " 'oun': 4173,\n",
       " 'buys': 27433,\n",
       " 'eddie': 13411,\n",
       " 'treas': 9393,\n",
       " 'sme': 23656,\n",
       " 'isto': 18572,\n",
       " '娜': 1936,\n",
       " 'se': 4097,\n",
       " 'lei': 25393,\n",
       " 'ܙ': 446,\n",
       " 'install': 7110,\n",
       " 'israel': 8258,\n",
       " 'frith': 20855,\n",
       " 'pilot': 7415,\n",
       " 'turrets': 10557,\n",
       " 'lobe': 25714,\n",
       " 'hawkins': 21464,\n",
       " '於': 2359,\n",
       " 'drama': 7800,\n",
       " 'sine': 26881,\n",
       " '颂': 3857,\n",
       " 'britten': 20117,\n",
       " 'charting': 16127,\n",
       " 'ternal': 10210,\n",
       " 'ه': 420,\n",
       " 'onge': 12890,\n",
       " 'coc': 16449,\n",
       " '紋': 3087,\n",
       " 'phae': 16699,\n",
       " '翡': 3177,\n",
       " 'artist': 6827,\n",
       " 'rians': 27762,\n",
       " 'cork': 26469,\n",
       " 'appealing': 18044,\n",
       " 'rebuff': 27694,\n",
       " 'aires': 14749,\n",
       " 'ilus': 21850,\n",
       " 'tactic': 19065,\n",
       " 'is': 4088,\n",
       " '1990': 7626,\n",
       " '←': 1093,\n",
       " 'nberg': 27110,\n",
       " 'semif': 15682,\n",
       " 'cribed': 12295,\n",
       " 'consultant': 17465,\n",
       " 'mccl': 17382,\n",
       " '𐎺': 4057,\n",
       " 'machines': 13196,\n",
       " 'dilemma': 26694,\n",
       " 'caragiale': 29173,\n",
       " 'inia': 21369,\n",
       " 'tai': 11523,\n",
       " 'gle': 9024,\n",
       " 'ulus': 15769,\n",
       " 'kilometre': 17302,\n",
       " 'mirkin': 27171,\n",
       " 'ជ': 959,\n",
       " 'forbes': 16014,\n",
       " 'chloe': 21435,\n",
       " 'tempered': 27675,\n",
       " 'improper': 22425,\n",
       " 'last': 4917,\n",
       " 'oria': 17329,\n",
       " 'tetr': 20606,\n",
       " 'easily': 8897,\n",
       " 'acan': 23876,\n",
       " 'boots': 16589,\n",
       " '広': 2104,\n",
       " 'selfish': 24629,\n",
       " '偃': 1525,\n",
       " 'sly': 25807,\n",
       " 'mega': 13273,\n",
       " 'genius': 14658,\n",
       " 'peer': 17347,\n",
       " 'eminem': 17900,\n",
       " 'xen': 19465,\n",
       " 'ecclesiast': 16373,\n",
       " 'dau': 23856,\n",
       " 'ghost': 10429,\n",
       " 'wor': 4225,\n",
       " 'ensis': 11853,\n",
       " 'gul': 12771,\n",
       " 'colleagues': 11855,\n",
       " 'flaws': 20302,\n",
       " '395': 25278,\n",
       " 'sexuality': 12571,\n",
       " 'ɖ': 139,\n",
       " 'q': 59,\n",
       " 'colony': 9285,\n",
       " 'crews': 10698,\n",
       " '129': 19460,\n",
       " 'walled': 18078,\n",
       " 'whate': 12184,\n",
       " 'cino': 24833,\n",
       " 'class': 4669,\n",
       " 'hur': 5216,\n",
       " 'haifa': 25653,\n",
       " 'escal': 14689,\n",
       " '飛': 3867,\n",
       " 'ჩ': 811,\n",
       " 'angr': 23071,\n",
       " 'odus': 25922,\n",
       " 'ទ': 966,\n",
       " 'д': 277,\n",
       " 'cohen': 16860,\n",
       " 'incorporated': 9180,\n",
       " 'widespread': 9318,\n",
       " 'eigh': 7366,\n",
       " 'tomato': 15855,\n",
       " 'colling': 28516,\n",
       " 'cies': 8967,\n",
       " 'ignorant': 28625,\n",
       " 'punct': 27133,\n",
       " 'fro': 8222,\n",
       " 'alous': 15341,\n",
       " 'compil': 26991,\n",
       " 'alternately': 28714,\n",
       " '840': 28592,\n",
       " 'teh': 28921,\n",
       " 'messenger': 20124,\n",
       " '|': 70,\n",
       " 'rabbi': 19379,\n",
       " '置': 3163,\n",
       " 'commended': 16006,\n",
       " 'ｄ': 4033,\n",
       " 'behaviour': 10726,\n",
       " 'valued': 15544,\n",
       " 'divergence': 26073,\n",
       " 'brezhnev': 22310,\n",
       " 'fifth': 6468,\n",
       " '⥇': 1206,\n",
       " 'journalist': 9892,\n",
       " 'realignment': 26836,\n",
       " '214': 26279,\n",
       " 'ர': 591,\n",
       " 'guns': 5830,\n",
       " 'yn': 11300,\n",
       " 'soviets': 14598,\n",
       " 'quartet': 19376,\n",
       " 'traject': 24946,\n",
       " 'administrator': 18280,\n",
       " '树': 2473,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.token_to_id(\"[SEP]\"))\n",
    "print(tokenizer.id_to_token(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the given sequence and pair. This method can process raw text sequences as well as already pre-tokenized sequences.\n",
    "output = tokenizer.encode(\n",
    "    sequence=\"Hello, y'all! How are you 😁 ?\",  # 未分好词\n",
    "    is_pretokenized=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?']\n",
      "[22334, 16, 67, 11, 4162, 5, 5380, 4172, 4787, 0, 35]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(output.tokens)\n",
    "print(output.ids)\n",
    "print(output.type_ids)  # The generated type IDs\n",
    "print(output.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'pre', 'token', 'ized', 'sequence', 'and', 'its', 'pair']\n",
      "[43, 4305, 26436, 4779, 7436, 4084, 4242, 5685]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_pair = tokenizer.encode(\n",
    "    sequence=[\"A\", \"pre\", \"tokenized\", \"sequence\"],  # 已经分好词\n",
    "    pair=[\"And\", \"its\", \"pair\"],\n",
    "    # Whether the input is already pre-tokenized\n",
    "    is_pretokenized=True\n",
    ")\n",
    "print(output_pair.tokens)\n",
    "print(output_pair.ids)\n",
    "print(output_pair.type_ids)\n",
    "print(output_pair.attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Post-Processors\n",
    "\n",
    "After the whole pipeline, we sometimes want to insert some special tokens before feed a tokenized string into a model like ”[CLS] My horse is amazing [SEP]”. The PostProcessor is the component doing just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Provides a way to specify templates in order to add the special tokens to each input sequence as relevant.\n",
    "# Let’s take BERT tokenizer as an example. It uses two special tokens, used to delimitate each sequence. [CLS] is always used at the beginning of the first sequence, and [SEP] is added at the end of both the first, and the pair sequences. The final result looks like this:\n",
    "# Then, we specify the template for sentence pairs, which should have the form \"[CLS] $A [SEP] $B [SEP]\" where $A represents the first sentence and $B the second one. The :1 added in the template represent the type IDs we want for each part of our input: it defaults to 0 for everything (which is why we don’t have $A:0) and here we set it to 1 for the tokens of the second sentence and the last \"[SEP]\" token.\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    # The template used for single sequences\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    # The template used when both sequences are specified\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    #  The list of special tokens used in each sequences\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n",
      "[1, 22334, 16, 67, 11, 4162, 5, 5380, 4172, 4787, 0, 35, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_temp = tokenizer.encode(\n",
    "    sequence=\"Hello, y'all! How are you 😁 ?\",  # 未分好词\n",
    "    is_pretokenized=False)\n",
    "\n",
    "print(output_temp.tokens)\n",
    "print(output_temp.ids)\n",
    "print(output_temp.type_ids)\n",
    "print(output_temp.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'a', 'pre', 'token', 'ized', 'sequence', '[SEP]', 'and', 'its', 'pair', '[SEP]']\n",
      "[1, 43, 4305, 26436, 4779, 7436, 2, 4084, 4242, 5685, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_pair_temp = tokenizer.encode(\n",
    "    sequence=[\"A\", \"pre\", \"tokenized\", \"sequence\"],  # 已经分好词\n",
    "    pair=[\"And\", \"its\", \"pair\"],\n",
    "    # Whether the input is already pre-tokenized\n",
    "    is_pretokenized=True\n",
    ")\n",
    "\n",
    "print(output_pair_temp.tokens)\n",
    "print(output_pair_temp.ids)\n",
    "print(output_pair_temp.type_ids)\n",
    "print(output_pair_temp.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=15, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
      "\n",
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', '[SEP]']\n",
      "[1, 22334, 16, 67, 11, 4162, 5, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "['[CLS]', 'how', 'are', 'you', '[UNK]', '?', ',', 'i', 'am', 'fine', '!', 'thank', 'you', '!', '[SEP]']\n",
      "[1, 5380, 4172, 4787, 0, 35, 16, 51, 4099, 7936, 5, 16593, 4787, 5, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_batch = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you 😁 ?, i am fine ! thank you!\"])\n",
    "print(output_batch, end='\\n\\n')\n",
    "\n",
    "# 第一个句子\n",
    "print(output_batch[0].tokens)  # 长度为8\n",
    "print(output_batch[0].ids)\n",
    "print(output_batch[0].type_ids)\n",
    "print(output_batch[0].attention_mask, end='\\n\\n')\n",
    "\n",
    "# 第二个句\n",
    "print(output_batch[1].tokens)  # 长度为15\n",
    "print(output_batch[1].ids)\n",
    "print(output_batch[1].type_ids)\n",
    "print(output_batch[1].attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.token_to_id('[PAD]'))\n",
    "\n",
    "# Enable the padding\n",
    "tokenizer.enable_padding(\n",
    "    pad_id=3,  # The id to be used when padding\n",
    "    pad_token=\"[PAD]\",  # The pad token to be used when padding\n",
    "    pad_type_id=0)  # The type id to be used when padding\n",
    "\n",
    "# Enable truncation\n",
    "tokenizer.enable_truncation(\n",
    "    max_length=10)  # 截断的最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
      "\n",
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', '[SEP]', '[PAD]', '[PAD]']\n",
      "[1, 22334, 16, 67, 11, 4162, 5, 2, 3, 3]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "\n",
      "['[CLS]', 'how', 'are', 'you', '[UNK]', '?', ',', 'i', 'am', '[SEP]']\n",
      "[1, 5380, 4172, 4787, 0, 35, 16, 51, 4099, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_batch_padding = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you 😁 ?, i am fine ! thank you!\"])\n",
    "print(output_batch_padding, end='\\n\\n')\n",
    "\n",
    "# 第一个句子(编码结果长度为10,不足部分通过'[PAD']填充)\n",
    "print(output_batch_padding[0].tokens)\n",
    "print(output_batch_padding[0].ids)\n",
    "print(output_batch_padding[0].type_ids)\n",
    "print(output_batch_padding[0].attention_mask, end='\\n\\n')\n",
    "\n",
    "# 第二个句子\n",
    "print(output_batch_padding[1].tokens)\n",
    "print(output_batch_padding[1].ids)\n",
    "print(output_batch_padding[1].type_ids)\n",
    "print(output_batch_padding[1].attention_mask)  # 截断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# WordPiece Decoder\n",
    "tokenizer.decoder = decoders.WordPiece(prefix='##')  # 选择解码器(根据Pre-tokenizers来选择)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"steward, y ' ts!\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the given list of ids back to a string\n",
    "tokenizer.decode([1, 22477, 16, 67, 11, 4190, 5, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save the :class:`~tokenizers.Tokenizer` to the file at the given path.\n",
    "tokenizer.save(\"../extra_dataset/save_train_tokenizer/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x1c4aaa44830>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a new :class:`~tokenizers.Tokenizer` from the file at the given path.\n",
    "tokenizer_load = Tokenizer.from_file(\"../extra_dataset/save_train_tokenizer/tokenizer-wiki.json\")\n",
    "tokenizer_load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
