{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "LABEL = data.Field(sequential=False,\n",
    "                   use_vocab=True) # 若字段Sentiment已经为数字,则这里可以设置use_vocal=False\n",
    "TEXT = data.Field(sequential=True, lower=True,\n",
    "                  dtype=torch.float32, # Example的数据类型;默认torch.long,一般不用进行设置\n",
    "                  include_lengths=True, # Whether to return a tuple of a padded minibatch and a list containing the lengths of each examples, or just a padded minibatch. Default: False.\n",
    "                  fix_length=10) #  A fixed length that all examples using this field will be padded to, or None for flexible sequence lengths. Default: None.\n",
    "\n",
    "train, val = data.TabularDataset.splits(path='test_text', train='train.csv', validation='val.csv',\n",
    "                                        format='csv', skip_header=True,\n",
    "                                        fields=[('PhraseId', None), ('SentenceId', None),\n",
    "                                                ('Phrase', TEXT), ('Sentiment', LABEL)])\n",
    "\n",
    "TEXT.build_vocab(train, vectors='glove.6B.100d', vectors_cache='vector_cache/')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator = data.Iterator.splits(datasets=(train, val),\n",
    "                                                      # Whether to shuffle examples between epochs\n",
    "                                                      shuffle=False,\n",
    "                                                      device=device,\n",
    "                                                      batch_sizes=(11, 5)) # 每个批次含Example(句子)的个数,其中train_iterator批次大小为10,valid_iterator批次大小为5;若设置参数batch_size=11,则批次大小均为11"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# 与上使用splits方法等价;若需对不同数据集设置不同的参数,则需分别进行设置(即采用下面的设置方法)\n",
    "train_iterator_ = data.Iterator(dataset=train, device=device, batch_size=11, shuffle=False)\n",
    "valid_iterator_ = data.Iterator(dataset=val, device=device, batch_size=5, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124848\n",
      "31212\n",
      "*****************\n",
      "11350\n",
      "6243\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Iterator __len__魔法方法的实现:\n",
    "def __len__(self):\n",
    "    if self.batch_size_fn is not None:\n",
    "        raise NotImplementedError\n",
    "    return math.ceil(len(self.dataset) / self.batch_size)\n",
    "\n",
    "'''\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print('*****************')\n",
    "print(len(train_iterator))\n",
    "print(len(valid_iterator))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.batch.Batch'>\n",
      "\n",
      "\n",
      "[torchtext.data.batch.Batch of size 11]\n",
      "\t[.Phrase]:('[torch.cuda.FloatTensor of size 10x11 (GPU 0)]', '[torch.cuda.FloatTensor of size 11 (GPU 0)]')\n",
      "\t[.Sentiment]:[torch.cuda.LongTensor of size 11 (GPU 0)]\n",
      "\n",
      "(tensor([[2.0000e+00, 6.6600e+02, 1.4200e+02, 1.7700e+02, 1.3000e+01, 9.0000e+01,\n",
      "         2.7100e+02, 1.2541e+04, 1.2100e+02, 4.0000e+00, 1.1000e+01],\n",
      "        [1.6850e+03, 1.1049e+04, 1.0000e+01, 5.5400e+02, 9.0000e+00, 7.0000e+00,\n",
      "         7.6000e+01, 1.0000e+00, 2.3000e+01, 1.1200e+03, 2.0000e+00],\n",
      "        [5.3700e+02, 1.5341e+04, 2.4640e+03, 4.3000e+01, 1.3876e+04, 1.1600e+02,\n",
      "         1.0000e+00, 1.0000e+00, 2.1000e+01, 6.8100e+02, 3.2000e+01],\n",
      "        [5.0000e+00, 1.0000e+00, 8.0000e+00, 6.3160e+03, 2.2700e+02, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 5.4000e+01, 1.0000e+00, 3.8800e+02],\n",
      "        [4.1980e+03, 1.0000e+00, 1.0000e+00, 2.1640e+03, 5.0000e+01, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 6.0000e+00],\n",
      "        [6.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.1439e+04],\n",
      "        [1.2652e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.6800e+02, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.4600e+02],\n",
      "        [4.7900e+02, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.8000e+01, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "        [1.5589e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 3.2950e+03, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "        [2.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 8.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       device='cuda:0'), tensor([10.,  3.,  4.,  5., 10.,  3.,  2.,  1.,  4.,  3.,  7.],\n",
      "       device='cuda:0'))\n",
      "\n",
      "tensor([[2.0000e+00, 6.6600e+02, 1.4200e+02, 1.7700e+02, 1.3000e+01, 9.0000e+01,\n",
      "         2.7100e+02, 1.2541e+04, 1.2100e+02, 4.0000e+00, 1.1000e+01],\n",
      "        [1.6850e+03, 1.1049e+04, 1.0000e+01, 5.5400e+02, 9.0000e+00, 7.0000e+00,\n",
      "         7.6000e+01, 1.0000e+00, 2.3000e+01, 1.1200e+03, 2.0000e+00],\n",
      "        [5.3700e+02, 1.5341e+04, 2.4640e+03, 4.3000e+01, 1.3876e+04, 1.1600e+02,\n",
      "         1.0000e+00, 1.0000e+00, 2.1000e+01, 6.8100e+02, 3.2000e+01],\n",
      "        [5.0000e+00, 1.0000e+00, 8.0000e+00, 6.3160e+03, 2.2700e+02, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 5.4000e+01, 1.0000e+00, 3.8800e+02],\n",
      "        [4.1980e+03, 1.0000e+00, 1.0000e+00, 2.1640e+03, 5.0000e+01, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 6.0000e+00],\n",
      "        [6.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.1439e+04],\n",
      "        [1.2652e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.6800e+02, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.4600e+02],\n",
      "        [4.7900e+02, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.8000e+01, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "        [1.5589e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 3.2950e+03, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "        [2.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 8.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([10.,  3.,  4.,  5., 10.,  3.,  2.,  1.,  4.,  3.,  7.],\n",
      "       device='cuda:0')\n",
      "\n",
      "torch.Size([10, 11])\n",
      "\n",
      "tensor([3, 1, 2, 2, 4, 1, 2, 1, 2, 1, 1], device='cuda:0')\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    print(type(batch), end='\\n\\n')\n",
    "    print(batch, end='\\n\\n')\n",
    "    print(batch.Phrase, end='\\n\\n') # 返回值为元组(由于include_lengths=True)\n",
    "    print(batch.Phrase[0], end='\\n\\n') # 元素为对应单词在单词表中的id,元素1对应'<pad>'表示填充\n",
    "    print(batch.Phrase[1], end='\\n\\n') # lengths of each examples(若设置include_lengths=Flase,则不含此项)\n",
    "    print(batch.Phrase[0].shape, end='\\n\\n') # 由于fix_length=10,batch_size=11,故batch.Phrase.shape=(10, 11)\n",
    "    print(batch.Sentiment)\n",
    "    print(batch.Sentiment.dtype)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,1685,537,5,4198,6,12652,479,15589,2,13247,1305,28,2,113,4308,6,\n",
      "666,11049,15341,\n",
      "142,10,2464,8,\n",
      "177,554,43,6316,2164,\n"
     ]
    }
   ],
   "source": [
    "# 可以看出,上面batch.Phrase每列元素对应一个Example(所有:fix_length=None;部分:根据fix_length的大小进行截取)单词在单词表中的id\n",
    "for i in range(4):\n",
    "    phrase = train.examples[i].__dict__['Phrase']\n",
    "    for j in phrase:\n",
    "        print(TEXT.vocab.stoi[j], end=',')\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}