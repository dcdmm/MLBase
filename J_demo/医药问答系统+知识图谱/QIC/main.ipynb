{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-31T07:57:35.021665Z",
     "start_time": "2024-10-31T07:57:30.778049Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "from model import Model"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-31T07:57:35.072327Z",
     "start_time": "2024-10-31T07:57:35.037290Z"
    }
   },
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "seed = 2022\n",
    "set_seed(seed)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-31T07:57:35.166666Z",
     "start_time": "2024-10-31T07:57:35.151037Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T07:57:35.198315Z",
     "start_time": "2024-10-31T07:57:35.182684Z"
    }
   },
   "source": [
    "label_to_id = {'病情诊断': 0, '病因分析': 1, '治疗方案': 2, '就医建议': 3, \n",
    "               '指标解读': 4, '疾病表述': 5, '后果表述': 6, '注意事项': 7,\n",
    "               '功效作用': 8, '医疗费用': 9, '其他': 10}"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-31T07:57:35.245193Z",
     "start_time": "2024-10-31T07:57:35.213943Z"
    }
   },
   "source": [
    "with open('KUAKE-QIC/KUAKE-QIC_train.json', encoding='utf-8') as f:\n",
    "    data_train = json.load(f)\n",
    "\n",
    "with open('KUAKE-QIC/KUAKE-QIC_dev.json', encoding='utf-8') as f:\n",
    "    data_valid = json.load(f)\n",
    "\n",
    "train = pd.DataFrame(data_train).iloc[:, 1:]\n",
    "train['label'] = train['label'].map(label_to_id)\n",
    "train = train.values.tolist()\n",
    "valid = pd.DataFrame(data_valid).iloc[:, 1:]\n",
    "valid['label'] = valid['label'].map(label_to_id)\n",
    "valid = valid.values.tolist()\n",
    "valid[:5]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['心肌缺血如何治疗与调养呢？', 2],\n",
       " ['19号来的月经，25号服用了紧急避孕药本月5号，怎么办？', 2],\n",
       " ['什么叫痔核脱出？什么叫外痔？', 5],\n",
       " ['您好，请问一岁三个月的孩子可以服用复方锌布颗粒吗？', 10],\n",
       " ['多发乳腺结节中药能治愈吗', 5]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-31T07:57:40.316182Z",
     "start_time": "2024-10-31T07:57:35.277451Z"
    }
   },
   "source": [
    "model_ckpt = \"nghuyong/ernie-health-zh\"\n",
    "\n",
    "token = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "print(token.model_input_names)\n",
    "pretrained = AutoModel.from_pretrained(model_ckpt)\n",
    "print(pretrained.num_parameters())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duanm\\anaconda3\\envs\\nlp_base\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "Some weights of ErnieModel were not initialized from the model checkpoint at nghuyong/ernie-health-zh and are newly initialized: ['ernie.pooler.dense.bias', 'ernie.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103404288\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T07:57:40.363050Z",
     "start_time": "2024-10-31T07:57:40.347426Z"
    }
   },
   "source": [
    "def get_collate_fn(tokenizer, max_len=512):\n",
    "    \"\"\"返回collate_fun函数(通过闭包函数引入形参)\"\"\"\n",
    "\n",
    "    def collate_fn(data):\n",
    "        sents = [i[0] for i in data]\n",
    "        labels = [i[1] for i in data]\n",
    "\n",
    "        # 批量编码句子\n",
    "        text_t = tokenizer(text=sents,\n",
    "                           truncation=True,\n",
    "                           padding=True,\n",
    "                           max_length=max_len,\n",
    "                           return_token_type_ids=True,\n",
    "                           return_attention_mask=True,\n",
    "                           return_tensors='pt')\n",
    "\n",
    "        input_ids = text_t['input_ids']\n",
    "        attention_mask = text_t['attention_mask']\n",
    "        token_type_ids = text_t['token_type_ids']\n",
    "        labels = torch.LongTensor(labels)\n",
    "        return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "# 数据处理\n",
    "dataloader_valid = torch.utils.data.DataLoader(dataset=valid,\n",
    "                                               batch_size=32,\n",
    "                                               shuffle=False,\n",
    "                                               collate_fn=get_collate_fn(token))\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset=train,\n",
    "                                               batch_size=32,\n",
    "                                               shuffle=True,\n",
    "                                               collate_fn=get_collate_fn(token))\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(dataloader_train):\n",
    "    print(input_ids.shape)\n",
    "    print(labels.shape)\n",
    "    break\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 33])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T07:57:41.028837Z",
     "start_time": "2024-10-31T07:57:40.409819Z"
    }
   },
   "source": [
    "# 损失函数\n",
    "criterion_cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model_bert_base = Model(copy.deepcopy(pretrained))  # 必须进行深拷贝(pretrained(模型子网络结构)会参与梯度更新)\n",
    "model_bert_base = model_bert_base.to(device)  # 模型设备切换\n",
    "\n",
    "# 优化器\n",
    "optimizer_adamw = torch.optim.AdamW(model_bert_base.parameters(), lr=2e-5)  # 模型设备切换必须在优化器定义前执行"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T07:57:41.075713Z",
     "start_time": "2024-10-31T07:57:41.044464Z"
    }
   },
   "source": [
    "# 模型训练\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    for idx, (input_ids, attention_mask, token_type_ids, labels) in enumerate(dataloader):\n",
    "        # 数据设备切换\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "\n",
    "        loss = criterion(out, labels)  # 每个step的损失值\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 20 == 0 and idx > 0:\n",
    "            predict = out.argmax(dim=1).cpu().numpy()\n",
    "            accuracy = accuracy_score(labels.cpu().numpy(), predict)  # 评估指标\n",
    "            print('| step {:5d} | loss {:8.5f} | accuracy {:8.5f} |'.format(idx, loss.item(), accuracy))\n",
    "\n",
    "\n",
    "# 模型验证\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    predict_list = []\n",
    "    y_true_list = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, token_type_ids, labels in dataloader:\n",
    "            # 数据设备切换\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "            predict_list.append(out.cpu())\n",
    "            y_true_list.extend(labels.tolist())\n",
    "\n",
    "    predict_all = torch.cat(predict_list, dim=0)  # 合并所有批次的预测结果\n",
    "    y_true_all = torch.tensor(y_true_list)  # 真实标签\n",
    "    accuracy = accuracy_score(y_true_all.numpy(), predict_all.argmax(dim=1).numpy())  # 验证数据集准确率\n",
    "    return accuracy"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T08:09:34.610364Z",
     "start_time": "2024-10-31T07:57:41.091725Z"
    }
   },
   "source": [
    "best_valid_acc = 0.0 \n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model_bert_base, dataloader_train, criterion_cross_entropy, optimizer_adamw, device)\n",
    "    valid_acc = evaluate(model_bert_base, dataloader_valid, device)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model_bert_base.state_dict(), 'torch_model.bin')\n",
    "    print('-' * 62)\n",
    "    print('| end of epoch {:5d} | time: {:5.2f}s | valid accuracy {:8.5f} |'.format(epoch, \n",
    "                                                                                    time.time() - epoch_start_time, \n",
    "                                                                                    valid_acc))\n",
    "    print('-' * 62)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step    20 | loss  2.28665 | accuracy  0.40625 |\n",
      "| step    40 | loss  2.17666 | accuracy  0.37500 |\n",
      "| step    60 | loss  2.13138 | accuracy  0.40625 |\n",
      "| step    80 | loss  1.99772 | accuracy  0.62500 |\n",
      "| step   100 | loss  2.05464 | accuracy  0.50000 |\n",
      "| step   120 | loss  1.85924 | accuracy  0.68750 |\n",
      "| step   140 | loss  1.76947 | accuracy  0.78125 |\n",
      "| step   160 | loss  1.91102 | accuracy  0.65625 |\n",
      "| step   180 | loss  2.07714 | accuracy  0.46875 |\n",
      "| step   200 | loss  2.02115 | accuracy  0.53125 |\n",
      "--------------------------------------------------------------\n",
      "| end of epoch     1 | time: 119.34s | valid accuracy  0.70639 |\n",
      "--------------------------------------------------------------\n",
      "| step    20 | loss  1.72270 | accuracy  0.81250 |\n",
      "| step    40 | loss  1.88285 | accuracy  0.68750 |\n",
      "| step    60 | loss  1.69196 | accuracy  0.87500 |\n",
      "| step    80 | loss  1.84011 | accuracy  0.71875 |\n",
      "| step   100 | loss  1.72644 | accuracy  0.81250 |\n",
      "| step   120 | loss  1.82838 | accuracy  0.75000 |\n",
      "| step   140 | loss  1.71038 | accuracy  0.84375 |\n",
      "| step   160 | loss  1.65951 | accuracy  0.90625 |\n",
      "| step   180 | loss  1.95025 | accuracy  0.59375 |\n",
      "| step   200 | loss  1.76904 | accuracy  0.78125 |\n",
      "--------------------------------------------------------------\n",
      "| end of epoch     2 | time: 150.71s | valid accuracy  0.75908 |\n",
      "--------------------------------------------------------------\n",
      "| step    20 | loss  1.73948 | accuracy  0.81250 |\n",
      "| step    40 | loss  1.73496 | accuracy  0.81250 |\n",
      "| step    60 | loss  1.82552 | accuracy  0.71875 |\n",
      "| step    80 | loss  1.63979 | accuracy  0.90625 |\n",
      "| step   100 | loss  1.80509 | accuracy  0.71875 |\n",
      "| step   120 | loss  1.67463 | accuracy  0.87500 |\n",
      "| step   140 | loss  1.80599 | accuracy  0.75000 |\n",
      "| step   160 | loss  1.74642 | accuracy  0.81250 |\n",
      "| step   180 | loss  1.67992 | accuracy  0.87500 |\n",
      "| step   200 | loss  1.67242 | accuracy  0.87500 |\n",
      "--------------------------------------------------------------\n",
      "| end of epoch     3 | time: 148.52s | valid accuracy  0.79795 |\n",
      "--------------------------------------------------------------\n",
      "| step    20 | loss  1.82446 | accuracy  0.75000 |\n",
      "| step    40 | loss  1.68711 | accuracy  0.84375 |\n",
      "| step    60 | loss  1.76836 | accuracy  0.78125 |\n",
      "| step    80 | loss  1.67455 | accuracy  0.87500 |\n",
      "| step   100 | loss  1.60769 | accuracy  0.93750 |\n",
      "| step   120 | loss  1.73747 | accuracy  0.81250 |\n",
      "| step   140 | loss  1.78730 | accuracy  0.75000 |\n",
      "| step   160 | loss  1.66874 | accuracy  0.87500 |\n",
      "| step   180 | loss  1.66559 | accuracy  0.87500 |\n",
      "| step   200 | loss  1.63975 | accuracy  0.90625 |\n",
      "--------------------------------------------------------------\n",
      "| end of epoch     4 | time: 145.24s | valid accuracy  0.80051 |\n",
      "--------------------------------------------------------------\n",
      "| step    20 | loss  1.60455 | accuracy  0.93750 |\n",
      "| step    40 | loss  1.66914 | accuracy  0.87500 |\n",
      "| step    60 | loss  1.68410 | accuracy  0.84375 |\n",
      "| step    80 | loss  1.69647 | accuracy  0.84375 |\n",
      "| step   100 | loss  1.70334 | accuracy  0.84375 |\n",
      "| step   120 | loss  1.75206 | accuracy  0.78125 |\n",
      "| step   140 | loss  1.63310 | accuracy  0.90625 |\n",
      "| step   160 | loss  1.66740 | accuracy  0.87500 |\n",
      "| step   180 | loss  1.66298 | accuracy  0.87500 |\n",
      "| step   200 | loss  1.68541 | accuracy  0.84375 |\n",
      "--------------------------------------------------------------\n",
      "| end of epoch     5 | time: 149.69s | valid accuracy  0.79949 |\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T08:09:47.755412Z",
     "start_time": "2024-10-31T08:09:34.643370Z"
    }
   },
   "source": [
    "model_best = Model(copy.deepcopy(pretrained))\n",
    "model_best.load_state_dict(torch.load('torch_model.bin'))\n",
    "model_best = model_best.to(device)\n",
    "best_valid_acc = evaluate(model_best, dataloader_valid, device)\n",
    "best_valid_acc"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duanm\\AppData\\Local\\Temp\\ipykernel_60096\\400932493.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_best.load_state_dict(torch.load('torch_model.bin'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8005115089514067"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T08:09:47.850656Z",
     "start_time": "2024-10-31T08:09:47.834844Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3812",
   "language": "python",
   "name": "env_3812"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
