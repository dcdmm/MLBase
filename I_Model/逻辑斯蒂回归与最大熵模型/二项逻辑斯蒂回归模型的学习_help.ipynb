{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;考虑二分类问题,其输出标记$ y \\in \\{ 0, 1\\} $,而线性回归模型产生的预测值$ z= \\mathbf{x}^T \\mathbf{w} + b $(这里$\\mathbf{x}^T$为矩阵行向量,$\\mathbf{w}$为矩阵列向量),由\"对数几率回归\"(logistic regression)(使用线性回归模型的预测结果去逼近真实标记的对数几率,对数几率函数是一种\"Sigmoid\"函数)可知\n",
    "\\begin{align}\n",
    "p(y=1|\\mathbf{x}) &= \\frac{e^{\\mathbf{x}^T \\mathbf{w} + b}}{1 + e^{\\mathbf{x}^T \\mathbf{w} + b}}  \\\\\n",
    "p(y=0|\\mathbf{x}) &= \\frac{1}{1 + e^{\\mathbf{x}^T \\mathbf{w} + b}} \n",
    "\\end{align}     \n",
    "\n",
    "&emsp;&emsp;于是,我们可通过\"极大似然法\"(maximum likelihood method)来估计$\\mathbf{w}$和$b$.给定数据集$ \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{m} $,对率回归\n",
    "模型最大化\"对数似然(log-likelihood)\"     \n",
    "$$ l(\\mathbf{w}, b)  = \\sum_{i=1}^{m} \\ln p(y_i | \\mathbf{x}_i; \\mathbf{w}, b) $$   \n",
    "即令每个样本属于其真实标记的概率越大越好.为便于讨论,令$ \\beta = (\\mathbf{w};b), \\hat{\\mathbf{x}} = (\\mathbf{x};1) $,则$ \\mathbf{x}^T \\mathbf{w} + b $可\n",
    "简写为$ \\hat{\\mathbf{x}}^T \\beta $.再令$ p_1(\\hat{\\mathbf{x}};\\beta) = p(y=1|\\hat{\\mathbf{x}};\\beta)$,$p_0(\\hat{\\mathbf{x}};\\beta) = p(y=0|\\hat{\\mathbf{x}};\\beta)= 1- p_1(\\hat{\\mathbf{x}};\\beta)$,则上式似然项(真实标记的概率)可重写为     \n",
    "$$ p_1(\\hat{\\mathbf{x}}_i; \\beta)^{y_i} p_0(\\hat{\\mathbf{x}}_i, \\beta)^{1-y_i} $$     \n",
    "取对数后有         \n",
    "$$ \\ln p(y_i|\\mathbf{x}_i;\\mathbf{w},b) = y_i \\ln p_1(\\hat{\\mathbf{x}}_i; \\beta) + (1-y_i) \\ln  p_0(\\hat{\\mathbf{x}}_i, \\beta) $$   \n",
    "则最终的对数似然函数可化简为     \n",
    "$$ l(\\beta) = \\sum_{i=1}^{m} (y_i \\hat{\\mathbf{x}}_i^T \\beta - \\ln(1+ e^{\\hat{\\mathbf{x}}_i^T \\beta}))  $$   \n",
    "&emsp;&emsp;对率回归模型的损失函数也可定义为\n",
    "     \n",
    "\\begin{equation}\n",
    "J(p_1(\\hat{\\mathbf{x}};\\beta), y)=\\begin{cases}\n",
    "\t\t-\\ln(p_1(\\hat{\\mathbf{x}};\\beta))=  \\ln(1 + e^{-(\\mathbf{x}^T \\mathbf{w} + b)}) & \\text{if} \\quad  y=1 \\qquad 此时\\mathbf{x}^T \\mathbf{w} + b 越大越好 \\\\\n",
    "        -\\ln(1 - p_1(\\hat{\\mathbf{x}};\\beta)) = \\ln(1 + e^{-(-(\\mathbf{x}^T \\mathbf{w} + b))}) & \\text{if} \\quad  y=0 \\\\\n",
    "     \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "合并后损失(交叉熵)函数为:$ -y_i \\ln p_1(\\hat{\\mathbf{x}}_i; \\beta) - (1-y_i) \\ln p_0(\\hat{\\mathbf{x}}_i, \\beta), \\quad y=0 \\, or \\, 1$\n",
    "\n",
    "&emsp;&emsp;当模型是条件概率分布时,若损失函数是对数损失函数,经验风险最小化等价于极大似然估计.故对数几率回归的损失函数(对率回归模型下,交叉熵损失为凸函数,而均方误差损失函数不为凸函数)可定义为\n",
    "$$ J(\\beta) = \\frac{1}{m} \\sum_{i=1}^{m} (-y_i \\hat{\\mathbf{x}}_i^T \\beta + \\ln(1+ e^{\\hat{\\mathbf{x}}_i^T \\beta}))  $$     \n",
    "转换为矩阵形式有    \n",
    "$$ J(\\beta) = \\frac{1}{m} \\sum ( -\\mathbf{y} \\times X \\mathbf{\\beta} + \\ln(1+ e^{X \\mathbf{\\beta}})) \\qquad其中:\\sum表示向量所有元素之和,\\times表示向量对应元素相乘 $$     \n",
    "&emsp;&emsp;对数几率回归的损失函数是关于$\\beta$的高阶可导连续凸函数(指数和的对数是凸函数),根据凸优化理论,经典的数值优化算法如梯度下降法(gradient descent method),牛顿法(Newton method)等都可求得其最优解,于是就得到      \n",
    "$$ \\beta^* = \\arg \\min_{\\beta} (\\beta) $$     \n",
    "&emsp;&emsp;以牛顿法为例,其第$t+1$轮迭代解的更新公式为      \n",
    "$$ \\beta^{t+1} = \\beta^{t} - \\left(\\frac{\\partial J(\\beta)}{\\partial \\beta \\partial \\beta^T}\\right)^{-1} \\frac{\\partial J(\\beta)}{\\partial \\beta} $$           \n",
    "其中关于$\\beta$的梯度向量为:     \n",
    "$$ \\frac{\\partial J(\\beta)}{\\partial \\beta} = \\frac{1}{m} \\sum_{i=1}^{m} \\hat{\\mathbf{x}}_i^T \\left (\\frac{e^{\\hat{\\mathbf{x}}_i^T \\beta}}{1+e^{\\hat{\\mathbf{x}}_i^T \\beta}}-y_i \\right)   $$    \n",
    "转换为矩阵形式有            \n",
    "$$ \\frac{\\partial J(\\beta)}{\\partial \\beta} = \\frac{1}{m} X^T \\left (\\frac{e^{ X \\mathbf{\\beta}}}{1+e^{X  \\mathbf{\\beta}}}- \\mathbf{y} \\right)   $$    \n",
    "关于$\\beta$的Hessian matrix为:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta \\partial \\beta^T} &= \\frac{\\partial \\left(   \\frac{1}{m} \\sum_{i=1}^{m} \\hat{\\mathbf{x}}_i^T \\left (\\frac{e^{\\hat{\\mathbf{x}}_i^T \\beta}}{1+e^{\\hat{\\mathbf{x}}_i^T \\beta}}-y_i \\right) \\right) }{\\partial \\beta^T}\\\\\n",
    "&= \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial \\left( \\hat{\\mathbf{x}}_i^T \\frac{e^{\\hat{\\mathbf{x}}_i^T \\beta}}{1+e^{\\hat{\\mathbf{x}}_i^T \\beta}} \\right)}{\\partial \\beta^T}\\\\\n",
    " &=\\frac{1}{m}\\sum_{i=1}^{m}\\hat{\\mathbf{x}}_i^T \\hat{\\mathbf{x}}_i \\frac{e^{\\hat{\\mathbf{x}}_i^T \\beta}}{1+e^{\\hat{\\mathbf{x}}_i^T \\beta}} \\frac{1}{1+e^{\\hat{\\mathbf{x}}_i^T \\beta}} \\\\\n",
    " &=\\frac{1}{m}\\sum_{i=1}^{m}\\hat{\\mathbf{x}}_i^T \\hat{\\mathbf{x}}_i  p_1(\\hat{\\mathbf{x}};\\beta) (1- p_1(\\hat{\\mathbf{x}};\\beta))\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}