{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74db4254-fe76-4c40-9969-551313a1a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset\n",
    "import collections\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77794e88-5662-4a06-9fd4-7f4b8f551dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"stanfordnlp/imdb\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05994a5c-3616-4bca-bf3f-2087d9a5e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"\n",
    "    Vocabulary for text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens=None, min_freq=2, reserved_tokens=None):\n",
    "        # tokens: 单词tokens\n",
    "        # min_freq: The minimum frequency needed to include a token in the vocabulary.\n",
    "        # reserved_tokens: 自定义tokens\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        counter = collections.Counter(tokens)\n",
    "        # Sort according to frequencies\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)  # 未在字典中则返回'<unk>'\n",
    "        return [self.__getitem__(token) for token in tokens]  # 递归\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        \"\"\"第indices位置处的token\"\"\"\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        \"\"\"Index for the unknown token\"\"\"\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "split_list = []\n",
    "for i in data['train']:\n",
    "    split_list.extend([tok.text for tok in spacy_en.tokenizer(i['text'])])\n",
    "\n",
    "vocab = Vocab(split_list, min_freq=1, reserved_tokens=['<pad>', '<SOS>', '<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7bf8e8c-d50f-44d1-bcd4-86b9f9c19b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b21b58253b7469eaffe108161f9f314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 200])\n",
      "tensor([[-0.0715,  0.0935,  0.0237,  ...,  0.3362,  0.0306,  0.2558],\n",
      "        [ 0.1765,  0.2921, -0.0021,  ..., -0.2077, -0.2319, -0.1081],\n",
      "        [ 0.1229,  0.5804, -0.0696,  ..., -0.0392, -0.1624, -0.0967],\n",
      "        ...,\n",
      "        [-0.0020,  0.0202, -0.0244,  ...,  0.0142, -0.8224, -0.3703],\n",
      "        [ 0.1291, -0.2605,  0.0139,  ...,  0.1384, -0.0146,  0.4337],\n",
      "        [-0.7300,  0.5164, -0.5798,  ...,  0.3581,  1.1576,  0.2573]])\n"
     ]
    }
   ],
   "source": [
    "class Vectors:\n",
    "    def __init__(self, name, max_vectors=None) -> None:\n",
    "        self.vectors = None\n",
    "        self.name = name\n",
    "        self.max_vectors = max_vectors\n",
    "        self.itos = None\n",
    "        self.stoi = None\n",
    "        self.cache()\n",
    "\n",
    "    def cache(self):\n",
    "        with open(self.name, \"r\", encoding='utf-8') as f:\n",
    "            read_value = f.readlines()\n",
    "\n",
    "        all_value, itos = [], []\n",
    "        for i in tqdm(range(len(read_value))):\n",
    "            l_split = read_value[i].split(' ')\n",
    "            itos.append(l_split[0])\n",
    "            all_value.append([float(i.strip()) for i in l_split[1: ]])\n",
    "        all_value = torch.tensor(all_value)\n",
    "        self.vectors = all_value\n",
    "        self.itos = itos\n",
    "        num_lines = len(self.vectors)\n",
    "        if not self.max_vectors or self.max_vectors > num_lines:\n",
    "            self.max_vectors = num_lines\n",
    "        self.vectors = self.vectors[:self.max_vectors, :]\n",
    "        self.itos = self.itos[:self.max_vectors]\n",
    "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vectors)\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        if token in self.stoi:\n",
    "            return self.vectors[self.stoi[token]]\n",
    "        else:\n",
    "            dim = self.vectors.shape[1]\n",
    "            return torch.Tensor.zero_(torch.Tensor(dim))\n",
    "        \n",
    "    def get_vecs_by_tokens(self, tokens):\n",
    "        indices = [self[token] for token in tokens]\n",
    "        vecs = torch.stack(indices)\n",
    "        return vecs\n",
    "\n",
    "# 预训练词向量\n",
    "vec1 = Vectors(name=\"glove.6B.200d.txt\", max_vectors=25000)\n",
    "print(vec1.vectors.shape)\n",
    "print(vec1.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cc85943-22c9-438c-8ffb-46d21f9b8f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121069, 200])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4552,  0.2761, -0.3108,  ...,  0.6674, -0.2191,  0.3745],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = vec1.get_vecs_by_tokens(vocab.idx_to_token)\n",
    "\n",
    "print(pretrained_embeddings.shape)\n",
    "print(pretrained_embeddings)  # 模型词向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f7a45ea-6928-43af-870d-4c7fa02e4f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_map_style_dataset(iter_data):\n",
    "    r\"\"\"Convert iterable-style dataset to map-style dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    class _MapStyleDataset(Data.Dataset):\n",
    "\n",
    "        def __init__(self, iter_data):\n",
    "            self._data = list(iter_data)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self._data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self._data[idx]\n",
    "\n",
    "    return _MapStyleDataset(iter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "709370cc-b705-4ded-8c25-c2547d47c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = data['train'], data['test'] \n",
    "\n",
    "train_data = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e0a08e2-47ab-4d75-890c-ee69e40c1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(len(train_data) * 0.7)\n",
    "train_dataset, valid_dataset = random_split(train_data,\n",
    "                                            [num_train, len(train_data) - num_train])  # 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6fe0c131-b1b4-4734-ae07-55b90e0404c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform = lambda x: [vocab['<SOS>']] + [vocab[token] for token in\n",
    "                                               [tok.text for tok in spacy_en.tokenizer(x)]] + [vocab['<EOS>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf8c2174-f3d6-4a54-b436-b0ebd3e131c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,     2,     2,  ...,     2,     2,     2],\n",
      "        [   27,  3707,    13,  ...,   597, 53216,   360],\n",
      "        [29471,   102,   251,  ...,   102,     5,  1117],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0]])\n",
      "tensor([130, 128, 714, 234, 148, 336, 464, 541, 325, 118,  78, 143, 298, 145,\n",
      "        176, 259, 251, 184, 166, 171, 237, 151, 139, 393, 201, 308, 141, 370,\n",
      "        235, 370, 387, 230, 155, 135, 162, 363, 430, 192, 161, 116, 131, 835,\n",
      "        215, 480, 157, 241, 148, 231, 149, 124, 262,  77, 710, 301, 345, 474,\n",
      "        248, 167, 533, 224, 180, 120,  88, 531])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    对文本标签和文本内容进行处理使之可以用于pack_padded_sequence操作\n",
    "    Parameters\n",
    "    ---------\n",
    "    batch : 每个batch数据\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    label_tensor : 每个batch数据文本标签的处理输出\n",
    "    text_pad : 每个batch数据文本内容的处理输出\n",
    "    lengths : 每个batch数据文本内容的真实长度\n",
    "    \"\"\"\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for i in batch:\n",
    "        _text, _label = i['text'], i['label']\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_transform(_text))\n",
    "        lengths.append(len(processed_text))\n",
    "        text_list.append(processed_text)\n",
    "    label_tensor = torch.tensor(label_list)\n",
    "    text_pad = pad_sequence(text_list, batch_first=False, padding_value=0)\n",
    "    lengths = torch.tensor(lengths)  # 真实长度\n",
    "    return text_pad, lengths, label_tensor\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True,\n",
    "                              collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "for text, length, label in train_dataloader:\n",
    "    print(text)\n",
    "    print(length)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6afbb949-5a23-4b95-bc09-dfce113a1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_size, embedding_size = pretrained_embeddings.shape\n",
    "hidden_size = 256\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "out_size = 2\n",
    "num_layers = 2\n",
    "lr = 0.001  # 学习率\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e839475-182f-4726-bcae-982b3e734086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textrnn_model_torch import TextRNN\n",
    "%run textrnn_model_torch.py\n",
    "\n",
    "net = TextRNN(vocab_size=vocal_size,\n",
    "              embedding_size=embedding_size,\n",
    "              hidden_size=hidden_size,\n",
    "              num_layers=num_layers,\n",
    "              dropout_ratio=dropout,\n",
    "              bidirectional=True,\n",
    "              out_size=out_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b19f246a-6862-4c7b-963a-5dfbbd92638b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.4552,  0.2761, -0.3108,  ...,  0.6674, -0.2191,  0.3745],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用模型预训练词向量矩阵\n",
    "net.embed.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55f611a6-bfdb-4a80-8ffe-f2e3a131989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "541ad778-2c37-4fee-8f15-774ef2c60451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_evaluate_c import Trainer\n",
    "%run train_evaluate_c.py\n",
    "\n",
    "t_and_v = Trainer(net, optimizer, criterion, 5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae4cae57-fa49-4c02-b2dc-593433ad2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_acc(predict_all, y_true):\n",
    "    predict = predict_all.argmax(-1)\n",
    "    label = y_true\n",
    "    acc = accuracy_score(label, predict)\n",
    "    return {\"acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cd4cdba-5ed5-4e2b-8908-1491e2b94f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  [0    /17500 (0  %)]\tLoss: 0.689401\tacc: 0.625000\n",
      "Train Epoch: 0  [3200 /17500 (18 %)]\tLoss: 0.645733\tacc: 0.625000\n",
      "Train Epoch: 0  [6400 /17500 (36 %)]\tLoss: 0.636727\tacc: 0.578125\n",
      "Train Epoch: 0  [9600 /17500 (55 %)]\tLoss: 0.556295\tacc: 0.703125\n",
      "Train Epoch: 0  [12800/17500 (73 %)]\tLoss: 0.706302\tacc: 0.500000\n",
      "Train Epoch: 0  [16000/17500 (91 %)]\tLoss: 0.547815\tacc: 0.703125\n",
      "Train Epoch: 0  [17500/17500 (100%)]\tLoss: 0.522246\tacc: 0.750000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train Epoch: 1  [0    /17500 (0  %)]\tLoss: 0.638961\tacc: 0.625000\n",
      "Train Epoch: 1  [3200 /17500 (18 %)]\tLoss: 0.628456\tacc: 0.515625\n",
      "Train Epoch: 1  [6400 /17500 (36 %)]\tLoss: 0.483002\tacc: 0.781250\n",
      "Train Epoch: 1  [9600 /17500 (55 %)]\tLoss: 0.340300\tacc: 0.875000\n",
      "Train Epoch: 1  [12800/17500 (73 %)]\tLoss: 0.486037\tacc: 0.828125\n",
      "Train Epoch: 1  [16000/17500 (91 %)]\tLoss: 0.393623\tacc: 0.828125\n",
      "Train Epoch: 1  [17500/17500 (100%)]\tLoss: 0.538384\tacc: 0.821429\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train Epoch: 2  [0    /17500 (0  %)]\tLoss: 0.345571\tacc: 0.890625\n",
      "Train Epoch: 2  [3200 /17500 (18 %)]\tLoss: 0.481906\tacc: 0.750000\n",
      "Train Epoch: 2  [6400 /17500 (36 %)]\tLoss: 0.300785\tacc: 0.859375\n",
      "Train Epoch: 2  [9600 /17500 (55 %)]\tLoss: 0.226440\tacc: 0.906250\n",
      "Train Epoch: 2  [12800/17500 (73 %)]\tLoss: 0.311839\tacc: 0.890625\n",
      "Train Epoch: 2  [16000/17500 (91 %)]\tLoss: 0.330417\tacc: 0.859375\n",
      "Train Epoch: 2  [17500/17500 (100%)]\tLoss: 0.495683\tacc: 0.857143\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train Epoch: 3  [0    /17500 (0  %)]\tLoss: 0.410582\tacc: 0.859375\n",
      "Train Epoch: 3  [3200 /17500 (18 %)]\tLoss: 0.104115\tacc: 0.984375\n",
      "Train Epoch: 3  [6400 /17500 (36 %)]\tLoss: 0.083532\tacc: 0.968750\n",
      "Train Epoch: 3  [9600 /17500 (55 %)]\tLoss: 0.095775\tacc: 0.984375\n",
      "Train Epoch: 3  [12800/17500 (73 %)]\tLoss: 0.093131\tacc: 0.984375\n",
      "Train Epoch: 3  [16000/17500 (91 %)]\tLoss: 0.144165\tacc: 0.953125\n",
      "Train Epoch: 3  [17500/17500 (100%)]\tLoss: 0.137647\tacc: 0.964286\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train Epoch: 4  [0    /17500 (0  %)]\tLoss: 0.173683\tacc: 0.937500\n",
      "Train Epoch: 4  [3200 /17500 (18 %)]\tLoss: 0.104840\tacc: 0.968750\n",
      "Train Epoch: 4  [6400 /17500 (36 %)]\tLoss: 0.173603\tacc: 0.921875\n",
      "Train Epoch: 4  [9600 /17500 (55 %)]\tLoss: 0.302001\tacc: 0.906250\n",
      "Train Epoch: 4  [12800/17500 (73 %)]\tLoss: 0.045167\tacc: 0.984375\n",
      "Train Epoch: 4  [16000/17500 (91 %)]\tLoss: 0.033795\tacc: 1.000000\n",
      "Train Epoch: 4  [17500/17500 (100%)]\tLoss: 0.082543\tacc: 0.964286\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Training loss': [0.5827103853225708,\n",
       "  0.33858340978622437,\n",
       "  0.28198134899139404,\n",
       "  0.08237747848033905,\n",
       "  0.03502501919865608],\n",
       " 'Training acc': [0.646,\n",
       "  0.8652571428571428,\n",
       "  0.9044571428571428,\n",
       "  0.9729142857142857,\n",
       "  0.9910857142857142],\n",
       " 'Validation loss': [0.6017845869064331,\n",
       "  0.4423207640647888,\n",
       "  0.4647025465965271,\n",
       "  0.3755393922328949,\n",
       "  0.40965747833251953],\n",
       " 'Validation acc': [0.6261333333333333,\n",
       "  0.8298666666666666,\n",
       "  0.8545333333333334,\n",
       "  0.8714666666666666,\n",
       "  0.8692]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = t_and_v.train(train_dataloader, valid_dataloader, compute_metrics=compute_metrics_acc, verbose=50)\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e169ab86-27b2-4eb5-8781-d0a9c5938b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    \"\"\"预测句子的评价\"\"\"\n",
    "    model.eval()\n",
    "    processed_text = torch.tensor(text_transform(sentence)).to(device)\n",
    "    processed_text = processed_text.unsqueeze(1)\n",
    "    length = [len(processed_text)]\n",
    "    prediction = torch.sigmoid(model(processed_text, length))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ce83ac2-db7a-4e36-a948-6a1b3bd3d44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6829, 0.3112]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, \"fuck, garbage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1488b96b-eb0c-45ad-b45e-61991cc8ae17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8031, 0.1820]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, \"This film is terrible\")  # 倾向于负面评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2ffbfb7a-ffd9-460b-9e41-f5f333fd7077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0947, 0.9147]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, \"This film is great\")  # 倾向于正面评价"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
