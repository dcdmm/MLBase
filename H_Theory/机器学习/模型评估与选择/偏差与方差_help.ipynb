{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;对测试样本$ \\mathbf{x} $,令$ y_D $ 为在数据集中的标记,$ y $ 为 $ \\mathbf{x}$ 的真实标记,$ f(\\mathbf{x};D) $为训练集$ D $ 上\n",
    "学得模型$ f$在$\\mathbf{x} $上的预测输出.以回归任务为例,学习算法的期望预测为        \n",
    "$$ \\bar{f}(\\mathbf{x}) = E_D[f(\\mathbf{x};D)] $$    \n",
    "使用样本数相同的不同训练集产生的方差为   \n",
    "$$ var(\\mathbf{x}) = E_D[ (f(\\mathbf{x};D) - \\bar{f}(\\mathbf{x}))^2 ] $$   \n",
    "噪声为   \n",
    "$$ \\epsilon^2 = E_D[ (y_D - y)^2] $$   \n",
    "期望输出与真实标记的差别称为偏差(bias),即   \n",
    "$$ bias^2(\\mathbf{x}) = ( \\bar{f}(\\mathbf{x}) - y)^2 $$  \n",
    "为了便于讨论,假定噪声期望为零,即$ E_D[y_D - y] = 0 $.通过简单的多项式展开合并,可对算法的期望泛化误差进行分解:   \n",
    "\n",
    "\\begin{align}\n",
    "E(f;D) &= E_D[(f(\\mathbf{x};D) - y_D)^2]   \\\\\n",
    "  &= E_D[(f(\\mathbf{x};D) - \\bar{f}(\\mathbf{x}) + \\bar{f}(\\mathbf{x}) - y_D ))^2]  \\\\\n",
    "  &= E_D[(f(\\mathbf{x};D) - \\bar{f}(\\mathbf{x}))^2] + E_D[(\\bar{f}(\\mathbf{x}) - y_D)^2] + E_D[2(f(\\mathbf{x};D) - \\bar{f}(\\mathbf{x}))(\\bar{f}(\\mathbf{x}) - y_D)] \\\\\n",
    "  &= E_D[(f(\\mathbf{x};D) - \\bar{f}(\\mathbf{x}))^2] + E_D[(\\bar{f}(\\mathbf{x}) - y_D)^2] \\qquad \\rightarrow f(\\mathbf{x};D)与y_D独立\\\\\n",
    "  &= E_D[(f(\\mathbf{x};D) - \\bar{f}(\\mathbf{x}))^2] + E_D[(\\bar{f}(\\mathbf{x}) - y + y - y_D)^2] \\\\\n",
    "  &= E_D[(f(\\mathbf{x};D) - \\bar{f}(\\mathbf{x}))^2] + E_D[(\\bar{f}(\\mathbf{x}) - y )^2] + E_D[(y-y_D)^2] + 2E_D[(\\bar{f}(\\mathbf{x}) - y)(y-y_D)] \\\\\n",
    "  &= E_D[(f(\\mathbf{x};D) - \\bar{f}(\\mathbf{x}))^2] + (\\bar{f}(\\mathbf{x}) - y )^2 + E_D[(y-y_D)^2]   \n",
    "\\end{align}   \n",
    "于是   \n",
    "$$ E(f;D) = bias^2(\\mathbf{x}) = var(\\mathbf{x}) + \\epsilon^2 $$   \n",
    "也就是说,泛化误差可分解为偏差,方差与噪声之和     \n",
    "&emsp;&emsp;偏差度量了学习算法的期望预测与真实结果的偏离程度,即刻画了学习算法本身的拟合程度;方差度量了同样大小的训练集的变动\n",
    "所导致的学习性能的变化,即刻画了数据扰动所造成的影响;噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界,即刻画了学习问题\n",
    "本身的难度.偏差-方差分解说明,泛化性能是由学习算法的能力,数据的充分性以及学习任务本身的难度所共同决定的.    \n",
    "&emsp;&emsp;一般来说,偏差和方差是有冲突的,这称为偏差-方差窘境(bias-variance dilemma).给定学习任务,假定我们所能控制学习算法的\n",
    "训练程度(即模型复杂度),则在训练不足时,学习器的拟合能力不够强,训练数据的扰动不足以使学习器产生显著变化,此时偏差主导了泛化错误率;随着\n",
    "训练程度的加深,学习器的拟合能力逐渐增加,训练数据发生的扰动渐渐能被学习器学到,方差逐渐主导了泛化错误率;在训练程度充足后,学习器的拟合程度\n",
    "已非常强,训练数据发生的轻微扰动都会导致学习器发生显著变化,若训练数据自身的,非全局的特征被学习器学到了,则将发生过拟合.    \n",
    "\n",
    "<img src=\"../../../Other/img/偏差与方差.jpg\" style=\"width:500px;height:700px;float:bottom\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
