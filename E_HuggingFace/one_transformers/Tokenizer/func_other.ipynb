{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4e653f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d21ea54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dbcb495d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '和', '我', '的', '祖', '国', '一', '刻', '也', '不', '能', '分', '离', '!']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participle = tokenizer.tokenize(\"我和我的祖国一刻也不能分离!\")\n",
    "participle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2de999a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2769, 1469, 2769, 4638, 4862, 1744, 671, 1174, 738, 679, 5543, 1146, 4895, 106]\n",
      "['我', '和', '我', '的', '祖', '国', '一', '刻', '也', '不', '能', '分', '离', '!']\n",
      "我 和 我 的 祖 国 一 刻 也 不 能 分 离!\n"
     ]
    }
   ],
   "source": [
    "# Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the vocabulary.\n",
    "tti = tokenizer.convert_tokens_to_ids(participle)\n",
    "print(tti)\n",
    "\n",
    "# Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and added tokens.\n",
    "itt = tokenizer.convert_ids_to_tokens(tti)\n",
    "print(itt)\n",
    "\n",
    "# Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special tokens and clean up tokenization spaces.\n",
    "print(tokenizer.decode(tti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5e4f46a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 我 和 我 的 祖 国 一 刻 也 不 能 分 离! [SEP]\n",
      "我 和 我 的 祖 国 一 刻 也 不 能 分 离!\n"
     ]
    }
   ],
   "source": [
    "list_of_token = tokenizer(\"我和我的祖国一刻也不能分离!\")['input_ids']\n",
    "\n",
    "print(tokenizer.decode(list_of_token))\n",
    "print(tokenizer.decode(list_of_token, \n",
    "                       # Whether or not to remove special tokens in the decoding.defaults to False\n",
    "                       skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6c8dd5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] 我 和 我 的 祖 国 一 刻 也 不 能 分 离! [SEP]', '[CLS] 1001 问 [SEP]']\n",
      "['我 和 我 的 祖 国 一 刻 也 不 能 分 离!', '1001 问']\n"
     ]
    }
   ],
   "source": [
    "list_of_tokens = tokenizer([\n",
    "    \"我和我的祖国一刻也不能分离!\",\n",
    "    \"1001问\"])['input_ids']\n",
    "\n",
    "# 批量解码\n",
    "print(tokenizer.batch_decode(list_of_tokens))\n",
    "print(tokenizer.batch_decode(list_of_tokens, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
