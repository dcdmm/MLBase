{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "multihead_attn = nn.MultiheadAttention(embed_dim=200,  # E_q(E_q必须能整除num_heads)\n",
    "                                       num_heads=5,  # 注意力的数目\n",
    "                                       # 默认kdim=None(即kdim=embed_dim)\n",
    "                                       kdim=100,  # E_k\n",
    "                                       # 默认vdim=None(即vdim=embed_dim)\n",
    "                                       vdim=50,  # E_v\n",
    "                                       dropout=0.1  # Dropout probability on `attn_output_weights`\n",
    "                                       )\n",
    "\n",
    "mask = torch.arange(4)[None, :] < torch.tensor([2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4])[:, None]\n",
    "mask = torch.unsqueeze(mask, 0)\n",
    "mask = torch.repeat_interleave(mask, 40, dim=0)\n",
    "mask = ~mask\n",
    "\n",
    "query = torch.randn(14, 8, 200)  # query.shape=(L, N, E_q);L is the target sequence length\n",
    "key = torch.randn(4, 8, 100)  # key.shape=(S, N, E_k);S is the source sequence length\n",
    "value = torch.randn(4, 8, 50)  # value.shape=(S, N, E_v)\n",
    "attn_output, attn_output_weights = multihead_attn(query=query,\n",
    "                                                  key=key,\n",
    "                                                  value=value,\n",
    "                                                  # attn_mask.shape=(N * num_heads, L, S)\n",
    "                                                  # a True value indicates that the corresponding position is not allowed to attend\n",
    "                                                  attn_mask=mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([14, 8, 200])"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ★★★★★注意: pytorch输出的是头的平均注意力分数(tensorflow输出的是所有头的注意力分数)\n",
    "\"\"\"\n",
    "内部机制:\n",
    "if need_weights:\n",
    "    # average attention weights over heads\n",
    "    attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "    return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "else:\n",
    "    return attn_output, None\n",
    "\"\"\"\n",
    "# attn_output.shape=(L, N, E_q)\n",
    "attn_output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 14, 4])"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attn_output_weights.shape=(N, L, S)\n",
    "attn_output_weights.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "tensor([[0.4631, 0.6135, 0.0000, 0.0000],\n",
      "        [0.4181, 0.5873, 0.0000, 0.0000],\n",
      "        [0.4142, 0.6970, 0.0000, 0.0000],\n",
      "        [0.3883, 0.7228, 0.0000, 0.0000],\n",
      "        [0.5364, 0.3238, 0.2351, 0.0000],\n",
      "        [0.4114, 0.3255, 0.2799, 0.0000],\n",
      "        [0.3952, 0.3840, 0.2464, 0.0000],\n",
      "        [0.3306, 0.4838, 0.2259, 0.0000],\n",
      "        [0.3868, 0.3762, 0.2670, 0.0000],\n",
      "        [0.1457, 0.4392, 0.2390, 0.2213],\n",
      "        [0.2471, 0.1886, 0.1975, 0.3660],\n",
      "        [0.2645, 0.2381, 0.3361, 0.2382],\n",
      "        [0.3545, 0.2596, 0.1521, 0.2359],\n",
      "        [0.3103, 0.2017, 0.2972, 0.3019]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(mask[0, :, :])\n",
    "print(attn_output_weights[0, :, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_proj_weight.shape= torch.Size([200, 200])\n",
      "k_proj_weight.shape= torch.Size([200, 100])\n",
      "v_proj_weight.shape= torch.Size([200, 50])\n",
      "in_proj_bias.shape= torch.Size([600])\n",
      "out_proj.weight.shape= torch.Size([200, 200])\n",
      "out_proj.bias.shape= torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "for i, j in multihead_attn.named_parameters():\n",
    "    print(str(i) + \".shape=\", j.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}