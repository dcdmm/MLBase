{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask']\n",
      "['[CLS]', '[SEP]', '<unk>', '<pad>', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "主要预训练模型:\n",
    "'albert-base-v2'\n",
    "'albert-xxlarge-v2'\n",
    "'''\n",
    "tokenizer = AutoTokenizer.from_pretrained('albert-base-v2')\n",
    "print(tokenizer.model_input_names)\n",
    "print(tokenizer.all_special_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "AlbertModel(\n  (embeddings): AlbertEmbeddings(\n    (word_embeddings): Embedding(30000, 128, padding_idx=0)\n    (position_embeddings): Embedding(512, 128)\n    (token_type_embeddings): Embedding(2, 128)\n    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (encoder): AlbertTransformer(\n    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n    (albert_layer_groups): ModuleList(\n      (0): AlbertLayerGroup(\n        (albert_layers): ModuleList(\n          (0): AlbertLayer(\n            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (attention): AlbertAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (attention_dropout): Dropout(p=0, inplace=False)\n              (output_dropout): Dropout(p=0, inplace=False)\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            )\n            (ffn): Linear(in_features=768, out_features=3072, bias=True)\n            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): NewGELUActivation()\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (pooler): Linear(in_features=768, out_features=768, bias=True)\n  (pooler_activation): Tanh()\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"albert-base-v2\")\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   2, 3934,   55,   34,  186, 1854,   42,   22,   43,  101,    9,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "[CLS] replace me by any text you'd like.[SEP]\n"
     ]
    }
   ],
   "source": [
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "print(encoded_input.keys())\n",
    "print(tokenizer.decode(encoded_input['input_ids'].tolist()[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0633,  0.6634,  1.2338,  ..., -1.5131, -0.4445,  1.2011],\n",
      "         [-0.2914, -0.5385, -1.6138,  ...,  0.2044,  2.1072, -0.3526],\n",
      "         [ 0.3940,  0.8559, -0.5069,  ...,  0.8633,  0.4893,  0.2798],\n",
      "         ...,\n",
      "         [ 0.4754, -1.4797, -0.7564,  ...,  1.2648,  1.6309,  0.4099],\n",
      "         [ 0.0298,  0.1406,  0.2338,  ..., -0.2372,  0.6055, -0.0437],\n",
      "         [ 0.0726,  0.1270, -0.0512,  ..., -0.0985,  0.1229,  0.2115]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.3040e-01,  1.0424e-01,  3.9711e-01, -4.7384e-01, -1.3758e-02,\n",
      "         -9.8533e-01, -5.0131e-02,  1.2260e-02, -5.6495e-02, -9.9441e-01,\n",
      "          9.4768e-01, -1.4119e-01, -5.6679e-01, -8.5202e-01, -8.8070e-01,\n",
      "          2.3836e-01, -1.9595e-01, -1.6009e-01,  9.9556e-01,  6.8593e-02,\n",
      "         -6.8246e-01, -9.9791e-01,  9.9637e-01,  9.3427e-01,  9.4481e-01,\n",
      "          1.9624e-01, -1.2116e-01, -9.9659e-01, -9.5589e-01,  1.7546e-01,\n",
      "         -9.9503e-01, -4.9911e-02, -5.2337e-02, -1.7399e-01, -4.7024e-02,\n",
      "          2.5040e-01,  2.4297e-02,  9.8834e-01,  7.8854e-02, -2.1457e-02,\n",
      "         -2.1279e-01, -9.8895e-01, -6.3510e-01, -2.0707e-01, -2.1754e-01,\n",
      "         -2.9198e-01,  3.6226e-02, -9.9439e-01,  7.4158e-01,  2.8631e-01,\n",
      "          1.9829e-01,  1.4791e-01,  1.1378e-01, -6.8500e-01, -6.1387e-01,\n",
      "         -6.3339e-02,  1.3852e-01,  1.4212e-01,  9.9989e-01, -9.5492e-01,\n",
      "         -5.7281e-02, -4.1080e-02,  6.0182e-02, -6.2924e-01,  1.5932e-01,\n",
      "         -1.6249e-01, -2.6717e-01,  9.9913e-01,  3.4856e-01,  9.7228e-01,\n",
      "         -2.1798e-01,  1.1642e-02, -5.9979e-01,  7.3010e-03,  9.2392e-01,\n",
      "         -9.6832e-02,  8.4510e-01, -1.8919e-01,  7.8178e-01, -9.9320e-01,\n",
      "          4.9024e-01, -5.1955e-02,  2.9479e-02,  1.0081e-01, -7.1091e-01,\n",
      "         -8.9632e-01, -8.5785e-02, -9.9852e-01, -2.4799e-02,  8.0193e-01,\n",
      "         -1.1089e-01,  8.0454e-02,  7.4114e-02, -9.9972e-01, -2.1198e-01,\n",
      "         -5.2243e-02, -9.9954e-01, -1.6374e-01, -1.7113e-01,  8.5360e-02,\n",
      "          6.3804e-01,  1.5071e-01, -6.2198e-02,  1.0385e-01,  7.7855e-02,\n",
      "          2.0418e-01, -6.8647e-02,  9.7039e-01, -2.1730e-01,  9.9775e-01,\n",
      "         -8.0917e-01,  2.9006e-01, -3.2499e-01,  8.8365e-01,  1.2317e-01,\n",
      "          5.1994e-01,  8.1244e-02,  9.3032e-01, -9.9928e-01, -6.9538e-02,\n",
      "         -1.6410e-01,  9.7568e-02, -1.2148e-01, -7.4609e-01, -1.1170e-01,\n",
      "         -1.9996e-01, -2.6327e-01, -6.4652e-01,  9.8205e-01, -8.4030e-01,\n",
      "          8.2175e-01, -2.6660e-01, -9.5082e-01,  4.0787e-01, -1.1160e-01,\n",
      "          9.9751e-01,  1.8755e-01,  3.2216e-01, -5.1530e-02, -1.9900e-02,\n",
      "          9.5602e-01,  8.4380e-02, -2.2763e-01, -1.7340e-02,  1.6868e-01,\n",
      "         -2.1190e-01,  8.8368e-01,  2.4513e-01, -2.8049e-01,  9.9056e-01,\n",
      "         -9.5177e-01,  9.3813e-01,  6.4960e-01,  5.1082e-01, -9.9371e-01,\n",
      "          3.8080e-01,  9.9112e-01, -8.4105e-01,  9.2692e-01,  1.2104e-01,\n",
      "          2.0366e-01, -9.2437e-01, -9.9752e-01,  1.0286e-01, -9.5947e-01,\n",
      "         -7.6836e-02,  9.9712e-01, -6.8112e-01,  8.3198e-01, -9.5980e-01,\n",
      "          3.4711e-01, -2.7769e-01,  1.0016e-01,  9.3410e-01, -2.3233e-01,\n",
      "         -2.4486e-01, -1.0878e-01,  9.2204e-01, -8.8590e-03,  3.4276e-01,\n",
      "          7.8825e-01, -9.9858e-01, -2.2769e-01, -4.8769e-02,  2.1545e-01,\n",
      "         -1.2009e-01, -6.5136e-02, -8.5835e-01, -8.7286e-02, -4.1550e-02,\n",
      "          2.6171e-01, -9.9077e-01, -2.8487e-02,  7.2256e-01,  9.9412e-01,\n",
      "         -1.7670e-01, -1.9143e-01, -9.6841e-01,  9.3920e-01, -9.9170e-01,\n",
      "          4.1563e-03,  9.6685e-01,  9.3978e-01, -1.2538e-01,  5.3484e-03,\n",
      "          9.3723e-01, -9.6679e-01,  4.7937e-01, -9.9658e-01,  2.0717e-01,\n",
      "         -9.9657e-01,  7.7960e-01, -9.9593e-01, -1.7146e-01, -9.7448e-01,\n",
      "          9.9937e-01,  9.3058e-01,  1.3929e-01,  9.1244e-01,  9.5851e-02,\n",
      "          6.8748e-02, -8.1573e-02, -9.9998e-01,  3.0384e-01, -2.2618e-01,\n",
      "         -6.4892e-02,  6.5703e-01,  8.8592e-02,  9.4796e-01, -9.9780e-01,\n",
      "         -9.2430e-01, -5.2151e-01,  7.9665e-02, -6.7506e-01, -8.7814e-01,\n",
      "          1.0002e-01, -1.0749e-01, -7.4753e-02, -2.8732e-02, -9.9971e-01,\n",
      "          9.9923e-01, -1.6980e-01,  2.2332e-01,  9.6405e-02, -6.1845e-01,\n",
      "          9.8297e-01, -7.8361e-02, -8.6407e-01, -2.6640e-02,  9.9739e-01,\n",
      "          9.9426e-01, -5.2483e-02,  5.4563e-03,  1.5402e-01,  4.4536e-01,\n",
      "          2.8992e-01, -9.9268e-01, -9.9930e-01, -9.9597e-01, -1.8221e-01,\n",
      "         -8.8461e-01,  1.4176e-02,  4.0301e-01, -9.9386e-01,  1.3669e-01,\n",
      "          9.5573e-01,  9.9780e-01,  9.7170e-01, -1.1924e-01, -8.5885e-02,\n",
      "         -2.4560e-01,  6.9359e-02,  9.9765e-01, -8.1740e-01,  6.6565e-01,\n",
      "         -9.3592e-01, -9.8520e-01, -9.2749e-02, -1.3484e-01, -2.5550e-01,\n",
      "         -2.0975e-02, -8.6724e-01,  3.5679e-02, -5.2313e-02,  9.8861e-01,\n",
      "         -9.3719e-01,  8.7973e-01, -8.6758e-01, -9.9509e-01, -1.7937e-01,\n",
      "          5.2548e-02, -3.7444e-03,  9.9190e-01,  2.2331e-01, -9.9869e-01,\n",
      "         -9.3823e-01,  4.6489e-01,  9.2864e-01,  3.9182e-01, -9.9488e-01,\n",
      "         -1.9102e-01,  1.0388e-01, -8.0940e-03,  9.9682e-01, -1.6855e-01,\n",
      "          2.7679e-01,  3.0604e-01, -7.3693e-02, -2.4646e-01,  9.2795e-01,\n",
      "         -9.1661e-01,  1.1272e-01, -8.5794e-01, -2.4332e-01,  2.1810e-01,\n",
      "         -8.8934e-01,  2.1975e-01, -8.0795e-01,  7.1513e-01,  9.6914e-01,\n",
      "          1.8955e-01, -7.8794e-02,  9.9655e-01, -9.9800e-01,  8.2730e-03,\n",
      "         -9.6072e-01,  9.1349e-01, -9.5742e-01, -9.8602e-01, -3.1262e-01,\n",
      "         -7.8506e-01,  1.7597e-01, -1.0734e-02, -9.3480e-01, -2.2196e-01,\n",
      "         -9.9489e-01, -1.8864e-01,  8.3310e-02,  9.4341e-01,  9.1731e-01,\n",
      "         -9.8354e-01,  1.8452e-01, -9.4208e-01, -2.0079e-01,  8.8149e-02,\n",
      "         -1.9714e-01,  5.7268e-01, -5.4985e-02,  8.9442e-03, -9.9879e-01,\n",
      "         -1.7795e-01,  8.2120e-01,  2.1594e-02,  8.6678e-01,  5.3190e-02,\n",
      "         -1.0110e-01, -8.0046e-01, -7.3905e-01,  2.8800e-01,  9.8988e-01,\n",
      "          9.0597e-01, -5.6624e-01,  2.6335e-01, -1.0306e-01, -3.3526e-01,\n",
      "          9.9869e-01, -9.7301e-01,  9.9492e-01, -9.4553e-01,  1.3357e-01,\n",
      "         -9.9363e-01,  9.7325e-01,  8.8394e-01,  8.2483e-01,  1.5881e-02,\n",
      "         -6.4678e-01, -9.9092e-01, -4.2763e-02,  1.9504e-01,  2.5421e-01,\n",
      "          7.2128e-02,  9.6918e-01, -2.1326e-01,  9.1057e-01, -9.6023e-01,\n",
      "          3.9495e-01, -2.8381e-01,  9.0247e-01, -9.1458e-01,  9.9940e-01,\n",
      "         -9.2819e-01,  1.3806e-01, -7.8345e-01,  9.9988e-01,  6.9718e-02,\n",
      "         -1.8317e-01, -9.8927e-01, -8.3768e-01,  8.0200e-02, -2.6761e-01,\n",
      "          9.6231e-01,  1.3731e-01, -7.1935e-01,  9.0147e-01,  9.8725e-01,\n",
      "         -9.9234e-01, -8.5953e-02,  9.8630e-01,  4.6050e-01, -4.2923e-03,\n",
      "          1.2314e-01,  1.8173e-02,  7.6867e-01, -7.0130e-02,  9.8951e-01,\n",
      "          1.2075e-01,  9.7611e-01, -9.9897e-01, -9.9830e-01,  8.1365e-01,\n",
      "          1.5328e-01,  9.8608e-01,  6.9806e-02, -1.1681e-01, -2.6997e-01,\n",
      "          1.6115e-02,  4.1884e-02,  1.9147e-01, -2.1898e-01,  8.1056e-01,\n",
      "          4.6624e-02,  9.9253e-01, -3.6798e-01, -9.9428e-01, -9.9023e-01,\n",
      "          2.7284e-02,  2.5765e-03,  4.0158e-02,  1.9896e-01,  9.6002e-01,\n",
      "          1.3095e-01, -8.9153e-01, -2.5747e-02,  4.2793e-03, -1.2539e-01,\n",
      "         -8.1178e-01,  9.9654e-01, -9.9204e-01,  7.1752e-01,  1.0877e-01,\n",
      "         -8.1735e-01,  7.7196e-01,  8.5298e-01,  1.2152e-01, -9.8694e-01,\n",
      "          8.0595e-02, -9.6307e-01, -2.1160e-05,  1.8762e-01, -9.7350e-01,\n",
      "          1.1438e-01,  7.7677e-01,  1.0459e-01,  9.9229e-01,  6.0646e-01,\n",
      "         -2.6780e-02, -9.2755e-01,  9.9997e-01, -8.7929e-01, -9.9954e-01,\n",
      "         -1.6270e-01, -1.5950e-01,  2.7264e-01, -2.0389e-01, -3.0113e-01,\n",
      "         -1.7844e-01, -8.0561e-01,  1.2145e-01,  9.8868e-01, -9.4858e-01,\n",
      "          1.1354e-01, -9.9623e-01,  9.9868e-01, -2.3344e-02, -8.3782e-01,\n",
      "          9.3409e-01,  9.8557e-01,  9.8938e-01,  5.7522e-02, -9.9476e-01,\n",
      "         -3.3061e-02, -9.3644e-01, -9.9435e-01, -8.1908e-02, -9.7548e-01,\n",
      "          9.4893e-01, -1.0676e-01,  1.9106e-01,  9.2662e-01,  8.7443e-01,\n",
      "          2.7110e-01,  1.7567e-01, -9.6381e-01, -9.6451e-01,  2.4716e-01,\n",
      "         -8.9801e-01, -1.0089e-01, -9.9962e-01, -8.5845e-01, -9.0564e-01,\n",
      "         -9.9377e-01, -2.4379e-01, -9.5094e-01, -1.2883e-01, -9.8153e-01,\n",
      "          1.5932e-01,  7.6847e-02, -1.7706e-01,  2.3749e-01, -1.2751e-01,\n",
      "         -1.3370e-01,  1.7868e-01, -2.8548e-01,  9.9736e-01,  1.8111e-01,\n",
      "          9.9730e-01,  9.9030e-01,  9.7614e-01, -1.9741e-01, -8.9286e-01,\n",
      "          1.8896e-01,  9.6889e-01, -5.8823e-01, -7.2168e-01,  6.8454e-01,\n",
      "         -9.9668e-01,  6.7637e-01, -8.1214e-01,  8.4033e-01, -3.3454e-01,\n",
      "         -9.0599e-01, -9.3052e-01,  1.1943e-01, -9.9994e-01, -8.9492e-01,\n",
      "         -9.6641e-01, -1.2389e-01,  9.2887e-01, -9.9283e-01,  9.9151e-01,\n",
      "          4.4438e-02, -8.6406e-01,  9.6787e-01,  2.3232e-02, -9.8346e-01,\n",
      "         -1.8052e-01,  1.5202e-01,  9.9728e-01,  2.8673e-01,  9.9235e-01,\n",
      "          5.4324e-02, -9.8034e-01,  9.6542e-01,  7.1250e-01,  1.8919e-01,\n",
      "          9.1965e-02, -4.9665e-01, -9.6901e-01,  8.7764e-01,  2.8738e-01,\n",
      "          1.7140e-02,  9.6889e-01, -6.6511e-01, -9.4274e-02,  6.9735e-02,\n",
      "         -1.9111e-01, -9.9670e-01, -2.0449e-01, -9.7285e-01, -3.2914e-01,\n",
      "         -9.9269e-01, -9.3664e-01,  9.8493e-01, -1.4492e-01, -7.4314e-02,\n",
      "         -1.4385e-01, -5.6965e-02,  3.4197e-01,  8.8605e-01, -9.8582e-01,\n",
      "          2.2948e-01, -1.3791e-01, -6.5163e-01,  4.4889e-01,  1.5442e-01,\n",
      "          3.9279e-02, -1.5688e-01,  9.9702e-01,  3.1549e-01,  3.6276e-01,\n",
      "         -6.4498e-02,  4.9822e-02, -8.6313e-02, -7.6554e-02, -1.8607e-01,\n",
      "          8.1280e-01, -8.0227e-02, -8.8221e-01,  1.4556e-01,  2.3963e-01,\n",
      "          1.9192e-01,  9.4473e-01,  9.9943e-01, -1.5803e-02, -2.6156e-01,\n",
      "          1.8460e-01, -9.8726e-01,  9.6216e-01,  2.0181e-01,  9.9235e-01,\n",
      "          9.9628e-01,  1.2431e-01,  2.6691e-01, -4.8454e-01,  9.9948e-01,\n",
      "          9.8428e-01,  1.5863e-02,  8.8658e-01, -1.2680e-01,  4.9018e-02,\n",
      "          8.8174e-02,  9.8536e-01, -9.2226e-01,  5.9000e-01, -5.2168e-02,\n",
      "          4.9463e-01,  9.5426e-01,  7.8303e-02,  9.0998e-01, -9.9902e-01,\n",
      "          2.7999e-01,  9.4103e-01, -9.9615e-01, -7.6390e-02,  9.8373e-01,\n",
      "         -9.9719e-01,  2.0421e-01,  1.0913e-01,  9.6795e-01, -1.8604e-02,\n",
      "         -1.6616e-01, -1.5763e-01,  3.0080e-01,  1.1461e-01,  9.9956e-01,\n",
      "         -2.4178e-01,  1.3591e-01,  9.9811e-01,  1.4802e-01,  2.7473e-01,\n",
      "         -8.0483e-02,  1.1487e-02,  9.9804e-01,  9.7819e-01, -9.7063e-01,\n",
      "          1.9274e-01, -2.4003e-01, -1.0820e-01,  9.9942e-01, -9.9841e-01,\n",
      "          1.5901e-01, -3.2751e-01, -6.7844e-01, -1.5671e-01,  9.1314e-01,\n",
      "          1.9293e-02,  2.0958e-02, -9.9940e-01, -2.8716e-02,  9.4031e-01,\n",
      "         -6.0922e-02,  9.9548e-01, -1.5843e-01, -2.5460e-01,  6.0510e-01,\n",
      "          7.8539e-01,  3.0462e-01,  8.9446e-01, -9.3100e-01, -1.5987e-01,\n",
      "         -3.9562e-01, -1.1334e-01,  1.0113e-01, -8.1847e-01, -1.2628e-02,\n",
      "         -3.6631e-02,  8.6863e-01, -7.2835e-02, -2.4185e-01, -9.5374e-01,\n",
      "          5.3152e-02, -7.8554e-02, -1.7119e-01, -2.3549e-01,  1.9469e-01,\n",
      "          1.1125e-01,  1.5572e-01, -9.3611e-01, -5.4279e-01, -1.7163e-01,\n",
      "         -3.6062e-01, -4.8281e-02,  1.6014e-01, -9.6124e-01, -1.2670e-01,\n",
      "          1.3828e-01, -1.6064e-02, -9.8009e-01, -9.6806e-01,  1.2154e-01,\n",
      "         -8.5674e-01,  1.3435e-01, -1.6827e-01,  1.2826e-01, -1.2779e-01,\n",
      "         -6.6941e-01, -1.0000e+00, -4.0340e-01, -2.2259e-01, -7.0547e-01,\n",
      "         -1.6767e-01, -5.5096e-01, -2.1272e-01,  9.9983e-01,  1.0000e+00,\n",
      "         -9.7035e-01, -1.1583e-01, -9.7028e-01,  2.2730e-01, -2.0397e-02,\n",
      "         -9.9602e-01, -2.5706e-01,  9.9988e-01,  1.9008e-02, -2.2819e-01,\n",
      "         -9.9888e-01, -8.0973e-01,  9.8552e-01, -7.5838e-01, -8.5575e-02,\n",
      "         -1.5278e-01,  2.3182e-01, -9.9073e-01, -5.2722e-01, -8.2800e-02,\n",
      "         -5.8628e-03, -6.8183e-02, -9.9694e-01, -3.2906e-02, -9.6243e-02,\n",
      "         -9.9885e-01, -3.2426e-01,  9.3074e-01,  2.4958e-01, -1.5258e-01,\n",
      "          4.7611e-01, -5.2418e-01, -3.9121e-02]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = model(**encoded_input)\n",
    "print(result.last_hidden_state)\n",
    "print(result.pooler_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(result.last_hidden_state.shape)\n",
    "print(result.pooler_output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}