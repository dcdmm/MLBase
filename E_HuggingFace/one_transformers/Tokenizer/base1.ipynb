{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁i', '▁love', '▁you', '!']\n",
      "['<s>', '▁i', '▁love', '▁you', '!', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer_m0 = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.1', token='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',\n",
    "                                          add_bos_token=False, add_eos_token=False)\n",
    "print(tokenizer_m0(\"i love you!\").tokens())\n",
    "\n",
    "tokenizer_m1 = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.1', token='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', \n",
    "                                          add_bos_token=True, add_eos_token=True)\n",
    "print(tokenizer_m1(\"i love you!\").tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer_m1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': 'C:\\\\Users\\\\duanm\\\\.cache\\\\huggingface\\\\hub\\\\models--mistralai--Mistral-7B-Instruct-v0.1\\\\snapshots\\\\2dcff66eac0c01dc50e4c41eea959968232187fe\\\\tokenizer.model',\n",
       " 'clean_up_tokenization_spaces': False,\n",
       " 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'add_bos_token': True,\n",
       " 'add_eos_token': True,\n",
       " 'use_default_system_prompt': False,\n",
       " 'add_prefix_space': None,\n",
       " 'legacy': False,\n",
       " 'additional_special_tokens': [],\n",
       " 'chat_template': \"{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n\\n{{- bos_token }}\\n{%- for message in loop_messages %}\\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\\n    {%- endif %}\\n    {%- if message['role'] == 'user' %}\\n        {%- if loop.first and system_message is defined %}\\n            {{- ' [INST] ' + system_message + '\\\\n\\\\n' + message['content'] + ' [/INST]' }}\\n        {%- else %}\\n            {{- ' [INST] ' + message['content'] + ' [/INST]' }}\\n        {%- endif %}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- ' ' + message['content'] + eos_token}}\\n    {%- else %}\\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\\n    {%- endif %}\\n{%- endfor %}\\n\",\n",
       " 'model_max_length': 1000000000000000019884624838656,\n",
       " 'pad_token': None,\n",
       " 'sp_model_kwargs': {},\n",
       " 'spaces_between_special_tokens': False,\n",
       " 'name_or_path': 'mistralai/Mistral-7B-Instruct-v0.1'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m1.init_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '!']\n",
      "['1', '2', '3', '!']\n"
     ]
    }
   ],
   "source": [
    "tokenizer_m0 = AutoTokenizer.from_pretrained('Qwen/Qwen3-4B',\n",
    "                                             add_bos_token=False, add_eos_token=False)\n",
    "print(tokenizer_m0(\"123!\").tokens())\n",
    "\n",
    "tokenizer_m1 = AutoTokenizer.from_pretrained('Qwen/Qwen3-4B',\n",
    "                                             add_bos_token=True, add_eos_token=True)\n",
    "print(tokenizer_m1(\"123!\").tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不添加特殊token的tokens: ['1', '2', '3', '!']\n",
      "添加特殊token的tokens: ['1', '2', '3', '!']\n"
     ]
    }
   ],
   "source": [
    "print(\"不添加特殊token的tokens:\", tokenizer_m0.convert_ids_to_tokens(tokenizer_m0.encode(\"123!\", add_special_tokens=True)))\n",
    "print(\"添加特殊token的tokens:\", tokenizer_m1.convert_ids_to_tokens(tokenizer_m1.encode(\"123!\", add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': 'C:\\\\Users\\\\duanm\\\\.cache\\\\huggingface\\\\hub\\\\models--Qwen--Qwen3-4B\\\\snapshots\\\\531c80e289d6cff3a7cd8c0db8110231d23a6f7a\\\\vocab.json',\n",
       " 'merges_file': 'C:\\\\Users\\\\duanm\\\\.cache\\\\huggingface\\\\hub\\\\models--Qwen--Qwen3-4B\\\\snapshots\\\\531c80e289d6cff3a7cd8c0db8110231d23a6f7a\\\\merges.txt',\n",
       " 'unk_token': None,\n",
       " 'bos_token': None,\n",
       " 'eos_token': AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'pad_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'add_bos_token': True,\n",
       " 'add_prefix_space': False,\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>'],\n",
       " 'chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(\\'<tool_response>\\') and message.content.endswith(\\'</tool_response>\\')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = \\'\\' %}\\n    {%- endif %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {%- set reasoning_content = \\'\\' %}\\n        {%- if message.reasoning_content is string %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if \\'</think>\\' in content %}\\n                {%- set reasoning_content = content.split(\\'</think>\\')[0].rstrip(\\'\\\\n\\').split(\\'<think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n                {%- set content = content.split(\\'</think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n<think>\\\\n\\' + reasoning_content.strip(\\'\\\\n\\') + \\'\\\\n</think>\\\\n\\\\n\\' + content.lstrip(\\'\\\\n\\') }}\\n            {%- else %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- \\'<think>\\\\n\\\\n</think>\\\\n\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}',\n",
       " 'clean_up_tokenization_spaces': False,\n",
       " 'errors': 'replace',\n",
       " 'model_max_length': 131072,\n",
       " 'split_special_tokens': False,\n",
       " 'add_eos_token': True,\n",
       " 'name_or_path': 'Qwen/Qwen3-4B'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m1.init_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat template tokens: ['<|im_start|>', 'user', 'Ċ', '1', '2', '3', '!', '<|im_end|>', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_m1 = AutoTokenizer.from_pretrained('Qwen/Qwen3-4B', add_bos_token=False, add_eos_token=False)\n",
    "messages = [{\"role\": \"user\", \"content\": \"123!\"}]\n",
    "encoded = tokenizer_m1.apply_chat_template(messages, tokenize=True)\n",
    "tokens = tokenizer_m1.convert_ids_to_tokens(encoded)\n",
    "print(\"Chat template tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " 151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " 151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " 151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " 151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " 151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " 151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " 151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " 151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " 151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " 151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " 151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m1.added_tokens_decoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
