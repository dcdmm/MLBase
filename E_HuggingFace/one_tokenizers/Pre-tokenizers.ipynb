{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, pre_tokenizers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(model=models.BPE(unk_token='[UNK]'))\n",
    "string = 'This pre-tokenizer splits tokens on spaces, and also on punctuation. Each occurence of a punctuation character will be treated separately.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', (0, 4)),\n",
       " ('pre', (5, 8)),\n",
       " ('-', (8, 9)),\n",
       " ('tokenizer', (9, 18)),\n",
       " ('splits', (19, 25)),\n",
       " ('tokens', (26, 32)),\n",
       " ('on', (33, 35)),\n",
       " ('spaces', (36, 42)),\n",
       " (',', (42, 43)),\n",
       " ('and', (44, 47)),\n",
       " ('also', (48, 52)),\n",
       " ('on', (53, 55)),\n",
       " ('punctuation', (56, 67)),\n",
       " ('.', (67, 68)),\n",
       " ('Each', (69, 73)),\n",
       " ('occurence', (74, 83)),\n",
       " ('of', (84, 86)),\n",
       " ('a', (87, 88)),\n",
       " ('punctuation', (89, 100)),\n",
       " ('character', (101, 110)),\n",
       " ('will', (111, 115)),\n",
       " ('be', (116, 118)),\n",
       " ('treated', (119, 126)),\n",
       " ('separately', (127, 137)),\n",
       " ('.', (137, 138))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# This pre-tokenizer simply splits using the following regex: \\w+|[^\\w\\s]+\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', (0, 4)),\n",
       " ('pre', (5, 8)),\n",
       " ('-', (8, 9)),\n",
       " ('tokenizer', (9, 18)),\n",
       " ('splits', (19, 25)),\n",
       " ('tokens', (26, 32)),\n",
       " ('on', (33, 35)),\n",
       " ('spaces', (36, 42)),\n",
       " (',', (42, 43)),\n",
       " ('and', (44, 47)),\n",
       " ('also', (48, 52)),\n",
       " ('on', (53, 55)),\n",
       " ('punctuation', (56, 67)),\n",
       " ('.', (67, 68)),\n",
       " ('Each', (69, 73)),\n",
       " ('occurence', (74, 83)),\n",
       " ('of', (84, 86)),\n",
       " ('a', (87, 88)),\n",
       " ('punctuation', (89, 100)),\n",
       " ('character', (101, 110)),\n",
       " ('will', (111, 115)),\n",
       " ('be', (116, 118)),\n",
       " ('treated', (119, 126)),\n",
       " ('separately', (127, 137)),\n",
       " ('.', (137, 138))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "\n",
    "# This pre-tokenizer splits tokens on spaces, and also on punctuation. Each occurence of a punctuation character will be treated separately.\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This pre', (0, 8)),\n",
       " ('-', (8, 9)),\n",
       " ('tokenizer splits tokens on spaces', (9, 42)),\n",
       " (',', (42, 43)),\n",
       " (' and also on punctuation', (43, 67)),\n",
       " ('.', (67, 68)),\n",
       " (' Each occurence of a punctuation character will be treated separately',\n",
       "  (68, 137)),\n",
       " ('.', (137, 138))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Punctuation()\n",
    "\n",
    "# This pre-tokenizer simply splits on punctuation as individual characters.\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### WhitespaceSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', (0, 4)),\n",
       " ('pre-tokenizer', (5, 18)),\n",
       " ('splits', (19, 25)),\n",
       " ('tokens', (26, 32)),\n",
       " ('on', (33, 35)),\n",
       " ('spaces,', (36, 43)),\n",
       " ('and', (44, 47)),\n",
       " ('also', (48, 52)),\n",
       " ('on', (53, 55)),\n",
       " ('punctuation.', (56, 68)),\n",
       " ('Each', (69, 73)),\n",
       " ('occurence', (74, 83)),\n",
       " ('of', (84, 86)),\n",
       " ('a', (87, 88)),\n",
       " ('punctuation', (89, 100)),\n",
       " ('character', (101, 110)),\n",
       " ('will', (111, 115)),\n",
       " ('be', (116, 118)),\n",
       " ('treated', (119, 126)),\n",
       " ('separately.', (127, 138))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "# This pre-tokenizer simply splits on the whitespace. Works like .split()\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ByteLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ĠThis', (0, 4)),\n",
       " ('Ġpre', (4, 8)),\n",
       " ('-', (8, 9)),\n",
       " ('tokenizer', (9, 18)),\n",
       " ('Ġsplits', (18, 25)),\n",
       " ('Ġtokens', (25, 32)),\n",
       " ('Ġon', (32, 35)),\n",
       " ('Ġspaces', (35, 42)),\n",
       " (',', (42, 43)),\n",
       " ('Ġand', (43, 47)),\n",
       " ('Ġalso', (47, 52)),\n",
       " ('Ġon', (52, 55)),\n",
       " ('Ġpunctuation', (55, 67)),\n",
       " ('.', (67, 68)),\n",
       " ('ĠEach', (68, 73)),\n",
       " ('Ġoccurence', (73, 83)),\n",
       " ('Ġof', (83, 86)),\n",
       " ('Ġa', (86, 88)),\n",
       " ('Ġpunctuation', (88, 100)),\n",
       " ('Ġcharacter', (100, 110)),\n",
       " ('Ġwill', (110, 115)),\n",
       " ('Ġbe', (115, 118)),\n",
       " ('Ġtreated', (118, 126)),\n",
       " ('Ġseparately', (126, 137)),\n",
       " ('.', (137, 138))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\n",
    "# This pre-tokenizer takes care of replacing all bytes of the given string with a corresponding representation, as well as splitting into words.\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d',\n",
       " '~',\n",
       " 'ğ',\n",
       " 'À',\n",
       " 'Ā',\n",
       " 'Q',\n",
       " '=',\n",
       " 'è',\n",
       " 'û',\n",
       " 'Ĥ',\n",
       " 'B',\n",
       " '*',\n",
       " 'V',\n",
       " 'Õ',\n",
       " '$',\n",
       " '±',\n",
       " '7',\n",
       " 'µ',\n",
       " 'ě',\n",
       " 'z',\n",
       " '¶',\n",
       " 'l',\n",
       " 'D',\n",
       " '¤',\n",
       " 'þ',\n",
       " 'Ĳ',\n",
       " 'Ä',\n",
       " 'Ń',\n",
       " 'Ì',\n",
       " 'î',\n",
       " 'ç',\n",
       " '#',\n",
       " '6',\n",
       " 'Ñ',\n",
       " '®',\n",
       " 'ª',\n",
       " 'G',\n",
       " 'ú',\n",
       " 'M',\n",
       " '³',\n",
       " '8',\n",
       " 'W',\n",
       " '¹',\n",
       " '¸',\n",
       " '0',\n",
       " 'ü',\n",
       " 'I',\n",
       " '.',\n",
       " 'ĕ',\n",
       " ':',\n",
       " 'S',\n",
       " 'g',\n",
       " 'E',\n",
       " ',',\n",
       " 'ĵ',\n",
       " 'Þ',\n",
       " 's',\n",
       " 'Ă',\n",
       " 'ô',\n",
       " '¼',\n",
       " 'N',\n",
       " 'É',\n",
       " '^',\n",
       " 'ï',\n",
       " '@',\n",
       " 'c',\n",
       " 'å',\n",
       " 'Ĵ',\n",
       " ')',\n",
       " '©',\n",
       " 'ã',\n",
       " 'æ',\n",
       " 'q',\n",
       " '|',\n",
       " 'w',\n",
       " 'é',\n",
       " '>',\n",
       " 'ñ',\n",
       " 'ß',\n",
       " 'Ĺ',\n",
       " 'ħ',\n",
       " 'F',\n",
       " 'P',\n",
       " 'T',\n",
       " 'Ę',\n",
       " 'Ô',\n",
       " '¿',\n",
       " 'ķ',\n",
       " 'í',\n",
       " 'ð',\n",
       " 'Ğ',\n",
       " 'ī',\n",
       " '&',\n",
       " 'ę',\n",
       " 'Y',\n",
       " 'Đ',\n",
       " '<',\n",
       " '«',\n",
       " 'C',\n",
       " '£',\n",
       " 'đ',\n",
       " '2',\n",
       " 'f',\n",
       " '\\\\',\n",
       " 'Ĉ',\n",
       " 'ý',\n",
       " 'U',\n",
       " 'Ħ',\n",
       " '%',\n",
       " '¾',\n",
       " 'Ó',\n",
       " '?',\n",
       " 'Ļ',\n",
       " ';',\n",
       " '¯',\n",
       " 'Î',\n",
       " '¦',\n",
       " '3',\n",
       " 'Ú',\n",
       " 'Ð',\n",
       " 'Ċ',\n",
       " 'Ö',\n",
       " 'È',\n",
       " 'ĺ',\n",
       " '4',\n",
       " 'Č',\n",
       " 'i',\n",
       " 'A',\n",
       " 'ı',\n",
       " 'Ŀ',\n",
       " 'ĸ',\n",
       " '_',\n",
       " 'Ò',\n",
       " 'Ï',\n",
       " '}',\n",
       " '+',\n",
       " '»',\n",
       " '²',\n",
       " '[',\n",
       " 't',\n",
       " 'ĭ',\n",
       " 'Ù',\n",
       " '×',\n",
       " 'ė',\n",
       " 'â',\n",
       " 'Ľ',\n",
       " 'Ą',\n",
       " '¡',\n",
       " '§',\n",
       " 'ą',\n",
       " 'J',\n",
       " 'ĳ',\n",
       " 'Ĭ',\n",
       " 'ó',\n",
       " 'Â',\n",
       " 'á',\n",
       " 'k',\n",
       " 'H',\n",
       " 'Ø',\n",
       " '`',\n",
       " 'y',\n",
       " 'h',\n",
       " 'Ď',\n",
       " 'Ġ',\n",
       " 'Ĕ',\n",
       " 'ľ',\n",
       " 'Z',\n",
       " 'p',\n",
       " 'Û',\n",
       " 'ò',\n",
       " 'Ē',\n",
       " '°',\n",
       " 'Ü',\n",
       " '{',\n",
       " '(',\n",
       " 'ă',\n",
       " '5',\n",
       " 'õ',\n",
       " 'ø',\n",
       " 'K',\n",
       " 'ġ',\n",
       " 'Ģ',\n",
       " 'Ê',\n",
       " 'j',\n",
       " \"'\",\n",
       " 'Ī',\n",
       " '½',\n",
       " '9',\n",
       " 'X',\n",
       " 'Ç',\n",
       " 'a',\n",
       " 'ù',\n",
       " 'Ě',\n",
       " 'u',\n",
       " 'm',\n",
       " 'ĝ',\n",
       " 'ċ',\n",
       " 'r',\n",
       " 'º',\n",
       " '÷',\n",
       " 'v',\n",
       " 'Ý',\n",
       " 'x',\n",
       " 'ŀ',\n",
       " 'Ķ',\n",
       " '/',\n",
       " '¬',\n",
       " '´',\n",
       " 'ê',\n",
       " '·',\n",
       " 'b',\n",
       " 'ļ',\n",
       " ']',\n",
       " 'ā',\n",
       " 'ĥ',\n",
       " 'Ĩ',\n",
       " 'İ',\n",
       " 'o',\n",
       " 'Į',\n",
       " 'ĩ',\n",
       " 'ď',\n",
       " 'O',\n",
       " '1',\n",
       " '!',\n",
       " 'R',\n",
       " 'Á',\n",
       " 'ł',\n",
       " '¨',\n",
       " 'ģ',\n",
       " 'Ė',\n",
       " 'Í',\n",
       " 'Ł',\n",
       " 'Ĝ',\n",
       " 'L',\n",
       " 'e',\n",
       " 'č',\n",
       " 'Æ',\n",
       " 'į',\n",
       " 'ē',\n",
       " 'Ć',\n",
       " 'ä',\n",
       " 'ë',\n",
       " 'ì',\n",
       " 'ÿ',\n",
       " 'ć',\n",
       " 'ĉ',\n",
       " '¥',\n",
       " 'n',\n",
       " '-',\n",
       " '\"',\n",
       " '¢',\n",
       " 'Ë',\n",
       " 'Ã',\n",
       " 'à',\n",
       " 'Å',\n",
       " 'ö']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the alphabet used by this PreTokenizer.\n",
    "# Since the ByteLevel works as its name suggests, at the byte level, it encodes each byte value to a unique visible character. This means that there is a total of 256 different characters composing this alphabet.\n",
    "pre_tokenizers.ByteLevel.alphabet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
