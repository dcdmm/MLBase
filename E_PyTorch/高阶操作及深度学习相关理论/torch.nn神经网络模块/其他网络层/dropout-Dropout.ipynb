{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src='../../../../Other/img/dropout.png'>\n",
    "\n",
    "&emsp;&emsp;可以看出,在上图中,通过随机减少每一层神经元的数目,能够把稠密的神经网络(左)变成稀疏的连接(右).\n",
    "神经网络的复杂性和神经元的个数有关,神经元个数越多,模型越复杂,也越容易过拟合.\n",
    "如果能够减少神经元的数目,那么就能减少神经网络过拟合的倾向.\n",
    "\n",
    "&emsp;&emsp;减少神经网络的连接实现起来相对比较复杂.一个等价的最简单的方式是把Dropout层输入张量的某些的元素随机置为零(使用一个和原始张量一样大小的掩码张量来实现).在训练过程中,这样做的结果可以使模型不容易过拟合.\n",
    "为了校正张量的元素被置为零造成的影响,需要对张量所有的元素做缩放.假设元素被随机置零的概率为$p$(一般设置为0.5),\n",
    "那么这个缩放因子应该是$1/(1-p)$.也就是说,丢弃层首先会根据输入张量的形状,以及元素置零的概率产生一个随机的掩码张量,然后把输入张量和这个掩码张量按元素相乘,\n",
    "最后对结果张量乘以缩放因子,输出最终结果,即为丢弃层的过程.\n",
    "在预测过程(forward)中,由于丢弃层会降低神经网络的准确率,所以应该跳过丢弃层(或者把随机置零的概率设置为零).\n",
    "在PyTorch中,可以通过调用nn.Module的trian方法和eval方法来切换丢弃层的训练和预测状态.\n",
    "\n",
    "<font color='red' size=4>集成学习角度的解释</font>:\n",
    "\n",
    "&emsp;&emsp;每做一次丢弃(若$p=0.5$),相当于从原始的网络中采样得到一个子网络.如果一个神经网络有$n$个神经元,那么总共可以采样出$2^n$个子网络.\n",
    "每次迭代都相当于训练一个不同的子网络,这些子网络都共享原始网络的参数.那么,最终的网络可以近似看作集成了指数级个不同网络的组合模型．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 2., 0., 0., 2., 2., 0., 0., 2., 2.],\n        [2., 2., 0., 0., 2., 2., 0., 0., 0., 2.],\n        [0., 2., 0., 2., 2., 0., 2., 2., 0., 2.],\n        [2., 2., 2., 0., 2., 0., 2., 0., 0., 2.],\n        [0., 2., 0., 2., 2., 2., 2., 2., 0., 2.],\n        [0., 0., 0., 0., 0., 2., 2., 2., 2., 0.],\n        [0., 2., 2., 0., 2., 0., 2., 2., 2., 0.],\n        [0., 0., 2., 2., 0., 0., 2., 2., 2., 2.],\n        [0., 0., 0., 0., 0., 2., 0., 2., 2., 2.],\n        [2., 0., 0., 2., 0., 0., 2., 2., 0., 2.]])"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "a = torch.ones((10, 10))\n",
    "drop_5 = F.dropout(a,\n",
    "                   p=0.5) # 元素归零的概率\n",
    "drop_5 # 进行了放缩,1 * (1/(1-0.5)) = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10.],\n        [ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 10.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_1 = F.dropout(a, p=0.9)\n",
    "drop_1 # 进行了放缩,1 * (1/(1-0.9)) = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_F = F.dropout(a, p=0.9,\n",
    "                   training=False) # apply dropout if is True. Default: True\n",
    "drop_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2., 2., 0., 2., 0., 2., 0., 2., 0., 0.],\n        [0., 2., 2., 2., 2., 0., 0., 0., 0., 0.],\n        [2., 2., 2., 2., 0., 2., 2., 0., 2., 0.],\n        [2., 0., 2., 2., 0., 2., 0., 2., 2., 2.],\n        [2., 2., 2., 0., 2., 2., 2., 0., 2., 2.],\n        [2., 2., 2., 2., 2., 0., 2., 0., 0., 0.],\n        [2., 0., 2., 2., 2., 2., 0., 2., 0., 0.],\n        [2., 2., 0., 2., 2., 0., 0., 0., 2., 0.],\n        [2., 0., 0., 2., 0., 2., 2., 0., 0., 0.],\n        [2., 2., 0., 2., 0., 0., 0., 0., 2., 0.]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 实现方式:\n",
    "def forward(self, input):\n",
    "    return F.dropout(input, self.p,\n",
    "                     self.training, # nn.Mudule的training属性\n",
    "                     self.inplace)\n",
    "'''\n",
    "# During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.\n",
    "# Each channel will be zeroed out independently on every forward call.\n",
    "Drop = nn.Dropout(p=0.5)\n",
    "Drop(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}