{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x1cd01a97b70>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "{'B': 0, 'I': 1, 'O': 2, '<START>': 3, '<STOP>': 4}"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "\n",
    "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "tag_to_ix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def log_sum_exp(vec):\n",
    "    \"\"\"Compute log sum exp in a numerically stable way for the forward algorithm\"\"\"\n",
    "\n",
    "    # vec.shape=[1, ?]\n",
    "    max_score = torch.max(vec)\n",
    "    # max_score_boradcast.shaep=[1, ?]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,  # 单词表的单词数目\n",
    "                 tag_to_ix,  # 实体标签(key)与标签编号(value)组成的字典\n",
    "                 embedding_dim,  # 输出词向量的维度大小\n",
    "                 hidden_dim):  # 隐含变量的维度大小的2倍(权重矩阵W_{ih}、W_{hh}中h的大小的2倍)\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)  # 实体标签种类个数\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)  # lstm后接的全连接层\n",
    "\n",
    "        # 转移得分矩阵\n",
    "        # self.transitions.shape=[self.taget_size, self.taget_size]\n",
    "        # self.transitions[i, j]:从j列对应的标签转移到i行对应的标签的得分\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "        # 其他标签转移到\"START_TAG\"的分数非常小(即不可能由其他标签转移到\"START_TAG\")\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        # \"STOP_TAG\"转移到所有其他标签的分数非常小(即不可能由\"STOP_TAG\"转移到其他标签)\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        # sentence.shape=[sen len, ]  # 这里每次输入一个句子\n",
    "\n",
    "        # embeds.shape=[sen len, 1, embedding_dim]\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        # lstm_out.shape=[sen len, 1, hidden_dim]\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # lstm_out.shape=[sen len, hidden_dim]\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        # lstm_feats.shape=[sen len, self.tagset_size]\n",
    "        lstm_feats = self.hidden2tag(lstm_out)  # 句子中每个词属于不同实体类别标签的概率(即发射分数)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        \"\"\"计算给定tag序列的分数\"\"\"\n",
    "        # feats.shape=[sen len, self.taget_size]\n",
    "\n",
    "        score = torch.zeros(1)\n",
    "        # 开头出处添加标签\"START_TAG\"的编号\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            # feat.shape=[self.tageta_size, ]\n",
    "\n",
    "            # self.transitions[tags[i + 1], tags[i]]:转移分数(从tags[i]转移到tags[i + 1]转移得分)\n",
    "            # feat[tags[i + 1]]:发射分数\n",
    "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        # 添加tag最后一个元素到\"STOP_TAG\"的转移得分\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        \"\"\"全部路径的分数计算\"\"\"\n",
    "        # feats.shape=[sen len, self.taget_size]\n",
    "\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "        forward_var = init_alphas  # 上一步的得分(previous)\n",
    "\n",
    "        for feat in feats:\n",
    "            # feat.shape=[self.taget_size, ]\n",
    "            alphas_t = []\n",
    "\n",
    "            for next_tag in range(self.tagset_size):  # 通过循环每次计算一个标签\n",
    "                # feat[next_tag]:当前步骤该标签的发射分数(obs[i])\n",
    "                # feat[next_tag].view(-1, 1).shape=[1, 1]\n",
    "                # emit_score.shape=[1, self.tag_size]  # 扩展obs[i]\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # trans_score.shape=[1, self.tagset_size]   # transition得分(所有其他标签转移到该标签的分数)\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)  # 更新上一步的得分(更新previous)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        # 总路径得分(TotalScore)\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        \"\"\"CRF损失函数\"\"\"\n",
    "\n",
    "        # feats.shape=[sen len, self.taget_size]\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        forward_score = self._forward_alg(feats)  # 全部路径总得分\n",
    "        gold_score = self._score_sentence(feats, tags)  # 最优路径得分\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        \"\"\"维特比算法求解最优路径\"\"\"\n",
    "        # feats.shape=[sen len, self.taget_size]\n",
    "\n",
    "        backpointers = []\n",
    "\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "        # forward_var.shape=[1, self.tagset_size]\n",
    "        forward_var = init_vvars\n",
    "\n",
    "        for feat in feats:\n",
    "            # feat.shape=[self.taget_size, ]\n",
    "            bptrs_t = []\n",
    "            viterbivars_t = []\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # 维特比算法记录最优路径时只考虑上一步的分数以及上一步tag转移到当前tag的转移分数(并不取决与当前tag的发射分数)\n",
    "                # next_tag_var.shaep=[1, self.taget_size]\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = torch.argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(torch.max(next_tag_var, dim=1).values)\n",
    "\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = torch.argmax(terminal_var)\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]\n",
    "        best_path.reverse()\n",
    "        return best_path\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return tag_seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "{'the': 0,\n 'wall': 1,\n 'street': 2,\n 'journal': 3,\n 'reported': 4,\n 'today': 5,\n 'that': 6,\n 'apple': 7,\n 'corporation': 8,\n 'made': 9,\n 'money': 10,\n 'georgia': 11,\n 'tech': 12,\n 'is': 13,\n 'a': 14,\n 'university': 15,\n 'in': 16}"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = [(\"the wall street journal reported today that apple corporation made money\".split(),\n",
    "                  \"B I I I O O O B I O O\".split()),\n",
    "                 (\"georgia tech is a university in georgia\".split(),\n",
    "                  \"B I O O O O B\".split())]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "word_to_ix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, embedding_dim=5, hidden_dim=4)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precheck_sent = torch.tensor([word_to_ix[w] for w in training_data[0][0]], dtype=torch.long)\n",
    "precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "precheck_tags  # 真实标签"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13.4273])\n",
      "[tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(1)]\n"
     ]
    }
   ],
   "source": [
    "# Check predictions before training\n",
    "with torch.no_grad():\n",
    "    print(model.neg_log_likelihood(precheck_sent, precheck_tags))  # 损失值较大\n",
    "    print(model(precheck_sent))  # 最优路径错误"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8400])\n",
      "[tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2)]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    for sentence, tags in training_data:\n",
    "        model.zero_grad()\n",
    "        sentence_in = torch.tensor([word_to_ix[w] for w in sentence], dtype=torch.long)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)  # 损失值\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    print(model.neg_log_likelihood(precheck_sent, precheck_tags))  # 损失值较小\n",
    "    print(model(precheck_sent))  # 最优路径正确"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}