{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 从头训练分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Models\n",
    "\n",
    "Models are the core algorithms used to actually tokenize, and therefore, they are the only mandatory component of a Tokenizer.\n",
    "\n",
    "| Name      | Description                                                  |\n",
    "| --------- | ------------------------------------------------------------ |\n",
    "| WordLevel | This is the “classic” tokenization algorithm. It let’s you simply map words to IDs without anything fancy. This has the advantage of being really simple to use and understand, but it requires extremely large vocabularies for a good coverage. Using this `Model` requires the use of a `PreTokenizer`. No choice will be made by this model directly, it simply maps input tokens to IDs. |\n",
    "| BPE       | One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding works by starting with characters, while merging those that are the most frequently seen together, thus creating new tokens. It then works iteratively to build new tokens out of the most frequent pairs it sees in a corpus. BPE is able to build words it has never seen by using multiple subword tokens, and thus requires smaller vocabularies, with less chances of having “unk” (unknown) tokens. |\n",
    "| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don’t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word). |\n",
    "| Unigram   | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.models.BPE at 0x2475f70c670>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An implementation of the BPE (Byte-Pair Encoding) algorithm\n",
    "model_BPE = BPE(unk_token='[UNK]')\n",
    "model_BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x2475a7e9690>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A :obj:`Tokenizer` works as a pipeline. It processes some raw text as input and outputs an :class:`~tokenizers.Encoding`.\n",
    "tokenizer = Tokenizer(\n",
    "    # The core algorithm that this Tokenizer should be using.\n",
    "    model=model_BPE)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### normalizer(可选)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Takes care of normalizing raw text before giving it to a Bert model.\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pre-tokenizers\n",
    "\n",
    "The PreTokenizer takes care of splitting the input according to a set of rules. This pre-processing lets you ensure that the underlying Model does not build tokens across multiple “splits”. For example if you don’t want to have whitespaces inside a token, then you can have a PreTokenizer that splits on these whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This pre-tokenizer simply splits using the following regex: `\\w+|[^\\w\\s]+`\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.trainers.BpeTrainer at 0x2475f75f330>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer capable of training a BPE model\n",
    "trainer = BpeTrainer(\n",
    "    # The size of the final vocabulary, including all tokens and alphabet.\n",
    "    vocab_size=30000,  # 默认:30000\n",
    "    # The minimum frequency a pair should have in order to be merged.\n",
    "    min_frequency=0,  # 默认:0\n",
    "    # A list of special tokens the model should know of.\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])  # 默认:[]\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['实战/PreTrained_Bert/wikitext-103-raw/wiki.test.raw',\n",
       " '实战/PreTrained_Bert/wikitext-103-raw/wiki.train.raw',\n",
       " '实战/PreTrained_Bert/wikitext-103-raw/wiki.valid.raw']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [f\"实战/PreTrained_Bert/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x2475a7e9690>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Tokenizer using the given files.\n",
    "tokenizer.train(\n",
    "    files=files,  # 文件路径或包含路径的列表\n",
    "    trainer=trainer)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backward': 24136,\n",
       " 'donate': 27168,\n",
       " 'bile': 18978,\n",
       " 'versely': 15042,\n",
       " 'launches': 26276,\n",
       " 'feature': 6699,\n",
       " 'morrow': 15197,\n",
       " 'taxes': 12291,\n",
       " 'guer': 16086,\n",
       " 'rue': 23786,\n",
       " 'nees': 29722,\n",
       " 'pam': 11734,\n",
       " 'latin': 7482,\n",
       " 'slash': 19841,\n",
       " 'aka': 8568,\n",
       " '迄': 3617,\n",
       " 'worsen': 16694,\n",
       " 'alternatively': 19805,\n",
       " '巡': 2075,\n",
       " 'ative': 4721,\n",
       " 'ros': 8064,\n",
       " 'milner': 29939,\n",
       " 'alaska': 12394,\n",
       " 'aped': 21673,\n",
       " 'ial': 4252,\n",
       " 'dispatched': 14238,\n",
       " 'elaborated': 21469,\n",
       " 'fractured': 23976,\n",
       " 'sig': 10757,\n",
       " 'monastery': 12748,\n",
       " 'busy': 14710,\n",
       " 'performer': 15535,\n",
       " '映': 2390,\n",
       " 'precis': 19453,\n",
       " 'pcs': 25726,\n",
       " 'asan': 21833,\n",
       " 'jong': 28904,\n",
       " 'sooner': 26694,\n",
       " 'pitching': 15602,\n",
       " 'shade': 20554,\n",
       " 'owner': 7583,\n",
       " 'sees': 9750,\n",
       " 'activision': 22660,\n",
       " 'photographers': 28273,\n",
       " 'hou': 4739,\n",
       " '1780': 21444,\n",
       " 'ethnic': 10276,\n",
       " '570': 26023,\n",
       " 'banana': 23691,\n",
       " 'transformation': 15751,\n",
       " 'widens': 29238,\n",
       " 'this': 4285,\n",
       " 'finals': 8967,\n",
       " 'thetic': 9606,\n",
       " 'spir': 5770,\n",
       " 'vall': 20123,\n",
       " 'golden': 7903,\n",
       " 'heba': 29599,\n",
       " 'bows': 19852,\n",
       " '禎': 3001,\n",
       " '皮': 2924,\n",
       " 'bai': 10631,\n",
       " 'swells': 29977,\n",
       " 'hurley': 21122,\n",
       " 'eur': 12494,\n",
       " 'vincent': 12259,\n",
       " 'airship': 29667,\n",
       " 'greeted': 18773,\n",
       " 'preval': 15302,\n",
       " 'depressions': 24142,\n",
       " 'spire': 24852,\n",
       " 'dre': 5692,\n",
       " 'signal': 10868,\n",
       " 'photographer': 16426,\n",
       " 'ʕ': 195,\n",
       " 'ule': 8814,\n",
       " '◦': 1172,\n",
       " 'hostages': 23571,\n",
       " 'enzy': 12445,\n",
       " 'antony': 25831,\n",
       " 'tum': 12241,\n",
       " 'traditionally': 11945,\n",
       " 'ih': 19718,\n",
       " 'fathers': 18172,\n",
       " 'suffered': 6857,\n",
       " 'onica': 12606,\n",
       " 'wooden': 9864,\n",
       " 'illon': 22994,\n",
       " 'pala': 23446,\n",
       " 'bruk': 26456,\n",
       " 'repel': 28162,\n",
       " 'follow': 4521,\n",
       " '撲': 2334,\n",
       " 'slim': 18227,\n",
       " 'portions': 10282,\n",
       " 'rounds': 10697,\n",
       " 'plagu': 20055,\n",
       " 'turf': 20817,\n",
       " 'strength': 7508,\n",
       " 'chemically': 27519,\n",
       " 'malaysia': 12513,\n",
       " 'ե': 328,\n",
       " 'barcel': 13805,\n",
       " 'microw': 26698,\n",
       " '绮': 3169,\n",
       " 'embryos': 26477,\n",
       " '菊': 3322,\n",
       " 'dispersed': 17918,\n",
       " 'eighty': 17587,\n",
       " 'paleo': 22229,\n",
       " 'ch': 4124,\n",
       " 'member': 5377,\n",
       " 'organisation': 10340,\n",
       " 'nitro': 13981,\n",
       " 'iel': 6517,\n",
       " '6': 26,\n",
       " 'ock': 24843,\n",
       " 'handlen': 26561,\n",
       " 'report': 6324,\n",
       " 'sly': 25826,\n",
       " 'nicki': 29138,\n",
       " 'hat': 7459,\n",
       " 'trait': 21092,\n",
       " '卦': 1696,\n",
       " 'apers': 9839,\n",
       " 'caravan': 29937,\n",
       " '475': 28812,\n",
       " 'paralleling': 26182,\n",
       " 'innis': 28492,\n",
       " 'cairn': 27239,\n",
       " 'airs': 25079,\n",
       " 'dd': 8902,\n",
       " '166': 20870,\n",
       " 'abi': 20947,\n",
       " 'raises': 22633,\n",
       " 'ellie': 22067,\n",
       " 'scottish': 8012,\n",
       " 'categor': 16441,\n",
       " 'margaret': 11018,\n",
       " 'napoleon': 10573,\n",
       " 'bridgwater': 26389,\n",
       " 'rar': 12758,\n",
       " 'ɑ': 134,\n",
       " 'itz': 8805,\n",
       " 'ᛠ': 959,\n",
       " 'claiming': 9314,\n",
       " 'liquid': 11390,\n",
       " 'sule': 18798,\n",
       " 'imitate': 29817,\n",
       " 'airports': 18325,\n",
       " 'leo': 13220,\n",
       " 'gilmour': 23428,\n",
       " 'beyond': 7739,\n",
       " 'doorway': 25618,\n",
       " 'elda': 15978,\n",
       " 'capac': 7769,\n",
       " 'demons': 17889,\n",
       " 'hartman': 26408,\n",
       " 'fade': 23530,\n",
       " 'schizophren': 22768,\n",
       " 'employing': 19192,\n",
       " 'atche': 19168,\n",
       " 'karnat': 19682,\n",
       " 'mps': 19122,\n",
       " 'bass': 6866,\n",
       " 'frederic': 23618,\n",
       " 'ድ': 921,\n",
       " '↦': 1107,\n",
       " 'pharmaceutical': 24609,\n",
       " 'northampton': 20018,\n",
       " 'natalie': 22818,\n",
       " '虚': 3386,\n",
       " 'silva': 20191,\n",
       " 'emission': 13561,\n",
       " 'humph': 16515,\n",
       " 'padd': 15953,\n",
       " 'originality': 24964,\n",
       " '72': 9142,\n",
       " 'progressive': 12272,\n",
       " 'paralle': 8158,\n",
       " 'wład': 25657,\n",
       " 'anky': 25864,\n",
       " 'alamo': 27000,\n",
       " 'tankers': 28979,\n",
       " 'desperately': 24437,\n",
       " 'shorts': 17738,\n",
       " 'ahl': 12462,\n",
       " 'nier': 17854,\n",
       " 'terr': 5435,\n",
       " 'chancel': 21775,\n",
       " 'petro': 12305,\n",
       " 'distracted': 22217,\n",
       " 'thoughts': 14355,\n",
       " '46': 6993,\n",
       " 'specialized': 15004,\n",
       " 'udo': 16499,\n",
       " 'caen': 22448,\n",
       " 'vir': 5053,\n",
       " 'mtv': 9001,\n",
       " 'coincided': 18132,\n",
       " '袖': 3421,\n",
       " 'gradual': 17401,\n",
       " 'carson': 18286,\n",
       " '費': 3545,\n",
       " '能': 3231,\n",
       " 'mccor': 20287,\n",
       " 'restaurants': 14859,\n",
       " 'title': 5580,\n",
       " 'nasser': 19429,\n",
       " 'rho': 7742,\n",
       " 'fountain': 14534,\n",
       " 'ഹ': 670,\n",
       " 'bypass': 11938,\n",
       " 'bak': 10620,\n",
       " '亮': 1452,\n",
       " 'odia': 23144,\n",
       " 'wealth': 7299,\n",
       " 'ark': 5151,\n",
       " 'pup': 9017,\n",
       " 'regon': 9431,\n",
       " 'ballad': 11470,\n",
       " '麻': 3982,\n",
       " 'ayr': 27589,\n",
       " '坑': 1836,\n",
       " 'ᚢ': 934,\n",
       " 'ayles': 25074,\n",
       " 'bronze': 10063,\n",
       " 'cuth': 28093,\n",
       " 'philharmonic': 21180,\n",
       " 'stay': 7579,\n",
       " 'practiced': 17378,\n",
       " 'bucharest': 27969,\n",
       " 'computers': 14945,\n",
       " 'manila': 17304,\n",
       " 'irl': 26651,\n",
       " 'measurements': 15124,\n",
       " 'spitfire': 21845,\n",
       " 'gonna': 17010,\n",
       " '1906': 11064,\n",
       " 'stature': 23348,\n",
       " 'scrapping': 27606,\n",
       " 'bridg': 24002,\n",
       " 'onies': 22668,\n",
       " 'vance': 25298,\n",
       " 'newark': 16292,\n",
       " 'revelation': 16975,\n",
       " 'electronics': 16823,\n",
       " 'enrollment': 21185,\n",
       " 'clarence': 18652,\n",
       " 'jewellery': 24363,\n",
       " 'imet': 10108,\n",
       " 'lighter': 13255,\n",
       " 'aesthet': 20977,\n",
       " 'nhl': 8698,\n",
       " 'passport': 26688,\n",
       " 'cooperative': 18512,\n",
       " 'ades': 6097,\n",
       " 'artery': 27629,\n",
       " 'mckinley': 13773,\n",
       " 'concl': 6077,\n",
       " 'bethlehem': 24669,\n",
       " 'ams': 6640,\n",
       " 'provider': 29181,\n",
       " 'months': 5606,\n",
       " '1000': 12512,\n",
       " '津': 2673,\n",
       " 'gregg': 25112,\n",
       " 'invested': 16726,\n",
       " 'init': 5120,\n",
       " 'undercover': 26425,\n",
       " '𐭕': 4091,\n",
       " '杲': 2458,\n",
       " 'spikes': 23990,\n",
       " 'maureen': 28850,\n",
       " 'ڠ': 429,\n",
       " 'fault': 12722,\n",
       " 'sculptures': 17487,\n",
       " 'pud': 20199,\n",
       " 'holotype': 28111,\n",
       " '間': 3759,\n",
       " '分': 1617,\n",
       " 'inheritance': 18989,\n",
       " 'prote': 5359,\n",
       " 'bamboo': 23910,\n",
       " 'huck': 29993,\n",
       " 'digitally': 18242,\n",
       " 'wise': 7492,\n",
       " 'cricke': 14197,\n",
       " 'backup': 15083,\n",
       " 'white': 5189,\n",
       " 'magdal': 25761,\n",
       " 'courtesy': 25643,\n",
       " 'م': 418,\n",
       " 'resistance': 8524,\n",
       " 'embank': 24497,\n",
       " 'pand': 12912,\n",
       " 'transatlantic': 27859,\n",
       " 'foll': 28597,\n",
       " '可': 1738,\n",
       " 'terminated': 17030,\n",
       " 'chronological': 28143,\n",
       " 'morrison': 11750,\n",
       " 'midst': 19064,\n",
       " '質': 3554,\n",
       " '冠': 1594,\n",
       " '関': 3760,\n",
       " 'inhg': 13766,\n",
       " 'incisors': 29288,\n",
       " '挑': 2310,\n",
       " '召': 1737,\n",
       " 'exh': 11180,\n",
       " 'debbie': 26953,\n",
       " 'nineteen': 17545,\n",
       " 'splend': 22265,\n",
       " 'palau': 28048,\n",
       " 'ჭ': 822,\n",
       " 'ramps': 19675,\n",
       " 'jenkins': 21282,\n",
       " '蕩': 3358,\n",
       " 'gamble': 29749,\n",
       " 'bold': 12485,\n",
       " 'dion': 20481,\n",
       " '日': 2375,\n",
       " 'တ': 773,\n",
       " '‡': 1050,\n",
       " 'livery': 24432,\n",
       " 'holden': 16079,\n",
       " 'dip': 8246,\n",
       " 'marred': 25102,\n",
       " 'round': 5229,\n",
       " 'neurons': 25426,\n",
       " '如': 1922,\n",
       " 'cha': 8408,\n",
       " 'itate': 13093,\n",
       " 'russian': 6988,\n",
       " 'spotting': 26634,\n",
       " 'adequate': 12613,\n",
       " 'rails': 19081,\n",
       " 'arsen': 8799,\n",
       " 'detect': 13300,\n",
       " 'fungal': 26595,\n",
       " 'convert': 14285,\n",
       " 'oun': 4201,\n",
       " 'छ': 474,\n",
       " 'prosecut': 10344,\n",
       " 'terrifying': 29570,\n",
       " 'glorious': 18998,\n",
       " 'classroom': 25046,\n",
       " 'tc': 28859,\n",
       " 'plateau': 15269,\n",
       " 'investment': 11609,\n",
       " 'ights': 5283,\n",
       " 'apol': 8440,\n",
       " 'rehabil': 15546,\n",
       " 'ayed': 6290,\n",
       " 'indiana': 9413,\n",
       " 'lloy': 12100,\n",
       " '1874': 15232,\n",
       " 'ł': 109,\n",
       " 'ϕ': 269,\n",
       " 'ucks': 13577,\n",
       " 'walk': 8678,\n",
       " '衢': 3415,\n",
       " 'streaming': 22518,\n",
       " 'forman': 5144,\n",
       " 'institutes': 27095,\n",
       " '髪': 3932,\n",
       " 'after': 4293,\n",
       " '111': 13218,\n",
       " '藝': 3371,\n",
       " 'exercise': 11691,\n",
       " 'treating': 13725,\n",
       " 'mee': 21924,\n",
       " 'rad': 5036,\n",
       " 'bies': 20580,\n",
       " 'pepsi': 29983,\n",
       " '鞋': 3852,\n",
       " 'cloverleaf': 22741,\n",
       " 'sø': 21797,\n",
       " 'insight': 18027,\n",
       " 'weddings': 29522,\n",
       " 'thousand': 7451,\n",
       " 'suggesting': 11131,\n",
       " 'authen': 13745,\n",
       " 'dow': 10353,\n",
       " 'noise': 12979,\n",
       " '1945': 7676,\n",
       " '汤': 2633,\n",
       " 'geared': 24321,\n",
       " '街': 3410,\n",
       " 'infr': 9540,\n",
       " 'radio': 5828,\n",
       " 'orche': 7664,\n",
       " 'domains': 22500,\n",
       " 'otherwise': 9675,\n",
       " '彪': 2168,\n",
       " '改': 2339,\n",
       " 'automobiles': 28785,\n",
       " 'ワ': 1377,\n",
       " 'harry': 8226,\n",
       " '拥': 2302,\n",
       " 'smallest': 16384,\n",
       " '書': 2416,\n",
       " 'circular': 12589,\n",
       " '恵': 2219,\n",
       " 'avery': 20902,\n",
       " 'corresponds': 23697,\n",
       " '子': 1952,\n",
       " '產': 2886,\n",
       " 'vable': 17967,\n",
       " 'slipknot': 26475,\n",
       " 'coron': 11442,\n",
       " '料': 2359,\n",
       " 'rural': 9002,\n",
       " 'julie': 16934,\n",
       " 'vim': 28761,\n",
       " 'ties': 5728,\n",
       " 'quincy': 25680,\n",
       " 'zoological': 28879,\n",
       " '巴': 2083,\n",
       " 'potato': 19408,\n",
       " '昏': 2386,\n",
       " 'returning': 7582,\n",
       " 'touches': 21153,\n",
       " 'increasing': 6702,\n",
       " 'sara': 14224,\n",
       " 'conan': 19604,\n",
       " 'advertised': 20142,\n",
       " '1882': 14372,\n",
       " '辰': 3610,\n",
       " 'watershed': 11895,\n",
       " 'various': 5622,\n",
       " 'sist': 9815,\n",
       " 'chio': 26031,\n",
       " 'ǂ': 126,\n",
       " 'younger': 8253,\n",
       " '井': 1442,\n",
       " 'ardo': 14106,\n",
       " 'fu': 6078,\n",
       " 'py': 6690,\n",
       " 'goof': 26457,\n",
       " 'converted': 8867,\n",
       " 'spend': 11278,\n",
       " 'rico': 10655,\n",
       " 'linnaeus': 20880,\n",
       " '揚': 2323,\n",
       " 'itchy': 26393,\n",
       " '457': 27177,\n",
       " 'ow': 5214,\n",
       " 'aquarium': 27711,\n",
       " 'santiago': 19269,\n",
       " 'shell': 7543,\n",
       " 'acknowledged': 11718,\n",
       " 'dispat': 11538,\n",
       " 'carchar': 28834,\n",
       " '与': 1392,\n",
       " 'dav': 5345,\n",
       " 'celebrit': 17441,\n",
       " 'parli': 6523,\n",
       " 'ples': 6260,\n",
       " 'bourne': 8573,\n",
       " 'killer': 11253,\n",
       " 'dismissal': 17831,\n",
       " 'synthesizers': 21630,\n",
       " 'holliday': 26320,\n",
       " 'referred': 7066,\n",
       " 'showed': 7380,\n",
       " 'controlled': 8328,\n",
       " '汽': 2638,\n",
       " 'raynor': 25412,\n",
       " 'falun': 23258,\n",
       " 'refit': 14775,\n",
       " 'graceful': 28106,\n",
       " 'whole': 7016,\n",
       " 'kha': 24474,\n",
       " 'ories': 7870,\n",
       " 'ద': 615,\n",
       " 'astron': 8827,\n",
       " 'shipment': 19811,\n",
       " 'templ': 22283,\n",
       " 'ibe': 8002,\n",
       " 'ire': 4321,\n",
       " 'seated': 16635,\n",
       " 'betty': 16521,\n",
       " '276': 29511,\n",
       " 'everyday': 16828,\n",
       " '去': 1713,\n",
       " '美': 3182,\n",
       " 'basalt': 29362,\n",
       " 'marquess': 24310,\n",
       " 'abbe': 28725,\n",
       " 'evacuated': 11782,\n",
       " 'dried': 18881,\n",
       " 'profound': 17215,\n",
       " '贝': 3557,\n",
       " 'jab': 13830,\n",
       " '118': 16674,\n",
       " 'centred': 18960,\n",
       " 'starvation': 25545,\n",
       " 'rudder': 23263,\n",
       " 'mihail': 26719,\n",
       " 'unes': 10272,\n",
       " 'synap': 28141,\n",
       " 'imentary': 29071,\n",
       " 'newspapers': 10477,\n",
       " 'shetland': 23146,\n",
       " 'opposite': 9652,\n",
       " 'sensibility': 24783,\n",
       " 'innate': 29859,\n",
       " 'striker': 16173,\n",
       " 'provocative': 24240,\n",
       " 'modore': 15137,\n",
       " 'additions': 17028,\n",
       " '馬': 3907,\n",
       " 'フ': 1358,\n",
       " 'xes': 18485,\n",
       " '法': 2660,\n",
       " '底': 2118,\n",
       " 'grid': 13079,\n",
       " 'kno': 4526,\n",
       " 'descend': 11282,\n",
       " 'stored': 13715,\n",
       " 'crete': 9369,\n",
       " 'hetti': 29922,\n",
       " 'dish': 9952,\n",
       " 'pector': 22876,\n",
       " 'educ': 5837,\n",
       " '用': 2888,\n",
       " 'truck': 12267,\n",
       " 'bery': 14826,\n",
       " 'koch': 24696,\n",
       " 'enem': 9134,\n",
       " 'sava': 21773,\n",
       " 'pentecost': 25877,\n",
       " 'jup': 11633,\n",
       " 'recognise': 21449,\n",
       " 'tanner': 27326,\n",
       " '姊': 1932,\n",
       " 'metr': 21459,\n",
       " 'aleppo': 24825,\n",
       " '爆': 2800,\n",
       " 'wearing': 9393,\n",
       " 'renov': 11469,\n",
       " 'exem': 12649,\n",
       " 'examples': 9851,\n",
       " 'gimmick': 25533,\n",
       " 'consciously': 28481,\n",
       " 'paste': 24051,\n",
       " 'than': 4471,\n",
       " '「': 1262,\n",
       " 'cus': 5198,\n",
       " 'activation': 19558,\n",
       " 'epidem': 24610,\n",
       " 'frankenstein': 25616,\n",
       " 'searching': 13987,\n",
       " 'booker': 17908,\n",
       " 'robe': 17734,\n",
       " '1807': 21863,\n",
       " 'browning': 22806,\n",
       " 'curse': 17059,\n",
       " '限': 3781,\n",
       " 'approved': 8482,\n",
       " 'city': 4647,\n",
       " 'dropping': 13418,\n",
       " 'beautiful': 8941,\n",
       " '遮': 3657,\n",
       " 'bowers': 26306,\n",
       " 'madurai': 29070,\n",
       " 'law': 5028,\n",
       " 'smithers': 27971,\n",
       " 'acceleration': 24997,\n",
       " 'raiding': 18315,\n",
       " 'uki': 11839,\n",
       " 'shortages': 19226,\n",
       " 'nn': 12829,\n",
       " 'expanded': 8334,\n",
       " '412': 25707,\n",
       " 'wrecked': 18665,\n",
       " 'jamaica': 14181,\n",
       " '‛': 1044,\n",
       " 'eager': 16789,\n",
       " 'harlem': 22073,\n",
       " 'reinstated': 21484,\n",
       " 'seekers': 25355,\n",
       " 'countless': 27097,\n",
       " 'asu': 24907,\n",
       " '朧': 2432,\n",
       " 'goodman': 18344,\n",
       " 'guine': 10986,\n",
       " 'filler': 29624,\n",
       " 'proceeded': 12316,\n",
       " 'psychop': 29956,\n",
       " 'malta': 13552,\n",
       " 'ibility': 6398,\n",
       " 'improvisation': 28523,\n",
       " 'hmas': 22202,\n",
       " 'event': 5005,\n",
       " 'reserves': 13820,\n",
       " 'influencing': 28549,\n",
       " 'nerve': 16865,\n",
       " 'depending': 10707,\n",
       " '黃': 3985,\n",
       " 'incentives': 26817,\n",
       " 'ako': 24589,\n",
       " 'vertebr': 11793,\n",
       " 'rockstar': 24276,\n",
       " 'cambrian': 24533,\n",
       " 'ottoman': 8484,\n",
       " 'hen': 5471,\n",
       " 'ɺ': 173,\n",
       " 'phillips': 14640,\n",
       " 'alley': 16107,\n",
       " 'militant': 24668,\n",
       " 'reznor': 25154,\n",
       " 'glas': 8236,\n",
       " 'aristocracy': 21571,\n",
       " '₤': 1070,\n",
       " 'cules': 12397,\n",
       " 'oste': 20812,\n",
       " 'denny': 24564,\n",
       " 'depict': 16457,\n",
       " 'manage': 12608,\n",
       " 'ongo': 10434,\n",
       " 'revolving': 25972,\n",
       " '质': 3561,\n",
       " 'waterloo': 20529,\n",
       " 'ican': 12950,\n",
       " 'lets': 9323,\n",
       " 'bis': 8915,\n",
       " 'yl': 8125,\n",
       " 'ngo': 21158,\n",
       " 'resolved': 12185,\n",
       " 'ន': 975,\n",
       " 'pull': 12300,\n",
       " 'shor': 8299,\n",
       " 'encourage': 14280,\n",
       " 'amine': 17491,\n",
       " 'eroded': 21435,\n",
       " 'quickest': 28868,\n",
       " 'leader': 5864,\n",
       " 'somewhere': 17197,\n",
       " 'negro': 17303,\n",
       " '610': 19695,\n",
       " 'devastation': 29432,\n",
       " 'gang': 8712,\n",
       " 'charted': 12138,\n",
       " 'teaming': 29060,\n",
       " 'sens': 8105,\n",
       " 'talent': 9932,\n",
       " 'aram': 22648,\n",
       " 'spheric': 12948,\n",
       " '紫': 3107,\n",
       " '霆': 3829,\n",
       " 'undoubted': 23889,\n",
       " 'hein': 13078,\n",
       " 'bulls': 20757,\n",
       " 'taxon': 11052,\n",
       " '蕭': 3359,\n",
       " '拐': 2296,\n",
       " 'blers': 23605,\n",
       " 'eat': 9191,\n",
       " 'meier': 26829,\n",
       " 'celest': 17676,\n",
       " 'models': 8855,\n",
       " 'dante': 21207,\n",
       " 'roy': 4761,\n",
       " 'vulgar': 24350,\n",
       " 'mercia': 21031,\n",
       " 'surpassing': 21048,\n",
       " 'ム': 1363,\n",
       " 'deeming': 28846,\n",
       " 'sust': 7560,\n",
       " 'forrest': 20373,\n",
       " 'derivative': 19821,\n",
       " 'detective': 13236,\n",
       " 'و': 421,\n",
       " 'attracts': 25647,\n",
       " 'posting': 17305,\n",
       " 'grae': 29173,\n",
       " 'enacted': 15713,\n",
       " 'ᠯ': 1005,\n",
       " 'v': 64,\n",
       " 'establishment': 9265,\n",
       " 'rode': 14170,\n",
       " 'gamesradar': 25642,\n",
       " 'collaborating': 25391,\n",
       " 'rebirth': 25069,\n",
       " 'fares': 24667,\n",
       " 'veland': 9453,\n",
       " 'economically': 19924,\n",
       " 'music': 4625,\n",
       " '拉': 2293,\n",
       " '芙': 3273,\n",
       " '890': 26547,\n",
       " '章': 3041,\n",
       " 'ऐ': 466,\n",
       " '007': 22188,\n",
       " 'bann': 12295,\n",
       " 'empress': 16550,\n",
       " '樗': 2558,\n",
       " 'od': 4290,\n",
       " 'precise': 15359,\n",
       " 'te': 4164,\n",
       " 'hanging': 14862,\n",
       " 'tand': 25009,\n",
       " 'integer': 26407,\n",
       " 'listeners': 17165,\n",
       " 'alth': 5597,\n",
       " 'niks': 22691,\n",
       " 'bisexual': 26010,\n",
       " 'proximity': 14773,\n",
       " 'commod': 21986,\n",
       " 'ign': 4308,\n",
       " 'incorporating': 16014,\n",
       " 'triangular': 19828,\n",
       " 'ironclads': 21034,\n",
       " '▢': 1167,\n",
       " 'fought': 8358,\n",
       " 'l': 54,\n",
       " 'unclear': 12143,\n",
       " 'contrasts': 22810,\n",
       " 'ය': 683,\n",
       " 'infloresc': 29717,\n",
       " '荔': 3302,\n",
       " 'seamount': 29652,\n",
       " 'withdrawal': 11765,\n",
       " '梶': 2516,\n",
       " 'zagreb': 17773,\n",
       " 'alling': 16892,\n",
       " 'algae': 24738,\n",
       " 'catches': 16575,\n",
       " 'archipel': 16698,\n",
       " 'mercenary': 27705,\n",
       " 'prevent': 6480,\n",
       " 'suitable': 10226,\n",
       " 'valve': 16294,\n",
       " 'sanctioned': 22097,\n",
       " 'romeo': 18707,\n",
       " '曼': 2418,\n",
       " 'debating': 29474,\n",
       " 'tov': 20458,\n",
       " '糎': 3092,\n",
       " 'shears': 29871,\n",
       " 'assumptions': 27384,\n",
       " 'ject': 4809,\n",
       " '男': 2894,\n",
       " 'kyle': 12919,\n",
       " 'outing': 17992,\n",
       " 'savoy': 19829,\n",
       " 'ira': 7082,\n",
       " 'extraction': 20497,\n",
       " 'exploding': 26376,\n",
       " 'lizard': 22178,\n",
       " 'repeal': 23972,\n",
       " 'neighborhoods': 16222,\n",
       " 'pl': 4185,\n",
       " 'orthodo': 11999,\n",
       " 'unacceptable': 25159,\n",
       " 'antim': 23587,\n",
       " 'fluence': 19986,\n",
       " 'destruction': 9389,\n",
       " 'coon': 17015,\n",
       " 'exc': 7643,\n",
       " 'berkshire': 26859,\n",
       " 'icht': 28826,\n",
       " 'accumulate': 25424,\n",
       " 'palette': 28094,\n",
       " 'hunted': 19041,\n",
       " 'chants': 29771,\n",
       " '疆': 2907,\n",
       " 'prefect': 16677,\n",
       " 'injection': 23394,\n",
       " 'dead': 6283,\n",
       " 'companies': 7357,\n",
       " 'vener': 19577,\n",
       " 'curator': 24693,\n",
       " 'chiefs': 13779,\n",
       " 'puzzles': 16069,\n",
       " 'craw': 11030,\n",
       " 'u2': 14632,\n",
       " 'believe': 7881,\n",
       " 'nba': 9350,\n",
       " '白': 2919,\n",
       " 'workshops': 21579,\n",
       " 'pem': 22771,\n",
       " 'ـ': 413,\n",
       " 'princes': 16933,\n",
       " 'ᚱ': 940,\n",
       " 'smo': 6950,\n",
       " 'temporarily': 11054,\n",
       " 'brom': 16501,\n",
       " 'carla': 29735,\n",
       " '箪': 3067,\n",
       " 'gaseous': 28709,\n",
       " '潜': 2744,\n",
       " 'lat': 5311,\n",
       " 'shepher': 15570,\n",
       " 'harper': 15118,\n",
       " 'eyck': 27864,\n",
       " 'averages': 16681,\n",
       " 'dred': 16530,\n",
       " 'insol': 26943,\n",
       " 'crates': 25015,\n",
       " 'igm': 29551,\n",
       " 'cautious': 22700,\n",
       " 'ascended': 23776,\n",
       " 'contaminated': 21573,\n",
       " 'brilliant': 12247,\n",
       " 'bauer': 22487,\n",
       " 'gy': 5110,\n",
       " 'altit': 11333,\n",
       " 'mccul': 26143,\n",
       " 'chronic': 12171,\n",
       " 'ର': 571,\n",
       " 'itates': 28494,\n",
       " '逯': 3641,\n",
       " 'metamor': 20435,\n",
       " 'riv': 17373,\n",
       " '歌': 2586,\n",
       " 'gond': 20646,\n",
       " 'attained': 11400,\n",
       " 'paraguay': 22273,\n",
       " 'streak': 12129,\n",
       " 'laun': 5922,\n",
       " 'lying': 8217,\n",
       " 'tgs': 29716,\n",
       " 'wic': 8465,\n",
       " 'ᡨ': 1013,\n",
       " 'vietnames': 10828,\n",
       " 'equally': 12733,\n",
       " 'gul': 12785,\n",
       " 'counted': 12833,\n",
       " 'skins': 17361,\n",
       " 'ketts': 22712,\n",
       " 'sporadic': 23979,\n",
       " 'bail': 16123,\n",
       " 'conclude': 19569,\n",
       " 'վ': 353,\n",
       " '芮': 3279,\n",
       " 'pen': 4877,\n",
       " '152': 15859,\n",
       " 'occupation': 10172,\n",
       " 'augusta': 22051,\n",
       " 'voting': 11961,\n",
       " 'atts': 26713,\n",
       " 'winehouse': 29897,\n",
       " '[CLS]': 1,\n",
       " 'ilai': 27438,\n",
       " 'pah': 29184,\n",
       " 'businesses': 10850,\n",
       " 'adjut': 24296,\n",
       " 'imprisonment': 15541,\n",
       " 'coldplay': 24992,\n",
       " 'maurit': 18672,\n",
       " 'serbia': 13491,\n",
       " 'montreal': 10913,\n",
       " 'iner': 11119,\n",
       " 'rift': 16929,\n",
       " 'translates': 25458,\n",
       " 'recruitment': 20195,\n",
       " 'lifting': 19119,\n",
       " 'neat': 24375,\n",
       " 'snat': 27362,\n",
       " 'gorbach': 28270,\n",
       " 'coen': 26620,\n",
       " 'colombian': 29032,\n",
       " 'oblig': 11527,\n",
       " '夷': 1906,\n",
       " 'ane': 4633,\n",
       " 'spencer': 14704,\n",
       " 'achu': 8722,\n",
       " 'erness': 23687,\n",
       " '—': 1038,\n",
       " 'chaplain': 24859,\n",
       " 'airplay': 15275,\n",
       " 'onse': 6443,\n",
       " 'ustr': 6962,\n",
       " 'graduates': 16830,\n",
       " 'ʗ': 197,\n",
       " 'ฤ': 724,\n",
       " '圣': 1828,\n",
       " 'cessions': 20992,\n",
       " '裁': 3423,\n",
       " 'agric': 8087,\n",
       " 'second': 4527,\n",
       " 'vigorous': 22214,\n",
       " 'else': 7606,\n",
       " 'identifies': 18524,\n",
       " 'orni': 16784,\n",
       " 'todd': 11434,\n",
       " 'freeze': 23540,\n",
       " 'das': 15158,\n",
       " 'relocated': 12461,\n",
       " 'jared': 24292,\n",
       " 'mole': 8906,\n",
       " 'mif': 20841,\n",
       " 'lovers': 17320,\n",
       " 'tzman': 24702,\n",
       " 'pseudonym': 20901,\n",
       " '根': 2490,\n",
       " 'become': 5394,\n",
       " 'coherent': 22338,\n",
       " 'ambush': 13744,\n",
       " 'near': 4770,\n",
       " '耳': 3202,\n",
       " 'influenza': 20675,\n",
       " 'doctrines': 29148,\n",
       " 'jolie': 26055,\n",
       " 'spells': 19176,\n",
       " 'shab': 26679,\n",
       " 'dubbed': 11428,\n",
       " 'cir': 5416,\n",
       " 'spro': 28447,\n",
       " 'researcher': 21281,\n",
       " 'eland': 10162,\n",
       " 'hooked': 27032,\n",
       " 'dispos': 16695,\n",
       " 'gent': 8878,\n",
       " 'niel': 10071,\n",
       " 'tics': 16202,\n",
       " 'grie': 19016,\n",
       " 'textures': 24598,\n",
       " 'tann': 17040,\n",
       " 'judged': 15438,\n",
       " 'octagonal': 25913,\n",
       " 'ڈ': 428,\n",
       " 'व': 494,\n",
       " '希': 2090,\n",
       " 'condemnation': 28798,\n",
       " 'ets': 7722,\n",
       " 'incend': 28679,\n",
       " 'numerous': 6808,\n",
       " 'indies': 11215,\n",
       " 'normandy': 13620,\n",
       " 'sensors': 27182,\n",
       " 'purchasing': 17174,\n",
       " 'drifted': 20666,\n",
       " 'ifiable': 21043,\n",
       " 'collaborations': 23106,\n",
       " 'twe': 14737,\n",
       " 'revive': 20715,\n",
       " 'seventeenth': 18468,\n",
       " 'clif': 11152,\n",
       " '轉': 3599,\n",
       " 'krajina': 25156,\n",
       " 'som': 11397,\n",
       " 'explosive': 14312,\n",
       " 'ז': 368,\n",
       " '謡': 3495,\n",
       " '狼': 2836,\n",
       " 'supreme': 8394,\n",
       " 'cey': 20284,\n",
       " 'valentine': 17527,\n",
       " 'italia': 18169,\n",
       " 'further': 5166,\n",
       " 'vijay': 18757,\n",
       " 'drey': 18964,\n",
       " 'rez': 20457,\n",
       " 'variety': 7104,\n",
       " 'benz': 15973,\n",
       " 'atis': 22236,\n",
       " 'mould': 18167,\n",
       " 'ctr': 11598,\n",
       " 'villagers': 19768,\n",
       " 'cranes': 28497,\n",
       " '駐': 3910,\n",
       " 'played': 4997,\n",
       " 'mccall': 25935,\n",
       " 'zies': 26971,\n",
       " 'avia': 12673,\n",
       " 'ಃ': 629,\n",
       " 'relied': 14550,\n",
       " 'consultant': 17497,\n",
       " '矮': 2958,\n",
       " 'inaugur': 20100,\n",
       " 'peaked': 7513,\n",
       " 'calak': 26560,\n",
       " 'coden': 20075,\n",
       " 'residential': 10311,\n",
       " 'drav': 29778,\n",
       " 'couple': 7938,\n",
       " 'scrapped': 14931,\n",
       " 'ప': 617,\n",
       " 'twc': 12376,\n",
       " 'preview': 19186,\n",
       " 'sympath': 19858,\n",
       " '命': 1769,\n",
       " '酈': 3685,\n",
       " 'envision': 17940,\n",
       " 'purely': 16728,\n",
       " 'cedar': 16161,\n",
       " 'ivid': 5846,\n",
       " 'designing': 17519,\n",
       " 'narrowly': 16350,\n",
       " 'slain': 26077,\n",
       " 'synthesizer': 20807,\n",
       " 'daft': 28857,\n",
       " 'preparatory': 22517,\n",
       " 'hemingway': 17499,\n",
       " 'delivers': 21405,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.token_to_id(\"[SEP]\"))\n",
    "print(tokenizer.id_to_token(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the given sequence and pair. This method can process raw text sequences as well as already pre-tokenized sequences.\n",
    "output = tokenizer.encode(\n",
    "    sequence=\"Hello, y'all! How are you 😁 ?\",  # 未分好词\n",
    "    is_pretokenized=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?']\n",
      "[22477, 16, 67, 11, 4190, 5, 5405, 4200, 4815, 0, 35]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(output.tokens)\n",
    "print(output.ids)\n",
    "print(output.type_ids)  # The generated type IDs\n",
    "print(output.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'pre', 'token', 'ized', 'sequence', 'and', 'its', 'pair']\n",
      "[43, 4333, 26647, 4806, 7453, 4112, 4269, 5704]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_pair = tokenizer.encode(\n",
    "    sequence=[\"A\", \"pre\", \"tokenized\", \"sequence\"],  # 已经分好词\n",
    "    pair=[\"And\", \"its\", \"pair\"],\n",
    "    # Whether the input is already pre-tokenized\n",
    "    is_pretokenized=True\n",
    ")\n",
    "print(output_pair.tokens)\n",
    "print(output_pair.ids)\n",
    "print(output_pair.type_ids)\n",
    "print(output_pair.attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Post-Processors\n",
    "\n",
    "After the whole pipeline, we sometimes want to insert some special tokens before feed a tokenized string into a model like ”[CLS] My horse is amazing [SEP]”. The PostProcessor is the component doing just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Provides a way to specify templates in order to add the special tokens to each input sequence as relevant.\n",
    "# Let’s take BERT tokenizer as an example. It uses two special tokens, used to delimitate each sequence. [CLS] is always used at the beginning of the first sequence, and [SEP] is added at the end of both the first, and the pair sequences. The final result looks like this:\n",
    "# Then, we specify the template for sentence pairs, which should have the form \"[CLS] $A [SEP] $B [SEP]\" where $A represents the first sentence and $B the second one. The :1 added in the template represent the type IDs we want for each part of our input: it defaults to 0 for everything (which is why we don’t have $A:0) and here we set it to 1 for the tokens of the second sentence and the last \"[SEP]\" token.\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    # The template used for single sequences\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    # The template used when both sequences are specified\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    #  The list of special tokens used in each sequences\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n",
      "[1, 22477, 16, 67, 11, 4190, 5, 5405, 4200, 4815, 0, 35, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_temp = tokenizer.encode(\n",
    "    sequence=\"Hello, y'all! How are you 😁 ?\",  # 未分好词\n",
    "    is_pretokenized=False)\n",
    "\n",
    "print(output_temp.tokens)\n",
    "print(output_temp.ids)\n",
    "print(output_temp.type_ids)\n",
    "print(output_temp.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'a', 'pre', 'token', 'ized', 'sequence', '[SEP]', 'and', 'its', 'pair', '[SEP]']\n",
      "[1, 43, 4333, 26647, 4806, 7453, 2, 4112, 4269, 5704, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_pair_temp = tokenizer.encode(\n",
    "    sequence=[\"A\", \"pre\", \"tokenized\", \"sequence\"],  # 已经分好词\n",
    "    pair=[\"And\", \"its\", \"pair\"],\n",
    "    # Whether the input is already pre-tokenized\n",
    "    is_pretokenized=True\n",
    ")\n",
    "\n",
    "print(output_pair_temp.tokens)\n",
    "print(output_pair_temp.ids)\n",
    "print(output_pair_temp.type_ids)\n",
    "print(output_pair_temp.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=15, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
      "\n",
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', '[SEP]']\n",
      "[1, 22477, 16, 67, 11, 4190, 5, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "['[CLS]', 'how', 'are', 'you', '[UNK]', '?', ',', 'i', 'am', 'fine', '!', 'thank', 'you', '!', '[SEP]']\n",
      "[1, 5405, 4200, 4815, 0, 35, 16, 51, 4127, 7961, 5, 16654, 4815, 5, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_batch = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you 😁 ?, i am fine ! thank you!\"])\n",
    "print(output_batch, end='\\n\\n')\n",
    "\n",
    "# 第一个句子\n",
    "print(output_batch[0].tokens)  # 长度为8\n",
    "print(output_batch[0].ids)\n",
    "print(output_batch[0].type_ids)\n",
    "print(output_batch[0].attention_mask, end='\\n\\n')\n",
    "\n",
    "# 第二个句\n",
    "print(output_batch[1].tokens)  # 长度为15\n",
    "print(output_batch[1].ids)\n",
    "print(output_batch[1].type_ids)\n",
    "print(output_batch[1].attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.token_to_id('[PAD]'))\n",
    "\n",
    "# Enable the padding\n",
    "tokenizer.enable_padding(\n",
    "    pad_id=3,  # The id to be used when padding\n",
    "    pad_token=\"[PAD]\",  # The pad token to be used when padding\n",
    "    pad_type_id=0)  # The type id to be used when padding\n",
    "\n",
    "# Enable truncation\n",
    "tokenizer.enable_truncation(\n",
    "    max_length=10)  # 截断的最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
      "\n",
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', '[SEP]', '[PAD]', '[PAD]']\n",
      "[1, 22477, 16, 67, 11, 4190, 5, 2, 3, 3]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "\n",
      "['[CLS]', 'how', 'are', 'you', '[UNK]', '?', ',', 'i', 'am', '[SEP]']\n",
      "[1, 5405, 4200, 4815, 0, 35, 16, 51, 4127, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_batch_padding = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you 😁 ?, i am fine ! thank you!\"])\n",
    "print(output_batch_padding, end='\\n\\n')\n",
    "\n",
    "# 第一个句子(编码结果长度为10,不足部分通过'[PAD']填充)\n",
    "print(output_batch_padding[0].tokens)\n",
    "print(output_batch_padding[0].ids)\n",
    "print(output_batch_padding[0].type_ids)\n",
    "print(output_batch_padding[0].attention_mask, end='\\n\\n')\n",
    "\n",
    "# 第二个句子\n",
    "print(output_batch_padding[1].tokens)\n",
    "print(output_batch_padding[1].ids)\n",
    "print(output_batch_padding[1].type_ids)\n",
    "print(output_batch_padding[1].attention_mask)  # 截断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# WordPiece Decoder\n",
    "tokenizer.decoder = decoders.WordPiece(prefix='##')  # 选择解码器(根据Pre-tokenizers来选择)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello, y'all!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the given list of ids back to a string\n",
    "tokenizer.decode([1, 22477, 16, 67, 11, 4190, 5, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save the :class:`~tokenizers.Tokenizer` to the file at the given path.\n",
    "tokenizer.save(\"../extra_dataset/save_train_tokenizer/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x247605dcf90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a new :class:`~tokenizers.Tokenizer` from the file at the given path.\n",
    "tokenizer_load = Tokenizer.from_file(\"../extra_dataset/save_train_tokenizer/tokenizer-wiki.json\")\n",
    "tokenizer_load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
