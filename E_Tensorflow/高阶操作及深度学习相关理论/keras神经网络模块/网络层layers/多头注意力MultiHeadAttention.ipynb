{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 参考torch.nn.MultiheadAttention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 8, 40)\n",
      "(10, 5, 8, 4)\n"
     ]
    }
   ],
   "source": [
    "layer = MultiHeadAttention(num_heads=5,  # Number of attention heads.\n",
    "                           key_dim=200,  # Size of each attention head for query and key.\n",
    "                           value_dim=50,  # Size of each attention head for value.\n",
    "                           dropout=0.1)  # Dropout probability.\n",
    "\n",
    "mask = tf.range(4)[None, :]  < tf.constant([2, 2, 2, 2, 3, 3, 3, 3])[:, None]\n",
    "mask = tf.expand_dims(mask, 0)\n",
    "mask = tf.repeat(mask, 10, axis=0)\n",
    "\n",
    "query = tf.random.stateless_uniform(shape=(10, 8, 40), seed=(1, 1))  # query.shape=(B, T, dim_q)\n",
    "value = tf.random.stateless_uniform(shape=(10, 4, 30), seed=(1, 1))  # value.shape=(B, S, dim_v)\n",
    "key = tf.random.stateless_uniform(shape=(10, 4, 20), seed=(1, 1))  # key.shape=(B, S, dim_k)\n",
    "\n",
    "attention_output, attention_scores = layer(query=query,\n",
    "                                           value=value,\n",
    "                                           # if not given, will use value for both key and value, which is the most common case.\n",
    "                                           key=key,\n",
    "                                           # 默认return_attention_scores=False\n",
    "                                           return_attention_scores=True,\n",
    "                                           # attention_mask.shape=(B, T, S)\n",
    "                                           # 1 indicates attention and 0 indicates no attention.(与pytorch含义相反)\n",
    "                                           attention_mask=mask,\n",
    "                                           # 训练模式还是评估模式(默认training=False)\n",
    "                                           training=True)\n",
    "# attention_output.shape=(B, T, E)\n",
    "# T is for target sequence shapes\n",
    "# E is the query input last dimension\n",
    "print(attention_output.shape)\n",
    "\n",
    "# attention_scores.shape=(B, num_heads, T, S)\n",
    "print(attention_scores.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True False False]\n",
      " [ True  True False False]\n",
      " [ True  True False False]\n",
      " [ True  True False False]\n",
      " [ True  True  True False]\n",
      " [ True  True  True False]\n",
      " [ True  True  True False]\n",
      " [ True  True  True False]], shape=(8, 4), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[0.49942783 0.50057214 0.         0.        ]\n",
      " [0.49945465 0.5005454  0.         0.        ]\n",
      " [0.5003021  0.49969792 0.         0.        ]\n",
      " [0.50053257 0.49946746 0.         0.        ]\n",
      " [0.3331651  0.33336708 0.33346784 0.        ]\n",
      " [0.33350733 0.33324602 0.33324662 0.        ]\n",
      " [0.33371982 0.33332738 0.33295283 0.        ]\n",
      " [0.3335802  0.33322385 0.33319595 0.        ]], shape=(8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(mask[0, :, :])\n",
    "print(attention_scores[0, 0, :, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_head_attention_25/query/kernel:0 (40, 5, 200)\n",
      "multi_head_attention_25/query/bias:0 (5, 200)\n",
      "multi_head_attention_25/key/kernel:0 (20, 5, 200)\n",
      "multi_head_attention_25/key/bias:0 (5, 200)\n",
      "multi_head_attention_25/value/kernel:0 (30, 5, 50)\n",
      "multi_head_attention_25/value/bias:0 (5, 50)\n",
      "multi_head_attention_25/attention_output/kernel:0 (5, 50, 40)\n",
      "multi_head_attention_25/attention_output/bias:0 (40,)\n"
     ]
    }
   ],
   "source": [
    "for i in layer.weights:\n",
    "    print(i.name, i.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}