{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PyTorch归一化层参数解释\n",
    "\n",
    "* num_features:\n",
    "    * BatchNorm1d:$C$ from an expected input of size $(N, C, L)$ or $L$ from input of size $(N, L)$\n",
    "    * BatchNorm2d:$C$ from an expected input of size $(N, C, H, W)$;若输入向量维度为:$()$,则具体计算如下所示:\n",
    "\n",
    "    $$ E(X_C) = \\frac{1}{N \\times H \\times W} \\sum_{N,H,W} X_C $$\n",
    "\n",
    "    $$ \\mathrm{var}(X_C) = \\frac{1}{N \\times H \\times W} \\sum_{N,H,W} \\left( X_C - E(X_C) \\right)^2 $$\n",
    "\n",
    "    * BatchNorm2d:$C$ from an expected input of size $(N, C, D, H, W)$\n",
    "\n",
    "* eps:浮点常数$\\epsilon$,为了防止计算过程中分母为零\n",
    "\n",
    "* affine:\n",
    "    * 如果affine=False,则固定$\\gamma=1,\\beta=0$.\n",
    "    * else,this module has learnable affine parameters.\n",
    "\n",
    "* momentum:the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1\n",
    "\n",
    "    $$ running\\_mean_{t} = (1 - momentum) \\times running\\_mean_{t-1} + momentum \\times \\hat{x}_t $$\n",
    "\n",
    "* track_running_stats:a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class bn(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        super(bn, self).__init__()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(C, eps=0, momentum=0.3,\n",
    "                                  # 若track_running_stats=False,则running_mean=None,running_var=None,\\mu和\\sigma由本批次数据计算得出\n",
    "                                  track_running_stats=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn1(x)\n",
    "\n",
    "\n",
    "net = bn(C=4) # num_features为通道C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]], grad_fn=<NativeBatchNormBackward>)\n",
      "tensor([0.3000, 0.3000, 0.3000, 0.3000])\n",
      "tensor([0.7000, 0.7000, 0.7000, 0.7000])\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]], grad_fn=<NativeBatchNormBackward>)\n",
      "tensor([0.5100, 0.5100, 0.5100, 0.5100])\n",
      "tensor([0.4900, 0.4900, 0.4900, 0.4900])\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]], grad_fn=<NativeBatchNormBackward>)\n",
      "tensor([0.6570, 0.6570, 0.6570, 0.6570])\n",
      "tensor([0.3430, 0.3430, 0.3430, 0.3430])\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]], grad_fn=<NativeBatchNormBackward>)\n",
      "tensor([0.7599, 0.7599, 0.7599, 0.7599])\n",
      "tensor([0.2401, 0.2401, 0.2401, 0.2401])\n"
     ]
    }
   ],
   "source": [
    "entry = torch.ones((2, 4, 1))\n",
    "\n",
    "for i in range(4):\n",
    "    print(net(entry)) # 训练过程:\\mu和\\sigma总是由本批次数据计算得出\n",
    "    print(net.bn1.running_mean) # 若track_running_stats=True,每次forward之后计算新的running_mean和running_var(模型会对其进行缓存)\n",
    "    print(net.bn1.running_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4900],\n",
       "         [0.4900],\n",
       "         [0.4900],\n",
       "         [0.4900]],\n",
       "\n",
       "        [[0.4900],\n",
       "         [0.4900],\n",
       "         [0.4900],\n",
       "         [0.4900]]], grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()\n",
    "'''\n",
    "测试过程:\n",
    "若self.trainning=False-->\n",
    "由于用于测试的mini-batch数据仅仅是整个数据非常小的一部分,统计特性就会和全局统计特性有着较大偏差容易造成统计特性的偏移,导致预测结果的不准确,\n",
    "故\\mu和\\sigma使用模型训练过程中得到的running_mean与running_var代替(self.trainning=False,track_running_stats=True)\n",
    "'''\n",
    "entry1 = torch.ones((2, 4, 1))\n",
    "t_out = net(entry1)\n",
    "t_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4900],\n",
       "         [0.4900],\n",
       "         [0.4900],\n",
       "         [0.4900]],\n",
       "\n",
       "        [[0.4900],\n",
       "         [0.4900],\n",
       "         [0.4900],\n",
       "         [0.4900]]], grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry2 = torch.ones((2, 4, 1))\n",
    "t_out2 = net(entry2)\n",
    "t_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]], grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train()\n",
    "entry3 = torch.ones((2, 4, 1))\n",
    "t_out3 = net(entry3) # 测试过程:若trainning=True-->此时\\mu和\\sigma仍由本批次的数据计算得出\n",
    "t_out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1., 1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.bn1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.bn1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 4, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry_2d = torch.ones(5, 6, 4, 4)\n",
    "bn_2d = nn.BatchNorm2d(num_features=6) # num_features=C=6\n",
    "bn_2d(entry_2d).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 3, 4, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry_3d = torch.ones(3, 7, 3, 4, 4)\n",
    "bn_3d = nn.BatchNorm3d(num_features=7) # num_features=C=7\n",
    "bn_3d(entry_3d).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
