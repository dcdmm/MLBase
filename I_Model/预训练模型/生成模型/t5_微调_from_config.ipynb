{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8bc64c-fa83-47f1-8612-f5e14532dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "514ae099-e936-4765-a7fc-9f09cbd86fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_from_disk(\"billsum\")['train']\n",
    "billsum = billsum.remove_columns(['Unnamed: 0', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19bb087c-8d54-4564-84e9-1fd5f9b49398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary'],\n",
       "        num_rows: 15159\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary'],\n",
       "        num_rows: 3790\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billsum = billsum.train_test_split(test_size=0.2)\n",
    "billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82b3024-177e-4822-b73c-e22e26b311b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fba4ce073a0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# transformer T5格式为:\n",
    "# - single sequence: `X </s>`\n",
    "# - pair of sequences: `A </s> B </s>`\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d646a5-5402-432d-914c-4e7e74e3ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Prefix the input with a prompt so T5 knows this is a summarization task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "764d3c95-99e4-4b5f-9eaa-e7bd8c44c8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02056264877319336,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 16,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9761e97c558486f9daf552de181eb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017180919647216797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0741feaf07c7438b82d7ce1d70d23e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15159\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3790\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T5ForConditionalGeneration forward函数函数签名:\n",
    "'''\n",
    "def forward(\n",
    "    self,\n",
    "    input_ids: Optional[torch.LongTensor] = None,\n",
    "    attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "    decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
    "    head_mask: Optional[torch.FloatTensor] = None,\n",
    "    decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
    "    cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "    encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    ") -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n",
    "'''\n",
    "tokenized_billsum = billsum.map(preprocess_function, batched=True)\n",
    "tokenized_billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d969fffc-d6b8-4a43-ad2d-fc261bc58873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"t5-small\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"relu\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": false,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 6,\n",
       "  \"num_heads\": 8,\n",
       "  \"num_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 30,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"summarize: \"\n",
       "    },\n",
       "    \"translation_en_to_de\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to German: \"\n",
       "    },\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to French: \"\n",
       "    },\n",
       "    \"translation_en_to_ro\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to Romanian: \"\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.23.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aea7dbd-a138-45e6-a982-71a8431a3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从头训练\n",
    "model = AutoModelForSeq2SeqLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86a33366-5a23-4280-a83b-ecac8343eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相当于torch.utils.data.DataLoader中collate_fn的作用(可以重写,参考K_demo/way_of_training/pytorch_transformer.ipynb)\n",
    "# Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "'''\n",
    "model ([`PreTrainedModel`]):\n",
    "    The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n",
    "    prepare the *decoder_input_ids*\n",
    "\n",
    "    This is useful when using *label_smoothing* to avoid calculating loss twice.\n",
    "'''\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d453e691-1009-463c-a541-96eccb79e769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for\\nevaluating automatic summarization and machine translation software in natural language processing.\\nThe metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\\n\\nNote that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\\n\\nThis metrics is a wrapper around Google Research reimplementation of ROUGE:\\nhttps://github.com/google-research/google-research/tree/master/rouge\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "rouge.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "837bf8ee-79ee-49a7-9dd0-f7b97c6dec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # predictions.shape=[batch_size, max(该批次生成句子长度)]\n",
    "    # labels.shape=[batch_size, max(该批次句子长度)]\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cb8216c-cb7c-47b4-ac41-b8825e7624e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/root/miniconda3/envs/env_3812/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15159\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9480\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9480' max='9480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9480/9480 1:35:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>7.885400</td>\n",
       "      <td>7.130400</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.090800</td>\n",
       "      <td>6.770035</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>6.806800</td>\n",
       "      <td>6.544214</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.124700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>6.604100</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>6.447800</td>\n",
       "      <td>6.223190</td>\n",
       "      <td>0.129800</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>0.114200</td>\n",
       "      <td>0.114300</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>6.299600</td>\n",
       "      <td>6.103204</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.135100</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>6.204100</td>\n",
       "      <td>6.006391</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.144900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>6.089900</td>\n",
       "      <td>5.906005</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>6.005600</td>\n",
       "      <td>5.822850</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.152400</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.927200</td>\n",
       "      <td>5.754549</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>5.846300</td>\n",
       "      <td>5.683355</td>\n",
       "      <td>0.173700</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.151400</td>\n",
       "      <td>0.151400</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>5.794100</td>\n",
       "      <td>5.633137</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.154200</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>5.743100</td>\n",
       "      <td>5.586354</td>\n",
       "      <td>0.173200</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>0.151200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>5.701500</td>\n",
       "      <td>5.549339</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.154400</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>5.655900</td>\n",
       "      <td>5.516887</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.148300</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.631800</td>\n",
       "      <td>5.495425</td>\n",
       "      <td>0.164900</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>5.606700</td>\n",
       "      <td>5.479128</td>\n",
       "      <td>0.175400</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>5.581700</td>\n",
       "      <td>5.468150</td>\n",
       "      <td>0.169400</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.147800</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-500\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-500/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-500/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-500/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-1000\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-1000/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-1000/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-1000/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-1500\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-1500/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-1500/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-1500/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-2000\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-2000/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-2000/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-2000/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-2500\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-2500/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-2500/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-2500/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-3000\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-3000/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-3000/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-3000/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-2500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-3500\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-3500/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-3500/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-3500/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-4000\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-4000/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-4000/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-4000/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-4500\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-4500/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-4500/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-4500/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-5000\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-5000/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-5000/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-5000/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-4500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-5500\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-5500/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-5500/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-5500/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-6000\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-6000/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-6000/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-6000/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-5500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-6500\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-6500/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-6500/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-6500/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-7000\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-7000/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-7000/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-7000/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-6500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-7500\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-7500/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-7500/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-7500/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-7000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-8000\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-8000/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-8000/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-8000/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-7500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-8500\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-8500/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-8500/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-8500/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-8000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, summary. If text, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3790\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_billsum_model_t5/checkpoint-9000\n",
      "Configuration saved in my_awesome_billsum_model_t5/checkpoint-9000/config.json\n",
      "Model weights saved in my_awesome_billsum_model_t5/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model_t5/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model_t5/checkpoint-9000/special_tokens_map.json\n",
      "Copy vocab file to my_awesome_billsum_model_t5/checkpoint-9000/spiece.model\n",
      "Deleting older checkpoint [my_awesome_billsum_model_t5/checkpoint-8500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9480, training_loss=6.133500523305644, metrics={'train_runtime': 5748.3281, 'train_samples_per_second': 26.371, 'train_steps_per_second': 1.649, 'total_flos': 4.103292737028096e+16, 'train_loss': 6.133500523305644, 'epoch': 10.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_awesome_billsum_model_t5\",\n",
    "    save_total_limit=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=10,\n",
    "\n",
    "    # 区别于TrainingArguments特有参数:\n",
    "    # predict_with_generate (bool, optional, defaults to False) — Whether to use generate to calculate generative metrics (ROUGE, BLEU).\n",
    "    predict_with_generate=True\n",
    ")\n",
    "\n",
    "# 继承自Trainer\n",
    "# predict方法:trainer.predict(tokenized_billsum[\"test\"], **gen_kwargs)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# generate参数含义参考:huggingface GenerationConfig类\n",
    "# 参考generate_help.md\n",
    "trainer._gen_kwargs = {\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"max_length\": 50,\n",
    "    \"min_length\": 0}  # 用于评估\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9882a1d-bd9f-4337-98ad-ef41e8607542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T5ForConditionalGeneration\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60a9f24f-785d-427a-ba51-e956c90dd5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    3,    9,    3,    9, 1015,    3,    9,    3,    9,    3,    9,\n",
       "            3,    9,    3,    9,    3,    9,    3,    9,    3,    9,    3,    9,\n",
       "            3,    9,    3,    9,    3,    9,    3,    9,    3,    9,    3,    9,\n",
       "            3,    9,    3,    9,    3,    9,    3,    9,    3,    9,    3,    9,\n",
       "            3,    9]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference\n",
    "inputs = tokenizer(\n",
    "    \"summarize: Shields a business entity from civil liability relating to any injury or death occurring at a facility of that entity in connection with a use of such facility by a nonprofit organization if: (1) the use occurs outside the scope of business of the business entity; (2) such injury or death occurs during a period that such facility is used by such organization; and (3) the business entity authorized the use of such facility by the organization. Makes this Act inapplicable to an injury or death that results from an act or omission of a business entity that constitutes gross negligence or intentional misconduct, including misconduct that: (1) constitutes a hate crime or a crime of violence or act of international terrorism for which the defendant has been convicted in any court; or (2) involves a sexual offense for which the defendant has been convicted in any court or misconduct for which the defendant has been found to have violated a Federal or State civil rights law. Preempts State laws to the extent that such laws are inconsistent with this Act, except State law that provides additional protection from liability. Specifies that this Act shall not be construed to supersede any Federal or State health or safety law. Makes this Act inapplicable to any civil action in a State court against a business entity in which all parties are citizens of the State if such State, citing this Act's authority and containing no other provision, enacts a statute declaring the State's election that this Act shall not apply to such action in the State.\",\n",
    "    return_tensors=\"pt\").input_ids\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(inputs, repetition_penalty=1.0, min_length=0, max_length=50)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24c5b915-f78c-44f2-b7ae-8ed7ec94c232",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a State a a a a a a a a a a a a a a a a a a a a a a\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3812",
   "language": "python",
   "name": "env_3812"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
