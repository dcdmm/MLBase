{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e653f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d21ea54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbcb495d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '和', '我', '的', '祖', '国', '一', '刻', '也', '不', '能', '分', '离', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participle = tokenizer.tokenize(\"我和我的祖国一刻也不能分离!\")\n",
    "participle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2de999a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2769, 1469, 2769, 4638, 4862, 1744, 671, 1174, 738, 679, 5543, 1146, 4895, 106]\n",
      "['我', '和', '我', '的', '祖', '国', '一', '刻', '也', '不', '能', '分', '离', '!']\n",
      "我 和 我 的 祖 国 一 刻 也 不 能 分 离!\n"
     ]
    }
   ],
   "source": [
    "tti = tokenizer.convert_tokens_to_ids(participle)\n",
    "print(tti)\n",
    "\n",
    "itt = tokenizer.convert_ids_to_tokens(tti)\n",
    "print(itt)\n",
    "\n",
    "print(tokenizer.decode(tti))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
