{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 输出爆炸分析\n",
    "1. 若随机变量$X,Y$相互独立,则有\n",
    "\n",
    "    $ E(XY) = E(X)E(Y) $\n",
    "\n",
    "    $ D(X+Y) = D(X) + D(Y)$\n",
    "\n",
    "    $D(X) = E(X^2) - \\left[E(X)\\right]^2$\n",
    "\n",
    "2. 故可得\n",
    "\n",
    "    \\begin{align}\n",
    "    D(XY) &= E(X^2Y^2) - [E(X)]^2[E(Y)]^2 \\\\\n",
    "    &=  D(X)D(Y) + D(X)[E(Y)]^2 + D(Y)[E(X)]^2\n",
    "    \\end{align}\n",
    "\n",
    "3. 若$X,Y$服从0均值1标准差的正态分布,则有\n",
    "\n",
    "    $D(XY)=D(X)D(Y)$\n",
    "\n",
    "4.  对于由$n$个神经元组成的全连接层有\n",
    "\n",
    "    $ H^1 = \\sum_{i=1}^n X_i w^T_{1i} $\n",
    "\n",
    "    $D(H^1) = n*(1*1) = n $\n",
    "\n",
    "5. 同理可得\n",
    "\n",
    "    $ H^2 = \\sum_{i=1}^n H^1_i w^T_{2i} $\n",
    "\n",
    "    $D(H^2) = n*(n*1) = n^2 $\n",
    "\n",
    "    $D(H^3) = n^{3}$\n",
    "\n",
    "    $\\vdots$\n",
    "\n",
    "    $D(H^m) = n^{m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:253.48338317871094\n",
      "layer:1, std:67583.1796875\n",
      "layer:2, std:17795620.0\n",
      "layer:3, std:4671546368.0\n",
      "layer:4, std:1204450099200.0\n",
      "layer:5, std:310462382080000.0\n",
      "layer:6, std:8.013987208547533e+16\n",
      "layer:7, std:2.0335619288321753e+19\n",
      "layer:8, std:5.019494845985134e+21\n",
      "layer:9, std:1.1908801481093347e+24\n",
      "layer:10, std:3.038827151994154e+26\n",
      "layer:11, std:7.847845850090988e+28\n",
      "layer:12, std:2.0367696902373843e+31\n",
      "layer:13, std:5.15286537343349e+33\n",
      "layer:14, std:1.3578871990112007e+36\n",
      "layer:15, std:inf\n",
      "layer:16, std:inf\n",
      "layer:17, std:inf\n",
      "layer:18, std:inf\n",
      "layer:19, std:inf\n",
      "layer:20, std:inf\n",
      "layer:21, std:inf\n",
      "layer:22, std:inf\n",
      "layer:23, std:inf\n",
      "layer:24, std:inf\n",
      "layer:25, std:inf\n",
      "layer:26, std:inf\n",
      "layer:27, std:inf\n",
      "layer:28, std:inf\n",
      "layer:29, std:inf\n",
      "layer:30, std:inf\n",
      "layer:31, std:nan\n",
      "output is nan in 31 layers\n",
      "tensor([[       -inf,  1.5891e+38,        -inf,  ..., -5.3846e+37,\n",
      "                 inf,         inf],\n",
      "        [-3.7783e+37, -2.6330e+38, -1.6608e+38,  ...,         inf,\n",
      "                -inf, -1.1436e+38],\n",
      "        [        inf,         inf,         inf,  ...,         inf,\n",
      "         -2.3777e+38,        -inf],\n",
      "        ...,\n",
      "        [ 1.0493e+38,  5.1593e+37, -2.0412e+38,  ...,        -inf,\n",
      "                -inf,         inf],\n",
      "        [       -inf, -5.2706e+37, -4.9698e+37,  ...,         inf,\n",
      "                -inf,  2.3045e+38],\n",
      "        [        inf,         inf,         inf,  ...,         inf,\n",
      "                 inf,        -inf]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for _ in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "\n",
    "            print(\"layer:{}, std:{}\".format(i, x.var())) # x的方差按指数级进行增长\n",
    "            if torch.isnan(x.var()): # 某一层方差为nan时跳出循环\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight) # 整体初始化为标正态分布(标准正态分布的随机抽样也是正态分布)\n",
    "\n",
    "\n",
    "layer_nums = 100\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "inputs = torch.randn((batch_size, neural_nums))\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "解决:将权重矩阵$W$的方差初始化为$\\frac{1}{n}$,此时有\n",
    "\n",
    "$ D(H^1) = n*(\\frac{1}{n}*1) = 1 $\n",
    "\n",
    "$ D(H^2) = n*(\\frac{1}{n}*1) = 1 $\n",
    "\n",
    "$ \\vdots $\n",
    "\n",
    "$ D(H^m) = n*(\\frac{1}{n}*1) = 1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:0.9735792279243469\n",
      "layer:1, std:0.99771648645401\n",
      "layer:2, std:0.9953316450119019\n",
      "layer:3, std:1.0132945775985718\n",
      "layer:4, std:0.9622108936309814\n",
      "layer:5, std:0.9523491859436035\n",
      "layer:6, std:0.9915835857391357\n",
      "layer:7, std:0.9701160788536072\n",
      "layer:8, std:0.9977512955665588\n",
      "layer:9, std:0.9572206139564514\n",
      "layer:10, std:0.9705260992050171\n",
      "layer:11, std:0.9550902843475342\n",
      "layer:12, std:0.9415923357009888\n",
      "layer:13, std:0.9503766894340515\n",
      "layer:14, std:0.9189853668212891\n",
      "layer:15, std:0.9159332513809204\n",
      "layer:16, std:0.8976818919181824\n",
      "layer:17, std:0.8763459324836731\n",
      "layer:18, std:0.8999841809272766\n",
      "layer:19, std:0.90842205286026\n",
      "layer:20, std:0.8796555995941162\n",
      "layer:21, std:0.8616349101066589\n",
      "layer:22, std:0.9095585942268372\n",
      "layer:23, std:0.9559931755065918\n",
      "layer:24, std:0.9570003151893616\n",
      "layer:25, std:1.005898118019104\n",
      "layer:26, std:1.002387523651123\n",
      "layer:27, std:1.0296342372894287\n",
      "layer:28, std:1.0083959102630615\n",
      "layer:29, std:0.9582086205482483\n",
      "layer:30, std:0.9601848125457764\n",
      "layer:31, std:0.9815183281898499\n",
      "layer:32, std:0.9306061267852783\n",
      "layer:33, std:0.8840558528900146\n",
      "layer:34, std:0.8852319121360779\n",
      "layer:35, std:0.8811249732971191\n",
      "layer:36, std:0.8838598132133484\n",
      "layer:37, std:0.8831741809844971\n",
      "layer:38, std:0.8454844355583191\n",
      "layer:39, std:0.8008571863174438\n",
      "layer:40, std:0.8097031116485596\n",
      "layer:41, std:0.8206263184547424\n",
      "layer:42, std:0.8503687977790833\n",
      "layer:43, std:0.8327146172523499\n",
      "layer:44, std:0.8609479665756226\n",
      "layer:45, std:0.9513745903968811\n",
      "layer:46, std:0.9442869424819946\n",
      "layer:47, std:0.9922189712524414\n",
      "layer:48, std:1.0253424644470215\n",
      "layer:49, std:0.9955881237983704\n",
      "layer:50, std:0.9938708543777466\n",
      "layer:51, std:1.0166078805923462\n",
      "layer:52, std:1.0771064758300781\n",
      "layer:53, std:1.1217222213745117\n",
      "layer:54, std:1.146207571029663\n",
      "layer:55, std:1.1235368251800537\n",
      "layer:56, std:0.9942228198051453\n",
      "layer:57, std:1.0820683240890503\n",
      "layer:58, std:1.1029993295669556\n",
      "layer:59, std:1.1122978925704956\n",
      "layer:60, std:1.0631464719772339\n",
      "layer:61, std:1.0587440729141235\n",
      "layer:62, std:1.01591157913208\n",
      "layer:63, std:1.0787800550460815\n",
      "layer:64, std:1.0720807313919067\n",
      "layer:65, std:1.098647952079773\n",
      "layer:66, std:0.9952507615089417\n",
      "layer:67, std:0.997542679309845\n",
      "layer:68, std:1.1660581827163696\n",
      "layer:69, std:1.1473134756088257\n",
      "layer:70, std:1.1395409107208252\n",
      "layer:71, std:1.2488716840744019\n",
      "layer:72, std:1.1614140272140503\n",
      "layer:73, std:1.1235929727554321\n",
      "layer:74, std:1.1391044855117798\n",
      "layer:75, std:1.184990644454956\n",
      "layer:76, std:1.2637629508972168\n",
      "layer:77, std:1.2575829029083252\n",
      "layer:78, std:1.3438684940338135\n",
      "layer:79, std:1.3388055562973022\n",
      "layer:80, std:1.2798047065734863\n",
      "layer:81, std:1.3586148023605347\n",
      "layer:82, std:1.4580947160720825\n",
      "layer:83, std:1.4152158498764038\n",
      "layer:84, std:1.4090113639831543\n",
      "layer:85, std:1.3244364261627197\n",
      "layer:86, std:1.4164801836013794\n",
      "layer:87, std:1.4663506746292114\n",
      "layer:88, std:1.4753021001815796\n",
      "layer:89, std:1.520526647567749\n",
      "layer:90, std:1.7175564765930176\n",
      "layer:91, std:1.6314905881881714\n",
      "layer:92, std:1.6642582416534424\n",
      "layer:93, std:1.6258838176727295\n",
      "layer:94, std:1.5068645477294922\n",
      "layer:95, std:1.5005888938903809\n",
      "layer:96, std:1.561438798904419\n",
      "layer:97, std:1.621287226676941\n",
      "layer:98, std:1.5580321550369263\n",
      "layer:99, std:1.7313354015350342\n",
      "tensor([[ 2.2452, -0.5267, -2.4733,  ...,  1.6496,  1.3514, -1.1961],\n",
      "        [-0.7937, -0.3547,  0.2335,  ...,  0.0227, -1.4465,  1.1601],\n",
      "        [ 1.1047, -0.7038, -1.3904,  ..., -0.7836, -0.4223,  0.2477],\n",
      "        ...,\n",
      "        [-1.0807, -0.0233,  1.2909,  ..., -1.6992, -1.3098,  1.4214],\n",
      "        [-0.1930,  0.5577,  0.6909,  ...,  0.8760,  0.8526, -0.4442],\n",
      "        [-1.9133,  1.2738,  1.9768,  ...,  0.2481,  0.3859, -0.0505]],\n",
      "       grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "class MLP_1(MLP):\n",
    "    def initialize(self): # 重写initialize方法\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                std = torch.sqrt(torch.tensor(1/self.neural_num, dtype=torch.float32))\n",
    "                nn.init.normal_(m.weight, std=std)\n",
    "\n",
    "\n",
    "layer_nums = 100\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "inputs = torch.randn((batch_size, neural_nums))\n",
    "\n",
    "net = MLP_1(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
