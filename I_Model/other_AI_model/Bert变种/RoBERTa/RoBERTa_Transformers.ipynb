{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n",
      "['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "主要预训练模型:\n",
    "'roberta-base'\n",
    "'roberta-large'\n",
    "'''\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "print(tokenizer.model_input_names)  # 动态编码\n",
    "print(tokenizer.all_special_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   0, 9064, 6406,  162,   30,  143, 2788,   47, 1017,  101,    4,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "<s>Replace me by any text you'd like.</s>\n"
     ]
    }
   ],
   "source": [
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "print(encoded_input.keys())\n",
    "print(tokenizer.decode(encoded_input['input_ids'].tolist()[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1146,  0.1103, -0.0149,  ..., -0.0809, -0.0018, -0.0271],\n",
      "         [-0.0225,  0.1612,  0.0556,  ...,  0.5366,  0.1196,  0.1576],\n",
      "         [ 0.0532, -0.0020,  0.0370,  ..., -0.4887,  0.1641,  0.2736],\n",
      "         ...,\n",
      "         [-0.1586,  0.0837,  0.1302,  ...,  0.3970,  0.1715, -0.0848],\n",
      "         [-0.1065,  0.1044, -0.0383,  ..., -0.1068, -0.0015, -0.0517],\n",
      "         [ 0.0059,  0.0758,  0.1228,  ...,  0.1037,  0.0075,  0.0976]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-2.8347e-03, -1.8850e-01, -2.1461e-01, -1.1530e-01,  1.3189e-01,\n",
      "          2.3539e-01,  2.7159e-01, -6.0856e-02, -8.3708e-02, -1.9298e-01,\n",
      "          2.6565e-01, -4.3370e-05, -1.1200e-01,  1.4636e-01, -1.4502e-01,\n",
      "          4.9390e-01,  2.0317e-01, -5.3051e-01,  7.5674e-02, -3.7784e-02,\n",
      "         -2.8706e-01,  7.9115e-02,  4.9498e-01,  3.6626e-01,  1.0755e-01,\n",
      "          4.5707e-02, -1.5804e-01,  1.3338e-02,  1.5470e-01,  2.6411e-01,\n",
      "          3.0425e-01,  3.5527e-02,  1.1397e-01,  2.5932e-01, -2.5477e-01,\n",
      "          6.6820e-02, -3.4011e-01,  8.4366e-03,  2.8969e-01, -1.9251e-01,\n",
      "         -8.1795e-02,  1.7997e-01,  1.9420e-01, -1.8720e-01, -1.0835e-01,\n",
      "          4.2660e-01,  2.5290e-01,  4.4049e-02, -1.5877e-01, -9.2707e-02,\n",
      "         -3.3307e-01,  3.7266e-01,  3.0196e-01,  2.2397e-01, -6.3287e-02,\n",
      "          4.3699e-02, -9.3048e-02,  2.8365e-01, -7.5851e-02, -7.6650e-02,\n",
      "         -1.2443e-01, -2.3357e-01,  7.3141e-03, -6.6388e-02,  4.7960e-02,\n",
      "         -1.3959e-01,  1.0733e-01, -1.6085e-01, -1.3563e-01,  3.1196e-02,\n",
      "         -1.1300e-01,  1.7004e-01,  1.6964e-01, -3.3313e-01, -3.1102e-01,\n",
      "          7.0277e-02, -6.3457e-01, -9.6144e-02,  3.4468e-01,  4.6423e-01,\n",
      "         -8.3255e-02,  2.4088e-01,  3.9045e-02,  2.1656e-01, -1.2806e-02,\n",
      "         -7.3343e-02, -4.6851e-02, -1.4373e-01,  1.4100e-01,  2.6473e-01,\n",
      "         -2.1426e-01, -4.6240e-01,  4.3216e-02,  3.9133e-02, -9.2348e-02,\n",
      "          2.2254e-02, -1.1372e-02, -1.1127e-01, -1.8471e-01, -1.8038e-01,\n",
      "          9.9089e-02, -2.4278e-01, -1.5152e-01,  2.6573e-01,  4.7380e-04,\n",
      "         -1.6355e-01,  8.2371e-04,  3.2038e-01,  5.0322e-02, -1.0096e-01,\n",
      "         -2.1142e-01,  4.7146e-01,  3.5390e-01, -3.4947e-02, -1.2648e-03,\n",
      "          1.7674e-01,  1.5086e-01, -2.9137e-01,  4.5807e-01, -3.3300e-01,\n",
      "         -3.9867e-03, -1.0464e-01,  1.4464e-01,  1.9178e-01, -2.0823e-01,\n",
      "          3.0469e-01,  1.5175e-01,  3.0945e-01,  2.1146e-01,  1.4162e-01,\n",
      "         -4.4261e-02,  1.3641e-01, -1.3945e-01,  1.5955e-01,  2.3872e-01,\n",
      "          1.2040e-01, -4.9609e-03, -3.4112e-01, -2.6020e-01,  3.0965e-01,\n",
      "          3.4374e-01,  1.3289e-01, -6.3243e-02,  2.0233e-01,  1.0470e-01,\n",
      "          2.4754e-01,  1.4736e-01, -4.4511e-01,  4.2347e-02,  3.5871e-01,\n",
      "          1.2213e-01,  1.5714e-01, -1.1579e-01, -3.0872e-01, -2.8117e-01,\n",
      "         -1.1094e-01,  5.6936e-02, -3.6070e-01, -1.2276e-01,  3.9022e-01,\n",
      "          8.4130e-02, -7.2016e-02, -1.9839e-01, -2.2041e-01, -1.1239e-02,\n",
      "         -1.2389e-01,  6.9023e-03,  9.8317e-02, -7.9866e-02, -4.6790e-01,\n",
      "         -1.3635e-01, -5.3511e-01, -9.6947e-02,  2.2063e-01, -3.5547e-01,\n",
      "          3.0791e-01, -2.9423e-01,  1.0540e-01,  4.1336e-01,  5.9913e-02,\n",
      "         -1.8542e-02, -2.4088e-01, -1.6369e-02,  9.3561e-02,  3.2514e-01,\n",
      "          2.9757e-01, -4.3569e-01,  1.0774e-01,  1.4302e-01,  3.1605e-01,\n",
      "          1.1054e-01, -8.9299e-02, -1.1941e-01,  1.7482e-01, -2.1888e-01,\n",
      "          1.7849e-01, -2.2932e-01,  2.0647e-01, -2.5988e-01, -2.5896e-01,\n",
      "          3.1579e-01, -4.5348e-01, -7.6572e-02,  8.1546e-02,  2.5454e-01,\n",
      "          6.5838e-03, -5.5890e-02, -1.0195e-01,  1.8580e-01,  1.7586e-01,\n",
      "          1.2009e-01, -4.2580e-01,  3.1833e-01, -3.5225e-02, -3.6717e-02,\n",
      "         -3.6468e-02,  1.8630e-01,  2.5572e-01,  9.9702e-02, -3.8584e-01,\n",
      "         -1.5646e-01,  1.3589e-01,  3.1458e-01, -2.6697e-01,  1.6693e-01,\n",
      "         -3.3316e-01, -4.3700e-01, -1.1081e-01,  2.4861e-01,  2.2493e-01,\n",
      "          1.9945e-01, -3.0226e-01,  1.7410e-01, -1.5682e-01, -4.5891e-01,\n",
      "         -3.8876e-01, -1.3655e-01,  2.3969e-01,  1.7983e-01,  1.6895e-01,\n",
      "          2.5715e-01,  3.0751e-02,  1.1278e-01,  1.6356e-01,  1.6140e-01,\n",
      "         -1.3235e-01,  1.5606e-01, -3.6884e-01, -4.3860e-02, -3.2912e-01,\n",
      "         -2.1013e-01, -2.2653e-01,  4.2579e-01, -2.5797e-01,  2.6423e-01,\n",
      "          4.3591e-01, -3.3986e-01, -1.0784e-01,  1.4810e-01,  1.8771e-01,\n",
      "          5.8792e-02, -9.3627e-02,  2.0970e-01,  2.0622e-01, -1.0515e-01,\n",
      "          2.8239e-01, -4.3975e-02,  2.9693e-01,  1.5088e-01,  4.6862e-02,\n",
      "          1.6996e-01,  1.3566e-01, -1.3045e-01,  8.1460e-02,  1.2664e-02,\n",
      "         -3.3464e-02, -2.8986e-01, -1.4230e-01,  2.3894e-01, -7.3702e-02,\n",
      "          2.3693e-02, -1.6792e-01, -2.9238e-02,  1.1320e-02,  4.2676e-01,\n",
      "         -3.7155e-01,  2.7734e-01,  5.9552e-02,  1.4843e-01, -2.3717e-01,\n",
      "         -2.4174e-01,  1.2127e-01,  2.3405e-01, -4.7163e-01,  4.4662e-02,\n",
      "          1.1726e-01,  9.0709e-02,  2.1785e-01,  2.8467e-01,  4.3689e-04,\n",
      "         -1.1491e-01,  5.6653e-01, -1.5376e-01, -1.3448e-01,  2.6769e-01,\n",
      "         -3.0039e-01, -3.0043e-01,  2.7399e-01, -1.9757e-02,  3.1912e-01,\n",
      "          1.4622e-01,  5.6607e-02,  6.1616e-02, -6.1817e-01,  8.0240e-02,\n",
      "         -4.8122e-01, -1.5592e-02,  5.3547e-02, -8.4474e-02, -2.3283e-01,\n",
      "          1.6090e-01,  3.1869e-01, -2.3078e-01, -1.9177e-02,  2.1890e-01,\n",
      "          4.8886e-02, -1.2004e-01,  4.8328e-01, -1.8755e-03,  2.2632e-01,\n",
      "         -5.1116e-02,  2.3901e-01, -2.3146e-01,  2.6983e-01, -2.9126e-01,\n",
      "         -1.2003e-01,  6.2455e-02,  7.8869e-02,  4.1980e-02, -7.5116e-02,\n",
      "         -3.9176e-01,  2.5679e-01, -1.1382e-02, -7.3775e-02, -5.6824e-02,\n",
      "          1.0898e-01, -2.5817e-02,  7.3328e-02,  8.8274e-02,  3.6668e-01,\n",
      "          2.4559e-01, -1.2493e-02, -4.1410e-01, -8.9574e-02, -1.2473e-01,\n",
      "          7.0269e-02,  6.0476e-02,  1.4108e-02,  4.8732e-01, -7.1245e-02,\n",
      "          2.6076e-02, -1.5084e-01,  2.8914e-01,  2.2343e-01,  1.7564e-01,\n",
      "          1.6365e-01,  5.4786e-02,  1.8040e-01, -4.7131e-02, -1.8931e-02,\n",
      "         -1.5203e-01, -2.5458e-01, -2.9443e-01,  2.3336e-01, -2.5514e-01,\n",
      "         -1.6871e-01,  1.6014e-01,  2.0769e-01, -1.6111e-01,  1.8165e-01,\n",
      "          3.4196e-01,  1.5221e-01, -1.6557e-01,  3.1800e-01, -7.6829e-02,\n",
      "          6.5035e-02,  3.0127e-01, -8.3677e-02,  2.0537e-01,  5.4490e-01,\n",
      "          2.5054e-01, -4.2671e-01, -5.8351e-02, -2.2732e-01,  1.4475e-02,\n",
      "          2.7506e-01, -1.2389e-01,  2.1066e-01,  3.8047e-01,  3.0992e-01,\n",
      "          4.9357e-01, -1.5483e-02, -1.1453e-01,  1.1932e-01,  2.3738e-01,\n",
      "          2.3809e-02, -2.0668e-01, -1.7490e-01,  3.0054e-01,  9.5040e-02,\n",
      "         -1.6448e-01,  8.2032e-03, -1.6251e-01,  4.5058e-02, -1.6121e-01,\n",
      "         -4.2871e-01,  4.4034e-02,  1.6746e-01, -5.0884e-01,  1.2617e-01,\n",
      "         -3.1699e-01,  3.6625e-02, -2.4639e-01,  2.6873e-01, -2.5727e-01,\n",
      "         -1.4897e-01,  4.3529e-01, -5.5864e-02,  2.7185e-02, -1.9407e-01,\n",
      "         -1.6114e-01,  2.9191e-02, -1.6526e-03, -7.2602e-02, -4.9690e-03,\n",
      "          3.5502e-01, -1.4824e-01,  7.0600e-02,  2.8513e-02,  2.1469e-01,\n",
      "         -7.3574e-02,  1.7018e-01, -2.6030e-03, -1.4642e-01, -4.1492e-01,\n",
      "          1.6521e-01, -2.2469e-01, -4.2976e-01, -4.2113e-01,  4.1487e-01,\n",
      "         -1.7290e-01, -2.8811e-01, -2.2153e-01, -2.5111e-01,  5.3529e-02,\n",
      "          2.4050e-01,  4.7290e-01, -4.0037e-01, -5.9299e-02,  4.9183e-01,\n",
      "         -5.9624e-02, -2.3099e-01,  2.7614e-01,  2.0807e-01, -3.2530e-01,\n",
      "          3.7258e-01,  2.6673e-01, -6.3106e-02,  5.1717e-02,  5.4289e-01,\n",
      "          1.1893e-01,  2.1599e-01, -2.1227e-01,  4.7760e-01, -2.6859e-01,\n",
      "          3.2262e-01, -1.7062e-01, -1.9771e-01, -2.4215e-01, -3.1962e-02,\n",
      "          3.6125e-01,  1.9359e-01, -4.5894e-01, -1.3200e-01,  4.3003e-02,\n",
      "          3.3126e-01, -4.2145e-01, -6.0110e-02,  4.1000e-02, -3.6422e-01,\n",
      "          1.2588e-01,  1.3854e-01,  2.4897e-01, -4.2243e-01, -6.7698e-02,\n",
      "          4.1120e-01, -3.4483e-01,  1.3896e-01,  3.0409e-01,  8.2978e-02,\n",
      "          3.6610e-01, -6.5310e-02, -8.2796e-03,  5.0149e-02, -2.6011e-01,\n",
      "         -2.2435e-02,  1.2717e-01,  5.8494e-01,  1.4487e-01, -4.0354e-01,\n",
      "          1.1067e-01,  2.6653e-01, -1.8659e-01,  3.3972e-01, -1.0567e-01,\n",
      "         -3.7917e-02,  2.6797e-01, -5.2882e-02,  1.6860e-01, -9.7774e-02,\n",
      "         -2.3715e-01, -3.5562e-01,  4.1702e-01, -1.9961e-01, -1.1364e-01,\n",
      "         -2.0353e-01, -1.0914e-01, -1.9304e-01,  9.5546e-03, -3.9396e-01,\n",
      "          3.6519e-01,  1.3993e-01, -2.1809e-01, -9.8051e-02, -1.0919e-01,\n",
      "         -2.0547e-01, -2.0266e-01, -2.9378e-01,  4.2927e-01, -1.8953e-01,\n",
      "         -4.9567e-01,  2.4299e-01, -1.3043e-02,  3.6800e-01,  4.0743e-03,\n",
      "          1.0481e-01, -6.7964e-02,  1.2875e-01,  8.9863e-02, -8.7973e-02,\n",
      "          2.8249e-01,  9.1169e-02, -5.9373e-01, -1.3107e-01, -2.6811e-01,\n",
      "          9.8535e-02,  2.3464e-01, -3.9614e-01,  1.5656e-02,  3.6724e-02,\n",
      "          2.0153e-01,  1.1375e-02, -1.2173e-01, -7.0875e-02,  4.3149e-01,\n",
      "          2.6859e-01,  3.0436e-01,  1.3308e-01,  2.3106e-01, -4.5164e-03,\n",
      "         -3.6247e-01,  1.8061e-02,  9.4915e-02, -1.9820e-01,  4.6976e-01,\n",
      "         -1.2277e-01, -3.9047e-01, -7.1323e-02,  4.2497e-01,  9.7055e-02,\n",
      "         -2.6554e-02, -3.3067e-02,  2.3281e-01,  1.9053e-01, -1.2797e-01,\n",
      "          2.1960e-01, -8.9729e-03, -1.4726e-01, -1.5024e-01,  9.6500e-02,\n",
      "         -2.3059e-01,  3.7448e-02, -1.4284e-01, -2.9568e-03, -2.2259e-01,\n",
      "          9.0303e-03, -2.2012e-01,  3.0411e-01, -3.5056e-01,  1.1357e-01,\n",
      "          3.1748e-02,  3.2109e-01, -3.7659e-01, -1.7942e-01, -2.8157e-02,\n",
      "          2.0363e-01,  2.7066e-01,  3.8169e-01,  2.1824e-02,  5.1432e-02,\n",
      "         -2.0284e-01, -2.8634e-01,  5.2265e-02, -2.1432e-01,  1.1748e-01,\n",
      "          8.6978e-02,  2.7207e-01, -3.1238e-01, -2.0362e-01,  2.3377e-01,\n",
      "         -5.4622e-02, -1.4171e-01,  4.9318e-01,  2.5240e-01,  1.8076e-01,\n",
      "          2.9939e-02,  2.3065e-01,  2.8543e-02, -1.6701e-01, -1.1991e-01,\n",
      "         -3.0575e-01,  1.0910e-01, -1.1004e-01, -4.9718e-02, -6.8488e-02,\n",
      "         -1.1613e-01, -2.6441e-01, -1.9368e-01,  1.5138e-01,  1.4259e-01,\n",
      "          3.0952e-02, -5.1871e-02, -1.8964e-02, -3.0977e-01,  3.2418e-01,\n",
      "          1.8257e-02,  1.1345e-01, -3.2933e-02,  5.4760e-02, -1.6449e-01,\n",
      "          2.4663e-01,  2.1938e-01,  5.0557e-02, -2.0592e-01, -4.8105e-02,\n",
      "         -3.3525e-01, -3.7126e-01,  2.9869e-02,  1.6168e-01,  1.3227e-01,\n",
      "         -9.2244e-02, -2.8055e-01,  5.7759e-03, -1.2485e-01,  1.9057e-01,\n",
      "          1.8671e-02, -1.8965e-01, -8.9002e-02, -7.3102e-02, -2.7037e-02,\n",
      "          9.7274e-02, -2.2742e-01, -2.0173e-01, -1.3191e-01, -8.9041e-02,\n",
      "         -8.0196e-02,  3.6282e-01, -8.2313e-02,  3.2301e-01, -1.6621e-01,\n",
      "          2.9981e-02, -2.1863e-01,  1.5416e-01, -7.0528e-02,  7.8769e-02,\n",
      "          3.0648e-01, -4.6061e-01, -1.2646e-01, -2.7547e-02, -1.9933e-01,\n",
      "         -1.6299e-01, -1.1201e-01, -1.8919e-02,  2.2978e-01, -3.7508e-01,\n",
      "          2.0549e-01, -1.2558e-01,  2.0741e-01, -7.4029e-02, -2.5238e-01,\n",
      "         -1.6511e-01, -1.0580e-03,  2.9722e-01, -3.3988e-01, -2.3128e-01,\n",
      "         -3.1352e-01, -1.2131e-01, -1.1570e-01, -2.9116e-01,  4.4779e-01,\n",
      "         -1.1711e-01, -5.9108e-02,  3.8968e-02,  4.3492e-01,  2.1890e-01,\n",
      "          1.7709e-01,  2.5733e-01,  1.5447e-02,  1.2343e-02,  1.1580e-01,\n",
      "         -4.9933e-01,  2.8145e-01, -2.6542e-01, -1.7272e-01,  2.0916e-04,\n",
      "          1.4980e-01, -5.8473e-02,  3.9033e-02, -1.5795e-01, -1.1659e-01,\n",
      "          2.3245e-01, -3.9180e-01, -1.6454e-02,  3.0201e-01,  1.5689e-01,\n",
      "         -2.9235e-01,  3.0020e-02,  1.2322e-01,  3.8744e-01,  5.0056e-02,\n",
      "         -2.4898e-01,  1.4628e-01, -3.6670e-01, -1.9514e-02, -2.1036e-01,\n",
      "         -3.3647e-01,  1.9012e-01, -1.1920e-01,  3.9577e-02, -6.7328e-02,\n",
      "         -3.0076e-01,  2.0926e-01, -6.3638e-02, -3.9904e-02,  4.5228e-01,\n",
      "          7.4620e-02, -1.1017e-01,  1.5910e-01,  3.6012e-02,  1.9745e-02,\n",
      "         -9.8928e-02,  2.8636e-01,  2.0934e-01, -3.2661e-01,  7.1475e-02,\n",
      "         -1.7098e-01, -2.6441e-02, -1.4120e-01]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = model(**encoded_input)\n",
    "print(result.last_hidden_state)\n",
    "print(result.pooler_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}