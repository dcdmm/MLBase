{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db10825-8ac2-4415-94fb-26d765d6da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from utils import loss_fun, MetricsCalculator\n",
    "from GlobalPointer import GlobalPointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95029ae3-18fb-46e3-b046-e6d1f3839039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffb47bd6-a5cc-417b-9390-73e321f32194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09345c6b-3c7e-42b6-bba7-63cfee267787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'bod', 1: 'dis', 2: 'sym', 3: 'mic', 4: 'pro', 5: 'ite', 6: 'dep', 7: 'dru', 8: 'equ'}\n"
     ]
    }
   ],
   "source": [
    "# dis:疾病\n",
    "# sym:临床表现\n",
    "# pro:医疗程序\n",
    "# equ:医疗设备\n",
    "# dru:药物\n",
    "# ite:医学检验项目\n",
    "# bod:身体\n",
    "# dep:科室\n",
    "# mic:微生物类\n",
    "ent2id = {\"bod\": 0, \"dis\": 1, \"sym\": 2, \"mic\": 3, \"pro\": 4, \"ite\": 5, \"dep\": 6, \"dru\": 7, \"equ\": 8}  # 9个实体类型\n",
    "\n",
    "id2ent = {}\n",
    "for k, v in ent2id.items(): id2ent[v] = k\n",
    "print(id2ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19df8085-dabc-4c87-8693-c92cf9fc81e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "5000\n",
      "['对儿童SARST细胞亚群的研究表明，与成人SARS相比，儿童细胞下降不明显，证明上述推测成立。', (3, 9, 0), (19, 24, 1)]\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    D = []\n",
    "    with open(path) as f:\n",
    "        for d in json.load(f):\n",
    "            D.append([d['text']])\n",
    "            for e in d['entities']:\n",
    "                start, end, label = e['start_idx'], e['end_idx'], e['type']\n",
    "                if start <= end:\n",
    "                    D[-1].append((start, end, ent2id[label]))\n",
    "    return D\n",
    "\n",
    "\n",
    "data_train = load_data('datasets/CMeEE_train.json')\n",
    "data_dev = load_data('datasets/CMeEE_dev.json')\n",
    "print(len(data_train))\n",
    "print(len(data_dev))\n",
    "print(data_dev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c1ffe75-40cd-489f-90ab-e71a1a1176de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['对儿童SARST细胞亚群的研究表明，与成人SARS相比，儿童细胞下降不明显，证明上述推测成立。', (3, 9, 0), (19, 24, 1)]\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"自定义Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "\n",
    "dataset_train = CustomDataset(data_train)\n",
    "dataset_valid = CustomDataset(data_dev)\n",
    "for i in dataset_valid:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef994ee1-3ed8-450d-859b-b546e36d8988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='hfl/chinese-roberta-wwm-ext', vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102267648\n"
     ]
    }
   ],
   "source": [
    "model_name = 'hfl/chinese-roberta-wwm-ext'\n",
    "\n",
    "tokenizer_fast = BertTokenizerFast.from_pretrained(model_name)\n",
    "print(tokenizer_fast)\n",
    "pretrained = BertModel.from_pretrained(model_name)\n",
    "print(pretrained.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "156b2077-3dad-4ce1-8fd3-7c5a20f08ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2190, 1036,  ...,    0,    0,    0],\n",
      "        [ 101, 4777, 4955,  ...,    0,    0,    0],\n",
      "        [ 101, 1728, 5445,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  123,  119,  ...,    0,    0,    0],\n",
      "        [ 101,  124,  119,  ...,    0,    0,    0],\n",
      "        [ 101,  125,  119,  ...,    0,    0,    0]])\n",
      "torch.Size([16, 122]) torch.Size([16, 122]) torch.Size([16, 122])\n",
      "torch.Size([16, 9, 122, 122])\n"
     ]
    }
   ],
   "source": [
    "def get_collate_fn(tokenizer, max_len=512):\n",
    "    \"\"\"返回collate_fun函数(通过闭包函数引入形参)\"\"\"\n",
    "\n",
    "    def collate_fn(data):\n",
    "        batch_size = len(data)\n",
    "        sentences_list = [i[0] for i in data]  # len(sentences_list) = batch_size\n",
    "        entities_list = [i[1:] for i in data]  # len(entities_list) = batch_size\n",
    "\n",
    "        # 长度>max_len ===> 截断\n",
    "        # 长度<=max_len ===> 当前批次最大长度\n",
    "        outputs = tokenizer(sentences_list, max_length=max_len, truncation=True, padding=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        input_ids = torch.tensor(outputs[\"input_ids\"], dtype=torch.int64)\n",
    "        attention_mask = torch.tensor(outputs[\"attention_mask\"], dtype=torch.int64)\n",
    "        token_type_ids = torch.tensor(outputs[\"token_type_ids\"], dtype=torch.int64)\n",
    "        offset_mapping = outputs[\"offset_mapping\"]  # return (char_start, char_end) for each token.\n",
    "\n",
    "        # (0, 0)表示特殊token(如:'[CLS]','[SEP'], '[PAD]'等)\n",
    "        # offset_mapping为左闭右开(故j[1] - 1表示该token结尾字符的位置)\n",
    "        # i表示第几个token(从0开始计数,包含特殊token)\n",
    "        start_mapping = [{j[0]: i for i, j in enumerate(i) if j != (0, 0)} for i in offset_mapping]\n",
    "        end_mapping = [{j[1] - 1: i for i, j in enumerate(i) if j != (0, 0)} for i in offset_mapping]\n",
    "\n",
    "        # 实体类别数量:len(ent2id)\n",
    "        labels = np.zeros((batch_size, len(ent2id), input_ids.shape[1], input_ids.shape[1]))\n",
    "        for i in range(batch_size):\n",
    "            for start, end, label in entities_list[i]:\n",
    "                if start in start_mapping[i] and end in end_mapping[i]:\n",
    "                    start = start_mapping[i][start]\n",
    "                    end = end_mapping[i][end]\n",
    "                    labels[i, label, start, end] = 1  # label实体类别中实体的位置\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=16, shuffle=True, collate_fn=get_collate_fn(tokenizer_fast))\n",
    "valid_loader = DataLoader(dataset_valid, batch_size=16, shuffle=False, collate_fn=get_collate_fn(tokenizer_fast))\n",
    "for j in valid_loader:\n",
    "    print(j[0])\n",
    "    print(j[0].shape, j[1].shape, j[2].shape)\n",
    "    print(j[-1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca49f306-ff54-4ad7-a301-d1085ebb6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GlobalPointer(pretrained, len(ent2id), 64).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd560357-3e5e-4298-a4af-be9e9cd42082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    for idx, (input_ids, attention_mask, token_type_ids, labels) in enumerate(dataloader):\n",
    "        # 数据设备切换\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        # labels.shape=[batch_size, ent_type_size, seq_len, seq_len]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # logits.shape=[batch_size, ent_type_size, seq_len, seq_len]\n",
    "        logits = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = loss_fun(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0 and idx > 0:\n",
    "            mc = MetricsCalculator()  # 计算查准率、查全率、F1 score \n",
    "            mc.calc_confusion_matrix(logits, labels)\n",
    "            print('| step {:5d} | loss {:8.5f} | precision {:8.5f} | recall {:8.5f} | f1 {:8.5f} |'.format(idx,\n",
    "                                                                                                           loss.item(),\n",
    "                                                                                                           mc.precision,\n",
    "                                                                                                           mc.recall,\n",
    "                                                                                                           mc.f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "448a401c-ee88-45eb-990c-977b8949e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型验证\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    mc = MetricsCalculator()\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, token_type_ids, labels in dataloader:\n",
    "            # 数据设备切换\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            # logits.shape=[batch_size, ent_type_size, seq_len, seq_len]\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            mc.calc_confusion_matrix(logits, labels)\n",
    "    return mc.precision, mc.recall, mc.f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dad9f793-0e1a-44c2-a8f6-1935d45d964b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step   100 | loss  1.19852 | precision  0.50000 | recall  0.02326 | f1  0.04444 |\n",
      "| step   200 | loss  0.68973 | precision  0.64444 | recall  0.45312 | f1  0.53211 |\n",
      "| step   300 | loss  0.80105 | precision  0.78571 | recall  0.17742 | f1  0.28947 |\n",
      "| step   400 | loss  0.79979 | precision  0.60000 | recall  0.25424 | f1  0.35714 |\n",
      "| step   500 | loss  0.66815 | precision  0.70000 | recall  0.50909 | f1  0.58947 |\n",
      "| step   600 | loss  0.53522 | precision  0.86364 | recall  0.54286 | f1  0.66667 |\n",
      "| step   700 | loss  0.64121 | precision  0.72340 | recall  0.46575 | f1  0.56667 |\n",
      "| step   800 | loss  0.63160 | precision  0.65385 | recall  0.54839 | f1  0.59649 |\n",
      "| step   900 | loss  1.01873 | precision  0.78846 | recall  0.44086 | f1  0.56552 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch:     0 | time: 279.43s | valid precision  0.65952 | valid recall  0.59908 | valid f1  0.62785 | train f1  0.68340 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss  0.50490 | precision  0.78846 | recall  0.66129 | f1  0.71930 |\n",
      "| step   200 | loss  0.59318 | precision  0.62264 | recall  0.54098 | f1  0.57895 |\n",
      "| step   300 | loss  0.38412 | precision  0.87805 | recall  0.60000 | f1  0.71287 |\n",
      "| step   400 | loss  0.45951 | precision  0.68750 | recall  0.67347 | f1  0.68041 |\n",
      "| step   500 | loss  0.35221 | precision  0.80000 | recall  0.57143 | f1  0.66667 |\n",
      "| step   600 | loss  0.63378 | precision  0.65000 | recall  0.24528 | f1  0.35616 |\n",
      "| step   700 | loss  0.53159 | precision  0.60976 | recall  0.44643 | f1  0.51546 |\n",
      "| step   800 | loss  0.43983 | precision  0.84783 | recall  0.75000 | f1  0.79592 |\n",
      "| step   900 | loss  0.42180 | precision  0.67442 | recall  0.59184 | f1  0.63043 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch:     1 | time: 277.13s | valid precision  0.67568 | valid recall  0.60283 | valid f1  0.63718 | train f1  0.72418 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss  0.57390 | precision  0.73333 | recall  0.66265 | f1  0.69620 |\n",
      "| step   200 | loss  0.49295 | precision  0.84444 | recall  0.52778 | f1  0.64957 |\n",
      "| step   300 | loss  0.97491 | precision  0.50943 | recall  0.38571 | f1  0.43902 |\n",
      "| step   400 | loss  0.46523 | precision  0.76364 | recall  0.63636 | f1  0.69421 |\n",
      "| step   500 | loss  0.41241 | precision  0.77358 | recall  0.75926 | f1  0.76636 |\n",
      "| step   600 | loss  0.42629 | precision  0.90000 | recall  0.63158 | f1  0.74227 |\n",
      "| step   700 | loss  0.36072 | precision  0.80000 | recall  0.72131 | f1  0.75862 |\n",
      "| step   800 | loss  0.40575 | precision  0.85000 | recall  0.65385 | f1  0.73913 |\n",
      "| step   900 | loss  0.36631 | precision  0.81395 | recall  0.74468 | f1  0.77778 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch:     2 | time: 272.31s | valid precision  0.69979 | valid recall  0.60160 | valid f1  0.64699 | train f1  0.77684 |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_loader, optimizer, device)\n",
    "    _, _, train_f1 = evaluate(model, train_loader, device)\n",
    "    valid_precision, valid_recall, valid_f1 = evaluate(model, valid_loader, device)\n",
    "    print('-' * 123)\n",
    "    print('| epoch: {:5d} | time: {:5.2f}s '\n",
    "          '| valid precision {:8.5f} '\n",
    "          '| valid recall {:8.5f} '\n",
    "          '| valid f1 {:8.5f} | train f1 {:8.5f} |'.format(epoch,\n",
    "                                                           time.time() - epoch_start_time,\n",
    "                                                           valid_precision,\n",
    "                                                           valid_recall,\n",
    "                                                           valid_f1,\n",
    "                                                           train_f1))\n",
    "    print('-' * 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b216314a-4113-4c89-bcad-975244a800e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ent_list = []\n",
    "\n",
    "for d in json.load(open('./datasets/CMeEE_test.json')):\n",
    "    text = d[\"text\"]\n",
    "\n",
    "    output = tokenizer_fast([text], return_offsets_mapping=True, max_length=512, truncation=True, padding=True)\n",
    "    input_ids = torch.tensor(output['input_ids'], dtype=torch.int64).to(device)\n",
    "    token_type_ids = torch.tensor(output['token_type_ids'], dtype=torch.int64).to(device)\n",
    "    attention_mask = torch.tensor(output['attention_mask'], dtype=torch.int64).to(device)\n",
    "    offset_mapping = output['offset_mapping']\n",
    "\n",
    "    one_ent_list = {'text': text, 'entities': []}\n",
    "    with torch.no_grad():\n",
    "        # logits.shape=[1, ent_type_size, seq_len, seq_len]\n",
    "        logits = model(input_ids, attention_mask, token_type_ids).cpu()\n",
    "        for _, l, start, end in zip(*torch.where(logits > 0.0)):  # 阈值(threshold)设置为0.0\n",
    "            ent_type = id2ent[l.item()]\n",
    "            # [offset_mapping[0][start]表示该实体开始token的位置信息\n",
    "            # [offset_mapping[0][end]表示该实体结尾token的位置信息\n",
    "            ent_char_span = [offset_mapping[0][start][0], offset_mapping[0][end][1]]\n",
    "            ent_text = text[ent_char_span[0]: ent_char_span[1]]\n",
    "            one_ent_list['entities'].append({\"start_idx\": ent_char_span[0],\n",
    "                                             \"end_idx\": ent_char_span[1] - 1,  # j[1] - 1表示该token结尾字符的位置\n",
    "                                             \"type\": ent_type,\n",
    "                                             \"entity\": ent_text})\n",
    "    all_ent_list.append(one_ent_list)  # 每次预测一条文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d97947fb-7313-42e3-9d3a-f0960d0036bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '六、新生儿疾病筛查的发展趋势自1961年开展苯丙酮尿症筛查以来，随着医学技术的发展，符合进行新生儿疾病筛查标准的疾病也在不断增加，无论在新生儿疾病筛查的病种，还是在新生儿疾病筛查的技术方法上，都有了非常显著的进步。', 'entities': [{'start_idx': 22, 'end_idx': 26, 'type': 'dis', 'entity': '苯丙酮尿症'}, {'start_idx': 2, 'end_idx': 8, 'type': 'pro', 'entity': '新生儿疾病筛查'}, {'start_idx': 22, 'end_idx': 28, 'type': 'pro', 'entity': '苯丙酮尿症筛查'}, {'start_idx': 68, 'end_idx': 74, 'type': 'pro', 'entity': '新生儿疾病筛查'}, {'start_idx': 82, 'end_idx': 88, 'type': 'pro', 'entity': '新生儿疾病筛查'}]}\n"
     ]
    }
   ],
   "source": [
    "print(all_ent_list[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3812",
   "language": "python",
   "name": "env_3812"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}