{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<font color='red' size=4>最大熵原理:</font>  \n",
    "&emsp;&emsp;最大熵原理是一种选择随机变量统计特性最符合客观情况的准则,也\n",
    "称为最大信息原理.随机量的概率分布是很难测定的,一般只能测得其各种均值(如数学期望、方差等)或\n",
    "已知某些限定条件下的值(如峰值、取值个数等),符合测得这些值的分布可有多种、以至无穷多种,通常,其中有\n",
    "一种分布的熵最大.选用这种具有最大熵的分布作为该随机变量的分布,是一种有效的\n",
    "处理方法和准则.这种方法虽有一定的主观性,但可以认为是最符合客观情况的一种选\n",
    "择.在投资时常常讲不要把所有的鸡蛋放在一个篮子里,这样可以降低风险.在信息处理中,这\n",
    "个原理同样适用.在数学上,这个原理称为最大熵原理.    \n",
    "&emsp;&emsp;假设分类模型是一个条件概率分布$ P(Y|X), X \\in \\mathcal{X} \\subseteq \\mathbf{R}^n $表示输入,$Y\\in \\mathcal{Y} $表示输\n",
    "出,$ \\mathcal{X} $和$\\mathcal{Y}$分别是输入和输出的集合.这个模型表示的是对于给定的输入$X$,以\n",
    "条件概率$ P(Y|X) $输出$Y$.   \n",
    "&emsp;&emsp;给定一个训练数据集    \n",
    "$$ T=\\{ (\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2),\\dots,(\\mathbf{x}_N, y_N) \\} $$   \n",
    "学习的目标是用最大熵原理选择最好的分类模型.     \n",
    "&emsp;&emsp;首先考虑模型应该满足的条件.给定训练数据集,可以确定联合分布$ P(X,Y) $的经验分布和\n",
    "边缘分布$ P(X) $的经验分布,分别以$ \\hat{P}(X,Y) $和$ \\hat{P}(X) $表示.这里    \n",
    "$$ \\hat{P}(X=\\mathbf{x}, Y=y)  = \\frac{v(X=\\mathbf{x}, Y=y)}{N}$$     \n",
    "$$\\hat{P}(X=\\mathbf{x}) = \\frac{v(X=\\mathbf{x})}{N}  $$    \n",
    "其中,$v(X=\\mathbf{x}, Y=y)$表示训练数据中样本$(\\mathbf{x}, y)$出现的频数,$ v(X=\\mathbf{x}) $表示训练数据中输入$\\mathbf{x}$出现的频数,$N$表示训练样本容量.   \n",
    "&emsp;&emsp;用特征函数(feature function) $ f(\\mathbf{x},y) $描述输入$\\mathbf{x}$和输出$y$之间的某一个事实.其定义是    \n",
    "\\begin{equation}\n",
    "f(\\mathbf{x}, y)=\\begin{cases}\n",
    "\t\t1, & \\text{if} \\quad  \\mathbf{x}与y满足某一事实 \\\\\n",
    "        0, & \\text{if} \\quad  否则\n",
    "     \\end{cases}\n",
    "\\end{equation}    \n",
    "它是一个二值函数,当$\\mathbf{x}$和$y$满足这个事实时取值为1,否则取值为0      \n",
    "&emsp;&emsp;特征函数$ f(\\mathbf{x}, y) $关于经验分布$ \\hat{P}(X,Y)  $的期望值,用$ E_{\\hat{P}} (f)$表示:    \n",
    "$$E_{\\hat{P}} (f) = \\sum_{\\mathbf{x}, y}\\hat{P}(\\mathbf{x},y)f(\\mathbf{x},y)$$   \n",
    "&emsp;&emsp;特征函数$ f(\\mathbf{x}, y) $关于模型$P(Y|X)$与经验分布$ \\hat{P}(X) $的期望值,用$ E_{P} (f)$表示:    \n",
    "$$E_{P} (f) = \\sum_{\\mathbf{x}, y}\\hat{P}(\\mathbf{x})P(y|\\mathbf{x})f(\\mathbf{x},y)$$    \n",
    "&emsp;&emsp;如果假设模型能够获取训练数据中的信息,那么就可以假设这两个期望值相等,即    \n",
    "$$ E_{\\hat{P}}(f) = E_{P}(f) $$   \n",
    "或    \n",
    "$$\\sum_{\\mathbf{x}, y}\\hat{P}(x)P(y|\\mathbf{x})f(\\mathbf{x},y) =   \\sum_{\\mathbf{x}, y}\\hat{P}(\\mathbf{x},y)f(\\mathbf{x},y) $$  \n",
    "将上式作为模型学习的约束条件,假设有$n$个特征函数$ f_i(x, y),i=1,2,\\dots,n $,那么就有$n$个约束条件.  \n",
    "\n",
    "<font color='red' size=4>定义:</font>  \n",
    "&emsp;&emsp;假设满足所有约束条件的模型集合为   \n",
    "$$ \\mathcal{C} = \\{ P \\in \\mathcal{P} | E_{P}(f_i) = E_{\\hat{P}}(f_i), \\quad i=1,2,\\dots, n \\} $$    \n",
    "定义在条件概率分布$ P(Y|X) $上的条件熵为:($P(x)$用$\\hat{P}(x)代替$).       \n",
    "$$ H(P) = -\\sum_{x, y} \\hat{P}(x) P(y|x) \\log P(y|x) \\qquad  \\qquad $$       \n",
    "则模型集合中$ \\mathcal{C} $中条件熵$H(P)$最大的模型称为最大熵模型.最大熵原理认为要选择的概率模型首先必须满足\n",
    "已"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}