{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '【病原和流行病学】狂犬病病毒（rabiesvirus）属弹状病毒科狂犬病病毒属。', 'entities': [{'start_idx': 9, 'end_idx': 13, 'type': 'mic', 'entity': '狂犬病病毒'}, {'start_idx': 15, 'end_idx': 25, 'type': 'mic', 'entity': 'rabiesvirus'}, {'start_idx': 28, 'end_idx': 31, 'type': 'mic', 'entity': '弹状病毒'}, {'start_idx': 33, 'end_idx': 37, 'type': 'mic', 'entity': '狂犬病病毒'}]}, {'text': '对儿童SARST细胞亚群的研究表明，与成人SARS相比，儿童细胞下降不明显，证明上述推测成立。', 'entities': [{'start_idx': 3, 'end_idx': 9, 'type': 'bod', 'entity': 'SARST细胞'}, {'start_idx': 19, 'end_idx': 24, 'type': 'dis', 'entity': '成人SARS'}]}, {'text': '研究证实，细胞减少与肺内病变程度及肺内炎性病变吸收程度密切相关。', 'entities': [{'start_idx': 10, 'end_idx': 10, 'type': 'bod', 'entity': '肺'}, {'start_idx': 10, 'end_idx': 13, 'type': 'sym', 'entity': '肺内病变'}, {'start_idx': 17, 'end_idx': 17, 'type': 'bod', 'entity': '肺'}, {'start_idx': 17, 'end_idx': 22, 'type': 'sym', 'entity': '肺内炎性病变'}]}]\n"
     ]
    }
   ],
   "source": [
    "with open('CMeEE_train.json', encoding='utf-8') as f:\n",
    "    data_raw = json.load(f)\n",
    "\n",
    "print(data_raw)  # 列表"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': '【病原和流行病学】狂犬病病毒（rabiesvirus）属弹状病毒科狂犬病病毒属。',\n 'entities': [{'start_idx': 9,\n   'end_idx': 13,\n   'type': 'mic',\n   'entity': '狂犬病病毒'},\n  {'start_idx': 15, 'end_idx': 25, 'type': 'mic', 'entity': 'rabiesvirus'},\n  {'start_idx': 28, 'end_idx': 31, 'type': 'mic', 'entity': '弹状病毒'},\n  {'start_idx': 33, 'end_idx': 37, 'type': 'mic', 'entity': '狂犬病病毒'}]}"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "毒\n",
      "s\n",
      "毒\n",
      "毒\n",
      "\n",
      "狂犬病病毒\n",
      "rabiesvirus\n",
      "弹状病毒\n",
      "狂犬病病毒\n"
     ]
    }
   ],
   "source": [
    "# 13表示结尾字符('毒')的位置\n",
    "print(data_raw[0]['text'][13])\n",
    "print(data_raw[0]['text'][25])\n",
    "print(data_raw[0]['text'][31])\n",
    "print(data_raw[0]['text'][37], end='\\n\\n')\n",
    "\n",
    "# 字符串索引不包括结尾位置元素\n",
    "print(data_raw[0]['text'][9:13 + 1])\n",
    "print(data_raw[0]['text'][15:25 + 1])\n",
    "print(data_raw[0]['text'][28:31 + 1])\n",
    "print(data_raw[0]['text'][33:37 + 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizerFast(name_or_path='hfl/chinese-roberta-wwm-ext', vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "['【',\n '病',\n '原',\n '和',\n '流',\n '行',\n '病',\n '学',\n '】',\n '狂',\n '犬',\n '病',\n '病',\n '毒',\n '（',\n 'ra',\n '##bi',\n '##es',\n '##vi',\n '##rus',\n '）',\n '属',\n '弹',\n '状',\n '病',\n '毒',\n '科',\n '狂',\n '犬',\n '病',\n '病',\n '毒',\n '属',\n '。']"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(data_raw[0]['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "[[(0, 0),\n  (0, 1),\n  (1, 2),\n  (2, 3),\n  (3, 4),\n  (4, 5),\n  (5, 6),\n  (6, 7),\n  (7, 8),\n  (8, 9),\n  (9, 10),\n  (10, 11),\n  (11, 12),\n  (12, 13),\n  (13, 14),\n  (14, 15),\n  (15, 17),\n  (17, 19),\n  (19, 21),\n  (21, 23),\n  (23, 26),\n  (26, 27),\n  (27, 28),\n  (28, 29),\n  (29, 30),\n  (30, 31),\n  (31, 32),\n  (32, 33),\n  (33, 34),\n  (34, 35),\n  (35, 36),\n  (36, 37),\n  (37, 38),\n  (38, 39),\n  (39, 40),\n  (0, 0)]]"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokenizer([data_raw[0]['text']],\n",
    "                    max_length=512, truncation=True, padding=True,\n",
    "                    return_offsets_mapping=True)\n",
    "offset_mapping = outputs[\"offset_mapping\"]\n",
    "offset_mapping  # return (char_start, char_end) for each token."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 17: 17, 19: 18, 21: 19, 23: 20, 26: 21, 27: 22, 28: 23, 29: 24, 30: 25, 31: 26, 32: 27, 33: 28, 34: 29, 35: 30, 36: 31, 37: 32, 38: 33, 39: 34}]\n",
      "[{0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 16: 16, 18: 17, 20: 18, 22: 19, 25: 20, 26: 21, 27: 22, 28: 23, 29: 24, 30: 25, 31: 26, 32: 27, 33: 28, 34: 29, 35: 30, 36: 31, 37: 32, 38: 33, 39: 34}]\n"
     ]
    }
   ],
   "source": [
    "# (0, 0)表示特殊token(如:'[CLS]','[SEP'], '[PAD]'等)\n",
    "# i表示第几个token(从0开始计数,包含特殊token)\n",
    "# j[1] - 1表示该token结尾字符的位置\n",
    "start_mapping = [{j[0]: i for i, j in enumerate(i) if j != (0, 0)} for i in offset_mapping]\n",
    "end_mapping = [{j[1] - 1: i for i, j in enumerate(i) if j != (0, 0)} for i in offset_mapping]\n",
    "print(start_mapping)\n",
    "print(end_mapping)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 13 mic 狂犬病病毒\n",
      "10 14\n",
      "15 25 mic rabiesvirus\n",
      "16 20\n",
      "28 31 mic 弹状病毒\n",
      "23 26\n",
      "33 37 mic 狂犬病病毒\n",
      "28 32\n"
     ]
    }
   ],
   "source": [
    "for i in data_raw[0]['entities']:\n",
    "    start_idx, end_idx, type, entity = i['start_idx'], i['end_idx'], i['type'], i['entity']\n",
    "    print(start_idx, end_idx, type, entity)\n",
    "    if start_idx in start_mapping[0] and end_idx in end_mapping[0]:\n",
    "        start_span = start_mapping[0][start_idx]\n",
    "        end_span = end_mapping[0][end_idx]\n",
    "        print(start_span, end_span)  # 该实体由第[start_span, end_span]的token组成(从0开始)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}