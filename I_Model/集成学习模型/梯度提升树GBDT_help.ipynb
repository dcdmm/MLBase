{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 提升树\n",
    "\n",
    "&emsp;&emsp;以决策树为基函数的提升方法称为提升树(boosting tree),对分类问题决策树是二叉分类树对回归问题的决策树是二叉回归树.\n",
    "提升树模型可以表示为决策树的加法模型:       \n",
    "$$ f_{T} (\\mathbf{x}) = \\sum_{t=1}^T T(\\mathbf{x}; \\Theta_t)  $$       \n",
    "其中,$T(x; \\Theta_t)$表示决策树,$ \\Theta_t $为决策树的参数,$T$为树的个数.       \n",
    "&emsp;&emsp;提升树算法采用前向分步算法.首先确定初始提升树$ f_0(\\mathbf{x}) = 0  $,第$ t $步的模型是      \n",
    "$$ f_t(\\mathbf{x}) = f_{t-1}(\\mathbf{x}) + T(\\mathbf{x};\\Theta_t) $$      \n",
    "其中,$ f_{t-1}(\\mathbf{x}) $为当前模型,通过经验风险极小化确定下一棵决策树的参数$ \\Theta_t $:           \n",
    "$$ \\hat{\\Theta}_{t}=\\arg \\min _{\\Theta_{t}} \\sum_{i=1}^{N} L\\left(y_{i}, f_{m-1}\\left(\\mathbf{x}_{i}\\right)+T\\left(\\mathbf{x}_{i} ; \\Theta_{t}\\right)\\right) $$        \n",
    "由于树的线性组合可以很好地拟合训练数据,即使数据中的输入与输出关系很复杂也是如此,所以提升树是一个高功能的学习算法.\n",
    "对于二类分类问题,提升树算法只需将AdaBoost算法中的基本分类器限制为二类分类树(CART分类树)即可.\n",
    "可以说这时的提升树算法是AdaBoost算法的特殊情况.                \n",
    "&emsp;&emsp;对于回归问题,若已知一个训练数据集$ D={(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\cdots, (\\mathbf{x}_N, y_N)}, \\mathbf{x}_i \\in \\mathcal{X} \\subseteq \\mathbb{R}^n$,$ \\mathcal{X}$为输入空间,\n",
    "$ y_i \\in \\mathcal{Y} \\subseteq \\mathbb{R}  $,$ \\mathcal{Y} $为输出空间.由CART回归树生成算法可知,\n",
    "如果将输入空间$ \\mathcal{X} $划分为$J$个互不相交的区域$ R_1, R_2, \\cdots,R_J $,并且在每个区域上确定输出的常量$ c_j $,那么树可表示为          \n",
    "$$ T(\\mathbf{x}, \\Theta) = \\sum_{j=1}^J c_j \\mathbb{I}( \\mathbf{x} \\in R_j) \\tag{1} $$         \n",
    "其中,参数$\\Theta=\\{ (R_1, c_1),(R_2, c_2), \\cdots, (R_J, c_J) \\}$表示树的区域划分和各区域上的常数,$J$是回归树的复杂度即叶节点个数.                         \n",
    "&emsp;&emsp;回归问题提升树使用以下前向分步算法:               \n",
    "\n",
    "\\begin{align} \n",
    "&f_0(\\mathbf{x}) = 0 \\\\\n",
    "&f_t(\\mathbf{x}) = f_{t-1}(\\mathbf{x}) + T(\\mathbf{x};\\Theta_t),\\quad t=1,2,\\cdots,T \\\\\n",
    "&f_{T} (\\mathbf{x}) = \\sum_{t=1}^T T(\\mathbf{x}; \\Theta_t)\n",
    "\\end{align}          \n",
    "\n",
    "在前向分步算法的第$t$步,给定当前模型$f_{t-1}(\\mathbf{x}) $,需求解      \n",
    "$$ \\hat{\\Theta}_{t}=\\arg \\min _{\\Theta_{t}} \\sum_{i=1}^{N} L\\left(y_{i}, f_{m-1}\\left(\\mathbf{x}_{i}\\right)+T\\left(\\mathbf{x}_{i} ; \\Theta_{t}\\right)\\right) $$          \n",
    "得到$ \\hat{\\Theta}_{t}$,即第$t$棵树的参数.                      \n",
    "&emsp;&emsp;当采用平方误差损失函数时,      \n",
    "$$ L(y, f(\\mathbf{x})) = (y - f(\\mathbf{x}))^2 $$             \n",
    "其损失函数为     \n",
    "\n",
    "\\begin{aligned}\n",
    "L\\left(y, f_{t-1}(\\mathbf{x})+T\\left(\\mathbf{x} ; \\Theta_{t}\\right)\\right) &=\\left[y-f_{t-1}(\\mathbf{x})-T\\left(\\mathbf{x} ; \\Theta_{m}\\right)\\right]^{2} \\\\\n",
    "&=\\left[r-T\\left(\\mathbf{x} ; \\Theta_{m}\\right)\\right]^{2}\n",
    "\\end{aligned}          \n",
    "\n",
    "其中,\n",
    "$$ r=y - f_{t-1}(\\mathbf{x}) $$           \n",
    "是当前模型拟合数据的残差(residual).所以,对回归问题的提升树算法来说,只需简单地拟合当前模型的残差.这样,算法是相当简单的.           \n",
    "\n",
    "<font color='red' size=4>回归问题的提升树算法:</font>\n",
    "输入:训练数据集$ D={(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\cdots, (\\mathbf{x}_N, y_N)}, \\mathbf{x}_i \\in \\mathcal{X} \\subseteq \\mathbb{R}^n,y_i \\in \\mathcal{Y} \\subseteq \\mathbb{R}$;        \n",
    "输出:提升树$ f_T(\\mathbf{x}) $              \n",
    "\n",
    "1. 初始化$ f_0(\\mathbf{x}) = 0  $\n",
    "\n",
    "2. 对$ t=1,2,\\cdots,T $     \n",
    "   1. 根据式(1)计算残差\n",
    "      $$ r_{ti} = y_i - f_{t-1}(\\mathbf{x}_i), \\quad i=1,2,\\cdots,N $$           \n",
    "   2. 拟合残差$ (\\mathbf{x}_1, r_{t1}), \\cdots, (\\mathbf{x}_i, r_{ti}),\\cdots,(\\mathbf{x}_N, r_{tN})$学习一个回归树,得到$ T(\\mathbf{x}, \\Theta_t) $          \n",
    "   3. 更新$ f_t(\\mathbf{x}) = f_{t-1}(\\mathbf{x}) + T(x;\\Theta_t) $                   \n",
    "   \n",
    "3. 得到回归问题提升树         \n",
    "$$ f_{T} (\\mathbf{x}) = \\sum_{t=1}^T T(\\mathbf{x}; \\Theta_t) $$\n",
    "\n",
    "\n",
    "\n",
    "### 梯度提升树\n",
    "&emsp;&emsp;提升树利用加法模型与前向分步算法实现学习的优化过程.当损失函数是平方损失(回归)和指数损失(分类)函数时,\n",
    "每一步优化是很简单的.但对一般损失函数而言,往往每一步优化并不那么容易.\n",
    "针对这一问题,Freidman提出了梯度提升(gradient boosting)算法.这是利用最速下降法的近似方法,其关键是利用损失函数的负梯度在当前模型的值        \n",
    "$$ -\\left[ \\frac{\\partial L(y, f(\\mathbf{x}))}{\\partial f(\\mathbf{x})}  \\right]_{f(\\mathbf{x}) = f_{t-1}(\\mathbf{x})} $$             \n",
    "作为回归问题提升树算法中残差的近似值,拟合一个回归树.       \n",
    "\n",
    "<font color='red' size=4>梯度提升算法:</font>             \n",
    "输入:训练数据集$ D={(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\cdots, (\\mathbf{x}_N, y_N)}, \\mathbf{x}_i \\in \\mathcal{X} \\subseteq \\mathbb{R}^n,y_i \\in \\mathcal{Y} \\subseteq \\mathbb{R}$;损失函数$ L(y, f(\\mathbf{x})) $;         \n",
    "输出:提升树$ f_T(\\mathbf{x}) $           \n",
    "\n",
    "1. 初始化     \n",
    "$$ f_{0}(\\mathbf{x})=\\arg \\min _{c} \\sum_{i=1}^{N} L\\left(y_{i}, c\\right) $$\n",
    "\n",
    "2. 对$ t=1,2,\\cdots,T $\n",
    "   1. 对$i=1,2,\\cdots,N$,计算       \n",
    "      $$ r_{ti} = -\\left[ \\frac{\\partial L(y, f(\\mathbf{x}))}{\\partial f(\\mathbf{x})}  \\right]_{f(\\mathbf{x}) = f_{t-1}(\\mathbf{x})} $$        \n",
    "   2. 对$ r_{ti}$拟合一个回归树,得到第$t$棵树的叶结点区域$R_{tj},j=1,2,\\cdots,J $       \n",
    "   3. 对$j=1,2,\\cdots,J$,计算\n",
    "      $$ c_{t j}=\\arg \\min _{c} \\sum_{\\mathbf{x}_{i} \\in R_{t j}} L\\left(y_{i}, f_{t-1}\\left(\\mathbf{x}_{i}\\right)+c\\right) $$           \n",
    "   4. 更新$$ f_{t}(\\mathbf{x})=f_{t-1}(\\mathbf{x})+\\sum_{j=1}^{J} c_{t j} I\\left(\\mathbf{x} \\in R_{t j}\\right) $$           \n",
    "\n",
    "3. 得到回归树      \n",
    "$$ \\hat{f}(\\mathbf{x})=f_{T}(\\mathbf{x})=\\sum_{t=1}^{T} \\sum_{j=1}^{J} c_{t j} I\\left(\\mathbf{x} \\in R_{t j}\\right) $$           \n",
    "\n",
    "&emsp;&emsp;算法的第1步初始化,估计使损失函数极小化的常数值,它是只有一个跟结点的树.第2.1步计算损失函数的负梯度在当前模型的值,\n",
    "将它作为残差的估计.对于平方损失函数,它就是通常所说的残差;对于一般损失函数,它就是残差的近似值.第2.2步估计回归树叶节点区域,\n",
    "以拟合残差的近似值.第2.3利用线性搜索估计叶节点区域的值,使损失函数极小化(因为损失函数不一定是平方损失函数,所以$c_{ti}$不一定是$ R_{mj} $上所有输入实例$ \\mathbf{x}_i $对应的输出$y_i$的均值).    \n",
    "第2.4步更新回归树.第3步得到输出的最终模型$ \\hat{f}(\\mathbf{x}) $.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
