{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e9ee61f-3ef8-480b-89e5-67682d81bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (BertModel, BertTokenizerFast)\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from model import RawGlobalPointer, ERENet\n",
    "from utils import sparse_multilabel_categorical_crossentropy, MetricsCalculator_CMeIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944c4849-692c-4c8a-bcc8-ec542dcb44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66d00d0f-0e37-429a-920e-f89aac3e98b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a953c6-9d39-458d-9042-82a8eb024ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    # example=[{'text': text0, 'spo_list': [(s0, p0, o0, s0_t, o0_t), (s0_0, p0_0, o0_0, s0_0_t, o0_0_t), xxxxxx]}, {'text': text1, 'spo_list': [(s1, p1, o1, s1_t, o1_t), xxxxxx]}, xxxxxx]\n",
    "    D = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line)\n",
    "            D.append({\n",
    "                \"text\": line[\"text\"],\n",
    "                \"spo_list\": [(spo[\"subject\"], spo[\"predicate\"], spo[\"object\"][\"@value\"], spo[\"subject_type\"],\n",
    "                              spo[\"object_type\"][\"@value\"])\n",
    "                             for spo in line[\"spo_list\"]]\n",
    "            })\n",
    "        return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911d8f29-7861-433c-a55c-542141931bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data('datasets_CMeIE/CMeIE_train.jsonl')\n",
    "valid_data = load_data('datasets_CMeIE/CMeIE_dev.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8be9cbd9-015d-484a-a2a8-d77cdc7bea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'疾病_预防_其他': 0, '疾病_阶段_其他': 1, '疾病_就诊科室_其他': 2, '其他_同义词_其他': 3, '疾病_辅助治疗_其他治疗': 4, '疾病_化疗_其他治疗': 5, '疾病_放射治疗_其他治疗': 6, '其他治疗_同义词_其他治疗': 7, '疾病_手术治疗_手术治疗': 8, '手术治疗_同义词_手术治疗': 9, '疾病_实验室检查_检查': 10, '疾病_影像学检查_检查': 11, '疾病_辅助检查_检查': 12, '疾病_组织学检查_检查': 13, '检查_同义词_检查': 14, '疾病_内窥镜检查_检查': 15, '疾病_筛查_检查': 16, '疾病_多发群体_流行病学': 17, '疾病_发病率_流行病学': 18, '疾病_发病年龄_流行病学': 19, '疾病_多发地区_流行病学': 20, '疾病_发病性别倾向_流行病学': 21, '疾病_死亡率_流行病学': 22, '疾病_多发季节_流行病学': 23, '疾病_传播途径_流行病学': 24, '流行病学_同义词_流行病学': 25, '疾病_同义词_疾病': 26, '疾病_并发症_疾病': 27, '疾病_病理分型_疾病': 28, '疾病_相关（导致）_疾病': 29, '疾病_鉴别诊断_疾病': 30, '疾病_相关（转化）_疾病': 31, '疾病_相关（症状）_疾病': 32, '疾病_临床表现_症状': 33, '疾病_治疗后症状_症状': 34, '疾病_侵及周围组织转移的症状_症状': 35, '症状_同义词_症状': 36, '疾病_病因_社会学': 37, '疾病_高危因素_社会学': 38, '疾病_风险评估因素_社会学': 39, '疾病_病史_社会学': 40, '疾病_遗传因素_社会学': 41, '社会学_同义词_社会学': 42, '疾病_发病机制_社会学': 43, '疾病_病理生理_社会学': 44, '疾病_药物治疗_药物': 45, '药物_同义词_药物': 46, '疾病_发病部位_部位': 47, '疾病_转移部位_部位': 48, '疾病_外侵部位_部位': 49, '部位_同义词_部位': 50, '疾病_预后状况_预后': 51, '疾病_预后生存率_预后': 52}\n"
     ]
    }
   ],
   "source": [
    "with open('datasets_CMeIE/53_schemas.jsonl', 'r', encoding='utf-8') as f:\n",
    "    schema = {}\n",
    "    for idx, item in enumerate(f):\n",
    "        item = json.loads(item.rstrip())\n",
    "        schema[item[\"subject_type\"] + \"_\" + item[\"predicate\"] + \"_\" + item[\"object_type\"]] = idx\n",
    "print(schema)  # 关系类型与id的字典映射\n",
    "\n",
    "id2schema = {}\n",
    "for k, v in schema.items():\n",
    "    id2schema[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "015746ed-fb31-4eda-8488-44984eac884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '急性胰腺炎@有研究显示，进行早期 ERCP （24 小时内）可以降低梗阻性胆总管结石患者的并发症发生率和死亡率； 但是，对于无胆总管梗阻的胆汁性急性胰腺炎患者，不需要进行早期 ERCP。', 'spo_list': [('急性胰腺炎', '影像学检查', 'ERCP', '疾病', '检查')]}\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, items):\n",
    "        self._items = items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._items)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self._items[index]\n",
    "\n",
    "        return {'text': item['text'],\n",
    "                'spo_list': item['spo_list']}\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(items=train_data)\n",
    "valid_dataset = CustomDataset(items=valid_data)\n",
    "\n",
    "for i in valid_dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8733358-6b0d-4b9a-a817-9a1a1364a9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.09734654426574707,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 19,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef8990b727945e582370418b3f643d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/19.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02646493911743164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 109540,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f8b8628594410bb85362c9cff74f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017536640167236328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 268961,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bed5f68de414f0c9f14c1ee81220cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014808177947998047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 2,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55170303d0cd40d882e63c1368afd837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.025739192962646484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 112,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa1e50ffd46431ebd5b8c824f71067e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015158891677856445,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 689,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bae7e562b94a4283c0f6a2d2584a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='hfl/chinese-roberta-wwm-ext', vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020651817321777344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 411578458,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b6a08ec9144fb2884a5125238e470a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102267648\n"
     ]
    }
   ],
   "source": [
    "tokenizer_fast = BertTokenizerFast.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "print(tokenizer_fast)\n",
    "\n",
    "pretrained = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "print(pretrained.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9faf3d1a-c3e2-4104-adbe-7cccb9708ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids.shape torch.Size([16, 207])\n",
      "entity_labels.shape torch.Size([16, 2, 11, 2])\n",
      "head_labels.shape torch.Size([16, 53, 9, 2])\n",
      "tail_labels.shape torch.Size([16, 53, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "def search(pattern, sequence):\n",
    "    \"\"\"从序列sequence中寻找子序列pattern.如果找到,返回pattern第一个元素在sequence中的index,否则返回-1\"\"\"\n",
    "    n = len(pattern)\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i:i + n] == pattern:\n",
    "            return i, i + n - 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def get_collate_fn(tokenizer, max_len=512):\n",
    "    \"\"\"返回collate_fun函数(通过闭包函数引入形参)\"\"\"\n",
    "\n",
    "    def collate_fn(data):\n",
    "        batch_size = len(data)\n",
    "\n",
    "        texts = [i['text'] for i in data]\n",
    "        encoder_text = tokenizer(texts, max_length=max_len, truncation=True, padding=True, return_tensors='pt')\n",
    "        input_ids, token_type_ids, attention_mask = encoder_text.values()\n",
    "\n",
    "        spo_lists = [i['spo_list'] for i in data]\n",
    "        entity_labels, head_labels, tail_labels = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            entity_labels_temp = [set(), set()]\n",
    "            head_labels_temp = [set() for _ in range(len(schema))]  # 每种关系的信息用一个列表表示\n",
    "            tail_labels_temp = [set() for _ in range(len(schema))]\n",
    "            spoes = set()\n",
    "\n",
    "            # example:(s0, p0, o0, s0_t, o0_t)\n",
    "            for s, p, o, s_t, o_t in spo_lists[i]:\n",
    "                s = tokenizer(s, add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)[\n",
    "                    'input_ids']\n",
    "                o = tokenizer(o, add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)[\n",
    "                    'input_ids']\n",
    "                p = schema[s_t + \"_\" + p + \"_\" + o_t]  # SPO关系约束字典中该关系对应id\n",
    "                # subject实体tokens列表(不含特殊token)在整个句子tokens列表(含特殊token)中的首尾位置\n",
    "                s_range = search(s, input_ids[i].tolist())\n",
    "                # object实体tokens列表(不含特殊token)在整个句子tokens列表(含特殊token)中的首尾位置\n",
    "                o_range = search(o, input_ids[i].tolist())\n",
    "                if s_range != -1 and o_range != -1:\n",
    "                    spoes.add((*s_range, p, *o_range))  # subject、predicate、object三元组关系是唯一的\n",
    "            for sh, se, p, oh, oe in spoes:\n",
    "                # 该句子所有不同subject实体(故entity_labels_temp[1]类型为集合)tokens列表(不含特殊token)在整个句子tokens列表(含特殊token)中的首尾位置\n",
    "                entity_labels_temp[0].add((sh, se))\n",
    "                # 该句子所有不同object实体(故entity_labels_temp[1]类型为集合)tokens列表(不含特殊token)在整个句子tokens列表(含特殊token)中的首尾位置\n",
    "                entity_labels_temp[1].add((oh, oe))\n",
    "                # 该句子所有不同subject实体tokens列表在整个句子tokens列表的首位置, object实体tokens列表在整个句子tokens列表的首位置\n",
    "                head_labels_temp[p].add((sh, oh))\n",
    "                # 该句子所有不同subject实体tokens列表在整个句子tokens列表的尾位置, object实体tokens列表在整个句子tokens列表的尾位置\n",
    "                tail_labels_temp[p].add((se, oe))\n",
    "\n",
    "            for label in entity_labels_temp + head_labels_temp + tail_labels_temp:\n",
    "                if not label:\n",
    "                    label.add((0, 0))\n",
    "\n",
    "            entity_labels_temp = [torch.tensor(list(i)) for i in entity_labels_temp]  # 内部set转换为list\n",
    "            # entity_labels_temp.shape=[longest sequence, 2, 2]\n",
    "            entity_labels_temp = torch.transpose(pad_sequence(entity_labels_temp, batch_first=True), 0,\n",
    "                                                 1)  # 填充第0个维度,其他维度必须相等或可广播\n",
    "            entity_labels.append(entity_labels_temp)\n",
    "\n",
    "            head_labels_temp = [torch.tensor(list(i)) for i in head_labels_temp]\n",
    "            head_labels_temp = torch.transpose(pad_sequence(head_labels_temp, batch_first=True), 0, 1)\n",
    "            head_labels.append(head_labels_temp)\n",
    "            tail_labels_temp = [torch.tensor(list(i)) for i in tail_labels_temp]\n",
    "            tail_labels_temp = torch.transpose(pad_sequence(tail_labels_temp, batch_first=True), 0, 1)\n",
    "            tail_labels.append(tail_labels_temp)\n",
    "\n",
    "            # entity_labels.shape=[batch_size, 2, longest sequence, 2]\n",
    "        entity_labels = torch.transpose(pad_sequence(entity_labels, batch_first=True), 1, 2)\n",
    "        # head_labels.shape=[batch_size, len(schema), longest sequence, 2]\n",
    "        head_labels = torch.transpose(pad_sequence(head_labels, batch_first=True), 1, 2)\n",
    "        tail_labels = torch.transpose(pad_sequence(tail_labels, batch_first=True), 1, 2)\n",
    "        return input_ids, attention_mask, token_type_ids, entity_labels, head_labels, tail_labels, texts, spo_lists\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=get_collate_fn(tokenizer_fast))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, collate_fn=get_collate_fn(tokenizer_fast))\n",
    "\n",
    "for i in train_loader:\n",
    "    print('input_ids.shape', i[0].shape)\n",
    "    print('entity_labels.shape', i[3].shape)\n",
    "    print('head_labels.shape', i[4].shape)\n",
    "    print('tail_labels.shape', i[5].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17091163-3bab-4f88-8cfa-23656773c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = pretrained.config.hidden_size\n",
    "mention_detect = RawGlobalPointer(hidden_size, 2, 64).to(device)  # 不提取实体类型(只识别subject、object对应的实体)\n",
    "s_o_head = RawGlobalPointer(hidden_size, len(schema), 64, RoPE=False, tril_mask=False).to(\n",
    "    device)  # 不需要设置tril_mask=False\n",
    "s_o_tail = RawGlobalPointer(hidden_size, len(schema), 64, RoPE=False, tril_mask=False).to(\n",
    "    device)  # 不需要设置tril_mask=False\n",
    "net = ERENet(copy.deepcopy(pretrained), mention_detect, s_o_head, s_o_tail).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bc67b28-4a79-4902-a350-013096e6956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spoes(logits1, logits2, logits3, texts, tokenizer, id2predicate):\n",
    "    logits1 = logits1.data.cpu().numpy()\n",
    "    logits2 = logits2.data.cpu().numpy()\n",
    "    logits3 = logits3.data.cpu().numpy()\n",
    "    batch_size = logits1.shape[0]\n",
    "    offset_mapping = tokenizer(texts, return_offsets_mapping=True)['offset_mapping']\n",
    "\n",
    "    # 序列开头与结尾特殊token('[CLS]', '[SEP]')处元素设置为无穷小\n",
    "    logits1[:, :, [0, -1]] -= np.inf\n",
    "    logits1[:, :, :, [0, -1]] -= np.inf\n",
    "    subjects, objects = [set() for _ in range(batch_size)], [set() for _ in range(batch_size)]\n",
    "    for b, l, h, t in zip(*np.where(logits1 > 0.0)):  # 阈值(threshold)设置为0.0\n",
    "        if l == 0:  # 不提取实体类型(只识别subjects、objects对应的实体)\n",
    "            subjects[b].add((h, t))\n",
    "        else:\n",
    "            objects[b].add((h, t))\n",
    "\n",
    "    spoes = [set() for _ in range(batch_size)]\n",
    "    for b in range(batch_size):\n",
    "        offset_mapping_b = offset_mapping[b]\n",
    "        text_b = texts[b]\n",
    "        # 计算subjects[b]与objects[b]所有可能关系的笛卡尔组合\n",
    "        for sh, st in subjects[b]:\n",
    "            for oh, ot in objects[b]:\n",
    "                p1s = np.where(logits2[b, :, sh, oh] > 0.0)[0]  # 阈值(threshold)设置为0.0\n",
    "                p2s = np.where(logits3[b, :, st, ot] > 0.0)[0]  # 阈值(threshold)设置为0.0\n",
    "                # 含义:首S(s_h,o_h|p) > 0 且 尾S(s_t,o_t|p) > 0\n",
    "                ps = set(p1s) & set(p2s)\n",
    "                for p in ps:\n",
    "                    sht_str = text_b[offset_mapping_b[sh][0]: offset_mapping_b[st][1]]\n",
    "                    oht_str = text_b[offset_mapping_b[oh][0]: offset_mapping_b[ot][1]]\n",
    "                    spoes[b].add((sht_str, id2predicate[p], oht_str))  # 添加预测结果:(subject, predicate, object)  \n",
    "    return spoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b283bb3-6aec-44da-962e-f092a72c51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(model, dataloader, optimizer, tokenizer, id2predicate, device):\n",
    "    model.train()\n",
    "\n",
    "    for idx, (\n",
    "    input_ids, attention_mask, token_type_ids, entity_labels, head_labels, tail_labels, texts, spo_lists) in enumerate(\n",
    "            dataloader):\n",
    "        # 数据设备切换\n",
    "        # input_ids.shape=[batch_size, seq_len]\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        entity_labels = entity_labels.to(device)\n",
    "        head_labels = head_labels.to(device)\n",
    "        tail_labels = tail_labels.to(device)\n",
    "\n",
    "        # logits1.shape=[batch_size, 2, seq_len, seq_len]\n",
    "        # logits2.shape=[batch_size, len(schema) seq_len, seq_len]\n",
    "        # logits3.shape=[batch_size, len(schema), seq_len, seq_len]\n",
    "        logits1, logits2, logits3 = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        loss1 = sparse_multilabel_categorical_crossentropy(y_true=entity_labels, y_pred=logits1)\n",
    "        loss2 = sparse_multilabel_categorical_crossentropy(y_true=head_labels, y_pred=logits2)\n",
    "        loss3 = sparse_multilabel_categorical_crossentropy(y_true=tail_labels, y_pred=logits3)\n",
    "        loss = sum([loss1, loss2, loss3]) / 3  # entities和relations之间的信息共享和交互\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0 and idx > 0:\n",
    "            y_pred = extract_spoes(logits1, logits2, logits3, texts, tokenizer, id2predicate)\n",
    "            mc = MetricsCalculator_CMeIE()  # 计算查准率、查全率、F1 score \n",
    "            mc.calc_confusion_matrix(y_pred, spo_lists)\n",
    "            print('| step {:5d} | loss {:9.5f} | precision {:8.5f} | recall {:8.5f} | f1 {:8.5f} |'.format(idx,\n",
    "                                                                                                           loss.item(),\n",
    "                                                                                                           mc.precision,\n",
    "                                                                                                           mc.recall,\n",
    "                                                                                                           mc.f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1753660a-cf30-4276-a9a4-100b98b268c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型验证\n",
    "def evaluate(model, dataloader, tokenizer, id2predicate, device):\n",
    "    model.eval()\n",
    "\n",
    "    mc = MetricsCalculator_CMeIE()  # 计算查准率、查全率、F1 score \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, token_type_ids, _, _, _, texts, spo_lists in dataloader:\n",
    "            # 数据设备切换\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "\n",
    "            logits1, logits2, logits3 = model(input_ids, attention_mask, token_type_ids)\n",
    "            y_pred = extract_spoes(logits1, logits2, logits3, texts, tokenizer, id2predicate)\n",
    "\n",
    "            mc.calc_confusion_matrix(y_pred, spo_lists)\n",
    "    return mc.precision, mc.recall, mc.f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ee2e5bb-ebd4-4923-8a07-a07621c35ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step   100 | loss 339.16815 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   200 | loss 151.01361 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   300 | loss 157.45125 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   400 | loss 139.54654 | precision  0.50000 | recall  0.02000 | f1  0.03846 |\n",
      "| step   500 | loss  85.37960 | precision  1.00000 | recall  0.02500 | f1  0.04878 |\n",
      "| step   600 | loss 103.00769 | precision  1.00000 | recall  0.05263 | f1  0.10000 |\n",
      "| step   700 | loss  79.83633 | precision  0.50000 | recall  0.05405 | f1  0.09756 |\n",
      "| step   800 | loss  87.48330 | precision  0.63636 | recall  0.15217 | f1  0.24561 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch:     0 | time: 295.65s | valid precision  0.62902 | valid recall  0.20465 | valid f1  0.30883 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss 103.91711 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   200 | loss  89.72753 | precision  0.71429 | recall  0.07692 | f1  0.13889 |\n",
      "| step   300 | loss  89.26766 | precision  0.60000 | recall  0.10526 | f1  0.17910 |\n",
      "| step   400 | loss 104.20004 | precision  0.80000 | recall  0.08889 | f1  0.16000 |\n",
      "| step   500 | loss  99.81656 | precision  0.75000 | recall  0.24000 | f1  0.36364 |\n",
      "| step   600 | loss  84.78838 | precision  0.45455 | recall  0.12195 | f1  0.19231 |\n",
      "| step   700 | loss  71.94133 | precision  0.87500 | recall  0.17073 | f1  0.28571 |\n",
      "| step   800 | loss  63.23231 | precision  0.86364 | recall  0.50000 | f1  0.63333 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch:     1 | time: 296.10s | valid precision  0.62779 | valid recall  0.31292 | valid f1  0.41766 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss 111.44698 | precision  0.42857 | recall  0.22222 | f1  0.29268 |\n",
      "| step   200 | loss  75.98532 | precision  0.78571 | recall  0.20755 | f1  0.32836 |\n",
      "| step   300 | loss  48.18111 | precision  0.82609 | recall  0.55882 | f1  0.66667 |\n",
      "| step   400 | loss  82.89573 | precision  0.88889 | recall  0.32000 | f1  0.47059 |\n",
      "| step   500 | loss  74.65108 | precision  0.72727 | recall  0.21053 | f1  0.32653 |\n",
      "| step   600 | loss  76.69109 | precision  0.71429 | recall  0.14706 | f1  0.24390 |\n",
      "| step   700 | loss  81.11944 | precision  0.73684 | recall  0.40580 | f1  0.52336 |\n",
      "| step   800 | loss  85.48904 | precision  0.70000 | recall  0.14000 | f1  0.23333 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch:     2 | time: 294.54s | valid precision  0.64988 | valid recall  0.39753 | valid f1  0.49331 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss 112.02722 | precision  0.62500 | recall  0.13889 | f1  0.22727 |\n",
      "| step   200 | loss  70.56333 | precision  0.95238 | recall  0.41667 | f1  0.57971 |\n",
      "| step   300 | loss  68.12782 | precision  0.83333 | recall  0.33898 | f1  0.48193 |\n",
      "| step   400 | loss  58.26944 | precision  0.75862 | recall  0.52381 | f1  0.61972 |\n",
      "| step   500 | loss  46.89254 | precision  0.90476 | recall  0.43182 | f1  0.58462 |\n",
      "| step   600 | loss  81.80117 | precision  0.92857 | recall  0.23214 | f1  0.37143 |\n",
      "| step   700 | loss  57.50639 | precision  0.69231 | recall  0.27273 | f1  0.39130 |\n",
      "| step   800 | loss  82.12763 | precision  0.81818 | recall  0.28571 | f1  0.42353 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch:     3 | time: 296.97s | valid precision  0.58743 | valid recall  0.51852 | valid f1  0.55082 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss  54.62727 | precision  0.28205 | recall  0.26190 | f1  0.27160 |\n",
      "| step   200 | loss  64.01536 | precision  0.91429 | recall  0.46377 | f1  0.61538 |\n",
      "| step   300 | loss  47.98179 | precision  0.90000 | recall  0.42857 | f1  0.58065 |\n",
      "| step   400 | loss  67.58041 | precision  0.50000 | recall  0.34146 | f1  0.40580 |\n",
      "| step   500 | loss  47.87622 | precision  0.89286 | recall  0.46296 | f1  0.60976 |\n",
      "| step   600 | loss  36.92064 | precision  0.74074 | recall  0.48780 | f1  0.58824 |\n",
      "| step   700 | loss  62.73376 | precision  0.85000 | recall  0.42500 | f1  0.56667 |\n",
      "| step   800 | loss  51.86985 | precision  0.86486 | recall  0.52459 | f1  0.65306 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch:     4 | time: 298.70s | valid precision  0.66338 | valid recall  0.45661 | valid f1  0.54091 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss  44.22430 | precision  0.78947 | recall  0.34884 | f1  0.48387 |\n",
      "| step   200 | loss  57.70642 | precision  0.85000 | recall  0.56667 | f1  0.68000 |\n",
      "| step   300 | loss  48.58065 | precision  0.88235 | recall  0.50000 | f1  0.63830 |\n",
      "| step   400 | loss  59.33863 | precision  0.81481 | recall  0.53659 | f1  0.64706 |\n",
      "| step   500 | loss  39.21595 | precision  0.78947 | recall  0.50000 | f1  0.61224 |\n",
      "| step   600 | loss  28.75983 | precision  0.91667 | recall  0.70968 | f1  0.80000 |\n",
      "| step   700 | loss  34.80506 | precision  0.95238 | recall  0.50000 | f1  0.65574 |\n",
      "| step   800 | loss  56.00348 | precision  0.57447 | recall  0.45763 | f1  0.50943 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch:     5 | time: 298.71s | valid precision  0.65212 | valid recall  0.50250 | valid f1  0.56761 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss  24.90588 | precision  0.94118 | recall  0.84211 | f1  0.88889 |\n",
      "| step   200 | loss  66.28650 | precision  0.62500 | recall  0.30303 | f1  0.40816 |\n",
      "| step   300 | loss  30.57642 | precision  0.83333 | recall  0.62500 | f1  0.71429 |\n",
      "| step   400 | loss  35.70528 | precision  0.93023 | recall  0.61538 | f1  0.74074 |\n",
      "| step   500 | loss  44.08858 | precision  0.94444 | recall  0.38636 | f1  0.54839 |\n",
      "| step   600 | loss  32.45492 | precision  0.84615 | recall  0.56410 | f1  0.67692 |\n",
      "| step   700 | loss  49.34639 | precision  0.91667 | recall  0.51163 | f1  0.65672 |\n",
      "| step   800 | loss  38.83755 | precision  0.90476 | recall  0.48718 | f1  0.63333 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch:     6 | time: 296.38s | valid precision  0.63244 | valid recall  0.53679 | valid f1  0.58070 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss  28.98619 | precision  0.89286 | recall  0.59524 | f1  0.71429 |\n",
      "| step   200 | loss  34.21201 | precision  0.66667 | recall  0.65116 | f1  0.65882 |\n",
      "| step   300 | loss  32.21350 | precision  1.00000 | recall  0.59091 | f1  0.74286 |\n",
      "| step   400 | loss  34.97155 | precision  0.88000 | recall  0.57895 | f1  0.69841 |\n",
      "| step   500 | loss  63.01566 | precision  0.82609 | recall  0.31148 | f1  0.45238 |\n",
      "| step   600 | loss  37.99680 | precision  0.91667 | recall  0.53659 | f1  0.67692 |\n",
      "| step   700 | loss  31.58501 | precision  0.88889 | recall  0.61538 | f1  0.72727 |\n",
      "| step   800 | loss  43.27798 | precision  0.77778 | recall  0.32558 | f1  0.45902 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch:     7 | time: 296.81s | valid precision  0.67310 | valid recall  0.50269 | valid f1  0.57554 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss  32.42800 | precision  0.89189 | recall  0.68750 | f1  0.77647 |\n",
      "| step   200 | loss  31.40068 | precision  0.79487 | recall  0.70455 | f1  0.74699 |\n",
      "| step   300 | loss  27.98589 | precision  0.97143 | recall  0.61818 | f1  0.75556 |\n",
      "| step   400 | loss  45.32328 | precision  0.84211 | recall  0.72727 | f1  0.78049 |\n",
      "| step   500 | loss  44.85577 | precision  0.91892 | recall  0.61818 | f1  0.73913 |\n",
      "| step   600 | loss  28.17715 | precision  0.88462 | recall  0.60526 | f1  0.71875 |\n",
      "| step   700 | loss  41.24396 | precision  0.85714 | recall  0.58065 | f1  0.69231 |\n",
      "| step   800 | loss  32.01694 | precision  0.92000 | recall  0.75410 | f1  0.82883 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch:     8 | time: 297.93s | valid precision  0.65432 | valid recall  0.53774 | valid f1  0.59033 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| step   100 | loss  33.39532 | precision  0.78261 | recall  0.70588 | f1  0.74227 |\n",
      "| step   200 | loss  30.17376 | precision  0.93182 | recall  0.64062 | f1  0.75926 |\n",
      "| step   300 | loss  18.30238 | precision  0.97143 | recall  0.80952 | f1  0.88312 |\n",
      "| step   400 | loss  20.70800 | precision  0.96774 | recall  0.90909 | f1  0.93750 |\n",
      "| step   500 | loss  36.45281 | precision  0.81250 | recall  0.76471 | f1  0.78788 |\n",
      "| step   600 | loss  22.99174 | precision  0.83871 | recall  0.66667 | f1  0.74286 |\n",
      "| step   700 | loss  53.10645 | precision  0.84848 | recall  0.36842 | f1  0.51376 |\n",
      "| step   800 | loss  42.31728 | precision  0.86667 | recall  0.28261 | f1  0.42623 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch:     9 | time: 296.77s | valid precision  0.61062 | valid recall  0.57656 | valid f1  0.59310 |\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    epoch_start_time = time.time()\n",
    "    train(net, train_loader, optimizer, tokenizer_fast, id2schema, device)\n",
    "    valid_precision, valid_recall, valid_f1 = evaluate(net, valid_loader, tokenizer_fast, id2schema, device)\n",
    "    print('-' * 100)\n",
    "    print('| epoch: {:5d} | time: {:5.2f}s '\n",
    "          '| valid precision {:8.5f} '\n",
    "          '| valid recall {:8.5f} '\n",
    "          '| valid f1 {:8.5f} |'.format(epoch,\n",
    "                                        time.time() - epoch_start_time,\n",
    "                                        valid_precision,\n",
    "                                        valid_recall,\n",
    "                                        valid_f1))\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf670b72-5fdd-42f6-83b9-98314843058d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3812",
   "language": "python",
   "name": "env_3812"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}