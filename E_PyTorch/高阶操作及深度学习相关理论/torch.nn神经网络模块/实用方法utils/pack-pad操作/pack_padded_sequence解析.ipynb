{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'to', 'choose', 'those', 'who', 'are', 'different', 'from', 'themselves', 'while', 'others', 'prefer', 'those', 'who', 'are', 'similar', 'to', 'themselves.']\n",
      "['People', 'choose', 'friends', 'in', 'differrent', 'ways.']\n",
      "['For', 'instance,', 'if', 'an', 'active', 'and', 'energetic', 'guy', 'proposes', 'to', 'his', 'equally', 'active', 'and', 'energetic', 'friends', 'that', 'they', 'should', 'have', 'some', 'activities,', 'it', 'is', 'more', 'likely', 'that', 'his', 'will', 'agree', 'at', 'once.']\n",
      "['When', 'people', 'have', 'friends', 'similar', 'to', 'themselves,', 'they', 'and', 'their', 'friends', 'chat,', 'play,', 'and', 'do', 'thing', 'together', 'natually', 'and', 'harmoniously.']\n",
      "['The', 'result', 'is', 'that', 'they', 'all', 'can', 'feel', 'relaxed', 'and', 'can', 'trully', 'enjoy', 'each', \"other's\", 'company.']\n"
     ]
    }
   ],
   "source": [
    "vocab = {}  # 词到索引的映射字典\n",
    "token_id = 1  # token_id=0预留给填充符号\n",
    "lengths = []  # 每句话的长度(即单词个数)\n",
    "\n",
    "with open('test.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        tokens = l.strip().split()  # 简单的英文分词(按空格切分)\n",
    "        print(tokens)\n",
    "        lengths.append(len(tokens))\n",
    "        for t in tokens:\n",
    "            if t not in vocab:\n",
    "                vocab[t] = token_id\n",
    "                token_id += 1\n",
    "lengths = torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18,  6, 32, 20, 16])\n",
      "{'like': 1, 'to': 2, 'choose': 3, 'those': 4, 'who': 5, 'are': 6, 'different': 7, 'from': 8, 'themselves': 9, 'while': 10, 'others': 11, 'prefer': 12, 'similar': 13, 'themselves.': 14, 'People': 15, 'friends': 16, 'in': 17, 'differrent': 18, 'ways.': 19, 'For': 20, 'instance,': 21, 'if': 22, 'an': 23, 'active': 24, 'and': 25, 'energetic': 26, 'guy': 27, 'proposes': 28, 'his': 29, 'equally': 30, 'that': 31, 'they': 32, 'should': 33, 'have': 34, 'some': 35, 'activities,': 36, 'it': 37, 'is': 38, 'more': 39, 'likely': 40, 'will': 41, 'agree': 42, 'at': 43, 'once.': 44, 'When': 45, 'people': 46, 'themselves,': 47, 'their': 48, 'chat,': 49, 'play,': 50, 'do': 51, 'thing': 52, 'together': 53, 'natually': 54, 'harmoniously.': 55, 'The': 56, 'result': 57, 'all': 58, 'can': 59, 'feel': 60, 'relaxed': 61, 'trully': 62, 'enjoy': 63, 'each': 64, \"other's\": 65, 'company.': 66}\n"
     ]
    }
   ],
   "source": [
    "print(lengths)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,  4.,  5.,\n",
       "          6., 13.,  2., 14.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [15.,  3., 16., 17., 18., 19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [20., 21., 22., 23., 24., 25., 26., 27., 28.,  2., 29., 30., 24., 25.,\n",
       "         26., 16., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 31., 29.,\n",
       "         41., 42., 43., 44.],\n",
       "        [45., 46., 34., 16., 13.,  2., 47., 32., 25., 48., 16., 49., 50., 25.,\n",
       "         51., 52., 53., 54., 25., 55.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [56., 57., 38., 31., 32., 58., 59., 60., 61., 25., 59., 62., 63., 64.,\n",
       "         65., 66.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros((len(lengths), max(lengths)))  # 将所有句子填充为最大的长度\n",
    "\n",
    "l_no = 0\n",
    "with open('test', 'r') as f:\n",
    "    for l in f:\n",
    "        tokens = l.strip().split()\n",
    "        for i in range(len(tokens)):\n",
    "            x[l_no, i] = vocab[tokens[i]]\n",
    "        l_no += 1\n",
    "\n",
    "x = torch.tensor(x, requires_grad=True)\n",
    "x  # 每句话的数字形式(通过vocab字典)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32, 20, 18, 16,  6])\n",
      "idx_sort: tensor([2, 3, 0, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# 所有句子长度(lenghts)按从大到小排序(降序),并返回排序后的索引idx_sort\n",
    "lengths_sort, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
    "print(lengths_sort)\n",
    "print(\"idx_sort:\", idx_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "idx_unsort: tensor([2, 4, 0, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# 对索引idx_sort进行从小到大排序,返回排序后的索引idx_unsort\n",
    "idx_sort_sort, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "print(idx_sort_sort)\n",
    "print(\"idx_unsort:\", idx_unsort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenghts1: tensor([32, 20, 18, 16,  6])\n",
      "x1的形状与内容:\n",
      " tensor([[20., 21., 22., 23., 24., 25., 26., 27., 28.,  2., 29., 30., 24., 25.,\n",
      "         26., 16., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 31., 29.,\n",
      "         41., 42., 43., 44.],\n",
      "        [45., 46., 34., 16., 13.,  2., 47., 32., 25., 48., 16., 49., 50., 25.,\n",
      "         51., 52., 53., 54., 25., 55.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,  4.,  5.,\n",
      "          6., 13.,  2., 14.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [56., 57., 38., 31., 32., 58., 59., 60., 61., 25., 59., 62., 63., 64.,\n",
      "         65., 66.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [15.,  3., 16., 17., 18., 19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]], dtype=torch.float64, grad_fn=<IndexBackward0>)\n",
      "torch.Size([5, 32])\n"
     ]
    }
   ],
   "source": [
    "x1 = x[idx_sort]  # 将x根据句子长度进行降序排列\n",
    "lengths1 = lengths[idx_sort]  # 排序后每个句子的长度(降序)\n",
    "print(\"lenghts1:\", lengths1)\n",
    "print(\"x1的形状与内容:\\n\", x1)\n",
    "print(x1.shape)  # 5个句子(可以理解为批次);每个批次的长度为32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2的形状与内容:\n",
      " tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,  4.,  5.,\n",
      "          6., 13.,  2., 14.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [15.,  3., 16., 17., 18., 19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [20., 21., 22., 23., 24., 25., 26., 27., 28.,  2., 29., 30., 24., 25.,\n",
      "         26., 16., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 31., 29.,\n",
      "         41., 42., 43., 44.],\n",
      "        [45., 46., 34., 16., 13.,  2., 47., 32., 25., 48., 16., 49., 50., 25.,\n",
      "         51., 52., 53., 54., 25., 55.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [56., 57., 38., 31., 32., 58., 59., 60., 61., 25., 59., 62., 63., 64.,\n",
      "         65., 66.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]], dtype=torch.float64, grad_fn=<IndexBackward0>)\n",
      "torch.Size([5, 32])\n"
     ]
    }
   ],
   "source": [
    "# 通过idx_unsort,x得到得到恢复\n",
    "x2 = x1[idx_unsort]\n",
    "print(\"x2的形状与内容:\\n\", x2)\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([20., 45.,  1., 56., 15., 21., 46.,  2., 57.,  3., 22., 34.,  3., 38.,\n",
      "        16., 23., 16.,  4., 31., 17., 24., 13.,  5., 32., 18., 25.,  2.,  6.,\n",
      "        58., 19., 26., 47.,  7., 59., 27., 32.,  8., 60., 28., 25.,  9., 61.,\n",
      "         2., 48., 10., 25., 29., 16., 11., 59., 30., 49., 12., 62., 24., 50.,\n",
      "         4., 63., 25., 25.,  5., 64., 26., 51.,  6., 65., 16., 52., 13., 66.,\n",
      "        31., 53.,  2., 32., 54., 14., 33., 25., 34., 55., 35., 36., 37., 38.,\n",
      "        39., 40., 31., 29., 41., 42., 43., 44.], dtype=torch.float64,\n",
      "       grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 2, 2, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# x1已经根据长度进行降序排序\n",
    "# lengths1为x1的长度\n",
    "x1_packed = nn.utils.rnn.pack_padded_sequence(input=x1,\n",
    "                                              lengths=lengths1,  # 每个批次序列的长度\n",
    "                                              # if True, the input is expected in B x T x * format.\n",
    "                                              batch_first=True)  # 这输入x1的形状为(B x T),故设置batch_fist=True\n",
    "print(x1_packed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### x1_packed解析\n",
    "* 序列的第一个位置的非零元素:20, 45, 1, 56, 15. batch_size=5\n",
    "* 序列的第二个位置的非零元素:46, 2, 57, 3, 22. batch_size=5\n",
    "* 序列的第三个位置的非零元素:34, 3, 38, 16, 23. batch_size=5\n",
    "* 序列的第四个位置的非零元素:16, 4, 31, 17, 24. batch_size=5\n",
    "* ......\n",
    "\n",
    "已经按长度降序排列,故sorted_indices=None,unsorted_indices=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([20., 45.,  1., 56., 15., 21., 46.,  2., 57.,  3., 22., 34.,  3., 38.,\n",
      "        16., 23., 16.,  4., 31., 17., 24., 13.,  5., 32., 18., 25.,  2.,  6.,\n",
      "        58., 19., 26., 47.,  7., 59., 27., 32.,  8., 60., 28., 25.,  9., 61.,\n",
      "         2., 48., 10., 25., 29., 16., 11., 59., 30., 49., 12., 62., 24., 50.,\n",
      "         4., 63., 25., 25.,  5., 64., 26., 51.,  6., 65., 16., 52., 13., 66.,\n",
      "        31., 53.,  2., 32., 54., 14., 33., 25., 34., 55., 35., 36., 37., 38.,\n",
      "        39., 40., 31., 29., 41., 42., 43., 44.], dtype=torch.float64,\n",
      "       grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 2, 2, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=tensor([2, 3, 0, 4, 1]), unsorted_indices=tensor([2, 4, 0, 1, 3]))\n"
     ]
    }
   ],
   "source": [
    "# x1没有根据长度进行降序排序\n",
    "# lengths为x1的长度\n",
    "# ★★★★★All RNN modules accept packed sequences as inputs\n",
    "x_packed = nn.utils.rnn.pack_padded_sequence(input=x, lengths=lengths, batch_first=True,\n",
    "                                             #  if True, the input is expected to contain sequences sorted by length in a decreasing order.\n",
    "                                             #  If False, the input will get sorted unconditionally. Default: True.\n",
    "                                             enforce_sorted=False)\n",
    "print(x_packed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### x_packed解析\n",
    "1. 先对x进行降序排列,故\n",
    "    * 序列的第一个位置的非零元素:20, 45, 1, 56, 15. batch_size=5\n",
    "    * 序列的第二个位置的非零元素:46, 2, 57, 3, 22. batch_size=5\n",
    "    * 序列的第三个位置的非零元素:34, 3, 38, 16, 23. batch_size=5\n",
    "    * 序列的第四个位置的非零元素:16, 4, 31, 17, 24. batch_size=5\n",
    "    * ......\n",
    "2. sorted_indice为对x的长度排序后的索引\n",
    "3. unsortd_indice为对sort_indice排序后的索引.其中x[sorted_indice][unsored_indice] = x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
