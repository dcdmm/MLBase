{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../../../../../Other/img/LSTM示意图.png\">\n",
    "\n",
    "&emsp;&emsp;长短期记忆网络(Long Short-Term Memory Network,LSTM)是循环神经网络的一个变体,可以有效地解决简单循环神经网络的梯度爆炸或消失问题.由上图可知LSTM 网络主要改进在以下两个方面.\n",
    "\n",
    "#### 新的内部状态\n",
    "\n",
    "&emsp;&emsp;LSTM 网络引入一个新的内部状态(internal state)$\\mathbf{c}_t \\in \\mathbb{R}^D$ 专门进行线性的循环信息传递,同时(非线性地)输出信息给隐藏层的外部状态$\\mathbf{h}_t \\in \\mathbb{R}^D$.内部状态$ \\mathbf{c}_t$通过下面公式计算:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{c}_{t} &=\\mathbf{f}_{t} \\odot \\mathbf{c}_{t-1}+\\mathbf{i}_{t} \\odot \\tilde{\\mathbf{c}}_{t}  \\tag{1} \\\\\n",
    "\\mathbf{h}_{t} &=\\mathbf{o}_{t} \\odot \\tanh \\left(\\mathbf{c}_{t}\\right) \\tag{2}\n",
    "\\end{align}\n",
    "\n",
    "其中$\\mathbf{f}_{t} \\in [0, 1]^D,\\mathbf{i}_{t} \\in [0, 1]^D$和$\\mathbf{o}_{t} \\in [0, 1]^D$为三个门(gate)来控制信息传递的路径;$\\odot $为向量元素乘积;$\\mathbf{c}_{t-1}$为上一时刻的记忆单元;$\\tilde{\\mathbf{c}}_{t}$是通过非线性函数得到的候选状态:\n",
    "\n",
    "$$ \\tilde{\\mathbf{c}}_{t}=\\tanh \\left( [W_{ih}]_c \\mathbf{x}_{t} + [W_{hh}]_c \\mathbf{h}_{t-1}+\\mathbf{b}_{c} \\right) $$\n",
    "\n",
    "在每个时刻$t$,LSTM网络的内部状态$\\mathbf{c}_{t}$记录了到当前时刻为止的历史信息.\n",
    "\n",
    "#### 门控机制\n",
    "\n",
    "&emsp;&emsp;在数字电路中,门(gate)为一个二值变量$\\{0, 1\\}$,0 代表关闭状态,不许任何信息通过;1 代表开放状态,允许所有信息通过.\n",
    "\n",
    "&emsp;&emsp;LSTM 网络引入门控机制(Gating Mechanism)来控制信息传递的路径.公式(1) 和公式(2) 中三个\"\"门\"\"分别为输入门$\\mathbf{i}_{t}$,遗忘门$\\mathbf{f}_{t}$ 和输出门$\\mathbf{o}_{t}$.这三个门的作用为\n",
    "\n",
    "1. 遗忘门$\\mathbf{f}_{t}$控制上一个时刻的内部状态$\\mathbf{c}_{t-1}$需要遗忘多少信息\n",
    "2. 输入门$\\mathbf{i}_{t}$控制当前时刻的候选状态$\\tilde{\\mathbf{c}}_{t}$有多少信息需要保存\n",
    "3. 输出门$\\mathbf{o}_{t}$控制当前时刻的内部状态$\\mathbf{c}_{t}$有多少信息需要输出给外部状态$\\mathbf{h}_{t}$\n",
    "\n",
    "&emsp;&emsp;当$\\mathbf{f}_{t}=0,\\mathbf{i}_{t}=1$时,记忆单元将历史信息清空,并将候选状态向量$\\tilde{\\mathbf{c}}_{t}$写入.但此时记忆单元$\\mathbf{c}_{t}$依然和上一时刻的历史信息相关.当$\\mathbf{f}_{t}=1,\\mathbf{i}_{t}=0$时,记忆单元将复制上一时刻的内容,不写入新的信息.\n",
    "\n",
    "&emsp;&emsp;LSTM 网络中的“门”是一种“软”门,取值在$(0, 1)$ 之间,表示以一定的比例允许信息通过.三个门的计算方式为：\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{i}_{t} &=\\sigma\\left([W_{ih}]_i \\mathbf{x}_{t}+ [W_{hh}]_i\\mathbf{h}_{t-1}+\\mathbf{b}_{i}\\right) \\tag{3} \\\\\n",
    "\\mathbf{f}_{t} &=\\sigma\\left([W_{ih}]_f \\boldsymbol{x}_{t}+[W_{hh}]_f \\mathbf{h}_{t-1}+\\mathbf{b}_{f}\\right)\\tag{4}  \\\\\n",
    "\\mathbf{o}_{t} &=\\sigma\\left( [W_{ih}]_o \\boldsymbol{x}_{t}+[W_{hh}]_{o} \\mathbf{h}_{t-1}+\\mathbf{b}_{o}\\right) \\tag{5} \n",
    "\\end{align}\n",
    "\n",
    "其中$\\sigma(*)$为Logistic 函数,其输出区间为$(0, 1)$,$\\mathbf{x}_{t}$为当前时刻的输入,$\\mathbf{h}_{t-1}$为上一时刻的外部状态.\n",
    "\n",
    "&emsp;&emsp;故LSTM的计算步骤为:\n",
    "\n",
    "1. 首先利用上一时刻的外部状态$\\mathbf{h}_{t-1}$和当前时刻的输入$\\mathbf{x}_{t}$，计算出三个门$\\mathbf{i}_{t},\\mathbf{g}_{t},\\mathbf{o}_{t}$以及候选状态$\\tilde{\\mathbf{c}}_{t} $\n",
    "2. 候选状态$\\tilde{\\mathbf{c}}_{t} $结合遗忘门$\\mathbf{f}_{t}$和输入门$\\mathbf{f}_{t}$来更新记忆单元$\\mathbf{c}_{t}$\n",
    "3. 结合输出门$\\mathbf{o}_{t}$,将内部状态的信息传递给外部状态$\\mathbf{h}_{t}$\n",
    "\n",
    "&emsp;&emsp;通过LSTM 循环单元,整个网络可以建立较长距离的时序依赖关系.公式(1)～公式(5) 可以简洁地描述为\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "\\tilde{\\mathbf{c}}_{t} \\\\\n",
    "\\mathbf{o}_{t}    \\\\\n",
    "\\mathbf{i}_{t}\t \\\\\n",
    "\\mathbf{f}_{t}\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "\\tanh \\\\\n",
    "\\sigma    \\\\\n",
    "\\sigma\t \\\\\n",
    "\\sigma\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left( W\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "\\mathbf{x}_t \\\\\n",
    "\\mathbf{h}_{t-1}    \\\\\n",
    "\\end{array}\n",
    "\\right] + \\mathbf{b}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{c}_{t} &=\\mathbf{f}_{t} \\odot \\mathbf{c}_{t-1}+\\mathbf{i}_{t} \\odot \\tilde{\\mathbf{c}}_{t} \\\\\n",
    "\\mathbf{h}_{t} &=\\mathbf{o}_{t} \\odot \\tanh \\left(\\mathbf{c}_{t}\\right)\n",
    "\\end{align}\n",
    "\n",
    "其中$\\mathbf{x}_t \\in \\mathbb{R}^M$为当前时刻的输入,$W = \\left[ W_{ih}, W_{hh} \\right] \\in \\mathbb{R}^{4D \\times (D+M)}$和$\\mathbf{b} \\in \\mathbb{R}^{4D}$为网络偏置参数.$\\tanh, \\sigma,\\sigma,\\sigma $\n",
    "\n",
    "分别作用与列向量$4D/4=D$个值,即$[W_{ih}]_i,[W_{ih}]_f,[W_{ih}]_o \\in \\mathbb{R}^{D \\times M}; [W_{hh}]_i,[W_{hh}]_f,[W_{hh}]_o \\in \\mathbb{R}^{D \\times D}$\n",
    "\n",
    "&emsp;&emsp;循环神经网络中的隐状态$\\mathbf{h}$存储了历史信息,可以看作一种记忆(Memory).在简单循环网络中,隐状态每个时刻都会被重写,因此可以看作一种短期记忆(Short-Term Memory).在神经网络中,长期记忆(Long-Term Memory)可以看作网络参数,隐含了从训练数据中学到的经验,其更新周期要远远慢于短期记忆.而在LSTM 网络中，记忆单元$\\mathbf{c}$可以在某个时刻捕捉到某个关键信息,并有能力将此关键信息保存一定的时间间隔.记忆单元$\\mathbf{c}$中保存信息的生命周期要长于短期记忆$\\mathbf{h}$,但又远远短于长期记忆.因此称为长短期记忆(Long Short-Term Memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
