{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch.utils.data as Data\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = load_dataset(\"dataset\")\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt = \"bert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at dataset/train/cache-9aa6937aa7a0509c.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c3eadf2fff4329b888ee39942f74a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at dataset/test/cache-b0d3f5da2aa8927a.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "\n",
    "# 批次处理,整个数据集同时进行处理\n",
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
    "emotions_encoded  # 原有数据与map函数新增数据('input_ids', 'token_type_ids', 'attention_mask')的联合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([0, 0, 3,  ..., 1, 3, 0]),\n",
       " 'input_ids': tensor([[  101,  1045,  2134,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2064,  ...,     0,     0,     0],\n",
       "         [  101, 10047,  9775,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1045,  2514,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2514,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2113,  ...,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded = emotions_encoded.remove_columns(['text'])  # 'text'列不参与训练(即不进入自定义模型forward函数)\n",
    "emotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
    "emotions_encoded['train'][:]  # 字典形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class Customize_Model(torch.nn.Module):\n",
    "    \"\"\"下游训练任务模型\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.classifier = torch.nn.Linear(768, num_labels)  # 多分类任务\n",
    "        self.pretrained = pretrained_model\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.loss_fct = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,  # ★★★★★训练阶段对应emotions_encoded['train']中的input_ids\n",
    "                attention_mask,  # ★★★★★训练阶段对应emotions_encoded['train']中的attention_mask\n",
    "                token_type_ids,  # ★★★★★训练阶段对应emotions_encoded['train']中的token_type_ids\n",
    "                labels=None):  # 标签;★★★★★训练阶段对应emotions_encoded['train']中的labels\n",
    "        outputs = self.pretrained(input_ids=input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:  # 若包含标签\n",
    "            loss = self.loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "\n",
    "        # ★★★★★\n",
    "        # 返回值为一个元组\n",
    "        # 元组的第一个元素必须为该批次训练数据的损失值\n",
    "        # 元素的第二个元素用于评估函数计算验证数据集(若有)的输出\n",
    "        return (loss, logits)\n",
    "\n",
    "\n",
    "num_labels = 6\n",
    "model_from_pretrained = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "model = Customize_Model(model_from_pretrained, num_labels)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"验证数据集评估函数\"\"\"\n",
    "    labels = pred.label_ids  # 对应自定义模型forward函数输入:labels\n",
    "    preds = pred.predictions  # 对应自定义模型forward函数返回值的第二个元素\n",
    "    preds_argmax = preds.argmax(-1)\n",
    "    f1 = f1_score(labels, preds_argmax, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds_argmax)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}  # return a dictionary string to metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:57, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.944800</td>\n",
       "      <td>0.323132</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>0.898926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.253600</td>\n",
       "      <td>0.214852</td>\n",
       "      <td>0.920500</td>\n",
       "      <td>0.920510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-emotion/checkpoint-250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-emotion/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-emotion/checkpoint-250/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-emotion/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-emotion/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-emotion/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.5991856842041016, metrics={'train_runtime': 118.4985, 'train_samples_per_second': 270.046, 'train_steps_per_second': 4.219, 'total_flos': 0.0, 'train_loss': 0.5991856842041016, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 主要调节的超参数\n",
    "batch_size = 64\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "training_args = TrainingArguments(\n",
    "    # The output directory where the model predictions and checkpoints will be written.\n",
    "    output_dir=model_name,\n",
    "    seed=42,\n",
    "\n",
    "    # Total number of training epochs to perform\n",
    "    num_train_epochs=2,  # 默认:3.0\n",
    "    # If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs. I\n",
    "    # max_steps=2000,  # 默认:-1\n",
    "\n",
    "    #  Maximum gradient norm (for gradient clipping).\n",
    "    max_grad_norm=1.0,  # 默认:1.0\n",
    "\n",
    "    # The batch size per GPU/TPU core/CPU for training.\n",
    "    per_device_train_batch_size=batch_size,  # 默认:8\n",
    "    # The batch size per GPU/TPU core/CPU for evaluation.\n",
    "    per_device_eval_batch_size=batch_size,  # 默认:8\n",
    "\n",
    "    # HuggingFace AdamW优化器超参数\n",
    "    # The initial learning rate for AdamW optimizer.\n",
    "    learning_rate=2e-5,  # 默认: 5e-5\n",
    "    # The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer.\n",
    "    weight_decay=0.01,  # 默认:0\n",
    "    # The beta1 hyperparameter for the AdamW optimizer.\n",
    "    adam_beta1=0.9,  # 默认:0.9\n",
    "    # The beta2 hyperparameter for the AdamW optimizer.\n",
    "    adam_beta2=0.999,  # 默认:0.999\n",
    "    # The epsilon hyperparameter for the AdamW optimizer.\n",
    "    adam_epsilon=1e-8,  # 默认:1e-8\n",
    "\n",
    "    # HuggingFace 不同学习率预热超参数\n",
    "    #  The scheduler type to use. See the documentation of SchedulerType for all possible values.\n",
    "    lr_scheduler_type='linear',  # 默认:'linear'\n",
    "    # Ratio of total training steps used for a linear warmup from 0 to learning_rate.\n",
    "    warmup_ratio=0.0,  # 默认:0.0\n",
    "    # Number of steps used for a linear warmup from 0 to learning_rate. Overrides any effect of warmup_ratio.\n",
    "    warmup_steps=0,  # 默认0\n",
    "\n",
    "    # The evaluation strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No evaluation is done during training.\n",
    "    # \"steps\": Evaluation is done (and logged) every eval_steps.\n",
    "    # \"epoch\": Evaluation is done at the end of each epoch.\n",
    "    evaluation_strategy=\"epoch\",  # 默认:'no'\n",
    "    # The logging strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No logging is done during training.\n",
    "    # \"epoch\": Logging is done at the end of each epoch.\n",
    "    # \"steps\": Logging is done every logging_steps.\n",
    "    logging_strategy='epoch',  # 默认:'steps'\n",
    "    # Number of update steps between two logs if logging_strategy=\"steps\".\n",
    "    # logging_steps=500,  # 默认:500\n",
    "    # The checkpoint save strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No save is done during training.\n",
    "    # \"epoch\": Save is done at the end of each epoch.\n",
    "    # \"steps\": Save is done every save_steps.\n",
    "    # Logger log level to use on the main process. Possible choices are the log levels as strings: ‘debug’, ‘info’, ‘warning’, ‘error’ and ‘critical’, plus a ‘passive’ level which doesn’t set anything and lets the application set the level.\n",
    "    log_level='passive',  # 默认'passive'\n",
    "    save_strategy='epoch',  # 默认:'steps'\n",
    "    # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
    "    # save_steps=500,  # 默认:500\n",
    "    disable_tqdm=False,  # 使用tqdm显示进度\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=emotions_encoded[\"train\"],  # 类型:datasets.arrow_dataset.Dataset\n",
    "    eval_dataset=emotions_encoded[\"validation\"],  # 类型:datasets.arrow_dataset.Dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer)\n",
    "trainer.train()  # 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 4.7823853 , -0.47211242, -0.8000455 , -0.8192299 , -1.4066333 ,\n",
       "        -1.8550745 ],\n",
       "       [ 4.7416277 , -0.36532468, -1.547922  , -0.49605307, -0.76526946,\n",
       "        -1.6814713 ],\n",
       "       [-1.4562929 ,  1.9604354 ,  3.1431172 , -1.2758981 , -1.7712132 ,\n",
       "        -1.4010117 ],\n",
       "       ...,\n",
       "       [-1.3463206 ,  4.849283  ,  0.2021552 , -2.022539  , -2.0852394 ,\n",
       "        -0.6736212 ],\n",
       "       [-2.4475095 ,  3.0625157 ,  2.5931797 , -1.4979831 , -1.6729348 ,\n",
       "        -1.100731  ],\n",
       "       [-1.7385969 ,  4.770994  ,  0.20426473, -2.086232  , -1.8465284 ,\n",
       "        -0.14392798]], dtype=float32), label_ids=array([0, 0, 2, ..., 1, 1, 1]), metrics={'test_loss': 0.2148517519235611, 'test_accuracy': 0.9205, 'test_f1': 0.9205104721800366, 'test_runtime': 1.9886, 'test_samples_per_second': 1005.755, 'test_steps_per_second': 16.092})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run prediction and returns predictions and potential metrics.\n",
    "preds_output = trainer.predict(emotions_encoded[\"validation\"])  # 类型:datasets.arrow_dataset.Dataset;包含标签\n",
    "preds_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.7823853  -0.47211242 -0.8000455  -0.8192299  -1.4066333  -1.8550745 ]\n",
      " [ 4.7416277  -0.36532468 -1.547922   -0.49605307 -0.76526946 -1.6814713 ]\n",
      " [-1.4562929   1.9604354   3.1431172  -1.2758981  -1.7712132  -1.4010117 ]\n",
      " ...\n",
      " [-1.3463206   4.849283    0.2021552  -2.022539   -2.0852394  -0.6736212 ]\n",
      " [-2.4475095   3.0625157   2.5931797  -1.4979831  -1.6729348  -1.100731  ]\n",
      " [-1.7385969   4.770994    0.20426473 -2.086232   -1.8465284  -0.14392798]]\n",
      "<class 'numpy.ndarray'>\n",
      "(2000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(preds_output.predictions)  # 预测结果\n",
    "print(type(preds_output.predictions))\n",
    "print(preds_output.predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.2148517519235611,\n",
       " 'test_accuracy': 0.9205,\n",
       " 'test_f1': 0.9205104721800366,\n",
       " 'test_runtime': 1.9886,\n",
       " 'test_samples_per_second': 1005.755,\n",
       " 'test_steps_per_second': 16.092}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = emotions_encoded[\"test\"].remove_columns(['label'])\n",
    "test_dataset  # 测试数据集不含标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7f95a6bfcbe0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_torch_dataset = Data.TensorDataset(test_dataset['input_ids'], test_dataset['token_type_ids'],\n",
    "                                        test_dataset['attention_mask'])\n",
    "test_torch_dataset  # 数据加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 10047,  3110,  2738, 11083,  2061, 10047,  2025,  2200, 12479,\n",
      "          2157,  2085,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101, 10047,  2039, 16616,  2026,  9927,  2138,  1045,  2514, 28543,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "dataLoader_test = Data.DataLoader(dataset=test_torch_dataset, batch_size=2)  # 数据处理\n",
    "for i in dataLoader_test:\n",
    "    print(i[0])\n",
    "    print(i[1])\n",
    "    print(i[2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.4552, -0.3450, -1.0418,  0.3081, -0.6800, -1.3975],\n",
      "        [ 3.6766, -0.7781, -0.8919,  0.0936, -0.5768, -1.5109],\n",
      "        [ 2.9260, -0.9456, -0.9254,  0.4611,  0.3105, -1.0101],\n",
      "        ...,\n",
      "        [-0.3021,  1.3146,  0.0343,  0.0378,  0.2261, -0.0418],\n",
      "        [-0.7132,  2.3728, -0.1887, -0.7444, -0.0694, -0.1836],\n",
      "        [-1.0886, -1.4676, -0.5053, -1.1809,  2.9800,  2.1502]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"预测不含标签的测试数据集\"\"\"\n",
    "    model.eval()  # Sets the module in evaluation mode.\n",
    "    predict_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in data_loader:\n",
    "            input_ids = i[0].to(device)\n",
    "            attention_mask = i[1].to(device)\n",
    "            token_type_ids = i[2].to(device)\n",
    "            output = model(input_ids, attention_mask, token_type_ids)[1]\n",
    "            predict_list.append(output)\n",
    "    predict_all = torch.cat(predict_list, dim=0)\n",
    "    return predict_all\n",
    "\n",
    "\n",
    "result = predict(model, dataLoader_test)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}