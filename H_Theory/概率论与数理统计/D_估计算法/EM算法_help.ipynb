{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### EM算法的导出           \n",
    "&emsp;&emsp;面对一个含有隐变量的概率模型,目标是极大化观测数据$Y$关于参数$\\theta$的对数似然函数,即极大化           \n",
    "\n",
    "\\begin{align}\n",
    "L(\\theta) &= \\log P(Y|\\theta) = \\log \\sum_{Z} P(Y,Z|\\theta)   \\\\\n",
    "\t\t  &= \\log \\left(   \\sum_{Z} \\frac{P(Y, Z, \\theta)}{P(\\theta)}   \\right) \\\\\n",
    "\t\t  &= \\log \\left(   \\sum_{Z} \\frac{P(Y| Z, \\theta) P(Z, \\theta) }{P(\\theta)}   \\right) \\\\\n",
    "\t\t  &= \\log \\left( \\sum_{Z}  P(Y|Z, \\theta) P(Z|\\theta) \\right) \\tag{1}\n",
    "\\end{align}     \n",
    "\n",
    "注意到这一极大化的主要困难是式(1)中有未观测数据并有包含和(或积分)的对数.$Y$和$Z$连在一起称为完全数据,观测值$Y$又称为不完全数据.          \n",
    "&emsp;&emsp;事实上,EM算法是通过迭代逐步近似极大化$L(\\theta)$的.假设在第$i$此迭代后$\\theta$的估计值是$\\theta^{(i)}$.\n",
    "我们希望新估计值$\\theta$能使$L(\\theta)$增加,即$ L(\\theta) > L(\\theta^{(i)}) $,并逐步达到极大值.为此,考虑二者的差:              \n",
    "$$ L(\\theta) - L(\\theta^{(i)})  = \\log \\left( \\sum_{Z}  P(Y|Z, \\theta) P(Z|\\theta) \\right) - \\log P(Y|\\theta^{(i)}) $$        \n",
    "利用Jensen不等式得到其下界      \n",
    "\n",
    "\\begin{align}\n",
    "L(\\theta) - L(\\theta^{(i)})  &= \\log \\left( \\sum_{Z} P(Z|Y,\\theta^{(i)}) \\frac{P(Y|Z, \\theta) P(Z|\\theta)}{ P(Z|Y,\\theta^{(i)}) } \\right) - \\log P(Y|\\theta^{(i)}) \\\\\n",
    "\t\t\t\t\t\t     &\\geq \\sum_{Z} P(Z|Y,\\theta^{(i)}) \\log \\frac{P(Y|Z, \\theta) P(Z|\\theta)}{ P(Z|Y,\\theta^{(i)}) } - \\log P(Y|\\theta^{(i)}) \\\\\n",
    "\t\t\t\t\t\t     &= \\sum_{Z} P(Z|Y,\\theta^{(i)}) \\log \\frac{P(Y|Z, \\theta) P(Z|\\theta)}{ P(Z|Y,\\theta^{(i)}) P(Y|\\theta^{(i)})}\n",
    "\\end{align}        \n",
    "\n",
    "令    \n",
    "$$ B(\\theta, \\theta^{(i)}) = L(\\theta^{(i)}) + \\sum_{Z} P(Z|Y,\\theta^{(i)}) \\log \\frac{P(Y|Z, \\theta) P(Z|\\theta)}{ P(Z|Y,\\theta^{(i)}) P(Y|\\theta^{(i)})} \\tag{2} $$       \n",
    "则          \n",
    "$$ L(\\theta) \\geq B(\\theta, \\theta^{(i)}) \\tag{3} $$        \n",
    "即函数$ B(\\theta, \\theta^{(i)}) $是$L(\\theta)$的一个下界,且由式(2)可得,   \n",
    "$$ L(\\theta^{(i)}) = B(\\theta^{(i)}, \\theta^{(i)}) \\tag{4} $$            \n",
    "因此,任何可以使$ B(\\theta, \\theta^{(i)})$增大的$\\theta$,也可以使$L(\\theta)$增大.为了使$L(\\theta)$有尽可能大的增长,\n",
    "选择$\\theta^{(i+1)}$使$B(\\theta, \\theta^{(i)})$达到极大,即      \n",
    "$$ \\theta^{(i+1)} = \\mathrm{arg} \\max_{\\theta} B(\\theta, \\theta^{(i)}) \\tag{5} $$    \n",
    "现在求$\\theta^{(i+1)}$的表达式.省去对$\\theta$的极大化而言是常数的项目,由式(5),式(2)可得    \n",
    "\n",
    "\\begin{align}\n",
    "\\theta^{(i+1)} &= \\mathrm{arg} \\max_{\\theta} \\left(  L(\\theta^{(i)}) + \\sum_{Z} P(Z|Y,\\theta^{(i)}) \\log \\frac{P(Y|Z, \\theta) P(Z|\\theta)}{ P(Z|Y,\\theta^{(i)}) P(Y|\\theta^{(i)})}  \\right) \\\\\n",
    "\t\t\t   &= \\mathrm{arg} \\max_{\\theta} \\left( \\sum_{Z} P(Z|Y,\\theta^{(i)}) \\log P(Y|Z, \\theta) P(Z|\\theta)\\right) \\\\\n",
    "\t\t\t   &= \\mathrm{arg} \\max_{\\theta} \\left( \\sum_{Z} P(Z|Y,\\theta^{(i)}) \\log P(Y|Z, \\theta) P(Z|\\theta)\\right) \\\\\n",
    "\t\t\t   &= \\mathrm{arg} \\max_{\\theta} \\left( \\sum_{Z} P(Z|Y,\\theta^{(i)}) \\log P(Y, Z| \\theta) \\right) \\\\\n",
    "\t\t\t   &= \\mathrm{arg} \\max_{\\theta} Q(\\theta, \\theta^{(i)}) \\tag{6}\n",
    "\\end{align}       \n",
    "\n",
    "其中,$ Q(\\theta, \\theta^{(i)}) = \\sum_{Z} P(Z|Y,\\theta^{(i)}) \\log P(Y, Z| \\theta) $         \n",
    "&emsp;&emsp;式(6)等价于一次EM算法的一次迭代,即求$ Q $函数及其极大化.EM算法是通过不断求解下界的极大化来逼近求解对数似然函数极大化的算法.      \n",
    "&emsp;&emsp;下图给出EM算法的直观解释.图中上方曲线为$L(\\theta)$,下方曲线为$ B(\\theta, \\theta^{(i)}) $ .由式(3),\n",
    "$ B(\\theta, \\theta^{(i)}) $为对数似然函数$L(\\theta)$的下界.由式(4),两个函数在点$\\theta = \\theta^{(i)}$ 处相等.\n",
    "由式(5)和式(6),EM算法找到下一个点$\\theta^{(i+1)}$使函数$B(\\theta, \\theta^{(i)}) $ 极大化,也使函数$ Q(\\theta, \\theta^{(i)}) $极大化.\n",
    "这时由于$ L(\\theta) \\geq B(\\theta, \\theta^{(i)}) $,函数$B(\\theta, \\theta^{(i)}) $的增加,\n",
    "保证对数似然函数$L(\\theta)$在每次迭代中也是增加的.EM算法在点$\\theta^{(i+1)}$重新计算$Q$函数的数值,进行下一次迭代.在这个过程中,\n",
    "对数似然函数$L(\\theta)$不断增大.从图可以推出EM算法不能保证找到全局最优值.   \n",
    "\n",
    "\n",
    "<img src='../../../Other/img/EM.png' style=\"width:650px;height:450px\">\n",
    "\n",
    "\n",
    "\n",
    "<font color='red' size=4>EM算法</font>            \n",
    "输入:观测变量数据$Y$,隐变量数据$Z$,联合分布$P(Y,Z|\\theta)$,条件分布$P(Z|Y,\\theta)$;     \n",
    "输出:模型参数$\\theta$;               \n",
    "\n",
    "1. 选择参数的初值$\\theta^{(0)}$,开始迭代      \n",
    "       \n",
    "2. E步:记$ \\theta^{(i)} $为第$i$次迭代参数$\\theta$的估计,在第$i+1$次迭代的E步,计算对数似然$\\log P(Y,Z|\\theta)$关于$Z$的期望     \n",
    "\n",
    "\\begin{align}\n",
    "Q(\\theta, \\theta^{(i)}) &= E_{Z|Y,\\theta^{(i)}} [ \\log P(Y, Z|\\theta] \\\\\n",
    "                    &= \\sum_{Z} P(Z|Y,\\theta^{(i)}) \\log P(Y, Z| \\theta) \\qquad \\text{注:省去了常数因子}\n",
    "\\end{align}       \n",
    " \n",
    "这里,$ P(Z|Y,\\theta^{(i)}) $是给定(所有)观测数据$ Y $和当前参数估计$ \\theta^{(i)} $下隐变量数据$ Z $的条件概率分布.        \n",
    "\n",
    "3. M步:求使$ Q(\\theta, \\theta^{(i)}) $极大化的$\\theta$,确定第$i+1$次迭代的参数的估计值$ \\theta^{(i+1)} $       \n",
    "$$ \\theta^{(i+1)} = \\mathrm{arg} \\max_{\\theta} Q(\\theta, \\theta^{(i)}) $$\n",
    "   \n",
    "4. 重复第(2)步和第(3)步,直到收敛          \n",
    "\n",
    "\n",
    "\n",
    "算法说明:     \n",
    "步骤1: 参数的初始可以任意选择,但需注意EM算法对初值是敏感的          \n",
    "步骤2: E步求$ Q(\\theta, \\theta^{(i)}) $.$Q$函数式中$Z$是未观测数据,$Y$是观测数据.注意,\n",
    "$Q(\\theta, \\theta^{(i)})$的第1个变元表示参数的当前估计值.第2个变元表示要极大化的参数        \n",
    "步骤3: M步求$ Q(\\theta, \\theta^{(i)}) $的极大化,得到$ \\theta^{(i+1)} $,完成一次迭代$ \\theta^{(i)} \\rightarrow \\theta^{(i+1)} $.每次迭代使似然函数增大或达到局部极值   \n",
    "步骤4: 给定停止迭代条件,一般是对较小的正数$ \\epsilon_1, \\epsilon_2 $,若满足      \n",
    "$$ ||\\theta^{(i+1)} -  \\theta^{(i)}|| < \\epsilon_1, \\text{或} ||Q(\\theta^{(i+1)}, \\theta^{(i)}) -Q(\\theta^{(i)}, \\theta^{(i)})   || < \\epsilon_2 $$          \n",
    "则停止迭代.           \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}