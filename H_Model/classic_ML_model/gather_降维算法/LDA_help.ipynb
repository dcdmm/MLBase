{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;LDA(线性判别分析)的思想非常朴素:给定训练样例集,设法将样例投影到一条直线上,使得同类样例的投影尽可能接近,异类样例的投影点尽可能远离;\n",
    "在对新样本进行分类时,将其投影到同样的这条直线上,再根据投影点的位置来确定新样本的类别.下图给出了一个二维示意图.\n",
    "\n",
    "<img src=\"../../Other/img/LDA.png\" style=\"width:650px;height:400px\">\n",
    "\n",
    "&emsp;&emsp;给定数据集$D = {(\\mathbf{x}_i, y_i)},y_i \\in \\{0, 1\\}$,令$X_i,\\boldsymbol{\\mu}_i, \\Sigma_i$分别表示第$i\\in \\{0,1\\}$类示例的集合,均值向量,协方差矩阵.若将数据投影到直线$\\mathbf{w}$上,则两类样本的中心在直线上的投影分别为$\\mathbf{w}^T \\boldsymbol{\\mu}_0$和$\\mathbf{w}^T \\boldsymbol{\\mu}_1$;\n",
    "若将所有样本点都投影到直线,上,则两类样本的协方差分别为$\\mathbf{w}^T \\Sigma_0 \\mathbf{w}$和$\\mathbf{w}^T \\Sigma_1 \\mathbf{w}$.\n",
    "由于直线是一维空间,因此$\\mathbf{w}^T \\boldsymbol{\\mu}_0, \\mathbf{w}^T \\boldsymbol{\\mu}_1,\\mathbf{w}^T \\Sigma_0 \\mathbf{w}$和$\\mathbf{w}^T \\Sigma_1 \\mathbf{w}$均为实数(标量).\n",
    "\n",
    "&emsp;&emsp;欲使同类样例的投影点尽可能接近,可以让同类样例投影点的协方差尽可能小,即$\\mathbf{w}^T \\Sigma_0 \\mathbf{w} + \\mathbf{w}^T \\Sigma_1 \\mathbf{w}$\n",
    "\n",
    "尽可能小;而欲使异类样例的投影点尽可能远离,可以让类中心之间的距离尽可能大打,即$||\\mathbf{w}^T \\boldsymbol{\\mu}_0 - \\mathbf{w}^T \\boldsymbol{\\mu}_1||$尽可能大.同时考虑二者,则可得到欲最大化的目标\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J &= \\frac{||\\mathbf{w}^T \\boldsymbol{\\mu}_0 - \\mathbf{w}^T \\boldsymbol{\\mu}_1||_2^2}{\\mathbf{w}^T \\Sigma_0 \\mathbf{w} + \\mathbf{w}^T \\Sigma_1 \\mathbf{w}} \\\\\n",
    "  &=\\frac{\\mathbf{w}^T(\\boldsymbol{\\mu}_0 - \\boldsymbol{\\mu}_1)(\\mu_0 - \\boldsymbol{\\mu}_1)^T \\mathbf{w}}{ \\mathbf{w}^T(\\Sigma_0 + \\Sigma_1) \\mathbf{w}}\n",
    "\\end{aligned}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;定义\"类间散度矩阵(within-class scatter matrix)\n",
    "\n",
    "\\begin{align}\n",
    "S_{\\mathbf{w}} &= \\Sigma_0 + \\sigma_1 \\\\\n",
    "               &= \\sum_{\\mathbf{x} \\in X_0} (\\mathbf{x} - \\boldsymbol{\\mu}_0) (\\mathbf{x} - \\boldsymbol{\\mu}_0)^T + \\sum_{\\mathbf{x} \\in X_1} (\\mathbf{x} - \\boldsymbol{\\mu}_1) (\\mathbf{x} - \\boldsymbol{\\mu}_1)^T\n",
    "\\end{align}\n",
    "\n",
    "以及\"类间散度矩阵\"(between-class scatter matrix)\n",
    "\n",
    "$$ S_b = (\\boldsymbol{\\mu}_0 - \\boldsymbol{\\mu}_1)(\\mu_0 - \\boldsymbol{\\mu}_1)^T $$\n",
    "\n",
    "则式(1)可重写为\n",
    "\n",
    "$$ J = \\frac{\\mathbf{w}^T S_b \\mathbf{w}}{ \\mathbf{w}^T S_{\\mathbf{w}} \\mathbf{w}} \\tag{2} $$\n",
    "\n",
    "这就是LDA欲最大化的目标,即$S_b$与$S_{\\mathbf{w}}$的\"广义瑞利商\"(generalized Rayleigh quotient).\n",
    "\n",
    "&emsp;&emsp;注意到式(2)的解与$\\mathbf{w}$的长度无关,只与其方向有关.不失一般性,令$ \\mathbf{w}^T S_{\\mathbf{w}} \\mathbf{w}=1$,则式(2)等价于\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min_{\\mathbf{w}} \\quad -\\mathbf{w}^T S_b \\mathbf{w} \\\\\n",
    "&s.t \\quad  \\mathbf{w}^T S_{\\mathbf{w}} \\mathbf{w}=1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由拉格朗日乘子法,上式等价于($S_b,S_{\\mathbf{w}}$均为对称矩阵)\n",
    "\n",
    "$$ S_b \\mathbf{w} = \\lambda S_{\\mathbf{w}} \\mathbf{w} \\tag{3} $$\n",
    "\n",
    "其中$\\lambda$是拉格朗日乘子.注意到$S_b \\mathbf{w}$的方向恒为$\\boldsymbol{\\mu}_0 - \\boldsymbol{\\mu}_1$(因为$(\\boldsymbol{\\mu}_0 - \\boldsymbol{\\mu}_1)^T$是标量),不妨令\n",
    "\n",
    "$$ S_b \\mathbf{w} = \\lambda(\\mu_0 - \\boldsymbol{\\mu}_1) $$\n",
    "\n",
    "代入式(3)即得\n",
    "\n",
    "$$ \\mathbf{w} = S_{\\mathbf{w}}^{-1} (\\mu_0 - \\boldsymbol{\\mu}_1) $$\n",
    "\n",
    "考虑到数值解的稳定性,在实践中通常是对$S_\\mathbf{w}$进行奇异值分解,即$S_\\mathbf{w}=U \\Sigma V^{T}$ ,这里$\\Sigma$是一个实对称矩阵,\n",
    "其对角线上的元素是 $S_\\mathbf{w}$的奇异值,然后再由$S_\\mathbf{w}^{-1} = V\\Sigma^{-1}U^T$得到$S_\\mathbf{w}^{-1}$.\n",
    "\n",
    "&emsp;&emsp;可以将LDA推广到多分类问题中,假定存在$N$个类,且第$i$类示例数为$m_i$,先定义\"全局散度矩阵\"\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "S_t &= S_b +  S_{\\mathbf{w}}  \\\\\n",
    "\t&= \\sum_{i=1}^m (\\mathbf{x}_i - \\boldsymbol{\\mu})(\\mathbf{x}_i - \\boldsymbol{\\mu})^T \n",
    "\\end{aligned}\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{\\mu}$是所有示例的均值向量,$m = \\sum_i m_i$,将类内散度矩阵$S_{\\mathbf{w}}$重定义为每个类别的散度矩阵之和,即\n",
    "\n",
    "$$ S_{\\mathbf{w}} = \\sum_{i=1}^N S_{\\mathbf{w}_i} \\tag{5} $$\n",
    "\n",
    "其中\n",
    "\n",
    "$$ S_{\\mathbf{w}_i} =\\sum_{\\mathbf{x} \\in X_i} (\\mathbf{x} - \\boldsymbol{\\mu}_i) (\\mathbf{x} - \\boldsymbol{\\mu}_i)^T \\tag{6} $$\n",
    "\n",
    "由式(4)(5)(6)可得\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "S_b &= S_t - S_{\\mathbf{w}} \\\\\n",
    "\t&= \\sum_{i=1}^N m_i(\\boldsymbol{\\mu}_i -\\boldsymbol{\\mu}) (\\boldsymbol{\\mu}_i -\\boldsymbol{\\mu})^T  \n",
    "\\end{aligned}\n",
    "\\tag{7}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;显然,多分类LDA可以有多种实现方法:使用$S_b,S_{\\mathbf{w}},S_t$三者中的任何两个即可,常见的一种实现是采用优化目标\n",
    "\n",
    "$$ \\max_{W} \\frac{\\mathrm{tr} (W^T S_b W)}{\\mathrm{tr} (W^T S_{\\mathbf{w}} W)} \\tag{8} $$\n",
    "\n",
    "&emsp;&emsp;若将$W$视为一个投影矩阵,则多分类LDA将样本投影到$d'$维空间.由式(7),$S_b$由$N$个矩阵的和组成,\n",
    "这些矩阵中只有$N-1$个矩阵是相互独立的,故$S_b$的秩最大等于$N-1$,因此最多有$N-1$个非零特征值,\n",
    "即有$d'\\leq N-1$($d'$通常远小于数据原有的属性数$d$).于是,可通过这个投影来减少样本点的维数,且投影过程中使用了类别信息,\n",
    "因此LDA也常被视为一种经典的监督降维技术.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
