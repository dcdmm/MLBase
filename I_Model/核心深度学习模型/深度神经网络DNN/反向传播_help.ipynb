{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../../../Other/img/bp_mul.png\" style=\"width:700px;height:500px\">\n",
    "\n",
    "符号说明:\n",
    "\n",
    "| 记号                                              | 含义                                                         |\n",
    "| :----- | :----- |\n",
    "| $ \\mathbf{a}^{\\mathrm{input}}$  <div style=\"width: 150px;height=100px\">                  | 输入层数据                                                   |\n",
    "| $L$   <div style=\"width: 150px;height=100px\">                                            | 神经网络层数                                                 |\n",
    "| $N_l$     <div style=\"width: 150px;height=100px\">                                        | 第$l$层神经元的个数                                          |\n",
    "| $f_l(*)$   <div style=\"width: 150px;height=100px\">                                  | 第$l$层神经元的激活函数                                      |\n",
    "| $W^l \\subseteq \\mathbb{R}^{N_{l} \\times N_{l-1}+1}$ | 第$l-1$层到第$l$层的权重矩阵($\\mathbf{w}^l_{i,}=[w^l_{i,1},w^l_{i,2},\\cdots,w^l_{i,N_{l-1}},b^l_i]$) |\n",
    "| $\\mathbf{a}^l \\subseteq \\mathbb{R}^{N_l}$  <div style=\"width: 150px;height=100px\">       | 第$l$层神经元的输入(净活性值)                                |\n",
    "| $\\mathbf{o}^l \\subseteq \\mathbb{R}^{N_l}$ <div style=\"width: 150px;height=100px\">        | 第$l$层神经元的输出(活性值)                                  |\n",
    "\n",
    "其中权重向量$\\mathbf{w}^l_{i,}$为权重矩阵的第$i$行;权重$w^l_{i, j}$为权重矩阵的第$i$行第$j$列处的元素;\n",
    "$\\mathbf{w}^l_{,-1}=[w^l_{1,-1}, w^l_{2,-1}, \\cdots, w^l_{N_t,-1}]^T=[b^l_0,b^l_1,\\cdots,b^l_{N_t}]^T$为权重矩阵的最后一列,即偏置向量;\n",
    "$\\mathbf{o}^l = [o^l_1,o^l_2,\\cdots,o^l_{N_l}, o^l_{N_l +1}] = [o^l_1,o^l_2,\\cdots,o^l_{N_l},1]$,$o^l_{i}$为第$l$层神经元的第$i$个输出(输出层即为$\\mathbf{o}^l = [o^l_1,o^l_2,\\cdots,o^l_{N_l}]$).\n",
    "\n",
    "&emsp;&emsp;令$\\mathbf{o}^0=[(\\mathbf{a}^{\\mathrm{input}})^T, 1]^T$,前馈神经网络通过不断迭代下面公式进行信息传播\n",
    "\n",
    "\\begin{align}\n",
    "&\\mathbf{a}^l = W^l \\mathbf{o}^{l-1} \\\\\n",
    "&\\mathbf{o}^{l} = \\left[ \\left( f_l(\\mathbf{a}^l) \\right)^T, 1 \\right]^T  \\quad \\text{注:若}l\\text{为最后一层,则}\\mathbf{o}^{l} = f_l(\\mathbf{a}^l) \\\\\n",
    "\\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "首先根据$l-1$层神经元的活性值(Activation)$\\mathbf{o}^{l-1}$计算出第$l$层神经元的净活性值(Net Activation)$\\mathbf{a}^l$,\n",
    "然后经过一个激活函数外加最末行额外增加1行(元素为1)得到第$l$层神经元的活性值$\\mathbf{o}^{l}$.因此,\n",
    "可以将每个网络层看作一个仿射变换(Affine Transformation)和一个非线性变换作用的结果.\n",
    "\n",
    "&emsp;&emsp;给定训练集为$D = \\{ \\mathbf{a}_m, y_m \\}_{m=1}^M$,将每个样本$\\mathbf{a}_m$输入前馈神经网络,得到网络输出为$ \\hat{y}_m$,\n",
    "其在数据集$D$上的结构风险函数为:\n",
    "\n",
    "$$ \\mathcal{R}(W)=\\frac{1}{M} \\sum_{m=1}^{M} \\mathcal{L}\\left(y_m, \\hat{y}_m \\right)+\\frac{1}{2} \\lambda\\|W^{b=0}\\|_{F}^{2} \\tag{2} $$\n",
    "\n",
    "其中$W$表示网络中所有权重矩阵;$W^{b=0}$表示令$W$中所有的偏置项$b$全为为0;$\\|W^{b=0}\\|_{F}^{2}$是正则化项,用来防止过拟合;\n",
    "$\\lambda>0$为超参数.$\\lambda$越大,$W^{b=0}$越接近于零矩阵,这里的$\\|W^{b=0}\\|_{F}^{2}$ 一般使用Frobenius范数:\n",
    "\n",
    "$$ \\|W^{b=0}\\|_{F}^{2}=\\sum_{l=1}^{L} \\sum_{i=1}^{N_{l}} \\sum_{j=1}^{N_{l-1}}\\left(w_{i j}^{(l)}\\right)^{2} \\tag{3} $$\n",
    "\n",
    "&emsp;&emsp;此时网络参数可以通过梯度下降法进行学习,在梯度下降法的每次迭代中,第$l$层的参数$W^l$更新方式为\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla W^l &= \\alpha \\frac{\\partial \\mathcal{R}(W)}{\\partial W^{l}} \\\\\n",
    "           &=\\alpha\\left(\\frac{1}{m} \\sum_{m=1}^{M}\\left(\\frac{\\partial \\mathcal{L}\\left(y_m,\\hat{y}_m \\right)}{\\partial W^{l}}\\right)+\\lambda W^{l;b=0} \\right) \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "其中$\\alpha$为学习率,易知$\\frac{\\partial \\|W^{b=0}\\|_{F}^{2}}{\\partial  W^l}$为标量对矩阵求导(这里采用分母布局).\n",
    "\n",
    "&emsp;&emsp;下面计算$\\frac{ \\partial \\mathcal{L}(y_m, \\hat{y}_m ) }{\\partial W^l}$,由于$\\frac{ \\partial \\mathcal{L}(y_m, \\hat{y}_m ) }{\\partial W^l}$的计算涉及对矩阵的微分,\n",
    "十分繁琐,因此我们先计算$ \\mathcal{L}(y_m, \\hat{y}_m )$关于参数矩阵中每个元素的偏导数$\\frac{ \\partial \\mathcal{L}(y_m, \\hat{y}_m ) }{\\partial w^l_{ij}}$,根据分母布局求导链式法则,\n",
    "\n",
    "$$ \\frac{\\partial\\mathcal{L}(y_m, \\hat{y}_m )}{\\partial w_{i j}^{l}}= \\frac{\\partial \\mathbf{a}^l}{\\partial w^l_{ij}}   \\frac{\\mathcal{L}(y_m, \\hat{y}_m )}{\\partial \\mathbf{a}^l}   \\tag{5} $$\n",
    "\n",
    "&emsp;&emsp;为了计算公式(5),先计算$\\frac{\\partial \\mathbf{a}^l}{\\partial w^l_{ij}} $,由于$\\mathbf{a}^l = W^l \\mathbf{o}^{l-1}$,故有\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{a}^l}{\\partial w^l_{ij}} &= \\left[ \\frac{a^l_1}{\\partial w^l_{ij}} ,\\cdots,\\frac{a^l_i}{\\partial w^l_{ij}} ,\\cdots,\\frac{a^l_{N_l-1}}{\\partial w^l_{ij}},\\frac{a^l_{N_l}}{\\partial w^l_{ij}}    \\right]    \\tag{6}    \\\\\n",
    "     & =\\left[ 0,\\cdots, \\frac{\\partial( \\mathbf{w}^l_{i,} \\mathbf{o}^{l-1} )}{\\partial w^l_{ij}} \\cdots, 0, 0 \\right] \\\\\n",
    "     &=\\left[ 0, \\cdots, o_j^{l-1},\\cdots, 0, 0 \\right] \\\\\n",
    "     &=\\mathbb{I}(o_j^{l-1}) \\subseteq \\mathbb{R}^{1 \\times N_l} \\tag{7}\n",
    "\\end{align}\n",
    "\n",
    "其中$\\mathbf{w}^l_{i,} \\mathbf{o}^{l-1} $表示向量$\\mathbf{w}^l_{i,}$与向量$\\mathbf{o}^{l-1}$的内积;\n",
    "$\\mathbb{I}(o_j^{l-1})$表示第$i$个元素为$o_j^{l-1}$,其余元素为0的行向量;式(6)采用的求导布局为分母布局.\n",
    "\n",
    "&emsp;&emsp;再计算$\\frac{\\mathcal{L}(y_m, \\hat{y}_m )}{\\partial \\mathbf{a}^l}$,\n",
    "偏导数$\\frac{\\mathcal{L}(y_m, \\hat{y}_m )}{\\partial \\mathbf{a}^l}$表示第$l$层神经元对最终损失的影响,\n",
    "也反映了最终损失对第$l$层神经元的敏感程度,因此一般称为第$l$层神经元的误差项,用$\\boldsymbol{\\delta}^l$来表示.\n",
    "\n",
    "$$ \\boldsymbol{\\delta}^l \\buildrel \\Delta \\over =\\frac{\\mathcal{L}(y_m, \\hat{y}_m )}{\\partial \\mathbf{a}^l} \\subseteq \\mathbb{R}^{N_l} $$\n",
    "\n",
    "误差项$\\delta^l$也间接反映了不同神经元对网络能力的贡献程度,从而比较好第解决了贡献度分配问题(Credit Assignment Problem,CAP).根据分母布局求导链式法则,有\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\delta}^l & \\buildrel \\Delta \\over =\\frac{\\mathcal{L}(y_m, \\hat{y}_m )}{\\partial \\mathbf{a}^l} \\\\\n",
    "\t\t &=  \\frac{\\partial \\mathbf{o}^l}{ \\partial \\mathbf{a}^l}   \\frac{\\partial \\mathbf{a}^{l+1}}{\\partial \\mathbf{o}^l} \\frac{\\mathcal{L}(y_m, \\hat{y}_m )}{\\partial \\mathbf{a}^{l+1}} \\\\\n",
    "\t\t &= \\frac{\\partial \\mathbf{o}^l}{ \\partial \\mathbf{a}^l}  \\frac{\\partial \\mathbf{a}^{l+1}}{\\partial \\mathbf{o}^l}  \\boldsymbol{\\delta}^{l+1 }  \\\\\n",
    "\t\t &= \\frac{\\partial \\mathbf{o}^l}{ \\partial \\mathbf{a}^l}  \\frac{\\partial W^{l+1} \\mathbf{o}^l }{\\partial \\mathbf{o}^l}   \\boldsymbol{\\delta}^{l+1 }                     \\\\\n",
    "\\text{若}l\\text{为最后一层-->}&=  \\frac{ \\partial  f_l(\\mathbf{a}^l) }{\\partial \\mathbf{a}^l}  (W^{l+1})^T \\boldsymbol{\\delta}^{l+1 } \\\\\n",
    "\\text{other-->}&=  \\frac{ \\partial \\left[ (f_l(\\mathbf{a}^l)^T, 1 \\right]^T ) }{\\partial \\mathbf{a}^l}  (W^{l+1})^T \\boldsymbol{\\delta}^{l+1 } \\\\\n",
    "\t\t &= A^l (W^{l+1})^T  \\boldsymbol{\\delta}^{l+1 } \\subseteq \\mathbb{R}^{N_l}\\tag{8}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "其中,根据分母布局求导法则,\n",
    "\n",
    "$$ \\frac{\\partial W^{l+1} \\mathbf{o}^l}{\\partial \\mathbf{o}^l} =  (W^{l+1})^T \\tag{9} $$\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial\\left[ (f_l(\\mathbf{a}^l)^T, 1 \\right]^T }{\\partial \\mathbf{a}^l} &=\n",
    "\\begin{bmatrix}\n",
    "\\frac{ \\partial f_l(a^l_{1})}{\\partial a^l_1} & \\frac{ \\partial f_l(a^l_{2})}{\\partial a^l_1} & \\cdots & \\frac{ \\partial f_l(a^l_{N_l})}{\\partial a^l_1} & \\frac{\\partial  1}{\\partial a^l_1} \\\\\n",
    "\\frac{ \\partial f_l(a^l_{1})}{\\partial a^l_2} & \\frac{ \\partial f_l(a^l_{2})}{\\partial a^l_2} & \\cdots & \\frac{ \\partial f_l(a^l_{N_l})}{\\partial a^l_2} & \\frac{\\partial 1}{\\partial a^l_2} \\\\\n",
    "\\vdots& \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "\\frac{ \\partial f_l(a^l_{1})}{\\partial a^l_{N_l}} & \\frac{ \\partial f_l(a^l_{2})}{\\partial a^l_{N_l}} & \\cdots & \\frac{ \\partial f_l(a^l_{N_l})}{\\partial a^l_{N_l}} & \\frac{\\partial 1}{\\partial a^l_{N_l -1}} \\\\\n",
    "\\end{bmatrix} \\\\\\\\\n",
    "\t\t&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{ \\partial f_l(a^l_{1})}{\\partial a^l_1} & 0 & \\cdots & 0 & 0 \\\\\n",
    "0 & \\frac{ \\partial f_l(a^l_{2})}{\\partial a^l_2} & \\cdots & 0 & 0\\\\\n",
    "\\vdots& \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\frac{ \\partial f_l(a^l_{N_l})}{\\partial a^l_{N_l}} & 0 \\\\\n",
    "\\end{bmatrix} \\\\\\\\\n",
    "\t\t&= A^l\n",
    "\t\t\\tag{10}\n",
    "\\end{align}\n",
    "\n",
    "&emsp;&emsp;根据公式(8)可以看出,第$l$层的误差项可以通过第$l+1$层的误差项计算得到,这就是误差我反向传播(BackPropagation,BP).\n",
    "\n",
    "&emsp;&emsp;于是第$l$层第$i$个神经元的误差可表示为\n",
    "\n",
    "$$ \\delta^l_i = \\frac{ \\partial f_l(a^l_{i})}{\\partial a^l_i} (\\mathbf{w}^{l+1}_{,i})^T \\boldsymbol{\\delta}^{l+1} \\tag{11} $$\n",
    "\n",
    "故反向传播的含义是:第$l$层一个神经元的误差(或敏感度)是所有与该神经元相连的第$l+1$层的误差项的权值和,再乘上该神经元上激活函数的导数.\n",
    "\n",
    "&emsp;&emsp;此时公式(5)可以写为\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial\\mathcal{L}(y_m, \\hat{y}_m )}{\\partial w_{i j}^{l}} &= \\mathbb{I}(o_j^{l-1}) \\boldsymbol{\\delta}^l \\\\\n",
    "\t\t&= \\left[ 0, \\cdots, o_j^{l-1},\\cdots, 0, 0 \\right] \\left[\\delta_1, \\cdots,\\delta_1,\\cdots,\\delta_{N_l}\\right]^T  \\\\\n",
    "\t\t&= \\delta_i^l o_j^{l-1}  \\tag{12} \\\\\n",
    "\t\t&= \\frac{ \\partial f_l(a^l_{i})}{\\partial a^l_i} (\\mathbf{w}^{l+1}_{,i})^T \\boldsymbol{\\delta}^{l+1}  o_j^{l-1} \\tag{13}\n",
    "\\end{align}\n",
    "\n",
    "其中$\\delta_i^l o_j^{l-1}$也可表示为向量$ \\boldsymbol{\\delta}^l $和向量$\\mathbf{o}^{l-1}$的外积的第$i$行第$j$列处的元素,\n",
    "故上式可进一步写为\n",
    "\n",
    "$$ \\left[ \\frac{\\partial \\mathcal{L}(y_m, \\hat{y}_m )}{\\partial w_{i j}^{l}} \\right]_{ij} = \\left[ \\boldsymbol{\\delta}^l \\left(\\mathbf{o}^{l-1}\\right)^T \\right]_{ij} $$\n",
    "\n",
    "&emsp;&emsp;因此,$\\mathcal{L}(y_m, \\hat{y}_m )$关于$l$层矩阵$W^l$的梯度为\n",
    "\n",
    "$$ \\frac{ \\partial \\mathcal{L}(y_m, \\hat{y}_m )}{\\partial W^l} =  \\boldsymbol{\\delta}^l \\left(\\mathbf{o}^{l-1}\\right)^T \\subseteq \\mathbb{R}^{N_l\\times N_{l-1}+1} \\tag{14} $$\n",
    "\n",
    "故第$l$层的参数$W^l$更新方式为\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta W^l &= \\alpha \\frac{\\partial \\mathcal{R}(W)}{\\partial W^{(l)}} \\\\\n",
    "           &=\\alpha\\left(\\frac{1}{m} \\sum_{m=1}^{M}\\left(\\frac{\\partial \\mathcal{L}\\left(y_m,\\hat{y}_m \\right)}{\\partial W^{(l)}}\\right)+\\lambda W^{(l);b=0} \\right) \\\\\n",
    "           &=\\alpha\\left(\\frac{1}{m} \\sum_{m=1}^{M} \\boldsymbol{\\delta}^l \\left(\\mathbf{o}^{l-1}\\right)^T +\\lambda W^{(l);b=0} \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}