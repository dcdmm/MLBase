{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "from sklearn.naive_bayes import MultinomialNB  # 基于多项式模型的朴素贝叶斯\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 以下代码可通过调用`from sklearn.feature_extraction.text import CountVectorizer`简洁实现"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def text_processing(folder_path, test_size=0.2):\n",
    "    \"\"\"\n",
    "    文本生成过程\n",
    "    :param folder_path:目录路径\n",
    "    :param test_size:测试集数据比例\n",
    "    :return:不重复的单词,训练集样本,测试集样本,训练集标签,测试集标签\n",
    "    \"\"\"\n",
    "    folder_list = os.listdir(folder_path)  # folder_path下的所有目录,返回值为列表\n",
    "    data_list = []  # 每个文件的分词\n",
    "    class_list = []  # 每个文件的标签\n",
    "\n",
    "    for folder in folder_list:\n",
    "        new_folder_path = os.path.join(folder_path, folder)  # 路径拼接\n",
    "        files = os.listdir(new_folder_path)  # new_folder_path下的所有.txt文件\n",
    "\n",
    "        for file in files:\n",
    "            with open(os.path.join(new_folder_path, file), 'r', encoding='UTF-8', errors='ignore') as fp:\n",
    "                raw = fp.read()\n",
    "            word_cut = jieba.cut(raw, cut_all=False)  # 精确模式,返回值为生成器\n",
    "            word_list = list(word_cut)\n",
    "            data_list.append(word_list)\n",
    "            class_list.append(folder)\n",
    "\n",
    "    train_data_list, test_data_list, train_class_list, test_class_list = train_test_split(data_list, class_list,\n",
    "                                                                                          test_size=test_size)  # 数据集划分\n",
    "\n",
    "    all_words_dict = {}  # 词频统计\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if word in all_words_dict:\n",
    "                all_words_dict[word] += 1\n",
    "            else:\n",
    "                all_words_dict[word] = 1\n",
    "\n",
    "    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f: f[1], reverse=True)  # 根据词频降序排列\n",
    "    all_words_list, _ = zip(*all_words_tuple_list)  # 解包\n",
    "    all_words_list = list(all_words_list)\n",
    "\n",
    "    return all_words_list, train_data_list, test_data_list, train_class_list, test_class_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10391\n",
      "['，', '的', '\\u3000', '。', '\\n', ';', '&', ' ', 'nbsp', '在', '、', '了', '“', '”', '是', '和', '：', '\\x00', '我', '中国']\n",
      "72\n",
      "18\n",
      "72\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# text_processing函数测试\n",
    "folder_path = 'Sample'  # 包含9中不同新闻的总文件夹\n",
    "all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = text_processing(folder_path,\n",
    "                                                                                                     test_size=0.2)\n",
    "print(len(all_words_list))  # 不重复单词数\n",
    "print(all_words_list[:20])\n",
    "\n",
    "print(len(train_data_list))  # 72个训练集样本\n",
    "print(len(test_data_list))  # 18个测试集样本\n",
    "\n",
    "print(len(train_class_list))  # 72个训练集标签\n",
    "print(len(test_class_list))  # 18个测试集标签"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def make_word_set(words_file):\n",
    "    \"\"\"生成停用词(去重)\"\"\"\n",
    "    words_set = set()  # 集合\n",
    "    with open(words_file, 'r', encoding='UTF-8', errors='ignore') as fp:\n",
    "        for line in fp.readlines():  # 循环取出每一行\n",
    "            word = line.strip()\n",
    "            if len(word) > 0 and word not in words_set:  # 去重\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428\n",
      "{'诸', '唯有', '把', '哪个', '不至于', '别说', '格里斯', '向', '对于', '哇', '看', '只怕', '因之', '这儿', '再有', '随时', '据', '以来', '反之', '些', '多么', '就算', '一', '当然', '各', '并不', '此', '与', '靠', '本着', '为着', '对待', '什么样', '根据', '小', '各位', '许多', '同时', '就是', '朝着', '前者', '要不', '直到', '其余', '不但', '至今', '及', '由', '她们', '跟', '因', '此间', '任何', '之所以', '则', '况且', '另外', '不如', '以便', '出来', '我们', '只消', '等等', '虽然', '再则', '正如', '何况', '及至', '你', '受到', '且', '最', '后者', '而且', '替代', '很', '被', '去', '或者说', '又', '以为', '如是', '无', '如果', '咱', '处在', '着', '如若', '依据', '继而', '然而', '一切', '为止', '吧', '分别', '诸位', '可是', '某个', '多会', '他', '总之', '尽管如此', '那么', '沿着', '针对', '介于', '打', '这么', '因此', '比', '说来', '与否', '可', '用来', '而已', '否则', '才能', '凡是', '至', '其', '如', '例如', '为何', '并非', '不过', '其中', '她', '其他', '还要', '然后', '或者', '不外乎', '甚至于', '某某', '哪些', '既然', '不管', '来说', '只', '还是', '了', '而', '趁', '那边', '拿', '此时', '对比', '怎么办', '甚至', '他人', '随', '大家', '万一', '个', '什么', '由此', '譬如', '不', '为什么', '但是', '仍旧', '什么的', '它', '只要', '于', '各自', '不单', '本地', '某些', '后', '自己', '要不然', '似的', '的话', '假如', '尽管', '已', '要是', '别人', '距', '出于', '得了', '既往', '如上', '有', '不光', '而后', '下', '这些', '这', '那般', '嘻嘻', '比如', '倘若', '接着', '哟', '不仅', '何以', '据此', '赖以', '每当', '只限于', '可以', '还有', '非但', '如同下', '何', '别', '这会', '那个', '这里', '本人', '既是', '就是说', '以致', '有关', '宁可', '可见', '既', '别处', '不是', '或', '得', '无论', '反而', '之', '即便', '以及', '固然', '您', '与其', '于是', '首先', '只有', '有些', '除此', '或是', '从而', '让', '便于', '啥', '怎么样', '若', '不料', '该', '关于', '凭', '全体', '依照', '来自', '有的', '彼时', '哪儿', '致', '随后', '除外', '谁', '较之', '同', '即', '如果说', '来', '但', '以免', '连同', '加以', '是', '那儿', '到', '咱们', '诸如', '此次', '对方', '以', '随着', '别的', '以上', '嘿嘿', '另', '亦', '果然', '的', '光是', '何时', '嘛', '全部', '就要', '还', '人们', '值此', '经过', '其它', '这个', '它们', '这般', '这样', '此地', '即使', '尔', '若是', '在', '毋宁', '如此', '却', '不然', '虽说', '沿', '不只', '若非', '使', '而外', '所在', '其次', '曾', '自从', '乃', '为', '简言之', '基于', '个人', '遵循', '儿', '而是', '向着', '不尽然', '并且', '怎么', '从', '用', '每', '为了', '多少', '甚而', '所以', '要么', '由于', '们', '一来', '那里', '和', '那时', '连带', '至于', '因为', '才是', '并', '那', '两者', '好', '人', '当地', '本身', '此处', '只限', '所有', '么', '又及', '那样', '谁人', '啦', '起', '当', '再', '开外', '乃至', '怎', '自身', '往', '逐步', '他们', '因而', '正值', '进而', '彼此', '不仅仅', '只因', '除非', '正巧', '仍', '为此', '今', '加之', '那些', '趁着', '某', '呵呵', '自', '有时', '以至', '鉴于', '给', '你们', '遵照', '一些', '上', '如何', '几', '在于', '虽', '哪怕', '这边', '除了', '个别', '我', '一旦', '从此', '凡', '照着', '不论', '嗡', '截至', '怎样', '所', '按照', '另一方面', '故而', '如下', '只需', '只是', '凭借', '不尽', '作为', '的确', '此外', '何处', '哪', '也', '正是'}\n"
     ]
    }
   ],
   "source": [
    "stopwords_file = './stopwords_cn.txt'  # 停用词文件\n",
    "stopwords_set = make_word_set(stopwords_file)  # 首先停用词去重\n",
    "print(len(stopwords_set))\n",
    "print(stopwords_set)  # 所有停用词"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def words_dict(all_words_list, deleteN, stopwords_set=None):\n",
    "    \"\"\"特征词选取\"\"\"\n",
    "    if stopwords_set is None:\n",
    "        stopwords_set = set()\n",
    "    feature_words = []\n",
    "    n = 1\n",
    "    for t in range(deleteN, len(all_words_list), 1):\n",
    "        if n > 1000:  # feature_words的维度这里选定频率前1000的词语\n",
    "            break\n",
    "\n",
    "        if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1 < len(\n",
    "                all_words_list[t]) < 5:  # 这里设定特征词需满足三个条件:不是数字;不在停用词表;长度2~4\n",
    "            feature_words.append(all_words_list[t])\n",
    "        n += 1\n",
    "\n",
    "    return feature_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一个', '游客', '公司', '导弹', '大陆', '旅游', '北京', '市场', '火炮', '认为', '进行', '台军', '考生', '没有', '已经', '时间', '各种', '志愿', '一种', '美国', '解放军', '作战', '成为', '企业', '支付', '仿制', '主要', '目前', '比赛', '问题', '发展', '很多', '远程', '可能', '五一', '黄金周', '通过', '射程', '选择', '完全', '分析', '记者', '技术', '学校', '词汇', '部署', '能力', '增长', '时候', '表示', '建设', '文章', '需要', '专业', '学习', '工作', '一定', '今年', '期间', '接待', '万人次', '毕业生', '开始', '部分', '填报', '亿美元', '使用', '部队', '情况', '现在', '专家', '电话', '比较', '相对', '阵地', '提供', '科学', '拥有', '资料', '必须', '达到', '收入', '销售', '几乎', '军事', '表现', '用户', '考试', '计划', '影响', '这是', '历史', '人数', '服务', '训练', '产品', '来源', '希望', '复习', '最后']\n"
     ]
    }
   ],
   "source": [
    "deleteN = 20  # 舍弃频率前20的词\n",
    "# 越靠前的词语出现的越频繁,有可能所有类别中都出现很多次,这类词去掉效果可能更好\n",
    "feature_words = words_dict(all_words_list, deleteN, stopwords_set)\n",
    "print(feature_words[:100])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def text_features(train_data_list, test_data_list, feature_words):\n",
    "    \"\"\"用0/1判断是否出现在特征词中\"\"\"\n",
    "\n",
    "    def text_features(text, feature_words):  # text的定义在下面\n",
    "        text_words = set(text)  # 样本去重\n",
    "        # 遍历每个样本词语,凡是样本的词语出现在1000个特征词里,就记录下来为1,否则为0\n",
    "        features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        return features\n",
    "\n",
    "    train_feature_list = [text_features(text, feature_words) for text in train_data_list]\n",
    "    test_feature_list = [text_features(text, feature_words) for text in test_data_list]\n",
    "\n",
    "    return train_feature_list, test_feature_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "18\n",
      "1000\n",
      "[1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_feature_list, test_feature_list = text_features(train_data_list, test_data_list, feature_words)\n",
    "print(len(train_feature_list))\n",
    "print(len(test_feature_list))\n",
    "print(len(test_feature_list[0]))  # 每个样本的维度都是1000\n",
    "print(test_feature_list[4][:200])  # 打印测试集的第5个样本的前200个值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def text_classifier(train_feature_list, test_feature_list,\n",
    "                    train_class_list, test_class_list):\n",
    "    \"\"\"朴素贝叶斯分类,同时输出准确率\"\"\"\n",
    "    classifier = MultinomialNB().fit(train_feature_list, train_class_list)\n",
    "    test_accuracy = classifier.score(test_feature_list, test_class_list)\n",
    "\n",
    "    return test_accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = text_classifier(train_feature_list, test_feature_list,\n",
    "                                train_class_list, test_class_list)\n",
    "print(test_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}