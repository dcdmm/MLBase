{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "KE_TpNaSZQ5n",
    "jupyter": {},
    "outputId": "e100b424-e5d9-45e5-ba3a-abe466edeb16",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0hfGEe2LB9s",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec78e0665704c33ab7e23f061184a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载mrpc数据集\n",
    "dataset = load_dataset('glue', 'mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4030B703046E478DBF8465C7106CF3E5",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = dataset['train'].to_pandas()\n",
    "df_val = dataset['test'].to_pandas()\n",
    "df_test = dataset['validation'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "nNs2FWNJSLSq",
    "jupyter": {},
    "outputId": "3157c71e-8b2d-4a86-9c3d-7433be8f7022",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3668, 4)\n",
      "(1725, 4)\n",
      "(408, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "irj7itV0UCF_",
    "jupyter": {},
    "outputId": "c7aedad8-f1c1-4407-f9d0-9c2d0b4b96b6",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother , whom he called \" ...</td>\n",
       "      <td>Referring to him as only \" the witness \" , Amr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick 's before selling the c...</td>\n",
       "      <td>Yucaipa bought Dominick 's in 1995 for $ 693 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10 , the ship 's owners had published ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT , Tab shares were up 19 cents ...</td>\n",
       "      <td>Tab shares jumped 20 cents , or 4.6 % , to set...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $ 2.11 , or about 11 percent , ...</td>\n",
       "      <td>PG &amp; E Corp. shares jumped $ 1.63 or 8 percent...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  Amrozi accused his brother , whom he called \" ...   \n",
       "1  Yucaipa owned Dominick 's before selling the c...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT , Tab shares were up 19 cents ...   \n",
       "4  The stock rose $ 2.11 , or about 11 percent , ...   \n",
       "\n",
       "                                           sentence2  label  idx  \n",
       "0  Referring to him as only \" the witness \" , Amr...      1    0  \n",
       "1  Yucaipa bought Dominick 's in 1995 for $ 693 m...      0    1  \n",
       "2  On June 10 , the ship 's owners had published ...      1    2  \n",
       "3  Tab shares jumped 20 cents , or 4.6 % , to set...      0    3  \n",
       "4  PG & E Corp. shares jumped $ 1.63 or 8 percent...      1    4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()  # label:whether the sentences in the pair are semantically equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3668 entries, 0 to 3667\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentence1  3668 non-null   object\n",
      " 1   sentence2  3668 non-null   object\n",
      " 2   label      3668 non-null   int64 \n",
      " 3   idx        3668 non-null   int32 \n",
      "dtypes: int32(1), int64(1), object(2)\n",
      "memory usage: 100.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()  # 没有缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tc1GQh7yEm4C",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, max_length, pretrained_model='albert-base-v2'):\n",
    "        self.data = data\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sent1 = str(self.data.loc[index, 'sentence1'])\n",
    "        sent2 = str(self.data.loc[index, 'sentence2'])\n",
    "\n",
    "        encoded_pair = self.tokenizer(text=sent1, text_pair=sent2,\n",
    "                                      padding='max_length',\n",
    "                                      truncation=True,\n",
    "                                      max_length=self.max_length,\n",
    "                                      return_tensors='pt')\n",
    "\n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)\n",
    "        attn_masks = encoded_pair['attention_mask'].squeeze(0)\n",
    "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)\n",
    "\n",
    "        label = self.data.loc[index, 'label']\n",
    "        return token_ids, attn_masks, token_type_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"albert-base-v2\"  # 预训练模型名称\n",
    "maxlen = 128\n",
    "bs = 32  # 批次大小\n",
    "\n",
    "train_set = CustomDataset(df_train, maxlen, bert_model)\n",
    "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SentencePairClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model=\"albert-base-v2\", hidden_size=768):\n",
    "        super(SentencePairClassifier, self).__init__()\n",
    "        self.bert_layer = AutoModel.from_pretrained(pretrained_model)\n",
    "        self.hidden_size = hidden_size  # 不同预训练模型有不同的隐藏层大小\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.cls_layer = nn.Linear(self.hidden_size, 1)  # 下游分类任务\n",
    "\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
    "        outputs = self.bert_layer(input_ids, attn_masks, token_type_ids)\n",
    "        logits = self.cls_layer(self.dropout(outputs['pooler_output']))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentencePairClassifier(\n",
       "  (bert_layer): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (cls_layer): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = SentencePairClassifier()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n",
    "    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            # 学习率预热(线性增加)\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        # 学习率线性衰减(最小为0)\n",
    "        # num_training_steps后学习率恒为0\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    loss, count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, token_type_ids, labels in dataloader:\n",
    "            seq, attn_masks, token_type_ids, labels = seq.to(device), attn_masks.to(device), token_type_ids.to(\n",
    "                device), labels.to(device)\n",
    "            logits = net(seq, attn_masks, token_type_ids)\n",
    "            loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            count += 1\n",
    "\n",
    "    return loss / count  # 每轮的平均损失\n",
    "\n",
    "\n",
    "def train_bert(net, criterion, opti, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n",
    "    for ep in range(epochs):\n",
    "        net.train()\n",
    "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(train_loader):\n",
    "            seq, attn_masks, token_type_ids, labels = seq.to(device), attn_masks.to(device), token_type_ids.to(\n",
    "                device), labels.to(device)\n",
    "            logits = net(seq, attn_masks, token_type_ids)\n",
    "            # logits.shape=(32, 1)\n",
    "            loss = criterion(logits.squeeze(-1), labels.to(torch.float))\n",
    "            ###########################################################################\n",
    "            # 梯度累加可以先累加多个batch的梯度再进行一次参数更新,相当于增大了batch-size\n",
    "            loss = loss / iters_to_accumulate\n",
    "            loss.backward()  # 每次获取1个batch的数据,计算1次梯度,梯度不清空,不断累加\n",
    "            if (it + 1) % iters_to_accumulate == 0:  # 累加一定次数后,根据累加的梯度更新网络参数,然后清空梯度,进行下一次循环\n",
    "                opti.step()\n",
    "                lr_scheduler.step()\n",
    "                opti.zero_grad()\n",
    "            ###########################################################################\n",
    "        val_loss = evaluate_loss(net, device, criterion, val_loader)\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep + 1, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6f797e5377fe4ab281003d537c15f279",
      "9b46878367244033ac0d96f6f83766cd",
      "b9b102512ad548ac96f5560eead35201",
      "cb3cf3d7089347a092499762c552f785",
      "4ae80720349c409bba8dffa6ba429e9e",
      "c74fcaee125642908ab8fa8905f0cf6f",
      "6a229158e0db415b8561dc22a8a4ab47",
      "ef906f1d1e444404bd88e789e14e977a",
      "80d383da347f41a6927d659dc5e1a855",
      "26425569e06e4fc5b32405bcdba37dad",
      "c6897db24c284263879bcc92e6d56f39",
      "7468d3da8c8547d38e146b8adee8948e",
      "e90a00d220bc4546923517f13daf29be",
      "13b7985246244fcb89926039ded8eb20",
      "3e108b6d60874974aaf0eea761e7da59",
      "afe53d24858d446592d1825713664d9b",
      "ac95062870cd4eb3afc93dfcc8ef9a6e",
      "d79473c349744b6ba4d1532889a3dd3b",
      "bac3fa1e8ff04a1680267702e35c89b1",
      "c425744e85d44659b143679eccca5b0f",
      "a9c4247af57f46d485a4f509eafc79e3",
      "6a4d58a9a2a84f09b3d341529de530ed",
      "c07487bb1e8c4415986d31752fa0d7f5",
      "9a003cc981484793b31167d114362ea4"
     ]
    },
    "colab_type": "code",
    "id": "VZWGPomoryxy",
    "jupyter": {},
    "outputId": "25d2f91d-ed8e-4d59-b6cf-cffaff9aaf4b",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iters_to_accumulate = 2\n",
    "lr = 2e-5  # 学习率\n",
    "epochs = 5  # 训练轮数\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # 损失函数\n",
    "opti = optim.AdamW(net.parameters(), lr=lr, weight_decay=1e-2)  # 优化器\n",
    "t_total = (len(train_loader) // iters_to_accumulate) * epochs\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=0,\n",
    "                                               num_training_steps=t_total)  # 学习率策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete! Validation Loss : 0.4671923650635613\n",
      "Epoch 2 complete! Validation Loss : 0.35165361370201464\n",
      "Epoch 3 complete! Validation Loss : 0.3257005587220192\n",
      "Epoch 4 complete! Validation Loss : 0.3517433353872211\n",
      "Epoch 5 complete! Validation Loss : 0.3637254762428778\n"
     ]
    }
   ],
   "source": [
    "train_bert(net, criterion, opti, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(net, device, dataloader):\n",
    "    net.eval()\n",
    "    probs_all = []\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, token_type_ids, _ in dataloader:\n",
    "            seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
    "            logits = net(seq, attn_masks, token_type_ids).squeeze(-1)\n",
    "            probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "            probs = probs.detach().cpu().numpy().squeeze(-1)\n",
    "            probs_all.extend(probs.tolist())\n",
    "    return np.array(probs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = CustomDataset(df_test, maxlen, bert_model)\n",
    "test_loader = DataLoader(test_set, batch_size=bs)\n",
    "test_probs_all = test_prediction(net, device, test_loader)\n",
    "\n",
    "threshold = 0.6  # 阈值\n",
    "test_bool_all = np.where(test_probs_all >= threshold, 1, 0)\n",
    "test_bool_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.79       129\n",
      "           1       0.90      0.91      0.91       279\n",
      "\n",
      "    accuracy                           0.87       408\n",
      "   macro avg       0.85      0.84      0.85       408\n",
      "weighted avg       0.87      0.87      0.87       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels_test = df_test['label']\n",
    "print(classification_report(labels_test, test_bool_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}