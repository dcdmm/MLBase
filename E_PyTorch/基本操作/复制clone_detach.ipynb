{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "| **Operation**   | **New/Shared memory** | **Still in computation graph** |\n",
    "| --------------- | --------------------- | ------------------------------ |\n",
    "| tor.clone()  | New                   | Yes                            |\n",
    "| tensor.detach() | Shared                | No                             |\n",
    "| tensor.clone().detach() | New                | No                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### torch.clone"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<CloneBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "a_y = a ** 2\n",
    "\n",
    "acl = torch.clone(a)  # Gradients propagating to the cloned tensor will propagate to the original tensor.\n",
    "acl.retain_grad()\n",
    "print(acl)\n",
    "acl_y = acl * 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "\n",
      "tensor(3.)\n",
      "tensor(5.)\n",
      "\n",
      "tensor(6.)\n",
      "tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "a_y.backward()\n",
    "print(a.grad, end='\\n\\n')\n",
    "\n",
    "acl_y.backward(retain_graph=True)\n",
    "print(acl.grad)  # acl为非叶tensor,通过acl_y.retain_grad()可输出acl.grad\n",
    "print(a.grad, end='\\n\\n')  # acl.grad会传递回给a.grad,因此a.grad=a.grad(2)+acl.grad(3)=5\n",
    "\n",
    "acl_y.backward()\n",
    "print(acl.grad)  # acl.grad由3累加到6\n",
    "print(a.grad)  # 此时a.grad=a.grad(2)+acl.grad(6)=8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<FillBackward0>)\n",
      "tensor(1., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "acl.fill_(0)\n",
    "\n",
    "print(acl)\n",
    "print(a)  # 不共享数据的物理内存"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### torch.detach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "b = torch.tensor([1., 2., 3., 4., 5., 6.], requires_grad=True)\n",
    "\n",
    "bde = torch.detach(b)\n",
    "print(bde)  # requires_grad=False,脱离计算图"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11., 12., 13., 14., 15., 16.])\n",
      "tensor([11., 12., 13., 14., 15., 16.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# detach函数与data属性的比较参考tensor相关属性.ipynb;backward注意1.ipynb\n",
    "bde.add_(10)  # 不能改变bde的地址\n",
    "\n",
    "print(bde)\n",
    "print(b)  # 共享数据的物理内存"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2319213526272\n",
      "2319211893952\n",
      "tensor([11., 12., 13., 14., 15., 16.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(id(bde))\n",
    "bde = bde + 100\n",
    "print(id(bde))  # bde地址改变\n",
    "print(b)  # b不变"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}