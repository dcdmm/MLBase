{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spicy Jalapeño\n",
      "Spicy Jalapeño\n",
      "14\n",
      "15\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "s1 = 'Spicy Jalape\\u00f1o'\n",
    "s2 = 'Spicy Jalapen\\u0303o'\n",
    "\n",
    "print(s1)\n",
    "print(s1)\n",
    "print(len(s1))\n",
    "print(len(s2))\n",
    "print(s1 == s2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准化方式NFC:\n",
      "t1: Spicy Jalapeño\n",
      "t2: Spicy Jalapeño\n",
      "14 14\n",
      "True\n",
      "\n",
      "标准化方式NFKC:\n",
      "t1: Spicy Jalapeño\n",
      "t2: Spicy Jalapeño\n",
      "14 14\n",
      "True\n",
      "\n",
      "标准化方式NFD:\n",
      "t1: Spicy Jalapeño\n",
      "t2: Spicy Jalapeño\n",
      "15 15\n",
      "True\n",
      "\n",
      "标准化方式NFKD:\n",
      "t1: Spicy Jalapeño\n",
      "t2: Spicy Jalapeño\n",
      "15 15\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 在处理Unicode字符串,需要确保所有字符串在底层有相同的表示\n",
    "# 字符串标准化\n",
    "normal_lst = ['NFC', 'NFKC', 'NFD', 'NFKD']\n",
    "for i in normal_lst:\n",
    "    print(\"标准化方式\" + i + \":\")\n",
    "    t1 = unicodedata.normalize(i, s1)\n",
    "    t2 = unicodedata.normalize(i, s2)\n",
    "    print(\"t1:\", t1)\n",
    "    print(\"t2:\", t2)\n",
    "    print(len(t1), len(t2))\n",
    "    print(t1 == t2, end='\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "strip_accents{‘ascii’, ‘unicode’}, default=None\n",
    "    Remove accents and perform other character normalization during the preprocessing step.\n",
    "    ‘ascii’ is a fast method that only works on characters that have an direct ASCII mapping.\n",
    "    ‘unicode’ is a slightly slower method that works on any characters. None (default) does nothing.\n",
    "\"\"\"\n",
    "vectorizer = CountVectorizer(strip_accents='ascii')  #  内部执行了上述的字符串标准化\n",
    "with open('data.txt', 'r', encoding='utf-8') as f:\n",
    "    a = f.readlines()\n",
    "\n",
    "X = vectorizer.fit_transform(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "{'age': 3,\n 'has': 70,\n 'reached': 135,\n 'the': 162,\n 'end': 52,\n 'of': 113,\n 'beginning': 22,\n 'word': 189,\n 'may': 100,\n 'be': 19,\n 'guilty': 69,\n 'in': 81,\n 'his': 76,\n 'seems': 143,\n 'to': 174,\n 'passing': 125,\n 'lot': 97,\n 'different': 44,\n 'life': 93,\n 'became': 20,\n 'appearance': 11,\n 'same': 140,\n 'day': 38,\n 'back': 16,\n 'past': 126,\n 'oneself': 117,\n 'paranoid': 123,\n 'weird': 184,\n 'belief': 23,\n 'disillusionment': 45,\n 'these': 165,\n 'days': 39,\n 'my': 108,\n 'mind': 103,\n 'been': 21,\n 'very': 180,\n 'messy': 102,\n 'constantly': 32,\n 'always': 8,\n 'feel': 55,\n 'should': 150,\n 'go': 64,\n 'do': 47,\n 'something': 153,\n 'or': 119,\n 'write': 190,\n 'twenty': 176,\n 'years': 193,\n 'trajectory': 175,\n 'deeply': 42,\n 'shallow': 147,\n 'suddenly': 159,\n 'it': 88,\n 'during': 50,\n 'childhood': 28,\n 'think': 167,\n 'lucky': 99,\n 'money': 106,\n 'and': 10,\n 'new': 110,\n 'clothes': 31,\n 'are': 12,\n 'necessary': 109,\n 'for': 58,\n 'year': 192,\n 'but': 26,\n 'as': 14,\n 'advance': 0,\n 'will': 188,\n 'more': 107,\n 'found': 59,\n 'that': 161,\n 'those': 168,\n 'things': 166,\n 'optional': 118,\n 'junior': 89,\n 'high': 74,\n 'school': 141,\n 'thought': 169,\n 'have': 71,\n 'crush': 36,\n 'on': 115,\n 'just': 90,\n 'means': 101,\n 'real': 136,\n 'growth': 68,\n 'over': 122,\n 'three': 170,\n 'later': 92,\n 'writing': 191,\n 'alumni': 7,\n 'peace': 127,\n 'isn': 87,\n 'really': 137,\n 'grow': 67,\n 'up': 177,\n 'is': 86,\n 'not': 111,\n 'so': 152,\n 'important': 80,\n 'then': 163,\n 'don': 48,\n 'want': 182,\n 'give': 63,\n 'vent': 179,\n 'out': 121,\n 'your': 194,\n 'inner': 84,\n 'voice': 181,\n 'can': 27,\n 'children': 29,\n 'feelings': 56,\n 'period': 128,\n 'was': 183,\n 'eventually': 53,\n 'infarction': 82,\n 'when': 186,\n 'graduation': 65,\n 'party': 124,\n 'throat': 171,\n 'again': 2,\n 'stood': 158,\n 'pitch': 130,\n 'he': 73,\n 'sweat': 160,\n 'profusely': 133,\n 'looked': 96,\n 'at': 15,\n 'thrown': 172,\n 'basketball': 18,\n 'hoops': 77,\n 'himself': 75,\n 'already': 5,\n 'remember': 138,\n 'person': 129,\n 'time': 173,\n 'ideas': 78,\n 'special': 154,\n 'clear': 30,\n 'line': 95,\n 'if': 79,\n 'nothing': 112,\n 'could': 34,\n 'shake': 146,\n 'also': 6,\n 'once': 116,\n 'seemed': 142,\n 'determined': 43,\n 'often': 114,\n 'backed': 17,\n 'last': 91,\n 'dislike': 46,\n 'cowardice': 35,\n 'finally': 57,\n 'there': 164,\n 'love': 98,\n 'miss': 104,\n 'like': 94,\n 'shadow': 145,\n 'doomed': 49,\n 'who': 187,\n 'green': 66,\n 'an': 9,\n 'arm': 13,\n 'injection': 83,\n 'self': 144,\n 'righteous': 139,\n 'spiritual': 155,\n 'moment': 105,\n 'sky': 151,\n 'dark': 37,\n 'air': 4,\n 'fresh': 60,\n 'factor': 54,\n 'after': 1,\n 'rained': 134,\n 'blue': 24,\n 'plaid': 131,\n 'shirt': 149,\n 'were': 185,\n 'broken': 25,\n 'into': 85,\n 'various': 178,\n 'shapes': 148,\n 'stationery': 157,\n 'from': 62,\n 'corner': 33,\n 'deep': 41,\n 'friendship': 61,\n 'declared': 40,\n 'encounter': 51,\n 'haven': 72,\n 'start': 156,\n 'planning': 132,\n 'youth': 195,\n 'our': 120}"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}