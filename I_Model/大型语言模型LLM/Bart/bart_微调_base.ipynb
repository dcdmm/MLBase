{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    model_name = '../all_models/bart-base/'\n",
    "    text_max_len = 768\n",
    "    summary_max_len = 256\n",
    "    batch_size = 8\n",
    "    epochs = 5\n",
    "    verbose = 100\n",
    "    lr = 1e-6  # 学习率\n",
    "    num_warmup_steps = 0\n",
    "    num_training_steps = math.ceil(14732 / batch_size) * epochs  # 向上取整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14732 818\n",
      "94 56\n",
      "Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "\n",
      "Amanda baked cookies and will bring Jerry some tomorrow.\n"
     ]
    }
   ],
   "source": [
    "class Dataset(Data.Dataset):\n",
    "    \"\"\"定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path):\n",
    "        with open(dataset_path) as f:\n",
    "            self.dataset = json.loads(f.read())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"定义索引方式\"\"\"\n",
    "        text = self.dataset[i]['dialogue']\n",
    "        summary = self.dataset[i]['summary']\n",
    "        return text, summary\n",
    "\n",
    "\n",
    "dataset_train = Dataset(\"../huggingface_dataset_samsum/train.json\")\n",
    "dataset_valid = Dataset(\"../huggingface_dataset_samsum/val.json\")\n",
    "\n",
    "print(len(dataset_train), len(dataset_valid))\n",
    "\n",
    "for text_, summary_ in dataset_train:\n",
    "    # 调用__getitem__方法\n",
    "    print(len(str(text_)), len(str(summary_)))\n",
    "    print(text_, end='\\n\\n')\n",
    "    print(summary_)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n",
      "BartTokenizer(name_or_path='../all_models/bart-base/', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True)\n",
      "139420416\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(CFG.model_name)\n",
    "print(tokenizer.model_input_names)\n",
    "print(tokenizer)\n",
    "\n",
    "model_bart = BartForConditionalGeneration.from_pretrained(CFG.model_name)\n",
    "model_bart = model_bart.to(device)\n",
    "print(model_bart.num_parameters())\n",
    "\n",
    "# 优化器\n",
    "optimizer_adamw = optim.AdamW(model_bart.parameters(), lr=CFG.lr)\n",
    "\n",
    "\n",
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n",
    "    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            # 学习率预热(线性增加)\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        # 学习率线性衰减(最小为0)\n",
    "        # num_training_steps后学习率恒为0\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "scheduler_lr = get_linear_schedule_with_warmup(optimizer_adamw, CFG.num_warmup_steps, CFG.num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842\n",
      "torch.Size([8, 233])\n",
      "torch.Size([8, 233])\n",
      "torch.Size([8, 49])\n"
     ]
    }
   ],
   "source": [
    "def get_collate_fn(tokenizer, text_max_len=512, summary_max_len=256):\n",
    "    \"\"\"返回collate_fun函数(通过闭包函数引入形参)\"\"\"\n",
    "\n",
    "    def collate_fn(data):\n",
    "        texts = [i[0] for i in data]\n",
    "        summarys = [i[1] for i in data]\n",
    "\n",
    "        texts_encode = tokenizer(texts,\n",
    "                                 max_length=text_max_len,\n",
    "                                 padding=True,\n",
    "                                 truncation=True,\n",
    "                                 return_tensors='pt')\n",
    "\n",
    "        summarys_encode = tokenizer(text_target=summarys,\n",
    "                                    max_length=summary_max_len,\n",
    "                                    padding=True,\n",
    "                                    truncation=True,\n",
    "                                    return_tensors='pt')\n",
    "\n",
    "        return {\"texts_input_ids\": texts_encode['input_ids'],\n",
    "                \"texts_attention_mask\": texts_encode['attention_mask'],\n",
    "                \"summarys_input_ids\": summarys_encode['input_ids']}\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "dataloader_valid = torch.utils.data.DataLoader(dataset=dataset_valid,\n",
    "                                               batch_size=CFG.batch_size,\n",
    "                                               collate_fn=get_collate_fn(tokenizer, CFG.text_max_len,\n",
    "                                                                         CFG.summary_max_len),\n",
    "                                               shuffle=False,\n",
    "                                               drop_last=False)\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset=dataset_train,\n",
    "                                               batch_size=CFG.batch_size,\n",
    "                                               collate_fn=get_collate_fn(tokenizer, CFG.text_max_len,\n",
    "                                                                         CFG.summary_max_len),\n",
    "                                               shuffle=True,\n",
    "                                               drop_last=False)\n",
    "\n",
    "print(len(dataloader_train))\n",
    "\n",
    "for i in dataloader_train:\n",
    "    print(i['texts_input_ids'].shape)  # [batch_size, text_max_len]\n",
    "    print(i['texts_attention_mask'].shape)\n",
    "    print(i['summarys_input_ids'].shape)  # [batch_size, summary_max_len]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    for idx, encode_data in enumerate(dataloader):\n",
    "        # 数据设备切换\n",
    "        input_ids = encode_data['texts_input_ids'].to(device)\n",
    "        attention_mask = encode_data['texts_attention_mask'].to(device)\n",
    "        labels = encode_data['summarys_input_ids'].to(device)\n",
    "        # replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "        labels[labels == tokenizer.pad_token_id] = - 100\n",
    "\n",
    "        '''\n",
    "        huggingfae源码:\n",
    "        if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            decoder_input_ids = shift_tokens_right(\n",
    "                labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "            )   ### Teacher Forcing \n",
    "        '''\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]  # BartForConditionalGeneration源码内封装损失函数:CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler_lr.step()\n",
    "\n",
    "        if idx % CFG.verbose == 0 and idx > 0:\n",
    "            print('| step {:5d} | loss {:8.5f} |'.format(idx, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型评估\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_blue_score = []\n",
    "    for idx, encode_data in enumerate(dataloader):\n",
    "        # 数据设备切换\n",
    "        input_ids = encode_data['texts_input_ids'].to(device)\n",
    "        attention_mask = encode_data['texts_attention_mask'].to(device)\n",
    "        labels = encode_data['summarys_input_ids'].to(device)\n",
    "\n",
    "        predictions = model.generate(input_ids=input_ids,\n",
    "                                     attention_mask=attention_mask,\n",
    "                                     max_length=256,\n",
    "                                     num_beams=2,\n",
    "                                     repetition_penalty=2.5,\n",
    "                                     length_penalty=1,\n",
    "                                     early_stopping=True)\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        all_blue_score.extend([sentence_bleu(references=[decoded_labels[i].split(' ')],\n",
    "                                             hypothesis=decoded_preds[i].split(' '),\n",
    "                                             weights=[0.25, 0.25, 0.25, 0.25],\n",
    "                                             smoothing_function=SmoothingFunction().method1)\n",
    "                               for i in range(predictions.shape[0])])\n",
    "\n",
    "    return np.array(all_blue_score).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step   100 | loss  2.50329 |\n",
      "| step   200 | loss  2.20362 |\n",
      "| step   300 | loss  2.45003 |\n",
      "| step   400 | loss  2.15884 |\n",
      "| step   500 | loss  2.31759 |\n",
      "| step   600 | loss  2.31060 |\n",
      "| step   700 | loss  2.31725 |\n",
      "| step   800 | loss  2.13361 |\n",
      "| step   900 | loss  2.23650 |\n",
      "| step  1000 | loss  2.02029 |\n",
      "| step  1100 | loss  1.68227 |\n",
      "| step  1200 | loss  2.31555 |\n",
      "| step  1300 | loss  2.15683 |\n",
      "| step  1400 | loss  1.78398 |\n",
      "| step  1500 | loss  2.19451 |\n",
      "| step  1600 | loss  1.88260 |\n",
      "| step  1700 | loss  1.83507 |\n",
      "| step  1800 | loss  1.82666 |\n",
      "------------------------------------------------------------\n",
      "| end of epoch     1 | time: 119.62s | valid blue  0.10503 |\n",
      "------------------------------------------------------------\n",
      "| step   100 | loss  2.10892 |\n",
      "| step   200 | loss  1.42785 |\n",
      "| step   300 | loss  1.94260 |\n",
      "| step   400 | loss  1.66786 |\n",
      "| step   500 | loss  1.98646 |\n",
      "| step   600 | loss  1.55752 |\n",
      "| step   700 | loss  1.92907 |\n",
      "| step   800 | loss  1.45522 |\n",
      "| step   900 | loss  2.08956 |\n",
      "| step  1000 | loss  1.88627 |\n",
      "| step  1100 | loss  1.59665 |\n",
      "| step  1200 | loss  1.79439 |\n",
      "| step  1300 | loss  1.57510 |\n",
      "| step  1400 | loss  1.79640 |\n",
      "| step  1500 | loss  1.63564 |\n",
      "| step  1600 | loss  1.60044 |\n",
      "| step  1700 | loss  1.45623 |\n",
      "| step  1800 | loss  1.18061 |\n",
      "------------------------------------------------------------\n",
      "| end of epoch     2 | time: 120.99s | valid blue  0.11075 |\n",
      "------------------------------------------------------------\n",
      "| step   100 | loss  1.57643 |\n",
      "| step   200 | loss  1.80415 |\n",
      "| step   300 | loss  1.89906 |\n",
      "| step   400 | loss  1.20961 |\n",
      "| step   500 | loss  1.14178 |\n",
      "| step   600 | loss  1.65653 |\n",
      "| step   700 | loss  1.45436 |\n",
      "| step   800 | loss  1.82643 |\n",
      "| step   900 | loss  1.52048 |\n",
      "| step  1000 | loss  1.20798 |\n",
      "| step  1100 | loss  1.83720 |\n",
      "| step  1200 | loss  1.58238 |\n",
      "| step  1300 | loss  1.51383 |\n",
      "| step  1400 | loss  1.74909 |\n",
      "| step  1500 | loss  1.42054 |\n",
      "| step  1600 | loss  1.49845 |\n",
      "| step  1700 | loss  1.55653 |\n",
      "| step  1800 | loss  1.80611 |\n",
      "------------------------------------------------------------\n",
      "| end of epoch     3 | time: 118.31s | valid blue  0.11906 |\n",
      "------------------------------------------------------------\n",
      "| step   100 | loss  1.36998 |\n",
      "| step   200 | loss  1.25346 |\n",
      "| step   300 | loss  1.66330 |\n",
      "| step   400 | loss  1.66540 |\n",
      "| step   500 | loss  1.82253 |\n",
      "| step   600 | loss  0.91348 |\n",
      "| step   700 | loss  1.43422 |\n",
      "| step   800 | loss  1.49610 |\n",
      "| step   900 | loss  1.15968 |\n",
      "| step  1000 | loss  1.55763 |\n",
      "| step  1100 | loss  1.80682 |\n",
      "| step  1200 | loss  1.19125 |\n",
      "| step  1300 | loss  1.64793 |\n",
      "| step  1400 | loss  1.67234 |\n",
      "| step  1500 | loss  1.31147 |\n",
      "| step  1600 | loss  1.36375 |\n",
      "| step  1700 | loss  1.55584 |\n",
      "| step  1800 | loss  1.32396 |\n",
      "------------------------------------------------------------\n",
      "| end of epoch     4 | time: 121.50s | valid blue  0.12726 |\n",
      "------------------------------------------------------------\n",
      "| step   100 | loss  1.36997 |\n",
      "| step   200 | loss  1.02548 |\n",
      "| step   300 | loss  1.12529 |\n",
      "| step   400 | loss  1.46023 |\n",
      "| step   500 | loss  1.19829 |\n",
      "| step   600 | loss  1.59073 |\n",
      "| step   700 | loss  1.36782 |\n",
      "| step   800 | loss  1.25559 |\n",
      "| step   900 | loss  1.48714 |\n",
      "| step  1000 | loss  1.53944 |\n",
      "| step  1100 | loss  1.35654 |\n",
      "| step  1200 | loss  1.17679 |\n",
      "| step  1300 | loss  1.22370 |\n",
      "| step  1400 | loss  1.45869 |\n",
      "| step  1500 | loss  1.40109 |\n",
      "| step  1600 | loss  1.41060 |\n",
      "| step  1700 | loss  1.44445 |\n",
      "| step  1800 | loss  1.46506 |\n",
      "------------------------------------------------------------\n",
      "| end of epoch     5 | time: 120.14s | valid blue  0.12964 |\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, CFG.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model_bart, dataloader_train, optimizer_adamw, device)\n",
    "    blue_val = evaluate(model_bart, dataloader_valid, device)\n",
    "\n",
    "    print('-' * 60)\n",
    "    print('| end of epoch {:5d} | time: {:5.2f}s | valid blue {:8.5f} |'.format(epoch,\n",
    "                                                                                time.time() - epoch_start_time,\n",
    "                                                                                blue_val))\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch13",
   "language": "python",
   "name": "torch13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}