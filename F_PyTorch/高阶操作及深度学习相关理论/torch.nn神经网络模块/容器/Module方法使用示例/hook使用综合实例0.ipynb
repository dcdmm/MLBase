{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = torch.normal(0, 1, size=(1000, 2))\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += torch.normal(0, 0.0001, size=labels.size()) # 添加噪音\n",
    "\n",
    "dataset = Data.TensorDataset(features, labels) # 数据包装\n",
    "\n",
    "data_iter = Data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module, ABC):\n",
    "    def __init__(self, n_feature):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(n_feature, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "net = LinearNet(2)\n",
    "loss = nn.MSELoss() # 损失函数为均方误差\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.03) # 优化器\n",
    "\n",
    "grad_list = list() # 模型每次梯度更新后的梯度\n",
    "input_list = list() # 模型前向传播前的输入\n",
    "weight_list = list() # 模型前向传播前的权重\n",
    "output_list = list() # 模型前向传播后的输出\n",
    "\n",
    "def grad_hook(grad):\n",
    "    grad_list.append(grad.clone())\n",
    "\n",
    "\n",
    "def forward_pre_hook(module, data_input):\n",
    "    b = module.weight.clone()\n",
    "    weight_list.append(b)\n",
    "    input_list.append(data_input)\n",
    "\n",
    "\n",
    "def forward_hook(module, data_input, data_output):\n",
    "    output_list.append(data_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 19.356615\n",
      "epoch 1, loss: 16.588722\n"
     ]
    }
   ],
   "source": [
    "# hook注册\n",
    "net.linear.weight.register_hook(grad_hook)\n",
    "net.linear.register_forward_pre_hook(forward_pre_hook)\n",
    "net.linear.register_forward_hook(forward_hook)\n",
    "\n",
    "for epoch in range(2):\n",
    "    l = 0\n",
    "    for X, y in data_iter:\n",
    "        output = net(X)\n",
    "        l = loss(output, y.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d, loss: %f' % (epoch, l.item()))\n",
    "    if l < 0.009:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "tensor([[0.5649, 0.2564]], grad_fn=<CloneBackward>)\n",
      "tensor([[0.8272, 0.0077]], grad_fn=<CloneBackward>)\n",
      "tensor([[ 0.8027, -0.4295]], grad_fn=<CloneBackward>)\n",
      "tensor([[ 0.8996, -0.4810]], grad_fn=<CloneBackward>)\n",
      "tensor([[ 0.8912, -0.5298]], grad_fn=<CloneBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(len(weight_list))\n",
    "for i in range(5):\n",
    "    print(weight_list[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "tensor([[-8.7409,  8.2897]])\n",
      "tensor([[ 0.8160, 14.5727]])\n",
      "tensor([[-3.2319,  1.7161]])\n",
      "tensor([[0.2816, 1.6280]])\n",
      "tensor([[-3.5301,  3.7507]])\n"
     ]
    }
   ],
   "source": [
    "print(len(grad_list))\n",
    "for i in range(5):\n",
    "     print(grad_list[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "(tensor([[ 0.7113,  0.4814],\n",
      "        [-0.6823, -0.3889],\n",
      "        [ 0.6921,  0.7602],\n",
      "        [ 1.5265, -1.1680],\n",
      "        [ 1.4280,  2.3250],\n",
      "        [ 1.5904, -1.7254],\n",
      "        [ 0.7305,  0.1300],\n",
      "        [-0.2738, -0.4559],\n",
      "        [-1.4554,  0.9210],\n",
      "        [ 0.9534, -0.0612]]),)\n",
      "(tensor([[-0.6283, -0.1799],\n",
      "        [ 0.5119, -0.6675],\n",
      "        [ 0.9803, -1.8322],\n",
      "        [ 0.6820,  0.5866],\n",
      "        [-0.5895, -1.8728],\n",
      "        [ 0.9480,  0.4579],\n",
      "        [-1.1571, -1.6336],\n",
      "        [-0.0024, -0.8734],\n",
      "        [-0.9567, -1.2456],\n",
      "        [ 0.0862, -0.3613]]),)\n",
      "(tensor([[ 0.0986, -0.5774],\n",
      "        [ 1.1459,  0.1607],\n",
      "        [ 0.0211, -0.6154],\n",
      "        [ 1.1798, -0.4697],\n",
      "        [ 0.0955,  2.1872],\n",
      "        [ 0.0987,  0.6975],\n",
      "        [ 0.1476,  1.2530],\n",
      "        [ 0.5516,  0.4058],\n",
      "        [-0.4137,  0.0755],\n",
      "        [ 0.3191,  0.8621]]),)\n",
      "(tensor([[-1.5407, -0.2731],\n",
      "        [ 0.0529,  0.9327],\n",
      "        [-0.0367, -0.7526],\n",
      "        [ 1.2101,  0.2804],\n",
      "        [-1.7342,  1.2527],\n",
      "        [-0.2329,  0.8468],\n",
      "        [-0.0986, -0.5983],\n",
      "        [-0.4033,  0.2065],\n",
      "        [-1.2104,  1.2632],\n",
      "        [-0.7609, -0.2172]]),)\n",
      "(tensor([[-0.8623, -0.5861],\n",
      "        [ 0.1020,  0.2834],\n",
      "        [-0.4047,  0.7923],\n",
      "        [ 0.7857,  0.0796],\n",
      "        [-0.7200,  0.2344],\n",
      "        [ 0.2483,  1.5497],\n",
      "        [ 1.0047, -1.0766],\n",
      "        [-0.9169, -0.4374],\n",
      "        [ 1.7466, -0.7594],\n",
      "        [-1.1313,  1.4553]]),)\n"
     ]
    }
   ],
   "source": [
    "print(len(input_list))\n",
    "for j in range(5):\n",
    "    print(input_list[j])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "tensor([[ 0.5253],\n",
      "        [-0.4852],\n",
      "        [ 0.5859],\n",
      "        [ 0.5629],\n",
      "        [ 1.4028],\n",
      "        [ 0.4561],\n",
      "        [ 0.4460],\n",
      "        [-0.2716],\n",
      "        [-0.5861],\n",
      "        [ 0.5230]], grad_fn=<MmBackward>)\n",
      "tensor([[-0.5211],\n",
      "        [ 0.4183],\n",
      "        [ 0.7968],\n",
      "        [ 0.5686],\n",
      "        [-0.5021],\n",
      "        [ 0.7877],\n",
      "        [-0.9697],\n",
      "        [-0.0087],\n",
      "        [-0.8009],\n",
      "        [ 0.0685]], grad_fn=<MmBackward>)\n",
      "tensor([[ 0.3271],\n",
      "        [ 0.8508],\n",
      "        [ 0.2812],\n",
      "        [ 1.1488],\n",
      "        [-0.8627],\n",
      "        [-0.2203],\n",
      "        [-0.4197],\n",
      "        [ 0.2685],\n",
      "        [-0.3645],\n",
      "        [-0.1142]], grad_fn=<MmBackward>)\n",
      "tensor([[-1.2547],\n",
      "        [-0.4010],\n",
      "        [ 0.3290],\n",
      "        [ 0.9539],\n",
      "        [-2.1627],\n",
      "        [-0.6168],\n",
      "        [ 0.1990],\n",
      "        [-0.4621],\n",
      "        [-1.6965],\n",
      "        [-0.5801]], grad_fn=<MmBackward>)\n",
      "tensor([[-0.4580],\n",
      "        [-0.0592],\n",
      "        [-0.7804],\n",
      "        [ 0.6580],\n",
      "        [-0.7658],\n",
      "        [-0.5998],\n",
      "        [ 1.4658],\n",
      "        [-0.5854],\n",
      "        [ 1.9589],\n",
      "        [-1.7793]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(len(output_list))\n",
    "for h in range(5):\n",
    "    print(output_list[h])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]], grad_fn=<SubBackward0>)"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前向传播前的输入 @ 前向传播前的权重.T = 前向传播后的输出\n",
    "input_list[0][0] @ weight_list[0].T - output_list[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}