{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## xgboost主要调节参数\n",
    "\n",
    "1. 其他参数\n",
    "    * booster\n",
    "    * n_jobs\n",
    "    * objective\n",
    "    * verbosity\n",
    "\n",
    "2. 树调节参数\n",
    "    * n_estimators\n",
    "    * max_depth(重要程度高)\n",
    "    * min_child_weight(重要程度高,与max_depth一起进行网格搜索)\n",
    "\n",
    "2. 防止过拟合参数\n",
    "    * gamma\n",
    "    * learning_rate(Learning rate shrinks the contribution of each tree by learning_rate)\n",
    "    * subsample\n",
    "    * colsample_bytree\n",
    "    * reg_alpha\n",
    "    * reg_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier  # XGBRegressor 同理\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = datasets.fetch_covtype().data[:3000]\n",
    "y = datasets.fetch_covtype().target[:3000]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(2250, 54)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape  # 数据集有54个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 2, 3, 4, 5, 6, 7])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)  # 7分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 1., 2., 3., 4., 5., 6.])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "y_train_new = enc.fit_transform(y_train.reshape(-1, 1))\n",
    "np.unique(y_train_new)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "y_test_new = enc.transform(y_test.reshape(-1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booster=gbtree,  score= 0.8333333333333334\n",
      "coef_ [[ 3.53587e-04 -9.98604e-05 -5.55529e-05 -1.69921e-04 -1.64735e-04\n",
      "  -2.32124e-04  4.63793e-04 -2.53355e-03 -1.18789e-03  7.77147e-03\n",
      "   6.73092e-03 -7.88560e-04  2.27223e-03 -3.38993e-04 -6.03233e-02\n",
      "  -2.33333e-02  5.65614e-02 -4.27438e-02  1.24921e-02  5.17271e-02\n",
      "  -3.91876e-02  1.86950e-03  2.23763e-03 -3.16162e-03 -3.83202e-02\n",
      "  -5.08220e-03  3.01689e-04 -1.81117e-03 -8.89841e-03 -2.21988e-03\n",
      "   1.11131e-02  3.07026e-02  3.33265e-03  2.42421e-03 -1.84742e-03\n",
      "   3.34636e-04  1.19614e-04 -1.77748e-03  3.98800e-03 -5.41300e-04\n",
      "  -7.51616e-04 -2.65181e-04 -2.02106e-03  2.12748e-04 -2.98124e-03\n",
      "   8.32792e-03  4.42223e-03 -6.29608e-03  3.12757e-03 -1.64373e-03\n",
      "  -2.29918e-04  3.54092e-03  1.64811e-03  2.16722e-03]\n",
      " [-3.41011e-03  4.60994e-03  2.05657e-04  2.18259e-03  5.88680e-03\n",
      "  -2.20931e-02 -2.53765e-03  4.03192e-03 -1.46478e-03  1.23012e-04\n",
      "   1.23198e-04 -4.08128e-03 -2.02230e-03 -1.14808e-04 -6.55815e-06\n",
      "   2.15381e-04 -2.43613e-01  3.39846e-01 -4.58731e+00 -1.85027e+01\n",
      "   7.12310e-01 -1.41182e+01 -4.20241e-01  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "  -2.42490e+00 -9.36636e-01 -1.20118e+00 -1.57780e+01  1.51425e+00\n",
      "   1.34928e+00 -1.32379e+01 -1.05851e+01 -3.26085e+00  4.29475e-01\n",
      "   1.99518e+00 -1.44059e+01  8.20828e-01 -1.16672e+01 -4.49004e+00\n",
      "  -1.21147e+01  1.33242e+00 -3.62573e-01 -5.49668e+00  4.75615e-01\n",
      "  -4.82536e+00 -4.25267e+00 -1.14737e+01  8.18882e-01]\n",
      " [-1.18985e+01 -5.16954e+00  1.27213e+00 -4.43969e+00 -5.89607e+00\n",
      "  -1.04732e+01  1.46986e+00  2.01574e+00 -7.17936e+00 -1.38267e+01\n",
      "  -6.04717e+00 -9.35922e+00  6.78266e-01 -1.26872e+01  3.04261e+00\n",
      "  -1.43829e+01  7.40061e-01 -4.43164e+00 -4.15954e+00 -1.15608e+01\n",
      "   9.54051e-01 -1.08851e+01 -5.22209e+00  5.99893e-01 -4.22327e+00\n",
      "  -3.94726e+00 -1.07535e+01  3.04487e+00 -1.14960e+01 -4.68777e+00\n",
      "  -1.00917e-01 -3.95339e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00 -5.89058e+00\n",
      "   7.05102e+00 -1.98453e+00 -2.09565e+00 -3.89146e+00 -2.81224e+00\n",
      "  -4.31954e+00 -5.87328e+00  8.47173e+00 -3.76068e+00 -3.12699e+00\n",
      "  -6.51926e+00 -4.02537e+00 -5.59950e+00 -9.65222e+00]\n",
      " [ 6.73668e-01  3.88886e-01  1.16740e+00 -1.70205e+00  3.53613e-01\n",
      "  -4.97309e+00 -4.39081e+00 -5.27608e+00 -3.52660e+00 -2.43068e+00\n",
      "   7.02368e+00 -5.61424e+00 -2.68789e+00  1.06786e-01  1.83213e+00\n",
      "  -6.90973e+00 -6.48169e+00 -1.38063e+01 -7.03520e+00 -1.42006e+01\n",
      "  -1.15213e+01  5.90995e-01  3.91022e+00 -4.14666e+00 -1.87030e+00\n",
      "   1.44162e+00 -4.52826e+00 -3.27589e+00 -4.35671e+00 -5.60434e+00\n",
      "   9.26584e+00 -3.75251e+00 -5.47150e+00 -3.17902e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  3.30661e-01  2.77374e-01 -3.34843e+00 -7.81457e+00\n",
      "   9.75106e-01 -6.28118e+00 -1.38856e+01 -6.40195e+00 -1.04876e+01\n",
      "   1.22771e+00  2.01382e+00 -7.12991e+00 -1.28310e+01]\n",
      " [-6.54019e+00 -1.39596e+01  2.17732e-01 -4.61723e+00 -4.13726e+00\n",
      "   2.38540e+00 -6.83791e+00 -1.42781e+01 -3.34395e-01  6.44055e-01\n",
      "  -6.69230e+00 -1.24458e+01  1.31215e+00 -1.16409e+01 -1.22177e+01\n",
      "   1.82958e-01  5.94790e-01 -8.96771e+00 -1.28661e+01 -2.34925e+00\n",
      "   2.08393e+00 -1.46908e+01  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  7.94485e-01\n",
      "   9.85041e-02 -5.26467e+00 -1.21576e+01 -1.26009e+01 -6.12360e+00\n",
      "  -1.36991e+01  2.15946e-01 -1.64356e-01 -8.56899e+00 -1.27578e+01\n",
      "   1.50460e+00 -1.29790e+01 -8.10793e-01  9.27296e-01  1.87381e+00\n",
      "  -9.01514e+00 -9.43487e+00 -1.31245e+00 -1.36948e+01 -1.22000e+01\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00]\n",
      " [ 0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00 -8.22676e+00  1.24223e+00 -8.64807e+00\n",
      "  -1.78824e+00  1.25563e+00 -1.44633e+01 -3.22687e+00  2.60856e-01\n",
      "   3.64080e-01 -8.90752e+00 -1.26271e+01  1.05621e+00 -8.18610e+00\n",
      "  -3.44777e-01  1.98564e-01  2.20179e-01 -9.25958e+00 -1.26862e+01\n",
      "   1.70735e+00 -7.86639e+00 -3.15177e+00  1.48354e+00  1.09382e+00\n",
      "  -9.21122e+00 -5.76378e+00 -2.82935e-02  4.23667e-01 -4.60257e+00\n",
      "  -1.11696e+01  2.34679e+00 -7.13023e+00 -5.49870e+00 -3.68639e-01\n",
      "  -1.30285e+01 -5.64662e+00  1.14827e+00  1.10307e+00]\n",
      " [-1.27043e+01 -4.52717e+00 -1.16380e+00  5.92993e-01 -4.70054e+00\n",
      "  -9.17869e+00  8.36718e-01 -5.32576e+00 -1.69195e+00 -1.21134e+01\n",
      "   1.63383e+00 -3.61865e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00\n",
      "   0.00000e+00  0.00000e+00  0.00000e+00  8.98410e-01 -1.52687e+00\n",
      "  -5.35798e+00 -1.25066e+01 -1.22169e+01 -6.44086e+00  2.48302e+00\n",
      "   5.04518e-01 -1.49815e+01 -5.16987e+00 -1.10089e+01 -1.33233e+01\n",
      "  -6.79404e+00  2.03216e+00  3.59665e-01 -1.36978e+01 -2.14180e+00\n",
      "  -3.84017e+00 -9.96576e+00 -4.63910e+00  2.33705e+00]]\n",
      "intercept_ [ 0.297171   0.281408  -0.630624  -0.421224   0.678056   0.0507381\n",
      " -1.17192  ]\n",
      "booster=gblinear,  score= 0.6613333333333333\n",
      "booster=dart,  score= 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# gbtree and dart use tree based models while gblinear uses linear functions.\n",
    "booster = ['gbtree', 'gblinear', 'dart']\n",
    "\n",
    "for i in booster:\n",
    "    xg = XGBClassifier(n_jobs=-1,  # xgboost虽属于boosting算法,但也可进行并行计算\n",
    "                       booster=i, use_label_encoder=False)  #  设置use_label_encoder=Fasle,和对标签进行标签编码避免警告\n",
    "\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    if i == 'gblinear':  # 此时没有feature_importances_属性\n",
    "        print(\"coef_\", xg.coef_)  # Coefficients property(当前仅当booster='gblinear')\n",
    "        print(\"intercept_\", xg.intercept_)  # Intercept (bias) property(当前仅当booster='gblinear')\n",
    "    print('booster=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# Verbosity of printing messages. Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug)\n",
    "xg = XGBClassifier(n_jobs=-1, use_label_encoder=False)  # 默认verbosity=1\n",
    "xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "print(xg.score(X_test, y_test_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective=reg:squarederror, score= 0.8333333333333334\n",
      "objective=binary:logistic, score= 0.8333333333333334\n",
      "objective=multi:softmax, score= 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# Specify the learning task and the corresponding learning objective\n",
    "# 可自定义\n",
    "objective = [\"reg:squarederror\",  # L2损失\n",
    "             \"binary:logistic\",  # logistic regression for binary classification, output probability\n",
    "             \"multi:softmax\"]  #  set XGBoost to do multiclass classification using the softmax objective\n",
    "\n",
    "for i in objective:\n",
    "    '''\n",
    "    objective : string, callable or None, optional (default=None)\n",
    "            Specify the learning task and the corresponding learning objective or\n",
    "            a custom objective function to be used (see note below).\n",
    "            Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
    "    '''\n",
    "    xgb = XGBClassifier(n_jobs=-1, use_label_encoder=False, objective=i)\n",
    "    xgb.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print(\"objective=\" + str(i) + ', score=', xgb.score(X_test, y_test_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10,  score= 0.8026666666666666\n",
      "n_estimators=20,  score= 0.816\n",
      "n_estimators=50,  score= 0.828\n",
      "n_estimators=100,  score= 0.8333333333333334\n",
      "n_estimators=200,  score= 0.8373333333333334\n",
      "n_estimators=500,  score= 0.8373333333333334\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [10, 20, 50, 100, 200, 500]  # 默认n_estimators=100\n",
    "\n",
    "for i in n_estimators:\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False,\n",
    "                       n_estimators=i)  # Number of gradient boosted trees\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('n_estimators=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=1,  score= 0.7626666666666667\n",
      "max_depth=3,  score= 0.8173333333333334\n",
      "max_depth=6,  score= 0.8333333333333334\n",
      "max_depth=9,  score= 0.84\n",
      "max_depth=12,  score= 0.8426666666666667\n",
      "max_depth=15,  score= 0.836\n",
      "max_depth=18,  score= 0.8373333333333334\n",
      "max_depth=21,  score= 0.8386666666666667\n"
     ]
    }
   ],
   "source": [
    "max_depth = [1, 3, 6, 9, 12, 15, 18, 21]  # 默认max_depth=6\n",
    "for i in max_depth:\n",
    "    # Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, max_depth=i)  #\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('max_depth=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_child_weight=0,  score= 0.8426666666666667\n",
      "min_child_weight=5,  score= 0.8266666666666667\n",
      "min_child_weight=10,  score= 0.8266666666666667\n",
      "min_child_weight=50,  score= 0.776\n",
      "min_child_weight=100,  score= 0.744\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = [0, 5, 10, 50, 100]  # 默认min_child_weight=1\n",
    "for i in min_child_weight:\n",
    "    \"\"\"\n",
    "    Minimum sum of instance weight (hessian) needed in a child. \n",
    "    If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, \n",
    "    then the building process will give up further partitioning. \n",
    "    In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. \n",
    "    The larger min_child_weight is, the more conservative the algorithm will be.\n",
    "    \"\"\"\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, min_child_weight=i)\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('min_child_weight=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma=0,  score= 0.8333333333333334\n",
      "gamma=0.3,  score= 0.8266666666666667\n",
      "gamma=0.9,  score= 0.82\n",
      "gamma=2.7,  score= 0.8106666666666666\n",
      "gamma=8.1,  score= 0.7746666666666666\n",
      "gamma=27.3,  score= 0.7386666666666667\n",
      "gamma=81.9,  score= 0.64\n"
     ]
    }
   ],
   "source": [
    "gamma = [0, 0.3, 0.9, 2.7, 8.1, 27.3, 81.9]  # 默认gamma=0\n",
    "for i in gamma:\n",
    "    # Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, gamma=i)\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('gamma=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learing_rate=0.01,  score= 0.7893333333333333\n",
      "learing_rate=0.02,  score= 0.792\n",
      "learing_rate=0.05,  score= 0.8146666666666667\n",
      "learing_rate=0.1,  score= 0.8213333333333334\n",
      "learing_rate=0.15,  score= 0.824\n",
      "learing_rate=0.3,  score= 0.8333333333333334\n",
      "learing_rate=0.5,  score= 0.8373333333333334\n",
      "learing_rate=0.7,  score= 0.8266666666666667\n",
      "learing_rate=0.9,  score= 0.8373333333333334\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.01, 0.02, 0.05, 0.1, 0.15, 0.3, 0.5, 0.7, 0.9]  # 默认learning_rate=0.3\n",
    "for i in learning_rate:\n",
    "    # Step size shrinkage used in update to prevents overfitting\n",
    "    # This is learning_rate parameters in the GBDT\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, learning_rate=i)  #\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('learing_rate=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample=0.1,  score= 0.8\n",
      "subsample=0.3,  score= 0.8186666666666667\n",
      "subsample=0.4,  score= 0.8213333333333334\n",
      "subsample=0.6,  score= 0.8333333333333334\n",
      "subsample=0.7,  score= 0.8333333333333334\n",
      "subsample=0.8,  score= 0.84\n",
      "subsample=0.85,  score= 0.8386666666666667\n",
      "subsample=0.95,  score= 0.8386666666666667\n",
      "subsample=1,  score= 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "subsample = [0.1, 0.3, 0.4, 0.6, 0.7, 0.8, 0.85, 0.95, 1]  # 默认subsample=1\n",
    "for i in subsample:\n",
    "    # Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting.\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, subsample=i)\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('subsample=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colsample_bytree=0.1,  score= 0.8\n",
      "colsample_bytree=0.3,  score= 0.8346666666666667\n",
      "colsample_bytree=0.4,  score= 0.836\n",
      "colsample_bytree=0.6,  score= 0.8466666666666667\n",
      "colsample_bytree=0.7,  score= 0.84\n",
      "colsample_bytree=0.8,  score= 0.8413333333333334\n",
      "colsample_bytree=0.85,  score= 0.8346666666666667\n",
      "colsample_bytree=0.95,  score= 0.8373333333333334\n",
      "colsample_bytree=1,  score= 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = [0.1, 0.3, 0.4, 0.6, 0.7, 0.8, 0.85, 0.95, 1]  # 默认colsample_bytree=1\n",
    "for i in subsample:\n",
    "    # colsample_bytree is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, colsample_bytree=i)  # column (feature) subsampling,即列采样比率\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('colsample_bytree=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_alpha=0,  score= 0.8333333333333334\n",
      "reg_alpha=0.25,  score= 0.8413333333333334\n",
      "reg_alpha=0.5,  score= 0.836\n",
      "reg_alpha=0.75,  score= 0.8346666666666667\n",
      "reg_alpha=1,  score= 0.8266666666666667\n",
      "reg_alpha=3,  score= 0.8266666666666667\n",
      "reg_alpha=9,  score= 0.7933333333333333\n"
     ]
    }
   ],
   "source": [
    "reg_alpha = [0, 0.25, 0.5, 0.75, 1, 3, 9]  # 默认reg_alpha=0\n",
    "for i in reg_alpha:\n",
    "    # L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, reg_alpha=i)\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('reg_alpha=' + str(i) + ',  score=', xg.score(X_test, y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_lambda=0,  score= 0.832\n",
      "reg_lambda=1,  score= 0.8333333333333334\n",
      "reg_lambda=3,  score= 0.832\n",
      "reg_lambda=9,  score= 0.8306666666666667\n",
      "reg_lambda=27,  score= 0.832\n",
      "reg_lambda=81,  score= 0.804\n"
     ]
    }
   ],
   "source": [
    "reg_lambda = [0, 1, 3, 9, 27, 81]  # 默认reg_lambda=1\n",
    "for i in reg_lambda:\n",
    "    # L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, reg_lambda=i)\n",
    "    xg.fit(X_train, y_train_new, eval_metric='mlogloss')\n",
    "    print('reg_lambda=' + str(i) + ',  score=', xg.score(X_test, y_test_new))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}