{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f96e29-0e7d-493f-b15a-da6a9bb6c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, BertConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import jsonlines\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import spearmanr\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2ef4d3-87e1-4d1a-836a-528354e2857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caaef9c2-9c61-49a3-84ee-63c0a9b6ca35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "MAXLEN = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c31a7af4-767b-40f7-b26f-287da710379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一个女人正走在街对面吃香蕉，而一个男人正紧跟在他的公文包后面。\n",
      "('一个戴着安全帽的男人在跳舞。', '一个戴着安全帽的男人在跳舞。', '5\\n')\n"
     ]
    }
   ],
   "source": [
    "def load_snli_data(filename):\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        return [line['origin'] for line in jsonlines.Reader(f)]\n",
    "\n",
    "\n",
    "def load_sts_data(filename):\n",
    "    with open(filename, 'r', encoding='utf8') as f:            \n",
    "        return [(line.split(\"||\")[1], line.split(\"||\")[2], line.split(\"||\")[3]) for line in f]   \n",
    "    \n",
    "\n",
    "# 增大预料规模,同时模型训练的同时增大batch_size\n",
    "# 解释:语料[a0, a1, a2, ......, an]应该是各不相同的,现实语料中存在相似样本\n",
    "# 如:a_v0, a_v0^'互为正样本,剩下a_v1, a_v1^',a_v2, a_v2^',......为负样本,此时若a_v1, a_v1^',a_v2, a_v2^',......中存在相似样本,则将产生干扰\n",
    "train_data_snli = load_snli_data('cnsd-snli-process/train.jsonl')\n",
    "train_data_sts = load_sts_data('STS-B/cnsd-sts-train.txt')\n",
    "train_data = train_data_snli + [_[0] for _ in train_data_sts]  # 两个数据集数据组合\n",
    "\n",
    "dev_data = load_sts_data('STS-B/cnsd-sts-dev.txt')\n",
    "test_data = load_sts_data('STS-B/cnsd-sts-test.txt')   \n",
    "\n",
    "print(train_data[0])\n",
    "print(dev_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6ee7b4e-d9c8-4b03-93f3-2e4bc1d6276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask']\n",
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "print(TOKENIZER.model_input_names)\n",
    "print(TOKENIZER.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909de640-e438-4bee-ab51-458ab1abca05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \"\"\"定义训练数据集\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def text_2_id(self, text):\n",
    "        # 将text分别输入到编码器中两次(每一个句子进行两次前向传播,得到两个不同的embeddings向量,互为正样本)\n",
    "        return TOKENIZER([text, text], max_length=MAXLEN, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.text_2_id(self.data[index])\n",
    "    \n",
    "    \n",
    "train_dataloader = DataLoader(TrainDataset(train_data), batch_size=BATCH_SIZE, shuffle=True)\n",
    "for i in train_dataloader:\n",
    "    print(i['input_ids'].shape)  # i['input_ids'].shape=[batch_size, 2, seq_len]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d32334-9ee4-4833-bec0-5346fc2a564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 64])\n",
      "torch.Size([128, 1, 64])\n",
      "tensor([5, 4, 2, 2, 2, 5, 2, 5, 3, 1, 5, 4, 0, 2, 5, 4, 3, 1, 3, 2, 1, 1, 4, 1,\n",
      "        2, 5, 5, 4, 3, 5, 5, 1, 4, 1, 3, 1, 0, 4, 4, 0, 4, 5, 0, 1, 0, 3, 0, 3,\n",
      "        4, 2, 3, 3, 0, 4, 2, 4, 3, 1, 0, 4, 2, 3, 1, 2, 2, 0, 3, 5, 0, 2, 4, 2,\n",
      "        3, 2, 1, 1, 0, 2, 0, 0, 3, 0, 0, 3, 2, 0, 3, 1, 0, 0, 0, 0, 0, 4, 3, 1,\n",
      "        2, 0, 1, 2, 0, 1, 0, 3, 5, 5, 5, 3, 2, 3, 1, 5, 5, 2, 5, 1, 1, 3, 2, 1,\n",
      "        4, 3, 4, 0, 1, 0, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"定义测试数据集\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def text_2_id(self, text):\n",
    "        return TOKENIZER(text, max_length=MAXLEN, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        da = self.data[index]        \n",
    "        return self.text_2_id([da[0]]), self.text_2_id([da[1]]), int(da[2])\n",
    "\n",
    "    \n",
    "dev_dataloader = DataLoader(TestDataset(dev_data), batch_size=BATCH_SIZE)\n",
    "for i, j, k in dev_dataloader:\n",
    "    print(i['input_ids'].shape)  # i['input_ids'].shape=[batch_size, 1, seq_len]\n",
    "    print(j['input_ids'].shape)\n",
    "    print(k)  # k.shape=[batch_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbda9ac6-d56e-4225-892a-a8d270cb7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimcseModel(nn.Module):\n",
    "    \"\"\"Simcse无监督模型,超参数参考见:https://kexue.fm/archives/8348\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained_model, pooling, dropout_ratio=0.1):\n",
    "        super(SimcseModel, self).__init__()\n",
    "        config = BertConfig.from_pretrained(pretrained_model)\n",
    "        \n",
    "        # 修改config的dropout参数\n",
    "        config.attention_probs_dropout_prob = dropout_ratio\n",
    "        config.hidden_dropout_prob = dropout_ratio          \n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model, config=config)\n",
    "        self.pooling = pooling\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        out = self.bert(input_ids, attention_mask, token_type_ids, output_hidden_states=True)\n",
    "\n",
    "        if self.pooling == 'cls':\n",
    "            return out.last_hidden_state[:, 0]  # shape=[batch_size, hidden_size]\n",
    "        \n",
    "        if self.pooling == 'pooler':\n",
    "            return out.pooler_output  # shape=[batch_size, hidden_size]\n",
    "        \n",
    "        if self.pooling == 'last-avg':\n",
    "            last = out.last_hidden_state.transpose(1, 2) # shape=[batch_size, hidden_size, seq_len]\n",
    "            return torch.avg_pool1d(last, kernel_size=last.shape[-1]).squeeze(-1)  # shape=[batch_size, hidden_size]\n",
    "        \n",
    "        if self.pooling == 'first-last-avg':\n",
    "            first = out.hidden_states[1].transpose(1, 2)  # shape=[batch_size, hidden_size, seq_len]\n",
    "            last = out.hidden_states[-1].transpose(1, 2)              \n",
    "            first_avg = torch.avg_pool1d(first, kernel_size=last.shape[-1]).squeeze(-1)  # shape=[batch_size, hidden_size]\n",
    "            last_avg = torch.avg_pool1d(last, kernel_size=last.shape[-1]).squeeze(-1)   # shape=[batch_size, hidden_size]\n",
    "            avg = torch.cat((first_avg.unsqueeze(1), last_avg.unsqueeze(1)), dim=1)     # shape=[batch_size, 2, hidden_size]\n",
    "            return torch.avg_pool1d(avg.transpose(1, 2), kernel_size=2).squeeze(-1)     # shape=[batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "980890f5-a18c-4970-aedc-8053e6dd16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simcse_unsup_loss(y_pred, temperature=0.05):\n",
    "    \"\"\"损失函数(无监督)\"\"\"\n",
    "    # y_pred.shape=[BATCH_SIZE * 2, hidden_size]\n",
    "    y_true = torch.arange(y_pred.shape[0], device=DEVICE)\n",
    "    # example:[1, 0, 3, 2, 5, 4, 7, 6, xxxxxx]\n",
    "    y_true = (y_true - y_true % 2 * 2) + 1\n",
    "    # sim.shape=[BATCH_SIZE * 2, BATCH_SIZE * 2]\n",
    "    sim = F.cosine_similarity(y_pred.unsqueeze(1), y_pred.unsqueeze(0), dim=-1)\n",
    "    # 相似度矩阵对角线处元素设置为很小的值(消除自身影响)\n",
    "    sim = sim - torch.eye(y_pred.shape[0], device=DEVICE) * 1e12\n",
    "    # 相似度矩阵除以温度系数\n",
    "    sim = sim / temperature\n",
    "    loss = F.cross_entropy(sim, y_true)  # 理解:将同一句子得到的embeddings向量对作为正样本对,其他句子得到的embeddings向量对作为负样本\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a1b0ac-d2c0-4414-ad37-1013475dcbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SimcseModel(pretrained_model='hfl/chinese-roberta-wwm-ext', pooling='cls', dropout_ratio=0.3)\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "780af8cd-9105-4500-9201-b0afd238d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型验证\n",
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    sim_tensor = torch.tensor([], device=DEVICE)\n",
    "    label_array = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for source, target, label in dataloader:\n",
    "            # label.shape=[batch_size]\n",
    "            # source_input_ids.shape=[batch_size, seq_len]\n",
    "            source_input_ids = source['input_ids'].squeeze(1).to(DEVICE)\n",
    "            source_attention_mask = source['attention_mask'].squeeze(1).to(DEVICE)\n",
    "            source_token_type_ids = source['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "            # source_pred.shape=[batch_size, hidden_size]\n",
    "            source_pred = model(source_input_ids, source_attention_mask, source_token_type_ids)\n",
    "            \n",
    "            # target_input_ids.shape=[batch_size, seq_len]\n",
    "            target_input_ids = target['input_ids'].squeeze(1).to(DEVICE)\n",
    "            target_attention_mask = target['attention_mask'].squeeze(1).to(DEVICE)\n",
    "            target_token_type_ids = target['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "            # target_pred.shape=[batch_size, hidden_size]\n",
    "            target_pred = model(target_input_ids, target_attention_mask, target_token_type_ids)\n",
    "            # sim.shape=[batch_size]\n",
    "            sim = F.cosine_similarity(source_pred, target_pred, dim=-1)  # result:是否相似\n",
    "            sim_tensor = torch.cat((sim_tensor, sim), dim=0)\n",
    "            label_array = np.append(label_array, np.array(label))  \n",
    "    \n",
    "    return spearmanr(label_array, sim_tensor.cpu().numpy()).correlation  # 斯皮尔曼相关系数(无序)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8268b536-24d7-4a20-95d1-58334053afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练与评估\n",
    "def train(model, train_dl, dev_dl, optimizer, best):\n",
    "    model.train()\n",
    "    \n",
    "    early_stop_batch = 0\n",
    "    for batch_idx, source in enumerate(train_dl, start=1):\n",
    "        real_batch_num = source['input_ids'].shape[0]\n",
    "        input_ids = source['input_ids'].view(real_batch_num * 2, -1).to(DEVICE)\n",
    "        attention_mask = source['attention_mask'].view(real_batch_num * 2, -1).to(DEVICE)\n",
    "        token_type_ids = source['token_type_ids'].view(real_batch_num * 2, -1).to(DEVICE)\n",
    "\n",
    "        out = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = simcse_unsup_loss(out)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('| step {:5d} | loss {:8.5f} |'.format(batch_idx, loss.item()))\n",
    "            corrcoef = eval(model, dev_dl)\n",
    "            model.train()\n",
    "            if best[0] < corrcoef:\n",
    "                best.clear()\n",
    "                best.append(corrcoef)\n",
    "                best.append(copy.deepcopy(model.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae3d0c48-152a-42c7-84de-558370bcca5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************0********************\n",
      "| step    10 | loss  1.65872 |\n",
      "| step    20 | loss  0.91445 |\n",
      "| step    30 | loss  0.65556 |\n",
      "| step    40 | loss  0.47245 |\n",
      "| step    50 | loss  0.33608 |\n",
      "| step    60 | loss  0.34597 |\n",
      "| step    70 | loss  0.24724 |\n",
      "| step    80 | loss  0.24938 |\n",
      "| step    90 | loss  0.19374 |\n",
      "| step   100 | loss  0.19065 |\n",
      "| step   110 | loss  0.21806 |\n",
      "| step   120 | loss  0.20168 |\n",
      "| step   130 | loss  0.16607 |\n",
      "| step   140 | loss  0.18110 |\n",
      "| step   150 | loss  0.14478 |\n",
      "| step   160 | loss  0.12090 |\n",
      "| step   170 | loss  0.13810 |\n",
      "| step   180 | loss  0.10538 |\n",
      "| step   190 | loss  0.11358 |\n",
      "| step   200 | loss  0.12241 |\n",
      "| step   210 | loss  0.10192 |\n",
      "| step   220 | loss  0.10163 |\n",
      "| step   230 | loss  0.09620 |\n",
      "| step   240 | loss  0.08635 |\n",
      "| step   250 | loss  0.08812 |\n",
      "| step   260 | loss  0.09918 |\n",
      "| step   270 | loss  0.08029 |\n",
      "| step   280 | loss  0.09616 |\n",
      "| step   290 | loss  0.08034 |\n",
      "| step   300 | loss  0.08143 |\n",
      "| step   310 | loss  0.07615 |\n",
      "| step   320 | loss  0.06814 |\n",
      "| step   330 | loss  0.06051 |\n",
      "| step   340 | loss  0.07804 |\n",
      "| step   350 | loss  0.07863 |\n",
      "| step   360 | loss  0.06526 |\n",
      "| step   370 | loss  0.06418 |\n",
      "| step   380 | loss  0.06852 |\n",
      "| step   390 | loss  0.06690 |\n",
      "| step   400 | loss  0.06313 |\n",
      "| step   410 | loss  0.06043 |\n",
      "| step   420 | loss  0.06039 |\n",
      "| step   430 | loss  0.06451 |\n",
      "| step   440 | loss  0.05686 |\n",
      "| step   450 | loss  0.05780 |\n",
      "| step   460 | loss  0.06044 |\n",
      "| step   470 | loss  0.04726 |\n",
      "| step   480 | loss  0.05172 |\n",
      "| step   490 | loss  0.05345 |\n",
      "| step   500 | loss  0.04643 |\n",
      "| step   510 | loss  0.04477 |\n",
      "| step   520 | loss  0.04430 |\n",
      "| step   530 | loss  0.04303 |\n",
      "| step   540 | loss  0.04764 |\n",
      "| step   550 | loss  0.04816 |\n",
      "| step   560 | loss  0.06136 |\n",
      "| step   570 | loss  0.04527 |\n",
      "| step   580 | loss  0.04859 |\n",
      "| step   590 | loss  0.04367 |\n",
      "| step   600 | loss  0.05245 |\n",
      "| step   610 | loss  0.04128 |\n",
      "| step   620 | loss  0.03333 |\n",
      "| step   630 | loss  0.03876 |\n",
      "| step   640 | loss  0.04061 |\n",
      "| step   650 | loss  0.04191 |\n",
      "| step   660 | loss  0.03825 |\n",
      "| step   670 | loss  0.03773 |\n",
      "| step   680 | loss  0.03643 |\n",
      "| step   690 | loss  0.03880 |\n",
      "| step   700 | loss  0.03649 |\n",
      "| step   710 | loss  0.04125 |\n",
      "| step   720 | loss  0.03381 |\n",
      "| step   730 | loss  0.03383 |\n",
      "| step   740 | loss  0.03011 |\n",
      "| step   750 | loss  0.04002 |\n",
      "| step   760 | loss  0.03575 |\n",
      "| step   770 | loss  0.03055 |\n",
      "| step   780 | loss  0.03154 |\n",
      "| step   790 | loss  0.03796 |\n",
      "| step   800 | loss  0.02604 |\n",
      "| step   810 | loss  0.03046 |\n",
      "| step   820 | loss  0.03192 |\n",
      "| step   830 | loss  0.02790 |\n",
      "| step   840 | loss  0.03746 |\n",
      "| step   850 | loss  0.03819 |\n",
      "| step   860 | loss  0.02445 |\n",
      "| step   870 | loss  0.02770 |\n",
      "| step   880 | loss  0.03398 |\n",
      "| step   890 | loss  0.03040 |\n",
      "| step   900 | loss  0.02846 |\n",
      "| step   910 | loss  0.03113 |\n",
      "| step   920 | loss  0.02193 |\n",
      "| step   930 | loss  0.03048 |\n",
      "| step   940 | loss  0.02176 |\n",
      "| step   950 | loss  0.02132 |\n",
      "| step   960 | loss  0.02643 |\n",
      "| step   970 | loss  0.02808 |\n",
      "| step   980 | loss  0.02224 |\n",
      "| step   990 | loss  0.02322 |\n",
      "| step  1000 | loss  0.01922 |\n",
      "| step  1010 | loss  0.02035 |\n",
      "| step  1020 | loss  0.02160 |\n",
      "| step  1030 | loss  0.02277 |\n",
      "| step  1040 | loss  0.01957 |\n",
      "| step  1050 | loss  0.02324 |\n",
      "| step  1060 | loss  0.01921 |\n",
      "| step  1070 | loss  0.01708 |\n",
      "| step  1080 | loss  0.02228 |\n",
      "| step  1090 | loss  0.01762 |\n",
      "| step  1100 | loss  0.01694 |\n",
      "| step  1110 | loss  0.02031 |\n",
      "| step  1120 | loss  0.01954 |\n",
      "| step  1130 | loss  0.02234 |\n",
      "| step  1140 | loss  0.02381 |\n",
      "| step  1150 | loss  0.01589 |\n",
      "| step  1160 | loss  0.02152 |\n",
      "| step  1170 | loss  0.01734 |\n",
      "| step  1180 | loss  0.01661 |\n",
      "********************1********************\n",
      "| step    10 | loss  0.01539 |\n",
      "| step    20 | loss  0.01613 |\n",
      "| step    30 | loss  0.01869 |\n",
      "| step    40 | loss  0.02339 |\n",
      "| step    50 | loss  0.01509 |\n",
      "| step    60 | loss  0.01622 |\n",
      "| step    70 | loss  0.01388 |\n",
      "| step    80 | loss  0.01770 |\n",
      "| step    90 | loss  0.01741 |\n",
      "| step   100 | loss  0.02012 |\n",
      "| step   110 | loss  0.01583 |\n",
      "| step   120 | loss  0.01931 |\n",
      "| step   130 | loss  0.01657 |\n",
      "| step   140 | loss  0.01550 |\n",
      "| step   150 | loss  0.01566 |\n",
      "| step   160 | loss  0.01731 |\n",
      "| step   170 | loss  0.01772 |\n",
      "| step   180 | loss  0.01185 |\n",
      "| step   190 | loss  0.01557 |\n",
      "| step   200 | loss  0.01301 |\n",
      "| step   210 | loss  0.01240 |\n",
      "| step   220 | loss  0.02352 |\n",
      "| step   230 | loss  0.01505 |\n",
      "| step   240 | loss  0.01557 |\n",
      "| step   250 | loss  0.01835 |\n",
      "| step   260 | loss  0.01209 |\n",
      "| step   270 | loss  0.01286 |\n",
      "| step   280 | loss  0.01651 |\n",
      "| step   290 | loss  0.01475 |\n",
      "| step   300 | loss  0.01200 |\n",
      "| step   310 | loss  0.01910 |\n",
      "| step   320 | loss  0.01138 |\n",
      "| step   330 | loss  0.01219 |\n",
      "| step   340 | loss  0.01323 |\n",
      "| step   350 | loss  0.01684 |\n",
      "| step   360 | loss  0.01220 |\n",
      "| step   370 | loss  0.01660 |\n",
      "| step   380 | loss  0.01399 |\n",
      "| step   390 | loss  0.01551 |\n",
      "| step   400 | loss  0.01486 |\n",
      "| step   410 | loss  0.01645 |\n",
      "| step   420 | loss  0.01200 |\n",
      "| step   430 | loss  0.01115 |\n",
      "| step   440 | loss  0.01724 |\n",
      "| step   450 | loss  0.01383 |\n",
      "| step   460 | loss  0.01356 |\n",
      "| step   470 | loss  0.01235 |\n",
      "| step   480 | loss  0.01162 |\n",
      "| step   490 | loss  0.01280 |\n",
      "| step   500 | loss  0.01008 |\n",
      "| step   510 | loss  0.00950 |\n",
      "| step   520 | loss  0.00929 |\n",
      "| step   530 | loss  0.01605 |\n",
      "| step   540 | loss  0.01439 |\n",
      "| step   550 | loss  0.01640 |\n",
      "| step   560 | loss  0.01105 |\n",
      "| step   570 | loss  0.01004 |\n",
      "| step   580 | loss  0.01186 |\n",
      "| step   590 | loss  0.01591 |\n",
      "| step   600 | loss  0.00880 |\n",
      "| step   610 | loss  0.01850 |\n",
      "| step   620 | loss  0.01639 |\n",
      "| step   630 | loss  0.02542 |\n",
      "| step   640 | loss  0.01004 |\n",
      "| step   650 | loss  0.01174 |\n",
      "| step   660 | loss  0.01313 |\n",
      "| step   670 | loss  0.01041 |\n",
      "| step   680 | loss  0.01230 |\n",
      "| step   690 | loss  0.00888 |\n",
      "| step   700 | loss  0.01266 |\n",
      "| step   710 | loss  0.01254 |\n",
      "| step   720 | loss  0.01318 |\n",
      "| step   730 | loss  0.01026 |\n",
      "| step   740 | loss  0.01052 |\n",
      "| step   750 | loss  0.01413 |\n",
      "| step   760 | loss  0.00817 |\n",
      "| step   770 | loss  0.00875 |\n",
      "| step   780 | loss  0.00972 |\n",
      "| step   790 | loss  0.01021 |\n",
      "| step   800 | loss  0.00942 |\n",
      "| step   810 | loss  0.00886 |\n",
      "| step   820 | loss  0.00922 |\n",
      "| step   830 | loss  0.01190 |\n",
      "| step   840 | loss  0.00968 |\n",
      "| step   850 | loss  0.00995 |\n",
      "| step   860 | loss  0.01072 |\n",
      "| step   870 | loss  0.00915 |\n",
      "| step   880 | loss  0.01060 |\n",
      "| step   890 | loss  0.00770 |\n",
      "| step   900 | loss  0.00994 |\n",
      "| step   910 | loss  0.00762 |\n",
      "| step   920 | loss  0.00968 |\n",
      "| step   930 | loss  0.00763 |\n",
      "| step   940 | loss  0.00810 |\n",
      "| step   950 | loss  0.00754 |\n",
      "| step   960 | loss  0.00730 |\n",
      "| step   970 | loss  0.00959 |\n",
      "| step   980 | loss  0.00909 |\n",
      "| step   990 | loss  0.00944 |\n",
      "| step  1000 | loss  0.00992 |\n",
      "| step  1010 | loss  0.00910 |\n",
      "| step  1020 | loss  0.01159 |\n",
      "| step  1030 | loss  0.01276 |\n",
      "| step  1040 | loss  0.01051 |\n",
      "| step  1050 | loss  0.00857 |\n",
      "| step  1060 | loss  0.00939 |\n",
      "| step  1070 | loss  0.00856 |\n",
      "| step  1080 | loss  0.00921 |\n",
      "| step  1090 | loss  0.01790 |\n",
      "| step  1100 | loss  0.00934 |\n",
      "| step  1110 | loss  0.00904 |\n",
      "| step  1120 | loss  0.01013 |\n",
      "| step  1130 | loss  0.00935 |\n",
      "| step  1140 | loss  0.01207 |\n",
      "| step  1150 | loss  0.01379 |\n",
      "| step  1160 | loss  0.00954 |\n",
      "| step  1170 | loss  0.00719 |\n",
      "| step  1180 | loss  0.02008 |\n"
     ]
    }
   ],
   "source": [
    "best = [0.0, None]  # 验证数据集最优相关系数与对应模型的状态字典\n",
    "for epoch in range(2):\n",
    "    print('*' * 20 + str(epoch) + '*' * 20)\n",
    "    train(model, train_dataloader, dev_dataloader, optimizer, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f77578ee-1799-49e5-8fb8-d5113b14580a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7457631647557355"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d1e4ec1-01c9-4478-ae06-5fe437e86389",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(TestDataset(test_data), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "418d30a5-fdb0-4c6e-a91b-3439749257e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_corrcoef: 0.74576316\n",
      "test_corrcoef: 0.69478349\n"
     ]
    }
   ],
   "source": [
    "best_model = SimcseModel(pretrained_model='hfl/chinese-roberta-wwm-ext', pooling='cls')\n",
    "best_model.load_state_dict(best[1])\n",
    "best_model.to(DEVICE)\n",
    "\n",
    "# 最优\n",
    "dev_corrcoef_best = eval(best_model, dev_dataloader)\n",
    "test_corrcoef_best = eval(best_model, test_dataloader)\n",
    "print(f'dev_corrcoef: {dev_corrcoef_best:.8f}')\n",
    "print(f'test_corrcoef: {test_corrcoef_best:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b49e804-3067-4e37-a304-828ca4c5bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_corrcoef: 0.71067608\n",
      "test_corrcoef: 0.66283171\n"
     ]
    }
   ],
   "source": [
    "dev_corrcoef_last = eval(model, dev_dataloader)\n",
    "test_corrcoef_last = eval(model, test_dataloader)\n",
    "\n",
    "# 未早停\n",
    "print(f'dev_corrcoef: {dev_corrcoef_last:.8f}')\n",
    "print(f'test_corrcoef: {test_corrcoef_last:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b99aefd-f7c2-487e-883b-e8a9e93092e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_corrcoef: 0.71675741\n",
      "test_corrcoef: 0.68251387\n"
     ]
    }
   ],
   "source": [
    "model_init = SimcseModel(pretrained_model='hfl/chinese-roberta-wwm-ext', pooling='cls')\n",
    "model_init.to(DEVICE)\n",
    "dev_corrcoef_init = eval(model_init, dev_dataloader)\n",
    "test_corrcoef_init = eval(model_init, test_dataloader)\n",
    "\n",
    "# 未训练\n",
    "print(f'dev_corrcoef: {dev_corrcoef_init:.8f}')\n",
    "print(f'test_corrcoef: {test_corrcoef_init:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a13666-9b33-4b8d-86bd-d1261f93ed0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3812",
   "language": "python",
   "name": "env_3812"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
