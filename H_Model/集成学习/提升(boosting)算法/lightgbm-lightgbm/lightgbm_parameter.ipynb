{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## lightgbm主要调节参数\n",
    "\n",
    "1. 其他参数\n",
    "    * boosting_type\n",
    "    * n_jobs==>xgboost\n",
    "    * verbosity==>xgboost\n",
    "    * objective==>xgboost\n",
    "\n",
    "2. 树调节参数\n",
    "    * n_estimators==>xgboost\n",
    "    * max_depth(重要程度高)==>xgboost\n",
    "    * min_child_weight(重要程度高,与max_depth一起进行网格搜索)==>xgboost\n",
    "    * min_child_samples\n",
    "    * num_leaves\n",
    "\n",
    "2. 防止过拟合参数\n",
    "    * subsample==>xgboost\n",
    "    * subsample_freq\n",
    "    * learning_rate==>xgboost\n",
    "    * colsample_bytree==>xgboost\n",
    "    * reg_alpha=0.0==>xgboost\n",
    "    * reg_lambda=0.0==>xgboost\n",
    "    * max_bin"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "X = datasets.fetch_covtype().data[:3000]\n",
    "y = datasets.fetch_covtype().target[:3000]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250, 54)\n",
      "[1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)  # 数据集有54个特征\n",
    "print(np.unique(y_train))  # 7分类"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booster=gbdt,  score= 0.8533333333333334\n",
      "booster=rf,  score= 0.7786666666666666\n",
      "booster=dart,  score= 0.8493333333333334\n",
      "booster=goss,  score= 0.836\n"
     ]
    }
   ],
   "source": [
    "boosting_type = ['gbdt', 'rf', 'dart', 'goss']\n",
    "\n",
    "for i in boosting_type:\n",
    "    if i == 'goss':\n",
    "        # 注意:Cannot use bagging in GOSS\n",
    "        lgb = LGBMClassifier(n_jobs=-1, boosting_type=i)\n",
    "    else:\n",
    "        # 注意:若boosting_type='rf',则必须进行bagging操作\n",
    "        '''\n",
    "        subsample_freq:frequency for bagging\n",
    "            0 means disable bagging; k means perform bagging at every k iteration.\n",
    "\n",
    "        sumsample:Subsample ratio of the training instances\n",
    "            * 0.0 < bagging_fraction <= 1.0\n",
    "            * to enable bagging, bagging_freq should be set to a non zero value as well\n",
    "        '''\n",
    "        lgb = LGBMClassifier(n_jobs=-1, boosting_type=i, subsample_freq=1, subsample=0.9, bagging_seed=1)\n",
    "    lgb.fit(X_train, y_train)\n",
    "    print('booster=' + str(i) + ',  score=', lgb.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "0.8453333333333334\n"
     ]
    }
   ],
   "source": [
    "# controls the level of LightGBM’s verbosity(< 0: Fatal, = 0: Error (Warning), = 1: Info, > 1: Debug\n",
    "lgb = LGBMClassifier(n_jobs=0, verbosity=0)  # 默认不输出其他信息\n",
    "lgb.fit(X_train, y_train)\n",
    "print(lgb.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective=regression, score= 0.8453333333333334\n",
      "objective=regression_l1, score= 0.8453333333333334\n",
      "objective=binary, score= 0.8453333333333334\n",
      "objective=softmax, score= 0.8453333333333334\n",
      "objective=cross_entropy, score= 0.8453333333333334\n"
     ]
    }
   ],
   "source": [
    "objective = [\"regression\",  # L2损失\n",
    "             \"regression_l1\",  # L1损失\n",
    "             \"binary\",  #  binary log loss classification\n",
    "             'softmax',\n",
    "             'cross_entropy']  # 交叉熵损失\n",
    "for i in objective:\n",
    "    '''\n",
    "    objective : string, callable or None, optional (default=None)\n",
    "            Specify the learning task and the corresponding learning objective or\n",
    "            a custom objective function to be used (see note below).\n",
    "            Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
    "    '''\n",
    "    lgb = LGBMClassifier(n_jobs=-1, objective=i)\n",
    "    lgb.fit(X_train, y_train)\n",
    "    print(\"objective=\" + str(i) + ', score=', lgb.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learing_rate=0.01,  score= 0.7826666666666666\n",
      "learing_rate=0.02,  score= 0.788\n",
      "learing_rate=0.05,  score= 0.7826666666666666\n",
      "learing_rate=0.1,  score= 0.7773333333333333\n",
      "learing_rate=0.15,  score= 0.7933333333333333\n",
      "learing_rate=0.3,  score= 0.8\n",
      "learing_rate=0.5,  score= 0.36533333333333334\n",
      "learing_rate=0.7,  score= 0.7946666666666666\n",
      "learing_rate=0.9,  score= 0.2786666666666667\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.01, 0.02, 0.05, 0.1, 0.15, 0.3, 0.5, 0.7, 0.9]  # 默认learning_rate=0.1\n",
    "for i in learning_rate:\n",
    "    lgb = LGBMClassifier(n_jobs=-1, learning_rate=i,\n",
    "                         cat_feature=0)  # 指定分类特征;默认自动\n",
    "    lgb.fit(X_train, y_train)\n",
    "    print('learing_rate=' + str(i) + ',  score=', lgb.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colsample_bytree=0.1,  score= 0.7866666666666666\n",
      "colsample_bytree=0.3,  score= 0.8293333333333334\n",
      "colsample_bytree=0.4,  score= 0.8493333333333334\n",
      "colsample_bytree=0.6,  score= 0.8466666666666667\n",
      "colsample_bytree=0.7,  score= 0.8573333333333333\n",
      "colsample_bytree=0.8,  score= 0.848\n",
      "colsample_bytree=0.85,  score= 0.852\n",
      "colsample_bytree=0.95,  score= 0.848\n",
      "colsample_bytree=1,  score= 0.8453333333333334\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = [0.1, 0.3, 0.4, 0.6, 0.7, 0.8, 0.85, 0.95, 1]\n",
    "for i in colsample_bytree:\n",
    "    lgb = LGBMClassifier(n_jobs=-1, colsample_bytree=i)  # 默认colsample_bytree=1.0\n",
    "    lgb.fit(X_train, y_train)\n",
    "    print('colsample_bytree=' + str(i) + ',  score=', lgb.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_alpha=0,  score= 0.8453333333333334\n",
      "reg_alpha=0.25,  score= 0.8453333333333334\n",
      "reg_alpha=0.5,  score= 0.8426666666666667\n",
      "reg_alpha=0.75,  score= 0.8386666666666667\n",
      "reg_alpha=1,  score= 0.836\n",
      "reg_alpha=3,  score= 0.832\n",
      "reg_alpha=9,  score= 0.7986666666666666\n"
     ]
    }
   ],
   "source": [
    "reg_alpha = [0, 0.25, 0.5, 0.75, 1, 3, 9]\n",
    "for i in reg_alpha:\n",
    "    lgb = LGBMClassifier(n_jobs=-1, reg_alpha=i)  # 默认reg_alpha=0.0\n",
    "    lgb.fit(X_train, y_train)\n",
    "    print('reg_alpha=' + str(i) + ',  score=', lgb.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_lambda=0,  score= 0.8453333333333334\n",
      "reg_lambda=1,  score= 0.8466666666666667\n",
      "reg_lambda=3,  score= 0.8373333333333334\n",
      "reg_lambda=9,  score= 0.84\n",
      "reg_lambda=27,  score= 0.8373333333333334\n",
      "reg_lambda=81,  score= 0.8093333333333333\n"
     ]
    }
   ],
   "source": [
    "reg_lambda = [0, 1, 3, 9, 27, 81]\n",
    "for i in reg_lambda:\n",
    "    lgb = LGBMClassifier(n_jobs=-1, reg_lambda=i)  # 默认reg_lambda=0.0\n",
    "    lgb.fit(X_train, y_train)\n",
    "    print('reg_lambda=' + str(i) + ',  score=', lgb.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_bin=50,  score= 0.848\n",
      "max_bin=120,  score= 0.848\n",
      "max_bin=255,  score= 0.8453333333333334\n",
      "max_bin=300,  score= 0.852\n",
      "max_bin=500,  score= 0.8506666666666667\n"
     ]
    }
   ],
   "source": [
    "max_bin = [50, 120, 255, 300, 500]\n",
    "for i in max_bin:\n",
    "    '''\n",
    "    max number of bins that feature values will be bucketed in\n",
    "    small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\n",
    "    '''\n",
    "    lgb = LGBMClassifier(n_jobs=-1, max_bin=i)  # 默认max_bin=255\n",
    "    lgb.fit(X_train, y_train)\n",
    "    print('max_bin=' + str(i) + ',  score=', lgb.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10,  score= 0.82\n",
      "n_estimators=20,  score= 0.84\n",
      "n_estimators=50,  score= 0.8506666666666667\n",
      "n_estimators=100,  score= 0.8453333333333334\n",
      "n_estimators=200,  score= 0.856\n",
      "n_estimators=500,  score= 0.852\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [10, 20, 50, 100, 200, 500]  # 默认n_estimators=100\n",
    "for i in n_estimators:\n",
    "    lgb = LGBMClassifier(n_jobs=-1, n_estimators=i)\n",
    "    lgb.fit(X_train, y_train)\n",
    "    print('n_estimators=' + str(i) + ',  score=', lgb.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=1,  score= 0.7586666666666667\n",
      "max_depth=3,  score= 0.8146666666666667\n",
      "max_depth=6,  score= 0.8373333333333334\n",
      "max_depth=9,  score= 0.848\n",
      "max_depth=12,  score= 0.852\n",
      "max_depth=15,  score= 0.844\n",
      "max_depth=18,  score= 0.8533333333333334\n",
      "max_depth=21,  score= 0.8453333333333334\n",
      "max_depth=-1,  score= 0.8453333333333334\n"
     ]
    }
   ],
   "source": [
    "max_depth = [1, 3, 6, 9, 12, 15, 18, 21, -1]\n",
    "for i in max_depth:\n",
    "    lgb = LGBMClassifier(n_jobs=-1, max_depth=i)  # 默认max_depth=-1\n",
    "    lgb.fit(X_train, y_train)\n",
    "    print('max_depth=' + str(i) + ',  score=', lgb.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_child_weight=10,  score= 0.8266666666666667\n",
      "min_child_weight=20,  score= 0.8146666666666667\n",
      "min_child_weight=30,  score= 0.792\n",
      "min_child_weight=40,  score= 0.768\n",
      "min_child_weight=50,  score= 0.7506666666666667\n",
      "min_child_weight=60,  score= 0.696\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = [10, 20, 30, 40, 50, 60]\n",
    "for i in min_child_weight:\n",
    "    lgb = LGBMClassifier(n_jobs=-1, min_child_weight=i)  # 默认min_child_weight=1e3\n",
    "    lgb.fit(X_train, y_train)\n",
    "    print('min_child_weight=' + str(i) + ',  score=', lgb.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_child_sample=10,  score= 0.86\n",
      "min_child_sample=20,  score= 0.8453333333333334\n",
      "min_child_sample=30,  score= 0.8453333333333334\n",
      "min_child_sample=40,  score= 0.844\n",
      "min_child_sample=50,  score= 0.8533333333333334\n",
      "min_child_sample=60,  score= 0.844\n"
     ]
    }
   ],
   "source": [
    "min_child_sample = [10, 20, 30, 40, 50, 60]\n",
    "for i in min_child_weight:\n",
    "    # minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "    lgb = LGBMClassifier(n_jobs=-1, min_child_samples=i)  # 默认min_child_sample=20\n",
    "    lgb.fit(X_train, y_train)\n",
    "    print('min_child_sample=' + str(i) + ',  score=', lgb.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_leaves=6,  score= 0.6026666666666667\n",
      "num_leaves=13,  score= 0.6053333333333333\n",
      "num_leaves=31,  score= 0.6053333333333333\n",
      "num_leaves=51,  score= 0.6053333333333333\n",
      "num_leaves=63,  score= 0.6053333333333333\n"
     ]
    }
   ],
   "source": [
    "num_leaves = [6, 13, 31, 51, 63]\n",
    "for i in num_leaves:\n",
    "    # max number of leaves in one tree\n",
    "    lgb = LGBMClassifier(n_jobs=-1, num_leaves=i, max_bin=2)  # 默认num_leaves=31\n",
    "    lgb.fit(X_train, y_train)\n",
    "    print('num_leaves=' + str(i) + ',  score=', lgb.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}