{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. 基本参数\n",
    "    * booster\n",
    "    * n_estimators\n",
    "    * learning_rate(学习速度调节)\n",
    "    * max_depth(重要程度高)\n",
    "    * min_child_weight(重要程度高,与max_depth一起进行网格搜索)\n",
    "\n",
    "2. 正则化调节\n",
    "    * gamma\n",
    "    * subsample\n",
    "    * colsample_bytree\n",
    "    * reg_alpha\n",
    "    * reg_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier # XGBRegressor 相同\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = datasets.fetch_covtype().data[:3000]\n",
    "y = datasets.fetch_covtype().target[:3000] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2250, 54)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape # 数据集有54个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train) # 7分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booster=gbtree,  score= 0.832\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Feature importance is not defined for Booster type gblinear",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-21-40f9d98c6cc6>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[0mxg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'gblinear'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mxg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_importances_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"coef_\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mxg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcoef_\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# Coefficients property(当前仅当booster='gblinear')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"intercept_\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mxg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintercept_\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# Intercept (bias) property(当前仅当booster='gblinear')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\CodeProgram\\anconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001B[0m in \u001B[0;36mfeature_importances_\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    688\u001B[0m                 'gbtree', 'dart'}:\n\u001B[0;32m    689\u001B[0m             raise AttributeError('Feature importance is not defined for Booster type {}'\n\u001B[1;32m--> 690\u001B[1;33m                                  .format(self.booster))\n\u001B[0m\u001B[0;32m    691\u001B[0m         \u001B[0mb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_booster\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    692\u001B[0m         \u001B[0mscore\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_score\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimportance_type\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimportance_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: Feature importance is not defined for Booster type gblinear"
     ]
    }
   ],
   "source": [
    "booster = ['gbtree',\n",
    "           'gblinear', # 适用于使用线性模型\n",
    "           'dart'] # 默认booster='gbtree'\n",
    "\n",
    "for i in booster:\n",
    "    xg = XGBClassifier(n_jobs=-1, # xgboost虽属于boosting算法,但也可进行并行计算\n",
    "                       booster=i)\n",
    "    xg.fit(X_train, y_train)\n",
    "    if i == 'gblinear': # 此时没有feature_importances_属性\n",
    "        print(\"coef_\", xg.coef_) # Coefficients property(当前仅当booster='gblinear')\n",
    "        print(\"intercept_\", xg.intercept_) # Intercept (bias) property(当前仅当booster='gblinear')\n",
    "    print('booster=' + str(i) + ',  score=', xg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10,  score= 0.7946666666666666\n",
      "n_estimators=20,  score= 0.8\n",
      "n_estimators=50,  score= 0.8226666666666667\n",
      "n_estimators=100,  score= 0.832\n",
      "n_estimators=200,  score= 0.8306666666666667\n",
      "n_estimators=500,  score= 0.832\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [10, 20, 50, 100, 200, 500] # 默认n_estimators=100\n",
    "\n",
    "for i in n_estimators:\n",
    "    xg = XGBClassifier(n_jobs=-1, \n",
    "                        n_estimators=i) # Number of gradient boosted trees\n",
    "    xg.fit(X_train, y_train)\n",
    "    print('n_estimators=' + str(i) + ',  score=', xg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learing_rate=0.01,  score= 0.7733333333333333\n",
      "learing_rate=0.02,  score= 0.7973333333333333\n",
      "learing_rate=0.05,  score= 0.8013333333333333\n",
      "learing_rate=0.1,  score= 0.8133333333333334\n",
      "learing_rate=0.15,  score= 0.816\n",
      "learing_rate=0.3,  score= 0.832\n",
      "learing_rate=0.5,  score= 0.8226666666666667\n",
      "learing_rate=0.7,  score= 0.836\n",
      "learing_rate=0.9,  score= 0.824\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.01, 0.02, 0.05, 0.1, 0.15, 0.3, 0.5, 0.7, 0.9] # 默认learning_rate=0.3\n",
    "for i in learning_rate:\n",
    "    xg = XGBClassifier(n_jobs=-1, learning_rate=i) # Step size shrinkage used in update to prevents overfitting\n",
    "    xg.fit(X_train, y_train)\n",
    "    print('learing_rate=' + str(i) + ',  score=', xg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=1,  score= 0.7466666666666667\n",
      "max_depth=3,  score= 0.8013333333333333\n",
      "max_depth=6,  score= 0.832\n",
      "max_depth=9,  score= 0.8306666666666667\n",
      "max_depth=12,  score= 0.8306666666666667\n",
      "max_depth=15,  score= 0.828\n",
      "max_depth=18,  score= 0.824\n",
      "max_depth=21,  score= 0.832\n"
     ]
    }
   ],
   "source": [
    "max_depth = [1, 3, 6, 9, 12, 15, 18, 21] # 默认max_depth=6\n",
    "for i in max_depth:\n",
    "    # Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit\n",
    "    xg = XGBClassifier(n_jobs=-1, max_depth=i, verbose=True)\n",
    "    xg.fit(X_train, y_train)\n",
    "    print('max_depth=' + str(i) + ',  score=', xg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_child_weight=0,  score= 0.8386666666666667\n",
      "min_child_weight=5,  score= 0.8266666666666667\n",
      "min_child_weight=10,  score= 0.8133333333333334\n",
      "min_child_weight=50,  score= 0.7786666666666666\n",
      "min_child_weight=100,  score= 0.732\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = [0, 5, 10, 50, 100] # 默认min_child_weight=1\n",
    "for i in min_child_weight:\n",
    "    \"\"\"\n",
    "    Minimum sum of instance weight (hessian) needed in a child. \n",
    "    If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, \n",
    "    then the building process will give up further partitioning. \n",
    "    In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. \n",
    "    The larger min_child_weight is, the more conservative the algorithm will be.\n",
    "    \"\"\"\n",
    "    xg = XGBClassifier(n_jobs=-1, min_child_weight=i) \n",
    "    xg.fit(X_train, y_train)\n",
    "    print('min_child_weight=' + str(i) + ',  score=', xg.score(X_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma=0,  score= 0.832\n",
      "gamma=0.3,  score= 0.828\n",
      "gamma=0.9,  score= 0.8186666666666667\n",
      "gamma=2.7,  score= 0.7866666666666666\n",
      "gamma=8.1,  score= 0.7653333333333333\n",
      "gamma=27.3,  score= 0.7293333333333333\n",
      "gamma=81.9,  score= 0.616\n"
     ]
    }
   ],
   "source": [
    "gamma = [0, 0.3, 0.9, 2.7, 8.1, 27.3, 81.9] # 默认gamma=0\n",
    "for i in gamma:\n",
    "    # Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "    xg = XGBClassifier(n_jobs=-1, gamma=i)\n",
    "    xg.fit(X_train, y_train)\n",
    "    print('gamma=' + str(i) + ',  score=', xg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample=0.1,  score= 0.7693333333333333\n",
      "subsample=0.3,  score= 0.8013333333333333\n",
      "subsample=0.4,  score= 0.8266666666666667\n",
      "subsample=0.6,  score= 0.8306666666666667\n",
      "subsample=0.7,  score= 0.816\n",
      "subsample=0.8,  score= 0.824\n",
      "subsample=0.85,  score= 0.824\n",
      "subsample=0.95,  score= 0.8293333333333334\n",
      "subsample=1,  score= 0.832\n"
     ]
    }
   ],
   "source": [
    "subsample = [0.1, 0.3, 0.4, 0.6, 0.7, 0.8, 0.85, 0.95, 1] # 默认subsample=1\n",
    "for i in subsample:\n",
    "    # Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting.\n",
    "    xg = XGBClassifier(n_jobs=-1, subsample=i)\n",
    "    xg.fit(X_train, y_train)\n",
    "    print('subsample=' + str(i) + ',  score=', xg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colsample_bytree=0.1,  score= 0.7586666666666667\n",
      "colsample_bytree=0.3,  score= 0.808\n",
      "colsample_bytree=0.4,  score= 0.8133333333333334\n",
      "colsample_bytree=0.6,  score= 0.832\n",
      "colsample_bytree=0.7,  score= 0.8306666666666667\n",
      "colsample_bytree=0.8,  score= 0.8293333333333334\n",
      "colsample_bytree=0.85,  score= 0.8293333333333334\n",
      "colsample_bytree=0.95,  score= 0.8346666666666667\n",
      "colsample_bytree=1,  score= 0.832\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = [0.1, 0.3, 0.4, 0.6, 0.7, 0.8, 0.85, 0.95, 1] # 默认colsample_bytree=1\n",
    "for i in subsample:\n",
    "    # colsample_bytree is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n",
    "    xg = XGBClassifier(n_jobs=-1, colsample_bytree=i)\n",
    "    xg.fit(X_train, y_train)\n",
    "    print('colsample_bytree=' + str(i) + ',  score=', xg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_alpha=0,  score= 0.832\n",
      "reg_alpha=0.25,  score= 0.8306666666666667\n",
      "reg_alpha=0.5,  score= 0.8346666666666667\n",
      "reg_alpha=0.75,  score= 0.8346666666666667\n",
      "reg_alpha=1,  score= 0.824\n",
      "reg_alpha=3,  score= 0.8133333333333334\n",
      "reg_alpha=9,  score= 0.7853333333333333\n"
     ]
    }
   ],
   "source": [
    "reg_alpha = [0, 0.25, 0.5, 0.75, 1, 3, 9] # 默认reg_alpha=0\n",
    "for i in reg_alpha:\n",
    "    # L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    xg = XGBClassifier(n_jobs=-1, reg_alpha=i)\n",
    "    xg.fit(X_train, y_train)\n",
    "    print('reg_alpha=' + str(i) + ',  score=', xg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_lambda=0,  score= 0.8213333333333334\n",
      "reg_lambda=1,  score= 0.832\n",
      "reg_lambda=3,  score= 0.832\n",
      "reg_lambda=9,  score= 0.828\n",
      "reg_lambda=27,  score= 0.8293333333333334\n",
      "reg_lambda=81,  score= 0.824\n"
     ]
    }
   ],
   "source": [
    "reg_lambda = [0, 1, 3, 9, 27, 81] # 默认reg_lambda=1\n",
    "for i in reg_lambda:\n",
    "    # L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    xg = XGBClassifier(n_jobs=-1, reg_lambda=i)\n",
    "    xg.fit(X_train, y_train)\n",
    "    print('reg_lambda=' + str(i) + ',  score=', xg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}