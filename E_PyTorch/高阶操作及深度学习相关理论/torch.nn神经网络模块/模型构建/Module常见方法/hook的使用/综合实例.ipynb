{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = torch.normal(0, 1, size=(1000, 2))\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += torch.normal(0, 0.0001, size=labels.size())  # 添加噪音\n",
    "\n",
    "dataset = Data.TensorDataset(features, labels)  # 数据包装\n",
    "\n",
    "data_iter = Data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module, ABC):\n",
    "    def __init__(self, n_feature):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(n_feature, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "net = LinearNet(2)\n",
    "loss = nn.MSELoss()  # 损失函数为均方误差\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.03)  # 优化器\n",
    "\n",
    "grad_list = list()  # 模型每次梯度更新后的梯度\n",
    "input_list = list()  # 模型前向传播前的输入\n",
    "weight_list = list()  # 模型前向传播前的权重\n",
    "output_list = list()  # 模型前向传播后的输出\n",
    "\n",
    "\n",
    "def grad_hook(grad):\n",
    "    grad_list.append(grad.clone())\n",
    "\n",
    "\n",
    "def forward_pre_hook(module, data_input):\n",
    "    b = module.weight.clone()\n",
    "    weight_list.append(b)\n",
    "    input_list.append(data_input)\n",
    "\n",
    "\n",
    "def forward_hook(module, data_input, data_output):\n",
    "    output_list.append(data_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 17.749737\n",
      "epoch 1, loss: 18.216667\n"
     ]
    }
   ],
   "source": [
    "# hook注册\n",
    "net.linear.weight.register_hook(grad_hook)\n",
    "net.linear.register_forward_pre_hook(forward_pre_hook)\n",
    "net.linear.register_forward_hook(forward_hook)\n",
    "\n",
    "for epoch in range(2):\n",
    "    l = 0\n",
    "    for X, y in data_iter:\n",
    "        output = net(X)\n",
    "        l = loss(output, y.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d, loss: %f' % (epoch, l.item()))\n",
    "    if l < 0.009:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "tensor([[0.4668, 0.1408]], grad_fn=<CloneBackward>)\n",
      "tensor([[0.4776, 0.0549]], grad_fn=<CloneBackward>)\n",
      "tensor([[ 0.5221, -0.2043]], grad_fn=<CloneBackward>)\n",
      "tensor([[ 0.5780, -0.2440]], grad_fn=<CloneBackward>)\n",
      "tensor([[ 0.7785, -0.8501]], grad_fn=<CloneBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(len(weight_list))\n",
    "for i in range(5):\n",
    "    print(weight_list[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "tensor([[-0.3616,  2.8623]])\n",
      "tensor([[-1.4814,  8.6381]])\n",
      "tensor([[-1.8654,  1.3237]])\n",
      "tensor([[-6.6819, 20.2030]])\n",
      "tensor([[-0.1126,  3.4293]])\n"
     ]
    }
   ],
   "source": [
    "print(len(grad_list))\n",
    "for i in range(5):\n",
    "    print(grad_list[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "(tensor([[-0.7883,  1.6140],\n",
      "        [ 0.0248,  1.7689],\n",
      "        [-0.6858, -0.3681],\n",
      "        [-0.6987, -0.2464],\n",
      "        [-0.4242,  0.6962],\n",
      "        [-0.1833, -0.9785],\n",
      "        [ 1.0593,  0.1734],\n",
      "        [ 0.0670,  0.0579],\n",
      "        [ 1.2751,  1.7562],\n",
      "        [ 1.3918,  1.4521]]),)\n",
      "(tensor([[ 0.9305,  0.1268],\n",
      "        [ 0.0809, -1.4031],\n",
      "        [-0.2083, -0.6231],\n",
      "        [-0.4751, -1.3295],\n",
      "        [ 0.8351, -0.6900],\n",
      "        [-1.1602,  1.1624],\n",
      "        [-0.8936,  2.1308],\n",
      "        [ 0.0517,  1.0283],\n",
      "        [-0.7746, -0.0962],\n",
      "        [-1.3538, -0.0091]]),)\n",
      "(tensor([[-0.9083, -0.3691],\n",
      "        [-0.3740,  0.2926],\n",
      "        [-0.5948,  0.6339],\n",
      "        [ 0.8718, -0.3834],\n",
      "        [ 1.3070,  0.9047],\n",
      "        [ 1.2523,  0.2241],\n",
      "        [-0.6087,  0.6298],\n",
      "        [-0.5499,  0.7431],\n",
      "        [-0.7246, -1.1253],\n",
      "        [ 0.6662, -0.3615]]),)\n",
      "(tensor([[ 0.4770, -0.4419],\n",
      "        [ 0.6602,  0.8987],\n",
      "        [ 0.4041, -2.3535],\n",
      "        [ 0.8901, -0.7783],\n",
      "        [ 0.5339, -1.4236],\n",
      "        [ 0.8708, -1.5129],\n",
      "        [ 0.5324, -1.9760],\n",
      "        [-1.2238, -0.2864],\n",
      "        [ 0.5887, -0.7100],\n",
      "        [-0.4377, -1.1484]]),)\n",
      "(tensor([[-0.3708,  0.6069],\n",
      "        [ 1.3161,  1.0460],\n",
      "        [ 0.0508, -0.0089],\n",
      "        [-0.1457, -1.2700],\n",
      "        [-1.2540, -1.0379],\n",
      "        [ 1.2203,  0.0154],\n",
      "        [ 0.3923,  0.0087],\n",
      "        [-0.2363, -1.0053],\n",
      "        [-0.3051, -0.3780],\n",
      "        [-0.5723,  0.7003]]),)\n"
     ]
    }
   ],
   "source": [
    "print(len(input_list))\n",
    "for j in range(5):\n",
    "    print(input_list[j])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "tensor([[-0.1408],\n",
      "        [ 0.2605],\n",
      "        [-0.3719],\n",
      "        [-0.3608],\n",
      "        [-0.1000],\n",
      "        [-0.2233],\n",
      "        [ 0.5189],\n",
      "        [ 0.0394],\n",
      "        [ 0.8424],\n",
      "        [ 0.8541]], grad_fn=<MmBackward>)\n",
      "tensor([[ 0.4514],\n",
      "        [-0.0384],\n",
      "        [-0.1337],\n",
      "        [-0.2999],\n",
      "        [ 0.3610],\n",
      "        [-0.4903],\n",
      "        [-0.3099],\n",
      "        [ 0.0812],\n",
      "        [-0.3753],\n",
      "        [-0.6471]], grad_fn=<MmBackward>)\n",
      "tensor([[-0.3988],\n",
      "        [-0.2550],\n",
      "        [-0.4400],\n",
      "        [ 0.5334],\n",
      "        [ 0.4975],\n",
      "        [ 0.6080],\n",
      "        [-0.4464],\n",
      "        [-0.4389],\n",
      "        [-0.1484],\n",
      "        [ 0.4216]], grad_fn=<MmBackward>)\n",
      "tensor([[ 0.3835],\n",
      "        [ 0.1623],\n",
      "        [ 0.8078],\n",
      "        [ 0.7044],\n",
      "        [ 0.6559],\n",
      "        [ 0.8724],\n",
      "        [ 0.7898],\n",
      "        [-0.6375],\n",
      "        [ 0.5135],\n",
      "        [ 0.0272]], grad_fn=<MmBackward>)\n",
      "tensor([[-0.8045],\n",
      "        [ 0.1354],\n",
      "        [ 0.0472],\n",
      "        [ 0.9661],\n",
      "        [-0.0939],\n",
      "        [ 0.9369],\n",
      "        [ 0.2980],\n",
      "        [ 0.6706],\n",
      "        [ 0.0838],\n",
      "        [-1.0409]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(len(output_list))\n",
    "for h in range(5):\n",
    "    print(output_list[h])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]], grad_fn=<SubBackward0>)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前向传播前的输入 @ 前向传播前的权重.T = 前向传播后的输出\n",
    "input_list[0][0] @ weight_list[0].T - output_list[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}