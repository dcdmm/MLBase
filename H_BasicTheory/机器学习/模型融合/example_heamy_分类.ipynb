{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcdmm\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from heamy.dataset import Dataset\n",
    "from heamy.estimator import Classifier\n",
    "from heamy.pipeline import ModelsPipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 54)\n",
      "(9000,)\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "data = fetch_covtype()\n",
    "\n",
    "X, y = data['data'][:10000], data['target'][:10000]\n",
    "\n",
    "ord = OrdinalEncoder()\n",
    "y_enc = ord.fit_transform(y.reshape(-1, 1))\n",
    "y_enc = y_enc.reshape(-1, )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.1, random_state=1)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset(eff47fde69d1e04cb6dc241bb4c1d9b5)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建数据集\n",
    "'''\n",
    "use_cache : bool, default True\n",
    "    If use_cache=True then preprocessing step will be cached until function code is changed.\n",
    "'''\n",
    "dataset = Dataset(X_train=X_train, y_train=y_train, X_test=X_test, y_test=None, use_cache=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2167.  129.   26. ...    0.    0.    0.]\n",
      " [2813.  117.   13. ...    0.    0.    0.]\n",
      " [2993.  286.   14. ...    0.    0.    0.]\n",
      " ...\n",
      " [2929.   75.   15. ...    0.    0.    0.]\n",
      " [2208.  317.   33. ...    0.    0.    0.]\n",
      " [2606.  356.   18. ...    0.    0.    0.]]\n",
      "\n",
      "[3. 1. 0. ... 4. 5. 1.]\n",
      "\n",
      "[[2979.   89.   18. ...    0.    0.    0.]\n",
      " [2083.   21.   28. ...    0.    0.    0.]\n",
      " [2322.  281.   17. ...    0.    0.    0.]\n",
      " ...\n",
      " [2306.  224.   25. ...    0.    0.    0.]\n",
      " [3029.  113.   14. ...    0.    0.    0.]\n",
      " [2882.   37.   10. ...    0.    0.    0.]]\n",
      "\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset.X_train, end='\\n\\n')\n",
    "print(dataset.y_train, end='\\n\\n')\n",
    "print(dataset.X_test, end='\\n\\n')\n",
    "print(dataset.y_test, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def xgb_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"参数必须为X_train,y_train,X_test,y_test\"\"\"\n",
    "    params = {'objective': 'multi:softprob',\n",
    "              \"eval_metric\": 'mlogloss',\n",
    "              \"verbosity\": 0,\n",
    "              'num_class': 7,\n",
    "              'nthread': -1}\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    model = xgb.train(params, dtrain, num_boost_round=300)\n",
    "    predict = model.predict(dtest)\n",
    "\n",
    "    return predict  # 返回值必须为X_test的预测\n",
    "\n",
    "\n",
    "def lgb_model(X_train, y_train, X_test, y_test,\n",
    "              **parameters):  # Classifier处对字典进行了解包,此处需要重新打包\n",
    "    if parameters is None:\n",
    "        parameters = {}\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "    model = lgb.train(params=parameters, train_set=lgb_train, num_boost_round=300)\n",
    "    predict = model.predict(X_test)\n",
    "\n",
    "    return predict\n",
    "\n",
    "\n",
    "def rf_model(X_train, y_train, X_test, y_test):\n",
    "    params = {\"n_estimators\": 100, \"n_jobs\": -1}\n",
    "    model = RandomForestClassifier(**params).fit(X_train, y_train)\n",
    "    predict = model.predict_proba(X_test)\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\"objective\": \"multiclass\",\n",
    "          \"num_class\": 7,\n",
    "          \"n_jobs\": -1,\n",
    "          \"verbose\": -4, \"metric\": (\"multi_logloss\",)}\n",
    "'''\n",
    "name : str, optional\n",
    "    The unique name of Estimator object.\n",
    "\n",
    "parameters : dict, optional\n",
    "    Arguments for estimator object.\n",
    "\n",
    "use_cache : bool, optional\n",
    "    if True then validate/predict/stack/blend results will be cached.\n",
    "'''\n",
    "model_xgb = Classifier(dataset=dataset, estimator=xgb_model, name='xgb', use_cache=False)\n",
    "model_lgb = Classifier(dataset=dataset, estimator=lgb_model, name='lgb',\n",
    "                       parameters=params,\n",
    "                       use_cache=False)\n",
    "model_rf = Classifier(dataset=dataset, estimator=rf_model,\n",
    "                      name='rf',  # 默认parameters=None\n",
    "                      use_cache=False)  # 默认use_cache=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<heamy.pipeline.ModelsPipeline at 0x221578995e0>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = ModelsPipeline(model_xgb, model_lgb, model_rf)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (log_loss): 0.36464098170245846\n",
      "Best Weights: [0.35853867 0.36476817 0.27669316]\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([0.35853867, 0.36476817, 0.27669316])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Finds optimal weights for weighted average of models.\n",
    "'''\n",
    "scorer : function\n",
    "    Scikit-learn like metric.\n",
    "\n",
    "test_size : float, default 0.2\n",
    "\n",
    "method : str\n",
    "    Type of solver. Should be one of:\n",
    "        ‘Nelder-Mead’\n",
    "        ‘Powell’\n",
    "        ‘CG’\n",
    "        ‘BFGS’\n",
    "        ‘Newton-CG’\n",
    "        ‘L-BFGS-B’\n",
    "        ‘TNC’\n",
    "        ‘COBYLA’\n",
    "        ‘SLSQP’\n",
    "        ‘dogleg’\n",
    "        ‘trust-ncg’\n",
    "'''\n",
    "# 使用留出法计算每个模型的评估结果\n",
    "# 求出最优权重组合使得整体评估结果最小\n",
    "pipeline.find_weights(scorer=log_loss, test_size=0.2)  # 输出最优权重组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Applies weighted mean to models.\n",
    "# 线性加权\n",
    "# pipeline.weight([0.5, 0.3, 0.2])  # 这里指定xgb模型权重0.5,lgb权重为0.3,rf权重为0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset(1585543b5c284b2fd7b13ceb7fb68816)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "k : int, default 5\n",
    "    Number of folds.\n",
    "\n",
    "stratify : bool, default False\n",
    "\n",
    "shuffle : bool, default True\n",
    "\n",
    "seed : int, default 100\n",
    "\n",
    "full_test : bool, default True\n",
    "    If True then evaluate test dataset on the full data otherwise take the mean of every fold.\n",
    "'''\n",
    "stack_ds = pipeline.stack(k=5,\n",
    "                          stratify=False,  # 是否为分层k折\n",
    "                          shuffle=True,\n",
    "                          seed=1,\n",
    "                          full_test=False)\n",
    "stack_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 21)\n",
      "(9000,)\n",
      "(1000, 21)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 第一层模型训练第二层模型的输出;这里xgb模型输出为概率,故有xgb_0, xgb_1,...,xgb_6\n",
    "print(stack_ds.X_train.shape)\n",
    "print(stack_ds.y_train.shape)\n",
    "print(stack_ds.X_test.shape)  # 第一层模型测试第二层模型的输出\n",
    "print(stack_ds.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[6.11388578e-01, 3.65444804e-01, 1.08756013e-03, ...,\n        4.41845714e-03, 1.42378642e-03, 1.55677176e-02],\n       [1.46281786e-03, 1.00738688e-02, 1.88128889e-01, ...,\n        3.73144024e-03, 7.89072609e-01, 5.71020859e-04],\n       [7.63720491e-04, 3.99136593e-03, 2.83299624e-02, ...,\n        1.69592047e-03, 9.61871688e-01, 2.44968597e-04],\n       ...,\n       [2.30384862e-03, 1.03668595e-02, 7.41239023e-01, ...,\n        6.78895418e-03, 1.09809566e-01, 1.34047346e-03],\n       [1.89951634e-01, 7.94199853e-01, 1.27640494e-03, ...,\n        8.70828391e-03, 2.26309865e-03, 3.18150280e-03],\n       [2.64335020e-01, 6.75597374e-01, 2.09267026e-03, ...,\n        4.77810202e-02, 2.61898589e-03, 6.89277645e-03]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacker = Classifier(dataset=stack_ds, estimator=LogisticRegression, parameters={\"solver\": 'lbfgs', \"max_iter\": 1000},\n",
    "                     use_cache=False)\n",
    "# stack_ds.X_test的预测结果\n",
    "predict_stack = stacker.predict()\n",
    "predict_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.858\n",
      "0.865\n",
      "0.863\n",
      "0.869\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(np.argmax(stack_ds.X_test.iloc[:, :7].values, axis=1), y_test))\n",
    "print(accuracy_score(np.argmax(stack_ds.X_test.iloc[:, 7:14].values, axis=1), y_test))\n",
    "print(accuracy_score(np.argmax(stack_ds.X_test.iloc[:, 14:].values, axis=1), y_test))\n",
    "\n",
    "# 通过stacking模型融合,准确率得到了提升\n",
    "print(accuracy_score(np.argmax(predict_stack, axis=1), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_t = stack_ds.X_test.iloc[:, :7].values\n",
    "lgb_t = stack_ds.X_test.iloc[:, 7:14].values\n",
    "rf_t = stack_ds.X_test.iloc[:, 14:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.84761098e-01, 3.08789063e-01, 8.00209028e-03, ...,\n        5.84277362e-02, 1.20066194e-02, 2.72107432e-02],\n       [1.20855528e-06, 5.54073804e-03, 2.86028596e-01, ...,\n        3.46179151e-07, 7.06023291e-01, 3.11759999e-06],\n       [8.00815949e-04, 1.60240044e-03, 3.45192607e-02, ...,\n        8.04382046e-04, 9.38060807e-01, 1.01507293e-07],\n       ...,\n       [4.44137708e-06, 1.46998647e-04, 6.23881210e-01, ...,\n        1.73260053e-06, 9.82145796e-02, 8.30107165e-07],\n       [1.61156785e-01, 7.90690982e-01, 2.17962280e-05, ...,\n        2.00077307e-02, 5.18871841e-06, 2.81107146e-02],\n       [2.37616060e-01, 5.92903073e-01, 1.11673729e-05, ...,\n        1.63860651e-01, 1.60057323e-03, 4.00082027e-03]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = 0.2 * xgb_t + 0.4 * lgb_t + 0.4 * rf_t\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.863\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(np.argmax(result, axis=1), y_test))  # 相比于线性加权,准确率得到了提升"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}