{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import collections\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### bert模型及其组件"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 24])\n"
     ]
    }
   ],
   "source": [
    "def masked_softmax(X, valid_lens=None):\n",
    "    \"\"\"通过在最后⼀个轴上遮蔽元素来执⾏softmax操作\"\"\"\n",
    "\n",
    "    def sequence_mask(X, valid_len, value=0):\n",
    "        \"\"\"Mask irrelevant entries in sequences\"\"\"\n",
    "        maxlen = X.size(1)\n",
    "        # 广播机制\n",
    "        mask = torch.arange(maxlen, device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = value\n",
    "        return X\n",
    "\n",
    "    if valid_lens is None:\n",
    "        return F.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # 被遮蔽的元素使用⼀个非常大的负值替换,使其softmax输出为0\n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
    "                          value=-1e6)\n",
    "        return F.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力\"\"\"\n",
    "\n",
    "    def __init__(self, dropout):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        # queries.shape = (b, ?q, d)\n",
    "        # keys.shape = (b, ?k, d)\n",
    "        # scores.shape = (b, ?q, d) x (b, d, ?k) = (b, ?q, ?k)\n",
    "        d = queries.shape[-1]\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # values.shape=(b, ?k, ?v)\n",
    "        # 返回值.shape=(b, ?q, ?k) x (b, ?k, ?v) = (b, ?q, ?v)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # 查询特征数目(E_q)\n",
    "                 query_size,\n",
    "                 # 键特征数目(E_k)\n",
    "                 key_size,\n",
    "                 # 值特征数目(E_v)\n",
    "                 value_size,\n",
    "                 # 多头数目\n",
    "                 num_heads, dropout, bias=False):  # 模仿pytorch的参数组成\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert query_size % num_heads == 0, \"query_size must be divisible by num_heads\"\n",
    "        # 可学习参数H^2 * 4\n",
    "        self.W_q = nn.Linear(query_size, query_size, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, query_size, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, query_size, bias=bias)\n",
    "        self.W_o = nn.Linear(query_size, query_size, bias=bias)\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def transpose_qkv(X, num_heads):\n",
    "        # 输入:X.shape=(N, L or S, E_q)\n",
    "        # X.shape=(N, L or S, num_heads, E_q / num_heads)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "        # X.shape=(N, num_heads, L or S, E_q / num_heads)\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        # 返回值.shape=(N * num_heads, L or S, E_q / num_heads)\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        \"\"\"\n",
    "        queries: 查询\n",
    "        keys: 键\n",
    "        values: 值\n",
    "        valid_lens: 计算attention_weights的有效长度\n",
    "        \"\"\"\n",
    "        # queries.shape=(N, L, E_q)\n",
    "        # self.W_q(queries).shape=(N, L, E_q)\n",
    "        # queries.shape=(N * num_heads, L, E_q / num_heads)\n",
    "\n",
    "        # keys.shape=(N, S, E_k)\n",
    "        # self.W_k(queries).shape=(N, S, E_q)\n",
    "        # keys.shape=(N * num_heads, S, E_q / num_heads)\n",
    "\n",
    "        # values.shape=(N, S, E_v)\n",
    "        # self.W_v(values).shape=(N, S, E_q)\n",
    "        # values.shape=(N * num_heads, S, E_q / num_heads)\n",
    "        queries = self.transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = self.transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = self.transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # E_q维度信息增加到batch_size维度上\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # output.shape=(N * num_heads, L, E_q / num_heads)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # output.shape=(N, num_heads, L, E_q / num_heads)\n",
    "        output = output.reshape(-1, self.num_heads, output.shape[1], output.shape[2])\n",
    "        # output.shape=(N, L, num_heads, E_q / num_heads)\n",
    "        output = output.permute(0, 2, 1, 3)\n",
    "        # output.shape=(N, L, E_q)\n",
    "        output_concat = output.reshape(output.shape[0], output.shape[1], -1)\n",
    "        # 返回值.shape=(N, L, E_q)\n",
    "        return self.W_o(output_concat)\n",
    "\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"残差连接和层归一化\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # 除mini-batch维度之外其他维度的列表(即进行层归一化的维度)\n",
    "                 normalized_shape,\n",
    "                 dropout):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 原因:句子长度不一致,并且各个batch的信息没什么关系\n",
    "        self.ln = nn.LayerNorm(normalized_shape)  # Normalized_shape is input.size()[1:]\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        # self.ln内部为残差连接\n",
    "        return self.ln(self.dropout(Y) + X)\n",
    "\n",
    "\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Networks\"\"\"\n",
    "\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs):\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        # 可学习参数:self.dense1权重矩阵,self.dense2权重矩阵\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"transformer编码器Block\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # 查询特征数目(E_q)\n",
    "                 query_size,\n",
    "                 # 键特征数目(E_k)\n",
    "                 key_size,\n",
    "                 # 值特征数目(E_v)\n",
    "                 value_size,\n",
    "                 # 除mini-batch维度之外其他维度的列表(即进行层归一化的维度)\n",
    "                 norm_shape,\n",
    "                 ffn_num_hiddens,\n",
    "                 # 多头数\n",
    "                 num_heads, dropout, use_bias=False):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(query_size, key_size, value_size, num_heads, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        # 多头注意力`forward`返回值的shape为:(N, L, E_q)\n",
    "        # 故PositionWiseFFN第一个线性层的输入维度为E_q\n",
    "        self.ffn = PositionWiseFFN(query_size, ffn_num_hiddens, query_size)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        # transformer编码器Block结构为:\n",
    "        # =+残差连接=>多头注意力==>add & Norm=+残差连接=>Feed Forward=>add & Norm\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))  # 多头自注意力\n",
    "        return self.addnorm2(Y, self.ffn(Y))\n",
    "\n",
    "\n",
    "encoder_blk = EncoderBlock(query_size=24,\n",
    "                           key_size=24,\n",
    "                           value_size=24,\n",
    "                           norm_shape=[100, 24],\n",
    "                           ffn_num_hiddens=48,\n",
    "                           num_heads=8,\n",
    "                           dropout=0.5)\n",
    "encoder_blk.eval()\n",
    "\n",
    "X = torch.ones((2, 100, 24))\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "\n",
    "# 可以看出Transformer编码器中的任何层都不会改变其输⼊的形状(可叠加多层)\n",
    "print(encoder_blk(X, valid_lens).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 8, 768])"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT encoder\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # 单词表的单词数目\n",
    "                 vocab_size,\n",
    "                 # 除mini-batch维度之外其他维度的列表(即进行层归一化的维度)\n",
    "                 norm_shape,\n",
    "                 # PositionWiseFFN层隐藏层大小\n",
    "                 ffn_num_hiddens,\n",
    "                 # 多头数\n",
    "                 num_heads,\n",
    "                 # bert编码器Block数\n",
    "                 num_layers,\n",
    "                 dropout,\n",
    "                 # 支持的最大序列长度\n",
    "                 max_len=1000,\n",
    "                 key_size=768, query_size=768, value_size=768):\n",
    "        super(BERTEncoder, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, query_size)\n",
    "        self.segment_embedding = nn.Embedding(2, query_size)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\", EncoderBlock(\n",
    "                query_size=query_size,\n",
    "                key_size=key_size,\n",
    "                value_size=value_size,\n",
    "                norm_shape=norm_shape,\n",
    "                num_heads=num_heads,\n",
    "                ffn_num_hiddens=ffn_num_hiddens,\n",
    "                dropout=dropout,\n",
    "                use_bias=True))\n",
    "\n",
    "        # BERT中,位置嵌⼊是可学习的\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, query_size))\n",
    "\n",
    "    def forward(self,\n",
    "                # 词元\n",
    "                tokens,\n",
    "                # 片段\n",
    "                segments,\n",
    "                valid_lens):\n",
    "        # tokens.shape=(N,  T)\n",
    "        # segments.shape=(N, T)\n",
    "        # X.shape=(N, T, vocab_size)\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)  # 词元嵌入+⽚段嵌⼊\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        # 返回值:X.shape=(N, T, vocab_size)\n",
    "        return X\n",
    "\n",
    "\n",
    "vocab_size, ffn_num_hiddens, num_heads = 10000, 1024, 4\n",
    "norm_shape, num_layers, dropout = [768], 2, 0.2\n",
    "encoder = BERTEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    norm_shape=norm_shape,\n",
    "    ffn_num_hiddens=ffn_num_hiddens,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout)\n",
    "\n",
    "tokens = torch.randint(0, vocab_size, (2, 8))  # 序列长度为8\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "# 不会改变其输⼊的形状\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 10000])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    \"\"\"BERT的遮蔽语⾔模型任务\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768):\n",
    "        super(MaskLM, self).__init__()\n",
    "        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.Linear(num_hiddens, vocab_size))\n",
    "\n",
    "    def forward(self,\n",
    "                # BERTEncoder的编码结果\n",
    "                X,\n",
    "                # 用于预测的词元位置\n",
    "                pred_positions):\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        # 假设batch_size=2, num_pred_positions=3\n",
    "        # 则batch_idx为tensor([0, 0, 0, 1, 1, 1])\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "        # 整数索引\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        # 输入:X.shaep=(N, L, query_size)\n",
    "        # 输出:mlm_Y_hat.shape=(N, len(flatten()), vocab_size)\n",
    "        return mlm_Y_hat\n",
    "\n",
    "\n",
    "mlm = MaskLM(10000, 768)\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)  # encoded_X.shape=(2, 8, 768)\n",
    "print(mlm_Y_hat.shape)\n",
    "\n",
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))  # 计算损失\n",
    "print(mlm_l.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"BERT的下⼀句预测任务\"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs):\n",
    "        super(NextSentencePred, self).__init__()\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X.shape=(batch size, hum_hiddens)\n",
    "        return self.output(X)\n",
    "\n",
    "\n",
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "nsp = NextSentencePred(encoded_X.shape[-1])\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "print(nsp_Y_hat.shape)\n",
    "\n",
    "nsp_y = torch.tensor([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "print(nsp_l.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 3, 10000])\n",
      "tensor([[ 0.0937, -0.0154]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    \"\"\"BERT模型\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # 单词表的单词数目\n",
    "                 vocab_size,\n",
    "                 num_hiddens,\n",
    "                 # 除mini-batch维度之外其他维度的列表(即进行层归一化的维度)\n",
    "                 norm_shape,\n",
    "                 ffn_num_hiddens,\n",
    "                 # 多头数\n",
    "                 num_heads,\n",
    "                 # bert编码器Block数\n",
    "                 num_layers,\n",
    "                 dropout,\n",
    "                 # 支持的最大序列长度\n",
    "                 max_len=1000,\n",
    "                 key_size=768, query_size=768, value_size=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            norm_shape=norm_shape,\n",
    "            ffn_num_hiddens=ffn_num_hiddens,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            max_len=max_len,\n",
    "            key_size=key_size,\n",
    "            query_size=query_size,\n",
    "            value_size=value_size)\n",
    "        self.mlm = MaskLM(\n",
    "            vocab_size=vocab_size,\n",
    "            num_hiddens=num_hiddens,\n",
    "            num_inputs=query_size)\n",
    "        self.hidden = nn.Sequential(nn.Linear(query_size, num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.nsp = NextSentencePred(num_hiddens)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None,\n",
    "                # 用于预测的词元位置\n",
    "                pred_positions=None):\n",
    "        # tokens.shape=(N, T)\n",
    "        # segments.shape=(N, T)\n",
    "        # encoded_X.shape=(N, T, query_size)\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            # mlm_Y_hat.shape=(N, len(flatten()), vocab_size)\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # encoded_X[:, 0, :].shape=(N, query_size)\n",
    "        # self.hidden(encoded_X[:, 0, :])).shape=(N, num_hiddens)\n",
    "        # nsp_Y_hat.shape=(N, 2)\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))  # 0为\"<cls>\"标记的索引\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat\n",
    "\n",
    "\n",
    "vocab_size, ffn_num_hiddens, num_heads = 10000, 1024, 4\n",
    "norm_shape, num_layers, dropout = [768], 2, 0.2\n",
    "\n",
    "bert_model = BERTModel(vocab_size=vocab_size,\n",
    "                       num_hiddens=1024,\n",
    "                       norm_shape=norm_shape,\n",
    "                       ffn_num_hiddens=ffn_num_hiddens,\n",
    "                       num_heads=num_heads,\n",
    "                       num_layers=num_layers,\n",
    "                       dropout=dropout)\n",
    "\n",
    "tokens = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8]])\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1]])\n",
    "mlm_positions = torch.tensor([[1, 5, 2]])\n",
    "\n",
    "encoded_X, mlm_Y_hat, nsp_Y_hat = bert_model(tokens, segments, pred_positions=mlm_positions)\n",
    "print(encoded_X.shape)\n",
    "print(mlm_Y_hat.shape)\n",
    "print(nsp_Y_hat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 数据处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "data": {
      "text/plain": "[['weather conditions often change rapidly , and afternoon thunderstorms are common in the <unk> ; <unk> and snow are possible year @-@ round',\n  \"an electrical storm on the mountain 's summit was considered remarkable enough to be reported in the july 1894 issue of science .\"],\n ['the <unk> creek mountains are in a very remote area of southeastern oregon and northern nevada , in harney and humboldt counties',\n  'the nearest human settlements are the <unk> ranch , about 20 miles ( 32 km ) directly north from the middle of the mountains ; fields , oregon , about 23 miles ( 37 km ) to the northwest ; <unk> , nevada , about 15 miles ( 24 km ) to the west ; and <unk> , nevada – oregon , about 30 miles ( 48 km ) to the east',\n  'the mountains are about 150 miles ( 240 km ) directly southwest of boise , idaho , and about 190 miles ( 310 km ) northeast of <unk> , nevada .'],\n ['the sculpture was constructed in sheffield by thomas <unk> studio , <unk> lucas , flint and <unk> and <unk> structures',\n  'it was approved at the start of 2003 , with the central core arriving in manchester on 13 june 2004',\n  'this was the largest load that could be transferred via road from the factory , and required a police escort',\n  'this central core was lifted into place in august 2004 , after which the 180 spikes could begin being attached',\n  'early estimates had given an optimistic completion date of july 2003 , which contributed to the sculpture gaining the nickname g of the bang .'],\n ['homarus gammarus was first given a binomial name by carl linnaeus in the tenth edition of his systema <unk> , published in 1758',\n  \"that name was cancer gammarus , since linnaeus ' concept of the genus cancer at that time included all large crustaceans .\"],\n ['the <unk> army pressed on after the monsoon season',\n  'in the following dry season of <unk> – 78 , c',\n  'december <unk> , a <unk> army of <unk> led by <unk> al @-@ din , son of <unk> <unk> <unk> , advanced to <unk> , which defended the <unk> pass',\n  'they occupied the fort and destroyed a large number of abandoned <unk>',\n  'but they found the heat excessive and returned .'],\n ['the median income for a household in the city was $ 25 @,@ <unk> , and the median income for a family was $ 31 @,@ <unk>',\n  'males had a median income of $ 29 @,@ <unk> versus $ 19 @,@ 702 for females',\n  'the per capita income for the city was $ 15 @,@ 255',\n  'about 24 @.@ 6 % of families and 28 @.@ 6 % of the population were below the poverty line , including 40 @.@ 8 % of those under age 18 and 22 @.@ 0 % of those age 65 or over .'],\n ['as the existing militia forces were unable to serve overseas under the provisions of the defence act 1903 , an all @-@ volunteer expeditionary force known as the australian imperial force ( aif ) was formed and recruitment began on 10 august 1914',\n  'the government pledged 20 @,@ 000 men , organised as one infantry division and one light horse brigade plus supporting units',\n  '<unk> and organisation was primarily regionally based and was undertaken under <unk> plans drawn up in 1912',\n  'the first commander was major general william bridges , who also assumed command of the 1st division',\n  'throughout the course of the conflict australian efforts were predominantly focused upon the ground war , although small air and naval forces were also committed .'],\n [\"although the importance of ' joint ' warfare had been highlighted during second world war when australian naval , ground and air units frequently served as part of single commands , the absence of a central authority continued to result in poor co @-@ ordination between the services in the post @-@ war era , with each organising and operating on the basis of a different military doctrine\",\n  \"the need for an integrated command structure received more emphasis during the australian military 's experiences in the vietnam war\",\n  'in 1973 , the secretary of the department of defence , arthur <unk> , submitted a report to the government that recommended the unification of the separate departments supporting each service into a single department and the creation of the post of chief of the defence force staff .'],\n ['following his term in the senate , <unk> resumed his legal practice in louisville',\n  'he sought another term as governor in 1927',\n  'this time he had the support of the louisville courier @-@ journal , which had been purchased by his ally , robert w. <unk>',\n  \"he was opposed by a powerful political machine known as the <unk> club , whose main interest was securing legislation to allow <unk> betting at the state 's horse <unk> .\"],\n ['mi6 officer james bond — agent <unk> — infiltrates a north korean military base , where colonel tan @-@ sun moon is illegally trading weapons for african blood <unk>',\n  \"after moon 's assistant <unk> discovers that bond is a british agent , the colonel attempts to kill bond and a <unk> chase <unk> , which ends with moon 's apparent death\",\n  \"bond survives , but is captured by north korean soldiers and imprisoned by the colonel 's father , general moon .\"]]"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _read_wiki(file_name):\n",
    "    with open(file_name, 'r', encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # 在WikiText-2数据集中,每行代表⼀个段落\n",
    "    # 保留至少有两句话的段落\n",
    "    # 大写字母转换为小写字母\n",
    "    # 使用句号作为分隔符来拆分句子\n",
    "    paragraphs = [line.strip().lower().split(' . ') for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    # paragraphs[0]表示一个段落\n",
    "    # paragraphsp[0][0]表示段落的第一句话\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "file_name = \"WikiText2/wiki.valid.tokens\"\n",
    "test_read_wiki = _read_wiki(file_name)[0:10]\n",
    "test_read_wiki"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "data": {
      "text/plain": "[[['weather',\n   'conditions',\n   'often',\n   'change',\n   'rapidly',\n   ',',\n   'and',\n   'afternoon',\n   'thunderstorms',\n   'are',\n   'common',\n   'in',\n   'the',\n   '<unk>',\n   ';',\n   '<unk>',\n   'and',\n   'snow',\n   'are',\n   'possible',\n   'year',\n   '@-@',\n   'round'],\n  ['an',\n   'electrical',\n   'storm',\n   'on',\n   'the',\n   'mountain',\n   \"'s\",\n   'summit',\n   'was',\n   'considered',\n   'remarkable',\n   'enough',\n   'to',\n   'be',\n   'reported',\n   'in',\n   'the',\n   'july',\n   '1894',\n   'issue',\n   'of',\n   'science',\n   '.']],\n [['the',\n   '<unk>',\n   'creek',\n   'mountains',\n   'are',\n   'in',\n   'a',\n   'very',\n   'remote',\n   'area',\n   'of',\n   'southeastern',\n   'oregon',\n   'and',\n   'northern',\n   'nevada',\n   ',',\n   'in',\n   'harney',\n   'and',\n   'humboldt',\n   'counties'],\n  ['the',\n   'nearest',\n   'human',\n   'settlements',\n   'are',\n   'the',\n   '<unk>',\n   'ranch',\n   ',',\n   'about',\n   '20',\n   'miles',\n   '(',\n   '32',\n   'km',\n   ')',\n   'directly',\n   'north',\n   'from',\n   'the',\n   'middle',\n   'of',\n   'the',\n   'mountains',\n   ';',\n   'fields',\n   ',',\n   'oregon',\n   ',',\n   'about',\n   '23',\n   'miles',\n   '(',\n   '37',\n   'km',\n   ')',\n   'to',\n   'the',\n   'northwest',\n   ';',\n   '<unk>',\n   ',',\n   'nevada',\n   ',',\n   'about',\n   '15',\n   'miles',\n   '(',\n   '24',\n   'km',\n   ')',\n   'to',\n   'the',\n   'west',\n   ';',\n   'and',\n   '<unk>',\n   ',',\n   'nevada',\n   '–',\n   'oregon',\n   ',',\n   'about',\n   '30',\n   'miles',\n   '(',\n   '48',\n   'km',\n   ')',\n   'to',\n   'the',\n   'east'],\n  ['the',\n   'mountains',\n   'are',\n   'about',\n   '150',\n   'miles',\n   '(',\n   '240',\n   'km',\n   ')',\n   'directly',\n   'southwest',\n   'of',\n   'boise',\n   ',',\n   'idaho',\n   ',',\n   'and',\n   'about',\n   '190',\n   'miles',\n   '(',\n   '310',\n   'km',\n   ')',\n   'northeast',\n   'of',\n   '<unk>',\n   ',',\n   'nevada',\n   '.']],\n [['the',\n   'sculpture',\n   'was',\n   'constructed',\n   'in',\n   'sheffield',\n   'by',\n   'thomas',\n   '<unk>',\n   'studio',\n   ',',\n   '<unk>',\n   'lucas',\n   ',',\n   'flint',\n   'and',\n   '<unk>',\n   'and',\n   '<unk>',\n   'structures'],\n  ['it',\n   'was',\n   'approved',\n   'at',\n   'the',\n   'start',\n   'of',\n   '2003',\n   ',',\n   'with',\n   'the',\n   'central',\n   'core',\n   'arriving',\n   'in',\n   'manchester',\n   'on',\n   '13',\n   'june',\n   '2004'],\n  ['this',\n   'was',\n   'the',\n   'largest',\n   'load',\n   'that',\n   'could',\n   'be',\n   'transferred',\n   'via',\n   'road',\n   'from',\n   'the',\n   'factory',\n   ',',\n   'and',\n   'required',\n   'a',\n   'police',\n   'escort'],\n  ['this',\n   'central',\n   'core',\n   'was',\n   'lifted',\n   'into',\n   'place',\n   'in',\n   'august',\n   '2004',\n   ',',\n   'after',\n   'which',\n   'the',\n   '180',\n   'spikes',\n   'could',\n   'begin',\n   'being',\n   'attached'],\n  ['early',\n   'estimates',\n   'had',\n   'given',\n   'an',\n   'optimistic',\n   'completion',\n   'date',\n   'of',\n   'july',\n   '2003',\n   ',',\n   'which',\n   'contributed',\n   'to',\n   'the',\n   'sculpture',\n   'gaining',\n   'the',\n   'nickname',\n   'g',\n   'of',\n   'the',\n   'bang',\n   '.']],\n [['homarus',\n   'gammarus',\n   'was',\n   'first',\n   'given',\n   'a',\n   'binomial',\n   'name',\n   'by',\n   'carl',\n   'linnaeus',\n   'in',\n   'the',\n   'tenth',\n   'edition',\n   'of',\n   'his',\n   'systema',\n   '<unk>',\n   ',',\n   'published',\n   'in',\n   '1758'],\n  ['that',\n   'name',\n   'was',\n   'cancer',\n   'gammarus',\n   ',',\n   'since',\n   'linnaeus',\n   \"'\",\n   'concept',\n   'of',\n   'the',\n   'genus',\n   'cancer',\n   'at',\n   'that',\n   'time',\n   'included',\n   'all',\n   'large',\n   'crustaceans',\n   '.']],\n [['the',\n   '<unk>',\n   'army',\n   'pressed',\n   'on',\n   'after',\n   'the',\n   'monsoon',\n   'season'],\n  ['in',\n   'the',\n   'following',\n   'dry',\n   'season',\n   'of',\n   '<unk>',\n   '–',\n   '78',\n   ',',\n   'c'],\n  ['december',\n   '<unk>',\n   ',',\n   'a',\n   '<unk>',\n   'army',\n   'of',\n   '<unk>',\n   'led',\n   'by',\n   '<unk>',\n   'al',\n   '@-@',\n   'din',\n   ',',\n   'son',\n   'of',\n   '<unk>',\n   '<unk>',\n   '<unk>',\n   ',',\n   'advanced',\n   'to',\n   '<unk>',\n   ',',\n   'which',\n   'defended',\n   'the',\n   '<unk>',\n   'pass'],\n  ['they',\n   'occupied',\n   'the',\n   'fort',\n   'and',\n   'destroyed',\n   'a',\n   'large',\n   'number',\n   'of',\n   'abandoned',\n   '<unk>'],\n  ['but',\n   'they',\n   'found',\n   'the',\n   'heat',\n   'excessive',\n   'and',\n   'returned',\n   '.']],\n [['the',\n   'median',\n   'income',\n   'for',\n   'a',\n   'household',\n   'in',\n   'the',\n   'city',\n   'was',\n   '$',\n   '25',\n   '@,@',\n   '<unk>',\n   ',',\n   'and',\n   'the',\n   'median',\n   'income',\n   'for',\n   'a',\n   'family',\n   'was',\n   '$',\n   '31',\n   '@,@',\n   '<unk>'],\n  ['males',\n   'had',\n   'a',\n   'median',\n   'income',\n   'of',\n   '$',\n   '29',\n   '@,@',\n   '<unk>',\n   'versus',\n   '$',\n   '19',\n   '@,@',\n   '702',\n   'for',\n   'females'],\n  ['the',\n   'per',\n   'capita',\n   'income',\n   'for',\n   'the',\n   'city',\n   'was',\n   '$',\n   '15',\n   '@,@',\n   '255'],\n  ['about',\n   '24',\n   '@.@',\n   '6',\n   '%',\n   'of',\n   'families',\n   'and',\n   '28',\n   '@.@',\n   '6',\n   '%',\n   'of',\n   'the',\n   'population',\n   'were',\n   'below',\n   'the',\n   'poverty',\n   'line',\n   ',',\n   'including',\n   '40',\n   '@.@',\n   '8',\n   '%',\n   'of',\n   'those',\n   'under',\n   'age',\n   '18',\n   'and',\n   '22',\n   '@.@',\n   '0',\n   '%',\n   'of',\n   'those',\n   'age',\n   '65',\n   'or',\n   'over',\n   '.']],\n [['as',\n   'the',\n   'existing',\n   'militia',\n   'forces',\n   'were',\n   'unable',\n   'to',\n   'serve',\n   'overseas',\n   'under',\n   'the',\n   'provisions',\n   'of',\n   'the',\n   'defence',\n   'act',\n   '1903',\n   ',',\n   'an',\n   'all',\n   '@-@',\n   'volunteer',\n   'expeditionary',\n   'force',\n   'known',\n   'as',\n   'the',\n   'australian',\n   'imperial',\n   'force',\n   '(',\n   'aif',\n   ')',\n   'was',\n   'formed',\n   'and',\n   'recruitment',\n   'began',\n   'on',\n   '10',\n   'august',\n   '1914'],\n  ['the',\n   'government',\n   'pledged',\n   '20',\n   '@,@',\n   '000',\n   'men',\n   ',',\n   'organised',\n   'as',\n   'one',\n   'infantry',\n   'division',\n   'and',\n   'one',\n   'light',\n   'horse',\n   'brigade',\n   'plus',\n   'supporting',\n   'units'],\n  ['<unk>',\n   'and',\n   'organisation',\n   'was',\n   'primarily',\n   'regionally',\n   'based',\n   'and',\n   'was',\n   'undertaken',\n   'under',\n   '<unk>',\n   'plans',\n   'drawn',\n   'up',\n   'in',\n   '1912'],\n  ['the',\n   'first',\n   'commander',\n   'was',\n   'major',\n   'general',\n   'william',\n   'bridges',\n   ',',\n   'who',\n   'also',\n   'assumed',\n   'command',\n   'of',\n   'the',\n   '1st',\n   'division'],\n  ['throughout',\n   'the',\n   'course',\n   'of',\n   'the',\n   'conflict',\n   'australian',\n   'efforts',\n   'were',\n   'predominantly',\n   'focused',\n   'upon',\n   'the',\n   'ground',\n   'war',\n   ',',\n   'although',\n   'small',\n   'air',\n   'and',\n   'naval',\n   'forces',\n   'were',\n   'also',\n   'committed',\n   '.']],\n [['although',\n   'the',\n   'importance',\n   'of',\n   \"'\",\n   'joint',\n   \"'\",\n   'warfare',\n   'had',\n   'been',\n   'highlighted',\n   'during',\n   'second',\n   'world',\n   'war',\n   'when',\n   'australian',\n   'naval',\n   ',',\n   'ground',\n   'and',\n   'air',\n   'units',\n   'frequently',\n   'served',\n   'as',\n   'part',\n   'of',\n   'single',\n   'commands',\n   ',',\n   'the',\n   'absence',\n   'of',\n   'a',\n   'central',\n   'authority',\n   'continued',\n   'to',\n   'result',\n   'in',\n   'poor',\n   'co',\n   '@-@',\n   'ordination',\n   'between',\n   'the',\n   'services',\n   'in',\n   'the',\n   'post',\n   '@-@',\n   'war',\n   'era',\n   ',',\n   'with',\n   'each',\n   'organising',\n   'and',\n   'operating',\n   'on',\n   'the',\n   'basis',\n   'of',\n   'a',\n   'different',\n   'military',\n   'doctrine'],\n  ['the',\n   'need',\n   'for',\n   'an',\n   'integrated',\n   'command',\n   'structure',\n   'received',\n   'more',\n   'emphasis',\n   'during',\n   'the',\n   'australian',\n   'military',\n   \"'s\",\n   'experiences',\n   'in',\n   'the',\n   'vietnam',\n   'war'],\n  ['in',\n   '1973',\n   ',',\n   'the',\n   'secretary',\n   'of',\n   'the',\n   'department',\n   'of',\n   'defence',\n   ',',\n   'arthur',\n   '<unk>',\n   ',',\n   'submitted',\n   'a',\n   'report',\n   'to',\n   'the',\n   'government',\n   'that',\n   'recommended',\n   'the',\n   'unification',\n   'of',\n   'the',\n   'separate',\n   'departments',\n   'supporting',\n   'each',\n   'service',\n   'into',\n   'a',\n   'single',\n   'department',\n   'and',\n   'the',\n   'creation',\n   'of',\n   'the',\n   'post',\n   'of',\n   'chief',\n   'of',\n   'the',\n   'defence',\n   'force',\n   'staff',\n   '.']],\n [['following',\n   'his',\n   'term',\n   'in',\n   'the',\n   'senate',\n   ',',\n   '<unk>',\n   'resumed',\n   'his',\n   'legal',\n   'practice',\n   'in',\n   'louisville'],\n  ['he', 'sought', 'another', 'term', 'as', 'governor', 'in', '1927'],\n  ['this',\n   'time',\n   'he',\n   'had',\n   'the',\n   'support',\n   'of',\n   'the',\n   'louisville',\n   'courier',\n   '@-@',\n   'journal',\n   ',',\n   'which',\n   'had',\n   'been',\n   'purchased',\n   'by',\n   'his',\n   'ally',\n   ',',\n   'robert',\n   'w.',\n   '<unk>'],\n  ['he',\n   'was',\n   'opposed',\n   'by',\n   'a',\n   'powerful',\n   'political',\n   'machine',\n   'known',\n   'as',\n   'the',\n   '<unk>',\n   'club',\n   ',',\n   'whose',\n   'main',\n   'interest',\n   'was',\n   'securing',\n   'legislation',\n   'to',\n   'allow',\n   '<unk>',\n   'betting',\n   'at',\n   'the',\n   'state',\n   \"'s\",\n   'horse',\n   '<unk>',\n   '.']],\n [['mi6',\n   'officer',\n   'james',\n   'bond',\n   '—',\n   'agent',\n   '<unk>',\n   '—',\n   'infiltrates',\n   'a',\n   'north',\n   'korean',\n   'military',\n   'base',\n   ',',\n   'where',\n   'colonel',\n   'tan',\n   '@-@',\n   'sun',\n   'moon',\n   'is',\n   'illegally',\n   'trading',\n   'weapons',\n   'for',\n   'african',\n   'blood',\n   '<unk>'],\n  ['after',\n   'moon',\n   \"'s\",\n   'assistant',\n   '<unk>',\n   'discovers',\n   'that',\n   'bond',\n   'is',\n   'a',\n   'british',\n   'agent',\n   ',',\n   'the',\n   'colonel',\n   'attempts',\n   'to',\n   'kill',\n   'bond',\n   'and',\n   'a',\n   '<unk>',\n   'chase',\n   '<unk>',\n   ',',\n   'which',\n   'ends',\n   'with',\n   'moon',\n   \"'s\",\n   'apparent',\n   'death'],\n  ['bond',\n   'survives',\n   ',',\n   'but',\n   'is',\n   'captured',\n   'by',\n   'north',\n   'korean',\n   'soldiers',\n   'and',\n   'imprisoned',\n   'by',\n   'the',\n   'colonel',\n   \"'s\",\n   'father',\n   ',',\n   'general',\n   'moon',\n   '.']]]"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    \"\"\"Split text lines into word or character tokens\"\"\"\n",
    "    if token == 'word':\n",
    "        # 根据空格切分句子\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type: ' + token)\n",
    "\n",
    "\n",
    "# paragraphs_tokenize[0]表示一个段落\n",
    "# paragraphs_tokenize[0][0]表示该段落的第一句话\n",
    "# paragraphs_tokenize[0][0][0]表示该段落的第一句话的第一个单词\n",
    "test_tokenize = [tokenize(paragraph, token='word') for paragraph in test_read_wiki]\n",
    "test_tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "def count_corpus(tokens):\n",
    "    \"\"\"Count token frequencies\"\"\"\n",
    "    # Here `tokens` is a 1D list or 2D list\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # Flatten a list of token lists into a list of tokens\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text\"\"\"\n",
    "\n",
    "    def __init__(self, tokens=None,\n",
    "                 # The minimum frequency needed to include a token in the vocabulary.\n",
    "                 min_freq=2,\n",
    "                 reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        counter = count_corpus(tokens)\n",
    "        # Sort according to frequencies\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {\n",
    "            token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        \"\"\"Index for the unknown token\"\"\"\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "def _get_next_sentence(\n",
    "        # 句子(列表表示)\n",
    "        sentence,\n",
    "        # sentence的下一个句子(列表表示)\n",
    "        next_sentence,\n",
    "        # 段落集合\n",
    "        paragraphs):\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        is_next = False\n",
    "        while True:\n",
    "            # paragraphs是三重列表的嵌套(故进行两次random.choice)\n",
    "            temp = random.choice(random.choice(paragraphs))\n",
    "            if temp != next_sentence:\n",
    "                # 此时next_sentence不再为sentence的下一句\n",
    "                next_sentence = temp\n",
    "                break\n",
    "    return sentence, next_sentence, is_next"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "当输⼊为单个文本时,BERT输⼊序列是特殊类别词元\"<cls>\"、文本序列的标记、以及特殊分隔词元\"<sep>\"的连结.\n",
    "当输⼊为文本对时,BERT输⼊序列是\"<cls>\"、第一个文本序列的标记、\"<sep>\"、第二个文本序列标记、以及\"<sep>\"的连结\n",
    "\n",
    "BERT输⼊序列的嵌⼊是词元嵌⼊、片段嵌⼊和位置嵌⼊的和,如下图所示:\n",
    "\n",
    "<img src=\"../../../Other/img/Bert输入.jpg\"  style=\"width:1000px;height:300px;float:bottom\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<cls>', 'a', 'b', 'c', 'd', '<sep>', 'A', 'B', 'C', '<sep>']\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"获取输⼊序列的词元及其⽚段索引\"\"\"\n",
    "    # 使用0标记词元序列A\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "\n",
    "    # 使用1标记词元序列B\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments\n",
    "\n",
    "\n",
    "tokens_1 = ['a', 'b', 'c', 'd']\n",
    "tokens_2 = ['A', 'B', 'C']\n",
    "tokens, segments = get_tokens_and_segments(tokens_1, tokens_2)\n",
    "\n",
    "print(tokens)\n",
    "print(segments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "data": {
      "text/plain": "[(['<cls>',\n   'weather',\n   'conditions',\n   'often',\n   'change',\n   'rapidly',\n   ',',\n   'and',\n   'afternoon',\n   'thunderstorms',\n   'are',\n   'common',\n   'in',\n   'the',\n   '<unk>',\n   ';',\n   '<unk>',\n   'and',\n   'snow',\n   'are',\n   'possible',\n   'year',\n   '@-@',\n   'round',\n   '<sep>',\n   'the',\n   '<unk>',\n   'army',\n   'pressed',\n   'on',\n   'after',\n   'the',\n   'monsoon',\n   'season',\n   '<sep>'],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False)]"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_nsp_data_from_paragraph(\n",
    "        # 段落\n",
    "        paragraph,\n",
    "        # 段落集合\n",
    "        paragraphs,\n",
    "        # 指定预训练期间的BERT输⼊序列的最大长度\n",
    "        max_len):\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_a_next, is_next = _get_next_sentence(paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # 输入序列包含1个'<cls>'词元和2个'<sep>'词元\n",
    "        if len(tokens_a) + len(tokens_a_next) + 3 > max_len:\n",
    "            continue\n",
    "        tokens, segments = get_tokens_and_segments(tokens_a, tokens_a_next)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph\n",
    "\n",
    "\n",
    "test_get_nsp_data_from_paragraph = _get_nsp_data_from_paragraph(test_tokenize[0], test_tokenize, 1024)\n",
    "test_get_nsp_data_from_paragraph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "预训练任务中,将随机选择15%的词元作为预测的遮蔽词元.要预测⼀个遮蔽词元而不使⽤标签作弊,\n",
    "一个简单的方法是用一个特殊的\"<mask>\"替换输⼊序列中的词元.然而,人造特殊词元\"<mask>\"不会出现在微调中.\n",
    "为了避免预训练和微调之间的这种不匹配,如果为预测而屏蔽词元(例如,在\"this movie is great\"中选择遮蔽和预测\"great\"),则在输⼊中将其替换为:\n",
    "\n",
    "    * 80%时间为特殊的\"<mask>\"词元(例如,\"this movie is great\"变为\"this movie is <mask>\"\n",
    "    * 10%时间为随机词元(例如,\"this movie is great\"变为\"this movie is drink\")\n",
    "    * 10%时间内为不变的标签词元(例如,\"this movie is great\"变为\"this movie is great\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "[([3,\n   0,\n   0,\n   0,\n   0,\n   0,\n   6,\n   8,\n   0,\n   0,\n   25,\n   0,\n   9,\n   5,\n   0,\n   2,\n   0,\n   8,\n   0,\n   25,\n   0,\n   0,\n   14,\n   0,\n   4,\n   5,\n   0,\n   0,\n   0,\n   25,\n   5,\n   0,\n   0,\n   6,\n   15,\n   0,\n   29,\n   10,\n   0,\n   21,\n   17,\n   0,\n   0,\n   0,\n   2,\n   0,\n   7,\n   5,\n   0,\n   2,\n   0,\n   6,\n   0,\n   2,\n   15,\n   2,\n   20,\n   16,\n   0,\n   21,\n   17,\n   12,\n   5,\n   0,\n   2,\n   0,\n   6,\n   0,\n   6,\n   15,\n   0,\n   20,\n   16,\n   0,\n   21,\n   2,\n   12,\n   5,\n   0,\n   0,\n   8,\n   0,\n   6,\n   0,\n   2,\n   0,\n   6,\n   15,\n   0,\n   20,\n   16,\n   2,\n   21,\n   17,\n   12,\n   5,\n   0,\n   4],\n  [5, 7, 15, 36, 37, 40, 44, 48, 49, 53, 55, 64, 75, 84, 91],\n  [0, 8, 0, 20, 16, 17, 5, 0, 0, 6, 0, 0, 17, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   5,\n   0,\n   0,\n   0,\n   25,\n   9,\n   11,\n   0,\n   0,\n   0,\n   7,\n   0,\n   0,\n   8,\n   0,\n   0,\n   6,\n   9,\n   0,\n   8,\n   0,\n   0,\n   4,\n   5,\n   0,\n   0,\n   0,\n   25,\n   2,\n   0,\n   0,\n   6,\n   2,\n   0,\n   20,\n   16,\n   2,\n   21,\n   17,\n   0,\n   0,\n   2,\n   5,\n   0,\n   7,\n   5,\n   0,\n   0,\n   0,\n   2,\n   0,\n   6,\n   15,\n   0,\n   2,\n   16,\n   2,\n   21,\n   2,\n   12,\n   5,\n   2,\n   0,\n   0,\n   6,\n   0,\n   6,\n   15,\n   2,\n   20,\n   16,\n   8,\n   21,\n   17,\n   12,\n   5,\n   0,\n   0,\n   8,\n   0,\n   6,\n   0,\n   0,\n   0,\n   6,\n   15,\n   0,\n   20,\n   16,\n   0,\n   2,\n   17,\n   12,\n   5,\n   0,\n   4],\n  [12, 28, 29, 33, 37, 42, 50, 55, 57, 59, 62, 69, 72, 81, 91],\n  [0, 25, 5, 15, 0, 0, 6, 20, 0, 17, 0, 0, 0, 6, 21],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   5,\n   2,\n   0,\n   0,\n   2,\n   5,\n   0,\n   0,\n   6,\n   15,\n   0,\n   20,\n   16,\n   0,\n   21,\n   17,\n   0,\n   0,\n   0,\n   5,\n   0,\n   7,\n   5,\n   0,\n   0,\n   0,\n   6,\n   0,\n   6,\n   2,\n   0,\n   20,\n   16,\n   0,\n   21,\n   2,\n   12,\n   5,\n   2,\n   0,\n   0,\n   6,\n   0,\n   6,\n   15,\n   8,\n   20,\n   2,\n   0,\n   21,\n   2,\n   12,\n   5,\n   0,\n   0,\n   2,\n   0,\n   6,\n   0,\n   0,\n   0,\n   2,\n   2,\n   0,\n   20,\n   16,\n   0,\n   21,\n   17,\n   12,\n   5,\n   0,\n   4,\n   0,\n   2,\n   2,\n   26,\n   5,\n   0,\n   19,\n   0,\n   10,\n   0,\n   0,\n   0,\n   12,\n   0,\n   0,\n   9,\n   5,\n   0,\n   0,\n   0,\n   7,\n   0,\n   2,\n   4],\n  [2, 5, 18, 30, 36, 39, 46, 48, 51, 56, 62, 63, 75, 76, 96],\n  [0, 25, 0, 15, 17, 0, 0, 16, 17, 8, 6, 15, 0, 0, 13],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   5,\n   0,\n   2,\n   0,\n   9,\n   0,\n   18,\n   0,\n   0,\n   0,\n   2,\n   0,\n   0,\n   6,\n   0,\n   8,\n   0,\n   8,\n   0,\n   0,\n   4,\n   0,\n   2,\n   0,\n   0,\n   5,\n   0,\n   2,\n   0,\n   2,\n   0,\n   5,\n   0,\n   0,\n   0,\n   20,\n   0,\n   26,\n   0,\n   0,\n   0,\n   4],\n  [3, 11, 23, 28, 30, 36],\n  [10, 6, 10, 7, 6, 9],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   10,\n   0,\n   0,\n   5,\n   0,\n   2,\n   0,\n   6,\n   0,\n   5,\n   0,\n   0,\n   0,\n   9,\n   2,\n   26,\n   0,\n   2,\n   0,\n   4,\n   0,\n   2,\n   5,\n   0,\n   0,\n   27,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   2,\n   0,\n   6,\n   8,\n   0,\n   11,\n   0,\n   0,\n   4],\n  [7, 16, 19, 23, 33, 34],\n  [7, 0, 0, 10, 0, 5],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   10,\n   5,\n   0,\n   0,\n   27,\n   0,\n   0,\n   0,\n   0,\n   2,\n   2,\n   5,\n   0,\n   6,\n   8,\n   0,\n   2,\n   0,\n   0,\n   4,\n   0,\n   0,\n   0,\n   10,\n   0,\n   0,\n   0,\n   9,\n   0,\n   0,\n   6,\n   0,\n   28,\n   2,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   4],\n  [11, 12, 18, 26, 33, 35],\n  [0, 0, 11, 0, 0, 5],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   0,\n   0,\n   10,\n   0,\n   0,\n   0,\n   9,\n   0,\n   0,\n   6,\n   0,\n   28,\n   5,\n   0,\n   2,\n   0,\n   0,\n   0,\n   0,\n   4,\n   0,\n   0,\n   0,\n   0,\n   2,\n   6,\n   8,\n   0,\n   0,\n   25,\n   0,\n   9,\n   5,\n   0,\n   2,\n   24,\n   8,\n   0,\n   2,\n   0,\n   0,\n   14,\n   0,\n   4],\n  [9, 16, 26, 30, 36, 37, 40],\n  [0, 0, 0, 0, 0, 0, 25],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   0,\n   0,\n   10,\n   0,\n   0,\n   11,\n   2,\n   0,\n   2,\n   0,\n   0,\n   9,\n   5,\n   2,\n   0,\n   7,\n   0,\n   0,\n   0,\n   6,\n   0,\n   9,\n   0,\n   4,\n   15,\n   0,\n   0,\n   0,\n   0,\n   7,\n   0,\n   8,\n   0,\n   2,\n   0,\n   0,\n   7,\n   5,\n   0,\n   0,\n   0,\n   2,\n   0,\n   0,\n   6,\n   0,\n   0,\n   0,\n   0,\n   0,\n   27,\n   0,\n   0,\n   0,\n   0,\n   8,\n   0,\n   0,\n   0,\n   0,\n   2,\n   2,\n   0,\n   0,\n   6,\n   0,\n   2,\n   4],\n  [7, 9, 14, 34, 42, 51, 61, 62, 65, 67],\n  [0, 18, 0, 0, 5, 7, 7, 0, 0, 13],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   5,\n   2,\n   0,\n   0,\n   2,\n   2,\n   5,\n   0,\n   0,\n   4,\n   0,\n   0,\n   0,\n   26,\n   5,\n   0,\n   2,\n   0,\n   10,\n   0,\n   0,\n   0,\n   2,\n   0,\n   0,\n   9,\n   5,\n   0,\n   0,\n   0,\n   7,\n   0,\n   13,\n   4],\n  [2, 5, 6, 17, 23],\n  [0, 26, 0, 19, 12],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   9,\n   5,\n   0,\n   0,\n   0,\n   7,\n   0,\n   0,\n   0,\n   2,\n   2,\n   4,\n   0,\n   0,\n   6,\n   11,\n   0,\n   0,\n   7,\n   2,\n   0,\n   18,\n   0,\n   0,\n   14,\n   0,\n   6,\n   0,\n   7,\n   0,\n   0,\n   0,\n   3,\n   0,\n   8,\n   0,\n   6,\n   28,\n   0,\n   2,\n   0,\n   0,\n   4],\n  [10, 11, 20, 24, 33, 35, 40],\n  [6, 0, 0, 0, 6, 12, 5],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   0,\n   6,\n   11,\n   0,\n   0,\n   7,\n   0,\n   2,\n   18,\n   0,\n   0,\n   14,\n   2,\n   2,\n   0,\n   7,\n   0,\n   0,\n   0,\n   2,\n   0,\n   12,\n   0,\n   6,\n   28,\n   0,\n   5,\n   0,\n   0,\n   4,\n   0,\n   2,\n   5,\n   0,\n   8,\n   0,\n   11,\n   0,\n   0,\n   2,\n   0,\n   0,\n   4],\n  [9, 14, 15, 21, 32, 33, 41],\n  [0, 0, 6, 6, 0, 0, 7],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3, 2, 0, 2, 0, 8, 2, 11, 0, 0, 7, 0, 0, 4, 0, 0, 0, 5, 0, 0, 8, 0, 13, 4],\n  [1, 3, 6, 21],\n  [0, 5, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n  True),\n ([3,\n   5,\n   2,\n   0,\n   22,\n   2,\n   0,\n   9,\n   5,\n   0,\n   10,\n   30,\n   0,\n   23,\n   2,\n   2,\n   2,\n   5,\n   0,\n   0,\n   2,\n   11,\n   0,\n   2,\n   30,\n   0,\n   23,\n   0,\n   4,\n   0,\n   29,\n   11,\n   0,\n   0,\n   7,\n   30,\n   0,\n   23,\n   0,\n   0,\n   30,\n   0,\n   23,\n   0,\n   22,\n   0,\n   4],\n  [2, 5, 14, 15, 16, 20, 23],\n  [0, 11, 0, 6, 8, 22, 10],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   8,\n   2,\n   0,\n   0,\n   29,\n   30,\n   0,\n   23,\n   0,\n   0,\n   30,\n   0,\n   23,\n   0,\n   2,\n   0,\n   4,\n   0,\n   0,\n   29,\n   0,\n   0,\n   0,\n   0,\n   0,\n   7,\n   0,\n   0,\n   6,\n   28,\n   0,\n   12,\n   5,\n   0,\n   2,\n   5,\n   0,\n   0,\n   2,\n   5,\n   0,\n   13,\n   4],\n  [2, 3, 6, 7, 16, 36, 40],\n  [29, 11, 7, 30, 22, 0, 7],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   2,\n   0,\n   0,\n   0,\n   22,\n   5,\n   0,\n   10,\n   30,\n   0,\n   23,\n   0,\n   4,\n   5,\n   0,\n   0,\n   0,\n   25,\n   5,\n   2,\n   0,\n   6,\n   15,\n   0,\n   20,\n   16,\n   0,\n   2,\n   17,\n   0,\n   2,\n   0,\n   5,\n   0,\n   2,\n   5,\n   0,\n   0,\n   0,\n   6,\n   0,\n   6,\n   15,\n   0,\n   20,\n   16,\n   0,\n   21,\n   17,\n   12,\n   5,\n   2,\n   0,\n   0,\n   6,\n   0,\n   2,\n   15,\n   0,\n   20,\n   2,\n   0,\n   21,\n   17,\n   12,\n   5,\n   0,\n   0,\n   8,\n   0,\n   6,\n   0,\n   0,\n   0,\n   6,\n   15,\n   0,\n   2,\n   16,\n   0,\n   21,\n   17,\n   12,\n   5,\n   0,\n   4],\n  [1, 5, 20, 24, 28, 31, 35, 50, 52, 57, 61, 78, 85],\n  [5, 22, 0, 0, 21, 0, 7, 12, 0, 6, 16, 20, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   24,\n   5,\n   0,\n   0,\n   0,\n   0,\n   0,\n   12,\n   0,\n   0,\n   2,\n   5,\n   0,\n   7,\n   2,\n   0,\n   0,\n   0,\n   6,\n   0,\n   2,\n   2,\n   0,\n   0,\n   0,\n   0,\n   24,\n   2,\n   0,\n   0,\n   0,\n   16,\n   0,\n   17,\n   10,\n   0,\n   8,\n   2,\n   0,\n   2,\n   0,\n   0,\n   2,\n   4,\n   5,\n   0,\n   0,\n   0,\n   23,\n   0,\n   0,\n   6,\n   0,\n   24,\n   0,\n   0,\n   0,\n   8,\n   0,\n   0,\n   2,\n   0,\n   2,\n   0,\n   0,\n   4],\n  [11, 15, 21, 22, 28, 38, 40, 43, 61, 63],\n  [0, 5, 0, 14, 5, 0, 26, 0, 0, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   2,\n   0,\n   0,\n   0,\n   23,\n   0,\n   2,\n   6,\n   0,\n   24,\n   0,\n   0,\n   0,\n   8,\n   0,\n   0,\n   27,\n   0,\n   0,\n   0,\n   0,\n   4,\n   0,\n   8,\n   0,\n   2,\n   0,\n   0,\n   0,\n   8,\n   10,\n   2,\n   0,\n   0,\n   0,\n   0,\n   0,\n   9,\n   0,\n   4],\n  [1, 4, 7, 17, 26, 32],\n  [5, 0, 0, 0, 10, 0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   8,\n   0,\n   2,\n   0,\n   0,\n   0,\n   8,\n   10,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   9,\n   0,\n   4,\n   0,\n   2,\n   2,\n   0,\n   0,\n   4,\n   0,\n   0,\n   7,\n   2,\n   0,\n   6,\n   28,\n   0,\n   12,\n   5,\n   0,\n   0,\n   5,\n   0,\n   0,\n   2,\n   5,\n   0,\n   13,\n   4],\n  [4, 20, 21, 24, 28, 29, 40],\n  [10, 0, 29, 0, 0, 0, 7],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   5,\n   0,\n   0,\n   10,\n   0,\n   0,\n   0,\n   0,\n   2,\n   0,\n   0,\n   0,\n   0,\n   7,\n   5,\n   8,\n   0,\n   4,\n   2,\n   5,\n   2,\n   7,\n   2,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   2,\n   0,\n   0,\n   6,\n   0,\n   0,\n   0,\n   8,\n   0,\n   0,\n   0,\n   0,\n   0,\n   13,\n   4],\n  [3, 9, 16, 19, 21, 23, 31],\n  [0, 6, 0, 0, 0, 5, 5],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   5,\n   0,\n   7,\n   0,\n   0,\n   0,\n   0,\n   29,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   19,\n   6,\n   0,\n   8,\n   0,\n   0,\n   0,\n   2,\n   24,\n   2,\n   7,\n   0,\n   0,\n   6,\n   5,\n   0,\n   7,\n   11,\n   0,\n   0,\n   2,\n   12,\n   2,\n   9,\n   0,\n   0,\n   14,\n   0,\n   0,\n   5,\n   0,\n   2,\n   5,\n   2,\n   14,\n   0,\n   0,\n   2,\n   0,\n   0,\n   0,\n   8,\n   0,\n   26,\n   5,\n   0,\n   7,\n   11,\n   0,\n   0,\n   0,\n   4,\n   5,\n   2,\n   22,\n   0,\n   2,\n   2,\n   0,\n   0,\n   0,\n   0,\n   0,\n   5,\n   0,\n   0,\n   19,\n   2,\n   9,\n   2,\n   0,\n   0,\n   4],\n  [18, 25, 27, 38, 40, 49, 51, 55, 71, 74, 75, 85, 86, 87],\n  [0, 0, 0, 0, 0, 9, 0, 6, 0, 0, 0, 0, 9, 5],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   5,\n   0,\n   22,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   5,\n   0,\n   2,\n   19,\n   0,\n   9,\n   5,\n   2,\n   0,\n   4,\n   9,\n   0,\n   6,\n   5,\n   0,\n   2,\n   5,\n   0,\n   7,\n   0,\n   6,\n   0,\n   0,\n   2,\n   0,\n   11,\n   0,\n   12,\n   5,\n   0,\n   27,\n   0,\n   5,\n   2,\n   7,\n   2,\n   0,\n   2,\n   0,\n   0,\n   2,\n   2,\n   11,\n   0,\n   0,\n   8,\n   5,\n   2,\n   7,\n   2,\n   0,\n   7,\n   0,\n   7,\n   5,\n   0,\n   0,\n   0,\n   13,\n   4],\n  [14, 19, 27, 35, 45, 47, 49, 52, 53, 59, 61],\n  [0, 0, 7, 6, 0, 5, 0, 0, 0, 0, 5],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   0,\n   0,\n   9,\n   24,\n   2,\n   6,\n   0,\n   0,\n   0,\n   2,\n   0,\n   9,\n   0,\n   4,\n   0,\n   0,\n   0,\n   2,\n   24,\n   0,\n   9,\n   0,\n   4],\n  [5, 6, 11, 19],\n  [5, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n  True),\n ([3,\n   0,\n   0,\n   0,\n   2,\n   24,\n   0,\n   9,\n   2,\n   4,\n   0,\n   29,\n   11,\n   0,\n   2,\n   7,\n   30,\n   0,\n   23,\n   0,\n   0,\n   30,\n   0,\n   23,\n   0,\n   2,\n   0,\n   4],\n  [4, 8, 14, 25],\n  [0, 0, 0, 22],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  False),\n ([3,\n   0,\n   8,\n   2,\n   29,\n   5,\n   0,\n   7,\n   5,\n   0,\n   0,\n   14,\n   0,\n   6,\n   28,\n   29,\n   0,\n   0,\n   18,\n   0,\n   0,\n   6,\n   0,\n   0,\n   0,\n   4,\n   0,\n   10,\n   0,\n   18,\n   11,\n   0,\n   2,\n   0,\n   0,\n   24,\n   5,\n   0,\n   0,\n   2,\n   2,\n   0,\n   0,\n   10,\n   0,\n   0,\n   12,\n   2,\n   0,\n   2,\n   0,\n   24,\n   0,\n   19,\n   0,\n   0,\n   13,\n   4],\n  [2, 3, 7, 32, 39, 40, 47, 49, 51],\n  [0, 0, 7, 0, 6, 0, 0, 0, 5],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   2,\n   0,\n   0,\n   0,\n   2,\n   0,\n   0,\n   0,\n   2,\n   0,\n   0,\n   0,\n   0,\n   6,\n   2,\n   2,\n   0,\n   14,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   2,\n   0,\n   0,\n   0,\n   4,\n   0,\n   0,\n   19,\n   0,\n   0,\n   0,\n   27,\n   0,\n   0,\n   11,\n   0,\n   2,\n   6,\n   2,\n   19,\n   0,\n   12,\n   0,\n   0,\n   8,\n   11,\n   0,\n   0,\n   0,\n   2,\n   28,\n   0,\n   0,\n   0,\n   19,\n   0,\n   0,\n   4],\n  [2, 6, 10, 16, 17, 26, 42, 44, 45, 55],\n  [0, 0, 11, 0, 0, 22, 0, 5, 0, 6],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True),\n ([3,\n   0,\n   0,\n   19,\n   0,\n   0,\n   0,\n   2,\n   2,\n   0,\n   11,\n   0,\n   0,\n   6,\n   5,\n   0,\n   0,\n   12,\n   0,\n   0,\n   8,\n   2,\n   0,\n   0,\n   0,\n   6,\n   28,\n   0,\n   0,\n   0,\n   19,\n   2,\n   0,\n   4,\n   0,\n   0,\n   6,\n   0,\n   0,\n   2,\n   18,\n   0,\n   0,\n   0,\n   8,\n   0,\n   18,\n   2,\n   0,\n   19,\n   0,\n   2,\n   0,\n   0,\n   13,\n   4],\n  [7, 8, 14, 21, 31, 39, 47, 51],\n  [27, 0, 5, 11, 0, 0, 5, 6],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1],\n  True)]"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    # 为遮蔽语言模型的输⼊创建新的词元副本\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "\n",
    "    pred_positions_and_labels = []\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        # 80%的时间:将词替换为\"<mask>\"词元\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10%的时间:保持词不变\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10%的时间:用随机词替换该词\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token  # 遮蔽后新的词元(可能包含替换的\"<mask>\"或随机词元)\n",
    "        # mlm_pred_position:遮蔽词元的位置\n",
    "        # tokens[mlm_pred_position]:遮蔽词元处真实的词元\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input_tokens, pred_positions_and_labels\n",
    "\n",
    "\n",
    "def _get_mlm_data_from_tokens(\n",
    "        # 词元序列列表\n",
    "        tokens,\n",
    "        vocab):\n",
    "    candidate_pred_positions = []\n",
    "    # tokens为⼀个字符串列表\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 在遮蔽语言模型任务中不会预测特殊词元\"<cls\">与\"<sep>\"\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    # 遮蔽语言模型任务中预测15%的随机词元\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens, candidate_pred_positions,\n",
    "                                                                      num_mlm_preds, vocab)\n",
    "\n",
    "    # 根据pred_positions_and_labels中的第一列排序(从小到大)\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    # vocab[mlm_input_tokens]:遮蔽后新的词元转换为数值矩阵\n",
    "    # vocab[mlm_pred_labels]:遮蔽词元处真实的词元转换为数值矩阵\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n",
    "\n",
    "\n",
    "# sentences[0]表示一句话\n",
    "# sentences[0][0]表示该句话的第一个单词\n",
    "# 此时不再区分段落\n",
    "sentences = [sentence for paragraph in test_tokenize for sentence in paragraph]\n",
    "\n",
    "vocab = Vocab(sentences, min_freq=5, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "\n",
    "examples = []\n",
    "for paragraph in test_tokenize:\n",
    "    examples.extend(_get_nsp_data_from_paragraph(paragraph, test_tokenize, 1024))\n",
    "\n",
    "test_get_mlm_data_from_tokens = [(_get_mlm_data_from_tokens(tokens, vocab) + (segments, is_next))\n",
    "                                 for tokens, segments, is_next in examples]\n",
    "test_get_mlm_data_from_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    max_num_mlm_preds = round(max_len * 0.15)  # 填充词元的总长度\n",
    "    all_token_ids, all_segments, valid_lens, = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "        # vocab['<pad>']:1\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (max_len - len(token_ids)), dtype=torch.long))\n",
    "        all_segments.append(torch.tensor(segments + [0] * (max_len - len(segments)), dtype=torch.long))\n",
    "        # valid_lens不包括\"<pad>\"的计数\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "\n",
    "        # len(pred_positions):真实的要预测填充词元的长度\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)),\n",
    "                                               dtype=torch.long))\n",
    "        # 填充词元的预测将通过乘以0权重在损失中过滤掉\n",
    "        all_mlm_weights.append(\n",
    "            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (max_num_mlm_preds - len(pred_positions)),\n",
    "                         dtype=torch.float32))\n",
    "        all_mlm_labels.append(\n",
    "            torch.tensor(mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "    return (all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels)\n",
    "\n",
    "\n",
    "class _WikiTextDataset(Data.Dataset):  # 继承Dataset类\n",
    "    \"\"\"包装成pytorch Data.DataSet数据集\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # paragraphs表示段落集合\n",
    "                 paragraphs,\n",
    "                 # 指定预训练期间的BERT输⼊序列的最大长度\n",
    "                 max_len):\n",
    "        paragraphs = [tokenize(paragraph, token='word') for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs for sentence in paragraph]\n",
    "\n",
    "        # 出现次数少于5次的不频繁词元将被过滤掉\n",
    "        self.vocab = Vocab(sentences, min_freq=5, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        # 获取下⼀句子预测任务的数据\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(paragraph, paragraphs, max_len))\n",
    "        # 获取遮蔽语言模型任务的数据\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next)) for tokens, segments, is_next\n",
    "                    in examples]\n",
    "\n",
    "        (self.all_token_ids,\n",
    "         self.all_segments,\n",
    "         self.valid_lens,\n",
    "         self.all_pred_positions,\n",
    "         self.all_mlm_weights,\n",
    "         self.all_mlm_labels,\n",
    "         self.nsp_labels) = _pad_bert_inputs(examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
    "                self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "def load_data_wiki(file_name, batch_size, max_len):\n",
    "    \"\"\"加载WikiText-2数据集\"\"\"\n",
    "    paragraphs = _read_wiki(file_name)\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "    return train_iter, train_set.vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    3,    15,   232,  ...,     1,     1,     1],\n",
      "        [    3,  1588,  3885,  ...,     1,     1,     1],\n",
      "        [    3,    30,   471,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    3,   311,     7,  ...,     1,     1,     1],\n",
      "        [    3,     9, 11797,  ...,     1,     1,     1],\n",
      "        [    3,    15,   522,  ...,     1,     1,     1]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([57., 41., 53., 47., 47., 42., 55., 31., 36., 49., 60., 63., 56., 39.,\n",
      "        53., 56., 61., 60., 55., 40., 25., 60., 52., 43., 54., 32., 40., 53.,\n",
      "        52., 39., 47., 46., 60., 38., 20., 34., 45., 41., 62., 61., 58., 50.,\n",
      "        48., 63., 28., 64., 52., 63., 39., 52., 39., 42., 64., 59., 43., 20.,\n",
      "        57., 50., 48., 53., 18., 48., 32., 62., 47., 47., 53., 53., 62., 38.,\n",
      "        58., 52., 56., 38., 63., 36., 64., 36., 61., 58., 55., 47., 52., 52.,\n",
      "        47., 31., 60., 46., 44., 43., 40., 47., 57., 51., 46., 59., 47., 38.,\n",
      "        64., 58., 39., 59., 63., 51., 46., 61., 41., 41., 57., 41., 45., 41.,\n",
      "        43., 47., 38., 49., 32., 46., 61., 59., 41., 56., 46., 63., 58., 52.,\n",
      "        53., 48., 59., 46., 26., 47., 37., 62., 41., 61., 29., 60., 49., 52.,\n",
      "        38., 40., 44., 62., 48., 56., 45., 33., 36., 34., 46., 57., 57., 53.,\n",
      "        36., 40., 62., 53., 59., 61., 44., 59., 61., 61., 64., 55., 60., 25.,\n",
      "        45., 49., 56., 56., 41., 58., 40., 27., 54., 58., 39., 57., 47., 61.,\n",
      "        36., 38., 56., 49., 43., 55., 46., 58., 46., 45., 33., 17., 40., 29.,\n",
      "        34., 48., 47., 51., 26., 58., 58., 56., 44., 38., 50., 52., 51., 64.,\n",
      "        44., 57., 11., 28., 33., 41., 59., 48., 21., 55., 43., 55., 54., 55.,\n",
      "        57., 43., 38., 42., 51., 49., 47., 57., 48., 33., 35., 51., 29., 64.,\n",
      "        64., 53., 34., 53., 29., 44., 35., 46., 37., 60., 52., 43., 35., 28.,\n",
      "        36., 53., 64., 40., 29., 49., 64., 48., 63., 42., 32., 50., 47., 49.,\n",
      "        62., 38., 61., 54., 44., 60., 50., 47., 57., 49., 37., 41., 40., 49.,\n",
      "        46., 56., 47., 51., 37., 55., 64., 52., 48., 45., 42., 58., 57., 51.,\n",
      "        24., 31., 48., 36., 47., 55., 28., 35., 59., 37., 46., 53., 45., 12.,\n",
      "        37., 49., 33., 59., 50., 24., 39., 48., 26., 49., 48., 27., 37., 51.,\n",
      "        46., 45., 44., 49., 60., 55., 43., 55., 47., 49., 56., 45., 43., 46.,\n",
      "        43., 45., 63., 38., 37., 61., 41., 52., 63., 36., 47., 58., 37., 42.,\n",
      "        59., 30., 55., 47., 44., 30., 43., 48., 60., 28., 23., 46., 37., 44.,\n",
      "        57., 49., 43., 46., 61., 60., 56., 32., 50., 53., 60., 58., 51., 59.,\n",
      "        55., 41., 59., 48., 54., 59., 46., 64., 33., 47., 29., 43., 53., 39.,\n",
      "        47., 30., 58., 47., 42., 56., 51., 27., 42., 52., 49., 52., 43., 55.,\n",
      "        45., 45., 53., 45., 55., 64., 42., 54., 43., 56., 26., 54., 28., 60.,\n",
      "        34., 42., 56., 61., 37., 53., 48., 59., 53., 40., 58., 52., 48., 58.,\n",
      "        44., 54., 51., 39., 62., 46., 38., 42., 58., 50., 58., 53., 51., 34.,\n",
      "        26., 34., 58., 46., 49., 64., 40., 59., 38., 54., 48., 44., 64., 33.,\n",
      "        38., 60., 51., 47., 62., 55., 44., 51., 37., 62., 32., 58., 60., 56.,\n",
      "        42., 55., 49., 45., 26., 50., 47., 51., 34., 54., 64., 42., 41., 42.,\n",
      "        35., 44., 41., 40., 16., 52., 43., 57., 30., 34., 46., 46., 51., 40.,\n",
      "        50., 23., 63., 49., 31., 48., 50., 57.])\n",
      "tensor([[ 8, 10, 21,  ..., 39, 47,  0],\n",
      "        [ 3,  9, 15,  ...,  0,  0,  0],\n",
      "        [ 3, 16, 18,  ..., 51,  0,  0],\n",
      "        ...,\n",
      "        [ 6, 11, 13,  ...,  0,  0,  0],\n",
      "        [ 2,  6, 16,  ..., 33,  0,  0],\n",
      "        [ 6,  8,  9,  ..., 51, 55,  0]])\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.]])\n",
      "tensor([[6230,  347, 5375,  ...,  151,   13,    0],\n",
      "        [  11,   15,    6,  ...,    0,    0,    0],\n",
      "        [  17,   12,   34,  ...,   12,    0,    0],\n",
      "        ...,\n",
      "        [ 712,    5,    6,  ...,    0,    0,    0],\n",
      "        [1808,  503,    5,  ..., 3628,    0,    0],\n",
      "        [   6,    6,   21,  ...,   11, 2804,    0]])\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "file_name = \"WikiText2/wiki.train.tokens\"\n",
    "train_iter, vocab = load_data_wiki(file_name, batch_size, max_len)\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y) in train_iter:\n",
    "    print(tokens_X)  # tokens_X.shape=(512, 64)\n",
    "    print(segments_X)\n",
    "    print(valid_lens_x)\n",
    "    print(pred_positions_X)\n",
    "    print(mlm_weights_X)\n",
    "    print(mlm_Y)\n",
    "    print(nsp_y)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "20256"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 预训练BERT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "net = BERTModel(len(vocab),\n",
    "                num_hiddens=128,\n",
    "                norm_shape=[128],\n",
    "                ffn_num_hiddens=256,\n",
    "                num_heads=2,\n",
    "                num_layers=2,\n",
    "                dropout=0.2,\n",
    "                key_size=128, query_size=128, value_size=128)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X, segments_X,\n",
    "                         valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y):\n",
    "    \"\"\"计算遮蔽语言模型和下⼀句子预测任务的损失\"\"\"\n",
    "    # forward函数前向传播\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X, valid_lens_x.reshape(-1), pred_positions_X)\n",
    "    # 计算遮蔽语言模型损失\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) * mlm_weights_X.reshape(-1, 1)\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "    # 计算下⼀句子预测任务的损失\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "    # BERT预训练的最终损失是遮蔽语言模型损失和下⼀句预测损失的和\n",
    "    l = mlm_l + nsp_l\n",
    "    return mlm_l, nsp_l, l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "def train_bert(train_iter, net, loss, vocab_size, devices,\n",
    "               # 训练的迭代步数,不是指训练的轮数\n",
    "               num_steps):\n",
    "    net = net.to(devices)\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)  # 优化器\n",
    "    step = 0\n",
    "    # 遮蔽语言模型损失\n",
    "    mlm_l_list = []\n",
    "    # 下⼀句预测损失\n",
    "    nsp_l_list = []\n",
    "    while step < num_steps:\n",
    "        for tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
    "            tokens_X = tokens_X.to(devices)\n",
    "            segments_X = segments_X.to(devices)\n",
    "            valid_lens_x = valid_lens_x.to(devices)\n",
    "            pred_positions_X = pred_positions_X.to(devices)\n",
    "            mlm_weights_X = mlm_weights_X.to(devices)\n",
    "            mlm_Y, nsp_y = mlm_Y.to(devices), nsp_y.to(devices)\n",
    "            trainer.zero_grad()\n",
    "            mlm_l, nsp_l, l = _get_batch_loss_bert(net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n",
    "                                                   pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
    "            mlm_l_list.append(mlm_l.item())\n",
    "            nsp_l_list.append(nsp_l.item())\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            step += 1\n",
    "            if step % 100 == 0:\n",
    "                print(\"mlm_l loss:\", mlm_l.item(), \"nsp_l loss:\", nsp_l.item())\n",
    "            if step == num_steps:\n",
    "                break\n",
    "    return mlm_l_list, nsp_l_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm_l loss: 5.0079145431518555 nsp_l loss: 0.6935237050056458\n",
      "mlm_l loss: 5.029735565185547 nsp_l loss: 0.6928146481513977\n",
      "mlm_l loss: 4.8301777839660645 nsp_l loss: 0.7137945890426636\n",
      "mlm_l loss: 4.816521167755127 nsp_l loss: 0.6952507495880127\n",
      "mlm_l loss: 4.766730308532715 nsp_l loss: 0.6945309638977051\n",
      "mlm_l loss: 4.831899642944336 nsp_l loss: 0.6966699361801147\n",
      "mlm_l loss: 4.990330219268799 nsp_l loss: 0.696281373500824\n",
      "mlm_l loss: 4.867680072784424 nsp_l loss: 0.6912459135055542\n",
      "mlm_l loss: 4.941139221191406 nsp_l loss: 0.6953710913658142\n",
      "mlm_l loss: 5.07868766784668 nsp_l loss: 0.6906602382659912\n",
      "mlm_l loss: 5.332531452178955 nsp_l loss: 0.6965398788452148\n",
      "mlm_l loss: 4.943722248077393 nsp_l loss: 0.6942748427391052\n",
      "mlm_l loss: 5.0994977951049805 nsp_l loss: 0.7019555568695068\n",
      "mlm_l loss: 5.074757099151611 nsp_l loss: 0.6983681917190552\n",
      "mlm_l loss: 5.264800071716309 nsp_l loss: 0.6968280076980591\n",
      "mlm_l loss: 5.185458183288574 nsp_l loss: 0.7016668319702148\n",
      "mlm_l loss: 5.251952171325684 nsp_l loss: 0.6976235508918762\n",
      "mlm_l loss: 5.165384769439697 nsp_l loss: 0.694433867931366\n",
      "mlm_l loss: 5.11295747756958 nsp_l loss: 0.6987256407737732\n",
      "mlm_l loss: 5.006335258483887 nsp_l loss: 0.6951941847801208\n",
      "[10.479865074157715, 8.31264877319336, 7.679129600524902, 6.39214563369751, 5.8175458908081055, 5.646623134613037, 5.675446510314941, 5.754960060119629, 5.706662654876709, 5.831921577453613, 5.6010918617248535, 5.777101516723633, 5.684859275817871, 5.721479415893555, 5.570734977722168, 5.675198078155518, 5.495322227478027, 5.675037384033203, 5.54376745223999, 5.507006645202637, 5.553008079528809, 5.5488176345825195, 5.616917133331299, 5.535983085632324, 5.48250150680542, 5.5023603439331055, 5.522000312805176, 5.42073392868042, 5.508167266845703, 5.535676956176758, 5.447153568267822, 5.355940818786621, 5.410728454589844, 5.45172119140625, 5.5901923179626465, 5.621163845062256, 5.5897345542907715, 5.523040294647217, 5.5122857093811035, 5.519755840301514, 5.578641414642334, 5.403016090393066, 5.523801803588867, 5.55531644821167, 5.461740016937256, 5.466622352600098, 5.49422550201416, 5.462524890899658, 5.415948867797852, 5.446105480194092, 5.388611316680908, 5.301318168640137, 5.358250617980957, 5.455982685089111, 5.384328365325928, 5.377086162567139, 5.38320779800415, 5.274073600769043, 5.331429481506348, 5.415081024169922, 5.3712944984436035, 5.471139430999756, 5.428740978240967, 5.371700763702393, 5.431704044342041, 5.383677959442139, 5.49606990814209, 5.408256530761719, 5.427276134490967, 5.4456305503845215, 5.400666236877441, 5.465773105621338, 5.464221954345703, 5.373061180114746, 5.42105770111084, 5.458639621734619, 5.406716346740723, 5.381041526794434, 5.451423645019531, 5.416795253753662, 5.462906837463379, 5.44065523147583, 5.357114315032959, 5.362878799438477, 5.292017936706543, 5.194145679473877, 5.324979305267334, 5.334282398223877, 5.323502063751221, 5.12430477142334, 5.123408317565918, 5.086727619171143, 5.099431037902832, 5.0359697341918945, 5.0334153175354, 5.132147312164307, 5.0717549324035645, 4.987308979034424, 5.078857421875, 5.0079145431518555, 4.958405494689941, 4.943982124328613, 4.880344390869141, 4.899112224578857, 4.915937423706055, 4.959835529327393, 4.9955830574035645, 4.862334728240967, 4.942872524261475, 4.892797470092773, 4.991301536560059, 4.962822914123535, 4.937948226928711, 5.025775909423828, 4.869335651397705, 4.883214473724365, 4.911135196685791, 4.932957172393799, 4.89949369430542, 4.878444194793701, 4.77767276763916, 4.958735466003418, 4.852108478546143, 4.9639058113098145, 4.892930507659912, 4.773200511932373, 4.782287120819092, 4.900240898132324, 4.9441938400268555, 5.0033488273620605, 4.902451515197754, 4.857772350311279, 4.840677261352539, 4.9183268547058105, 4.932402610778809, 4.848648548126221, 4.91221809387207, 4.961220741271973, 4.968077182769775, 4.853711128234863, 4.924248218536377, 4.826149940490723, 4.922983646392822, 4.922784805297852, 4.8486762046813965, 4.879526615142822, 4.902410507202148, 4.922537326812744, 4.865446090698242, 4.8402934074401855, 4.954257011413574, 4.913580894470215, 4.892297267913818, 4.882218360900879, 4.909310817718506, 4.870540618896484, 4.786752700805664, 4.8708086013793945, 4.873274326324463, 4.8402910232543945, 4.875611305236816, 4.869775772094727, 4.881042003631592, 4.952514171600342, 5.026271343231201, 4.902107238769531, 4.95825719833374, 5.043972969055176, 4.9063191413879395, 4.891351699829102, 4.8318305015563965, 4.881815433502197, 4.893507480621338, 4.8344621658325195, 4.801806449890137, 4.982505798339844, 4.875128746032715, 4.823307037353516, 4.907793998718262, 4.97895622253418, 4.88810920715332, 4.863877296447754, 4.859511852264404, 5.009963512420654, 5.021668910980225, 5.019266128540039, 5.004894733428955, 5.024456024169922, 4.943683624267578, 5.101028919219971, 5.177485942840576, 5.083004474639893, 5.104078769683838, 5.101249694824219, 5.120227336883545, 5.157015800476074, 5.165390491485596, 5.017776012420654, 5.171442985534668, 5.029735565185547, 4.963949680328369, 4.915372371673584, 4.98364782333374, 4.902719974517822, 4.899445533752441, 4.918919086456299, 4.898273944854736, 4.860844135284424, 4.980065822601318, 4.895653247833252, 5.037084579467773, 4.890525817871094, 4.887756824493408, 4.956324577331543, 4.857039451599121, 4.851027488708496, 4.816958904266357, 4.906845569610596, 4.868633270263672, 4.913453102111816, 4.8545379638671875, 4.821328639984131, 5.0420966148376465, 4.883002758026123, 5.036744594573975, 4.7365641593933105, 4.820181846618652, 4.870256423950195, 4.840094089508057, 4.85024881362915, 4.782703876495361, 4.85023832321167, 4.993537425994873, 4.8797478675842285, 4.8239545822143555, 4.881487846374512, 4.897592544555664, 4.913253307342529, 4.9061479568481445, 4.829346179962158, 4.835507392883301, 4.9507832527160645, 5.025656700134277, 4.954151630401611, 4.8186421394348145, 4.810480117797852, 4.780514717102051, 5.0001912117004395, 4.794849872589111, 4.841455459594727, 4.956998825073242, 4.836615085601807, 4.932780742645264, 4.844970226287842, 4.980130672454834, 4.9539313316345215, 4.813371181488037, 4.8763427734375, 4.898739814758301, 4.83566951751709, 4.8955302238464355, 4.803594589233398, 4.959864139556885, 4.748958110809326, 4.853501319885254, 4.729885578155518, 4.823479652404785, 4.845037460327148, 4.8994574546813965, 4.815876483917236, 4.878218173980713, 4.834809303283691, 4.918280601501465, 4.9138078689575195, 4.798436641693115, 4.901578426361084, 4.892171859741211, 4.792845726013184, 4.948843479156494, 4.937551975250244, 4.897831439971924, 4.80887508392334, 4.8840250968933105, 4.89235258102417, 4.764697074890137, 4.849213123321533, 4.9034552574157715, 4.888990879058838, 4.81598424911499, 4.8628411293029785, 4.81943941116333, 4.8526506423950195, 4.913575172424316, 4.840118885040283, 4.769186973571777, 4.856773376464844, 4.813961982727051, 4.877127170562744, 4.880669116973877, 4.8301777839660645, 4.978301525115967, 4.777905464172363, 4.847589015960693, 4.770133018493652, 4.735790252685547, 4.9571123123168945, 4.774339199066162, 4.858591556549072, 4.932504177093506, 4.800865173339844, 4.8823933601379395, 4.949519157409668, 4.7737650871276855, 4.950051307678223, 4.849956035614014, 4.843358993530273, 4.953421592712402, 4.845594882965088, 4.9103899002075195, 4.932376384735107, 4.780129909515381, 4.8133721351623535, 4.991294860839844, 4.818169116973877, 4.914535999298096, 5.002009391784668, 4.773026466369629, 4.778340816497803, 4.988157749176025, 4.9449262619018555, 4.798561096191406, 4.952543258666992, 4.804797649383545, 4.780460834503174, 4.807499885559082, 4.853896617889404, 4.920491695404053, 4.801621437072754, 4.8578667640686035, 4.806331157684326, 4.846810340881348, 4.850337982177734, 4.797661781311035, 4.754354953765869, 4.839626312255859, 4.809990406036377, 4.9075140953063965, 4.810868263244629, 4.913146018981934, 4.70273494720459, 4.82836389541626, 4.891472816467285, 4.812159061431885, 4.819380760192871, 4.886205196380615, 4.8399338722229, 4.832787990570068, 4.829469203948975, 4.743338108062744, 4.76553201675415, 4.841202259063721, 4.870558261871338, 4.910470008850098, 4.8465752601623535, 4.841795921325684, 4.883312225341797, 4.83567476272583, 4.890636920928955, 4.7990593910217285, 4.9066596031188965, 4.780176639556885, 4.831335067749023, 4.834926128387451, 4.829855442047119, 4.872430324554443, 4.7851128578186035, 4.811547756195068, 4.800918102264404, 4.862828731536865, 4.780864715576172, 4.952646732330322, 4.725395679473877, 4.852498531341553, 4.870835304260254, 5.014392852783203, 4.9402642250061035, 4.763185977935791, 4.858094215393066, 4.9478278160095215, 4.927563190460205, 4.951456546783447, 4.84918737411499, 4.96166467666626, 4.915491580963135, 4.934476375579834, 4.862053871154785, 4.7918314933776855, 4.861030578613281, 4.85020112991333, 4.816521167755127, 4.8418426513671875, 4.946598529815674, 4.917215824127197, 4.863160610198975, 4.772848606109619, 4.8711419105529785, 4.833748817443848, 4.7675652503967285, 4.896246910095215, 4.842645168304443, 4.773452281951904, 4.855315208435059, 4.837419509887695, 4.788714408874512, 4.832808017730713, 4.918002605438232, 4.865516185760498, 4.815986633300781, 4.878378391265869, 4.737114906311035, 4.801710605621338, 4.767381191253662, 4.828863143920898, 4.852729797363281, 4.894301414489746, 4.795433044433594, 4.977895736694336, 4.803250312805176, 4.804377555847168, 5.052034378051758, 4.8620924949646, 4.905611991882324, 4.808532238006592, 4.974499225616455, 4.891329765319824, 5.012494087219238, 4.9441680908203125, 5.042235374450684, 5.013676166534424, 5.101799964904785, 5.072586536407471, 5.033088684082031, 5.17344856262207, 5.088773727416992, 5.168562412261963, 5.095657825469971, 5.050797462463379, 5.074569225311279, 4.954497337341309, 5.049513339996338, 5.072228908538818, 4.843465805053711, 5.056358337402344, 4.97711181640625, 5.042965412139893, 5.004554271697998, 4.9139509201049805, 4.9129157066345215, 4.904313087463379, 4.9835710525512695, 4.869284629821777, 4.861167907714844, 4.950374603271484, 4.999871253967285, 4.912091255187988, 4.982019424438477, 4.885231018066406, 4.853174686431885, 4.858742713928223, 5.0246381759643555, 4.961054801940918, 4.893172740936279, 4.889838218688965, 4.900540351867676, 4.9697675704956055, 4.859836101531982, 4.918317794799805, 4.932976722717285, 4.958720684051514, 4.952486038208008, 5.034621715545654, 4.949357032775879, 4.882284641265869, 4.970770835876465, 4.942910194396973, 4.858755588531494, 4.927660942077637, 4.923951148986816, 4.787245273590088, 4.897830009460449, 4.9196600914001465, 4.858829498291016, 4.935553550720215, 4.817452430725098, 4.89130163192749, 4.929949760437012, 4.849085807800293, 4.975987434387207, 4.929795265197754, 4.766730308532715, 4.995109558105469, 4.9756317138671875, 4.83851957321167, 4.843717098236084, 4.905466556549072, 4.896578311920166, 4.900129795074463, 4.937170028686523, 4.971888542175293, 4.8941168785095215, 4.955633640289307, 4.876516342163086, 4.88418436050415, 4.791159152984619, 4.862752437591553, 4.882948875427246, 4.976248741149902, 4.838974952697754, 4.9302802085876465, 4.9119038581848145, 4.802540302276611, 4.898029804229736, 4.9257378578186035, 4.846880912780762, 4.8287248611450195, 4.830969333648682, 4.795075416564941, 4.822545051574707, 4.783064842224121, 4.906235694885254, 4.971224784851074, 4.735426902770996, 4.937670707702637, 4.850456714630127, 4.8680620193481445, 4.945371627807617, 4.8820271492004395, 4.906498432159424, 4.853299140930176, 4.898280143737793, 4.869894981384277, 4.822874546051025, 4.904990196228027, 4.7751665115356445, 4.939137935638428, 4.912277698516846, 4.807402610778809, 4.9155144691467285, 4.922115802764893, 4.975242614746094, 4.9329915046691895, 4.810337543487549, 4.749467849731445, 4.873662948608398, 4.881613254547119, 4.861032485961914, 4.8970255851745605, 4.844944953918457, 4.945504665374756, 4.907961845397949, 4.853128433227539, 4.829188823699951, 4.941971302032471, 4.869998455047607, 4.833054065704346, 4.850509166717529, 4.856948375701904, 4.88344144821167, 4.874589920043945, 4.838799476623535, 4.8724260330200195, 4.742987155914307, 4.879517555236816, 4.837889671325684, 4.8553595542907715, 4.820466041564941, 4.923385143280029, 4.9653730392456055, 4.8728251457214355, 4.949574947357178, 4.879518508911133, 4.820606708526611, 4.908611297607422, 4.822028160095215, 4.887909889221191, 4.824652671813965, 4.980862617492676, 5.005505561828613, 4.837616920471191, 4.874764442443848, 4.804924011230469, 4.888486862182617, 4.86440372467041, 4.930170059204102, 4.852072715759277, 4.85846471786499, 4.8803935050964355, 4.891520023345947, 4.852762222290039, 4.831899642944336, 4.90809965133667, 4.753916263580322, 4.936629295349121, 4.93406343460083, 4.841524600982666, 4.778948783874512, 4.851513862609863, 4.902289867401123, 4.825677871704102, 4.822517395019531, 4.849972248077393, 4.769810676574707, 4.7568583488464355, 4.850489616394043, 4.858685493469238, 4.9520487785339355, 4.779742240905762, 4.901329517364502, 4.883302211761475, 4.8733015060424805, 4.742684841156006, 4.857026100158691, 4.958275318145752, 4.969813346862793, 4.88311767578125, 4.817763328552246, 4.941175937652588, 4.834549903869629, 5.052419662475586, 4.902125358581543, 4.948055267333984, 4.881306171417236, 4.856849193572998, 4.999600887298584, 4.839297294616699, 4.837224960327148, 4.832777976989746, 4.858757972717285, 4.968242168426514, 4.916359901428223, 4.92754602432251, 4.766728401184082, 4.927431106567383, 4.880090713500977, 4.942408561706543, 4.917681694030762, 4.846240997314453, 4.903797626495361, 4.939064979553223, 4.898930549621582, 4.91174840927124, 4.8583784103393555, 4.885056972503662, 4.998497486114502, 5.027385234832764, 4.924383640289307, 4.883837699890137, 4.940883159637451, 4.9798665046691895, 4.931564807891846, 5.094325542449951, 4.95662784576416, 5.096514701843262, 4.958502769470215, 4.877053260803223, 4.933804988861084, 4.815670490264893, 5.000987529754639, 4.955714702606201, 4.934561252593994, 5.038730144500732, 4.839636325836182, 4.958617210388184, 5.005723476409912, 5.082552433013916, 5.049124717712402, 4.96098518371582, 5.022493839263916, 5.032040596008301, 4.981917381286621, 5.011484622955322, 4.998027801513672, 4.999660015106201, 4.9808149337768555, 5.037152290344238, 4.929807662963867, 4.940369606018066, 5.184407711029053, 5.010076522827148, 5.042753219604492, 5.035070896148682, 5.002666473388672, 4.990581035614014, 5.0933613777160645, 4.946747779846191, 4.925093650817871, 5.053478717803955, 4.827635288238525, 4.953771591186523, 4.990330219268799, 5.0631818771362305, 5.011109352111816, 5.032605171203613, 4.97274112701416, 4.901027202606201, 4.849542140960693, 4.944982528686523, 4.923044681549072, 5.001723289489746, 4.970516681671143, 4.98505163192749, 4.927006244659424, 4.928404808044434, 4.933736324310303, 4.8322343826293945, 4.856032848358154, 4.969353199005127, 5.015832424163818, 4.848662376403809, 4.921211242675781, 5.027261257171631, 4.916650772094727, 4.949711799621582, 4.894658088684082, 4.874053955078125, 4.921395778656006, 4.92046594619751, 4.981196403503418, 4.989304542541504, 5.0039825439453125, 4.9025139808654785, 4.909073829650879, 4.868941783905029, 4.990915298461914, 4.965474605560303, 4.861227989196777, 4.791544437408447, 4.911559581756592, 4.915250778198242, 4.844534873962402, 5.048593997955322, 4.823497772216797, 4.930572509765625, 4.9876203536987305, 4.917388439178467, 4.919291019439697, 4.980381011962891, 4.936223983764648, 4.937934398651123, 4.925343036651611, 4.911492347717285, 4.9060139656066895, 4.884943008422852, 4.861012935638428, 4.935199737548828, 4.928208827972412, 4.9090752601623535, 4.795553684234619, 4.981780052185059, 4.942728519439697, 4.896239280700684, 4.790427207946777, 4.956515312194824, 5.019418716430664, 4.932172775268555, 4.779885292053223, 4.963839054107666, 4.834293842315674, 4.817642688751221, 4.857090950012207, 4.932976722717285, 4.954160690307617, 5.079560279846191, 5.000247478485107, 4.83834171295166, 4.870997428894043, 4.865142345428467, 4.955745220184326, 4.8435258865356445, 4.931900501251221, 4.805573463439941, 4.917818546295166, 4.934776782989502, 4.881040573120117, 4.889463424682617, 4.793274879455566, 4.834729194641113, 4.883968830108643, 4.890366554260254, 4.833463668823242, 4.798077583312988, 4.911937713623047, 4.703286170959473, 4.820594310760498, 4.814054012298584, 4.92003059387207, 4.885904788970947, 4.856021404266357, 4.909790992736816, 4.867680072784424, 4.944260120391846, 4.888415813446045, 4.919349193572998, 4.895539283752441, 4.890873432159424, 4.769775867462158, 4.877862930297852, 4.879817008972168, 4.901982307434082, 4.864740371704102, 4.974339008331299, 4.873950958251953, 4.881669521331787, 4.798919200897217, 4.958056449890137, 4.872080326080322, 4.860196590423584, 4.670712471008301, 4.990800857543945, 4.933233261108398, 4.932716369628906, 4.924651145935059, 4.803351402282715, 4.872036457061768, 4.799780368804932, 4.688636302947998, 4.835476875305176, 4.858108997344971, 4.922670841217041, 4.992490768432617, 4.896157264709473, 4.838886260986328, 4.823812484741211, 4.894608497619629, 4.759144306182861, 4.954995632171631, 4.853614330291748, 4.788178443908691, 4.869241714477539, 4.945267677307129, 4.8131608963012695, 4.833661079406738, 4.949136734008789, 4.929605007171631, 4.879380702972412, 4.822112083435059, 4.877224445343018, 4.8079915046691895, 4.876995086669922, 4.871204853057861, 4.848352432250977, 4.911779880523682, 4.858049392700195, 4.886106967926025, 4.854732036590576, 4.965374946594238, 4.863099098205566, 4.871602535247803, 4.853290557861328, 4.971255302429199, 4.810242176055908, 4.853755474090576, 4.802657604217529, 4.833939552307129, 4.7823100090026855, 4.7826948165893555, 4.924610137939453, 4.789372444152832, 4.8509135246276855, 4.909141540527344, 4.8822431564331055, 4.9448957443237305, 4.875637531280518, 4.825397491455078, 4.869744300842285, 4.871622085571289, 4.797089576721191, 4.780522346496582, 4.772601127624512, 4.9042534828186035, 4.841095924377441, 4.875675201416016, 4.925627708435059, 4.759478569030762, 4.875639915466309, 4.825338840484619, 4.8931474685668945, 4.829287052154541, 4.845934867858887, 4.835909843444824, 4.816086769104004, 4.836145401000977, 4.950466156005859, 4.846220970153809, 4.838823318481445, 4.944983005523682, 4.921520709991455, 4.96879768371582, 4.930453300476074, 4.941139221191406, 4.9253411293029785, 5.007933139801025, 5.002240180969238, 5.109364032745361, 4.975811004638672, 4.984053611755371, 4.986569404602051, 5.065104007720947, 5.038511753082275, 5.1091718673706055, 4.913392066955566, 4.973642826080322, 5.058256149291992, 4.982005596160889, 4.994176387786865, 5.052244663238525, 5.100518226623535, 4.968625068664551, 5.095160007476807, 5.022759914398193, 4.995335578918457, 5.059608459472656, 5.024540424346924, 5.000341415405273, 4.964866638183594, 4.9544596672058105, 4.84966516494751, 5.061323165893555, 4.909722328186035, 4.936460018157959, 4.959897041320801, 4.919938564300537, 4.963565826416016, 4.872216701507568, 4.846697807312012, 4.9538655281066895, 4.825379371643066, 4.972653865814209, 4.8685832023620605, 4.879327774047852, 4.931296348571777, 4.778970718383789, 4.859289646148682, 4.903146266937256, 4.813662528991699, 4.766680717468262, 4.775050640106201, 4.786548137664795, 4.812189102172852, 4.866906642913818, 4.832487106323242, 4.857001304626465, 4.86677885055542, 4.894096374511719, 4.786361217498779, 4.917964458465576, 4.927262306213379, 4.819903373718262, 4.90359354019165, 4.900754451751709, 4.843082904815674, 4.8121771812438965, 4.877988338470459, 4.817818641662598, 4.913000583648682, 4.951999664306641, 4.752230167388916, 4.79326868057251, 4.847031116485596, 4.768874645233154, 4.877488136291504, 5.008163928985596, 4.918346405029297, 4.8943986892700195, 4.811541557312012, 4.900115013122559, 4.876605033874512, 4.735055446624756, 5.00214147567749, 4.897580623626709, 4.86245584487915, 4.957683563232422, 4.94222354888916, 4.94645881652832, 4.983553886413574, 5.006421089172363, 4.8662614822387695, 4.89091682434082, 4.9547858238220215, 4.8872880935668945, 5.065035343170166, 5.089046478271484, 5.071043968200684, 4.984629154205322, 5.105496883392334, 4.968535423278809, 5.049991130828857, 4.979973316192627, 5.000553607940674, 5.07868766784668, 5.101245880126953, 5.117181777954102, 5.141852855682373, 5.166790962219238, 5.213998794555664, 5.2289228439331055, 5.242764949798584, 5.202552795410156, 5.199869155883789, 5.310398578643799, 5.306485652923584, 5.115683078765869, 5.231549263000488, 5.0824995040893555, 5.187078475952148, 5.173403263092041, 5.212320327758789, 5.144893646240234, 5.288905143737793, 5.246615409851074, 5.204723834991455, 5.219264507293701, 5.2782697677612305, 5.321748733520508, 5.158188819885254, 5.200596809387207, 5.300965785980225, 5.2862935066223145, 5.243646621704102, 5.19950008392334, 5.184028148651123, 5.310333251953125, 5.223424434661865, 5.245043754577637, 5.251499652862549, 5.253670692443848, 5.222640514373779, 5.258784770965576, 5.389348030090332, 5.204545974731445, 5.290368556976318, 5.197049617767334, 5.151809215545654, 5.213109493255615, 5.207961082458496, 5.1639933586120605, 5.275548934936523, 5.259892463684082, 5.253340244293213, 5.354615211486816, 5.324602127075195, 5.347431659698486, 5.166315078735352, 5.322258949279785, 5.211071968078613, 5.373521327972412, 5.327582359313965, 5.206829071044922, 5.24399995803833, 5.303515911102295, 5.274425029754639, 5.263243675231934, 5.243725299835205, 5.250889778137207, 5.231029033660889, 5.202740669250488, 5.216978549957275, 5.23734712600708, 5.2748212814331055, 5.28617000579834, 5.270480632781982, 5.273682117462158, 5.258432865142822, 5.291306018829346, 5.281951904296875, 5.315070152282715, 5.31481409072876, 5.252996921539307, 5.317270755767822, 5.2287421226501465, 5.216673851013184, 5.146583080291748, 5.302666187286377, 5.2491631507873535, 5.2363080978393555, 5.302568435668945, 5.146386623382568, 5.299800395965576, 5.345887184143066, 5.224141597747803, 5.190792560577393, 5.300354480743408, 5.270727157592773, 5.281357765197754, 5.270985126495361, 5.335334300994873, 5.228139877319336, 5.1943135261535645, 5.1819233894348145, 5.332531452178955, 5.296527862548828, 5.23858642578125, 5.330620765686035, 5.248711109161377, 5.246052265167236, 5.121068000793457, 5.197251319885254, 5.347300052642822, 5.166682720184326, 5.204662322998047, 5.241555213928223, 5.163413047790527, 5.197858810424805, 5.280734539031982, 5.345672607421875, 5.26600980758667, 5.2122802734375, 5.240350246429443, 5.140328407287598, 5.197944641113281, 5.291308879852295, 5.2442827224731445, 5.217279434204102, 5.260023593902588, 5.218685626983643, 5.341039180755615, 5.350396156311035, 5.250983238220215, 5.245852947235107, 5.139167308807373, 5.228944778442383, 5.2484846115112305, 5.113510608673096, 5.1728596687316895, 5.259031295776367, 5.1258649826049805, 5.1733012199401855, 5.24553108215332, 5.243639945983887, 5.143969535827637, 5.057485580444336, 5.124386310577393, 5.101266384124756, 5.203014850616455, 5.074854373931885, 5.14709997177124, 4.998631954193115, 5.178366184234619, 5.024205207824707, 5.029810905456543, 5.144006729125977, 5.048855304718018, 5.066547393798828, 5.161630630493164, 5.175734519958496, 5.24531888961792, 5.11367130279541, 5.140605926513672, 5.104868412017822, 5.1165032386779785, 5.247287273406982, 5.135748863220215, 5.144444465637207, 5.1640753746032715, 5.052082538604736, 5.042232990264893, 5.050835132598877, 4.983679294586182, 5.0675554275512695, 5.089614391326904, 5.1675801277160645, 4.974210262298584, 5.117831707000732, 5.036204814910889, 5.049286365509033, 5.102893352508545, 5.030770778656006, 5.125227928161621, 5.038477897644043, 5.060118198394775, 4.995254039764404, 5.036772727966309, 5.072403907775879, 5.08558988571167, 5.024456024169922, 4.9899001121521, 5.026007175445557, 5.058930397033691, 5.02912712097168, 4.892099380493164, 5.0222954750061035, 5.173230171203613, 5.068039894104004, 4.922584056854248, 4.945079326629639, 5.122833251953125, 5.084813117980957, 4.986564636230469, 5.045495510101318, 4.943722248077393, 5.028012275695801, 5.103785991668701, 5.026026248931885, 5.08491849899292, 5.060037612915039, 5.11090087890625, 5.00982666015625, 5.080885410308838, 5.044143199920654, 5.087434768676758, 5.027766704559326, 5.05271053314209, 5.09342098236084, 5.070834159851074, 4.905158042907715, 5.066834449768066, 4.989264488220215, 4.98122501373291, 5.045976161956787, 5.008536338806152, 5.083457946777344, 4.970309257507324, 5.046273231506348, 4.9693379402160645, 5.040931224822998, 5.069090843200684, 5.001382827758789, 4.998475074768066, 4.997042179107666, 4.9319281578063965, 5.043121814727783, 5.04542350769043, 5.05906343460083, 5.1651434898376465, 5.087791442871094, 5.059314250946045, 5.097667217254639, 5.014814853668213, 5.0639872550964355, 5.083205223083496, 5.144164085388184, 5.120795726776123, 5.108456611633301, 5.177207946777344, 5.170416355133057, 5.196304798126221, 5.133599758148193, 5.241373538970947, 5.176558494567871, 5.153149127960205, 5.08312463760376, 5.208558082580566, 5.220449447631836, 5.317399024963379, 5.221331596374512, 5.221538543701172, 5.198609352111816, 5.130110263824463, 5.3040547370910645, 5.151366233825684, 5.207209587097168, 5.267491340637207, 5.356479167938232, 5.264277458190918, 5.186776638031006, 5.2233405113220215, 5.222937107086182, 5.23764705657959, 5.218535900115967, 5.189250946044922, 5.259349346160889, 5.183178424835205, 5.1995530128479, 5.3054118156433105, 5.238415718078613, 5.2450032234191895, 5.238223552703857, 5.3108954429626465, 5.201160907745361, 5.167675971984863, 5.225900650024414, 5.1406073570251465, 5.2406134605407715, 5.2034478187561035, 5.34165096282959, 5.158665180206299, 5.2392473220825195, 5.371157169342041, 5.254693984985352, 5.298525810241699, 5.213575839996338, 5.241640090942383, 5.190333366394043, 5.199768543243408, 5.248684883117676, 5.200165271759033, 5.1679887771606445, 5.265201568603516, 5.151693820953369, 5.0994977951049805, 5.22410774230957, 5.1527814865112305, 5.2590556144714355, 5.218822479248047, 5.1697773933410645, 5.239882469177246, 5.357335090637207, 5.180844783782959, 5.127706527709961, 5.240675926208496, 5.100804328918457, 5.150064468383789, 5.09900426864624, 5.221895694732666, 5.235661029815674, 5.156700611114502, 5.276567459106445, 5.222953796386719, 5.2507429122924805, 5.1140594482421875, 5.256196022033691, 5.181252479553223, 5.178009986877441, 5.140509128570557, 5.145194053649902, 5.268227577209473, 5.185287952423096, 5.114973545074463, 5.232194900512695, 5.298618793487549, 5.245486736297607, 5.263274192810059, 5.192060470581055, 5.188384056091309, 5.146679878234863, 5.180591106414795, 5.237849712371826, 5.246044158935547, 5.260313510894775, 5.2511420249938965, 5.287227153778076, 5.1913065910339355, 5.230311393737793, 5.238177299499512, 5.195878982543945, 5.302896499633789, 5.1807169914245605, 5.193578243255615, 5.233348369598389, 5.266077041625977, 5.202591896057129, 5.2408061027526855, 5.2843523025512695, 5.320724010467529, 5.2782301902771, 5.194422721862793, 5.140063762664795, 5.189137935638428, 5.1995954513549805, 5.271969318389893, 5.187505722045898, 5.178708553314209, 5.413363933563232, 5.230370044708252, 5.237405776977539, 5.280319690704346, 5.124882221221924, 5.272568702697754, 5.182923793792725, 5.264499664306641, 5.168546676635742, 5.133772850036621, 5.226983547210693, 5.219926834106445, 5.167755126953125, 5.259582996368408, 5.164122581481934, 5.245569229125977, 5.2719831466674805, 5.179056167602539, 5.195711135864258, 5.354429244995117, 5.215603351593018, 5.278121471405029, 5.258565902709961, 5.16533899307251, 5.262330532073975, 5.20473575592041, 5.139379978179932, 5.047997951507568, 5.179884910583496, 5.179948329925537, 5.159251689910889, 5.259551525115967, 5.229878902435303, 5.229298114776611, 5.1793107986450195, 5.257989406585693, 5.203641414642334, 5.074757099151611, 5.295801162719727, 5.097293853759766, 5.209052085876465, 5.3814215660095215, 5.188776016235352, 5.22347354888916, 5.211794376373291, 5.245537281036377, 5.139256477355957, 5.209644317626953, 5.217402935028076, 5.126460552215576, 5.162982940673828, 5.2274699211120605, 5.21988582611084, 5.187052249908447, 5.25566291809082, 5.301019191741943, 5.285307884216309, 5.163149356842041, 5.23933744430542, 5.1056342124938965, 5.20533561706543, 5.327649116516113, 5.14135217666626, 5.107724666595459, 5.212123394012451, 5.29037618637085, 5.223019599914551, 5.159293174743652, 5.244694232940674, 5.220022678375244, 5.175687789916992, 5.270506381988525, 5.189711570739746, 5.173386573791504, 5.2101826667785645, 5.221686363220215, 5.099200248718262, 5.213990211486816, 5.190656661987305, 5.296055793762207, 5.176671028137207, 5.207691192626953, 5.121190071105957, 5.194039821624756, 5.305388450622559, 5.116791725158691, 5.220515251159668, 5.186351299285889, 5.182478904724121, 5.148719787597656, 5.175754547119141, 5.15860652923584, 5.166593551635742, 5.277861595153809, 5.214634895324707, 5.166759967803955, 5.145779132843018, 5.21049165725708, 5.187431335449219, 5.159178733825684, 5.245377540588379, 5.1656670570373535, 5.128511428833008, 5.171866416931152, 5.136050701141357, 5.118294715881348, 5.197237014770508, 5.192028999328613, 5.203567028045654, 5.094966411590576, 5.0173726081848145, 5.18638277053833, 5.181364059448242, 5.086911678314209, 5.126617908477783, 5.252015590667725, 5.2255659103393555, 5.223758220672607, 5.215947151184082, 5.074207782745361, 5.040335655212402, 5.1824564933776855, 5.235185623168945, 5.286375522613525, 5.219391822814941, 5.27457332611084, 5.211461067199707, 5.197121620178223, 5.043448448181152, 5.173911094665527, 5.201232433319092, 5.2373456954956055, 5.217677116394043, 5.218223571777344, 5.139039039611816, 5.197098731994629, 5.140809059143066, 5.264800071716309, 5.244414329528809, 5.230217933654785, 5.260233402252197, 5.175125598907471, 5.22504186630249, 5.175495147705078, 5.120141506195068, 5.095320701599121, 5.180572986602783, 5.144064426422119, 5.157469749450684, 5.263670921325684, 5.199779033660889, 5.1878342628479, 5.170764923095703, 5.2150797843933105, 5.245936393737793, 5.206894874572754, 5.281473636627197, 5.273655891418457, 5.257072925567627, 5.295635223388672, 5.2954511642456055, 5.166843891143799, 5.1666765213012695, 4.9811601638793945, 5.095524311065674, 5.184107303619385, 5.073850631713867, 5.17349910736084, 5.2054548263549805, 5.200027942657471, 5.207082748413086, 5.174221992492676, 5.23978328704834, 5.110569953918457, 5.092743396759033, 5.183517932891846, 5.153933048248291, 5.145707130432129, 5.181844711303711, 5.201311111450195, 5.235205173492432, 5.1493706703186035, 5.211659908294678, 5.2319841384887695, 5.190112113952637, 5.1528778076171875, 5.37760591506958, 5.134612083435059, 5.14340877532959, 5.091578960418701, 5.149176120758057, 5.090084075927734, 5.224277019500732, 5.179230690002441, 5.229744911193848, 5.0976762771606445, 5.028605937957764, 5.079598426818848, 5.205208778381348, 5.157257080078125, 5.2396440505981445, 5.247193336486816, 5.221492290496826, 5.1574931144714355, 5.162774085998535, 5.176346778869629, 5.156365394592285, 5.207494258880615, 5.183244228363037, 5.169245719909668, 5.096856117248535, 5.051105976104736, 5.054104328155518, 5.110002517700195, 5.184576988220215, 5.135343551635742, 5.171650409698486, 5.190393924713135, 5.223208427429199, 5.111270427703857, 5.199070453643799, 5.167025089263916, 5.29718017578125, 5.240278720855713, 5.112101078033447, 5.159675598144531, 5.086381912231445, 5.083258152008057, 5.206532955169678, 5.1571574211120605, 5.242739200592041, 5.058018207550049, 5.22341251373291, 5.225005149841309, 5.258726596832275, 5.2798357009887695, 5.172595024108887, 5.185458183288574, 5.100800037384033, 5.228171348571777, 5.165401458740234, 5.162078380584717, 5.157622337341309, 5.243596076965332, 5.196863651275635, 5.124484539031982, 5.286782264709473, 5.224364757537842, 5.254160404205322, 5.141678810119629, 5.227031230926514, 5.244359016418457, 5.301453590393066, 5.287379264831543, 5.2633256912231445, 5.131112575531006, 5.172204971313477, 5.279871940612793, 5.237240314483643, 5.189054489135742, 5.207273960113525, 5.174655437469482, 5.281625747680664, 5.316286563873291, 5.186319828033447, 5.26171350479126, 5.201461315155029, 5.231600761413574, 5.2527241706848145, 5.352332592010498, 5.257775783538818, 5.356700420379639, 5.153758525848389, 5.253429889678955, 5.141574382781982, 5.132763385772705, 5.200626850128174, 5.167433738708496, 5.275087833404541, 5.1324262619018555, 5.155190467834473, 5.1651177406311035, 5.171978950500488, 5.22183895111084, 5.122200965881348, 5.242619037628174, 5.28569221496582, 5.216946601867676, 5.24233341217041, 5.219799518585205, 5.128670692443848, 5.238880634307861, 5.236569881439209, 5.184335708618164, 5.204543590545654, 5.1860737800598145, 5.175481796264648, 5.178715229034424, 5.179752349853516, 5.277651786804199, 5.147312641143799, 5.247105598449707, 5.11761999130249, 5.137866020202637, 5.198657035827637, 5.240964412689209, 5.163947582244873, 5.160049915313721, 5.219854831695557, 5.222921371459961, 5.169236183166504, 5.1446919441223145, 5.303443431854248, 5.190009117126465, 5.20584774017334, 5.2135210037231445, 5.2162933349609375, 5.126695156097412, 5.278468132019043, 5.148944854736328, 5.3507184982299805, 5.139065265655518, 5.1343793869018555, 5.355823516845703, 5.200171947479248, 5.206908226013184, 5.2784857749938965, 5.1883625984191895, 5.216953754425049, 5.194790363311768, 5.150987148284912, 5.206856727600098, 5.251954078674316, 5.2205023765563965, 5.221797466278076, 5.2420830726623535, 5.225231170654297, 5.251952171325684, 5.225581645965576, 5.23801851272583, 5.232413291931152, 5.197190284729004, 5.282357692718506, 5.195014476776123, 5.242274761199951, 5.234833717346191, 5.140932559967041, 5.088687419891357, 5.198570251464844, 5.168211936950684, 5.21241569519043, 5.160321235656738, 5.168052673339844, 5.25669527053833, 5.1223907470703125, 5.215882301330566, 5.158822059631348, 5.017178535461426, 5.144873142242432, 5.211824417114258, 5.158681392669678, 5.1326093673706055, 5.196941375732422, 5.280743598937988, 5.034672260284424, 5.207876682281494, 5.093253135681152, 5.1961989402771, 5.243217945098877, 5.128623008728027, 5.141439437866211, 5.207542419433594, 5.177936553955078, 5.239916801452637, 5.172717094421387, 5.237442493438721, 5.196407794952393, 5.129577159881592, 5.245632648468018, 5.149491310119629, 5.163207530975342, 5.195562839508057, 5.210110187530518, 5.075188636779785, 5.141764163970947, 5.2711286544799805, 5.220668792724609, 5.138150691986084, 5.089686870574951, 5.207929611206055, 5.1449713706970215, 5.146740436553955, 5.171260833740234, 5.103997230529785, 5.111077785491943, 5.060409069061279, 5.157471656799316, 5.134781837463379, 5.095216274261475, 5.07376766204834, 5.147387981414795, 5.156883716583252, 5.106298923492432, 5.1177568435668945, 5.043026447296143, 5.080163478851318, 5.163022518157959, 5.07288122177124, 5.1284308433532715, 5.09022331237793, 5.1351447105407715, 5.149619102478027, 5.102080345153809, 5.048727512359619, 5.176962375640869, 5.042922496795654, 5.08472204208374, 5.307918548583984, 5.11135196685791, 5.1224799156188965, 5.113085746765137, 5.185068130493164, 5.091733455657959, 5.191946506500244, 5.1239447593688965, 5.067857265472412, 5.153513431549072, 5.088793754577637, 5.15428352355957, 5.1305623054504395, 5.130148887634277, 5.194942474365234, 5.209660530090332, 5.0660786628723145, 5.117239952087402, 5.069201469421387, 5.10908317565918, 5.165384769439697, 5.173300743103027, 5.065772533416748, 5.066415309906006, 5.160470008850098, 5.0893144607543945, 5.10998010635376, 5.078733444213867, 5.106289386749268, 5.072682857513428, 5.0415568351745605, 5.121060371398926, 5.045521259307861, 5.018548011779785, 5.0847487449646, 5.064728736877441, 5.1214423179626465, 5.030675411224365, 5.044864177703857, 5.227166175842285, 5.058578968048096, 5.091362953186035, 4.977688789367676, 5.110528945922852, 5.085780620574951, 5.065677165985107, 5.093596458435059, 5.075728416442871, 5.124334335327148, 5.0843400955200195, 5.072445392608643, 5.092628002166748, 5.14624547958374, 5.060708045959473, 5.081963062286377, 5.215224266052246, 5.23323917388916, 5.11899471282959, 5.103447914123535, 5.024051189422607, 5.188249111175537, 5.138185024261475, 5.155882835388184, 5.073369979858398, 5.072380542755127, 5.0651469230651855, 5.166229248046875, 5.059403896331787, 5.06051778793335, 5.004784107208252, 5.132811546325684, 5.046794891357422, 5.138424396514893, 5.107791900634766, 5.101992607116699, 5.037224292755127, 5.03631067276001, 5.081674098968506, 5.102488040924072, 5.094245910644531, 5.156705856323242, 5.130110263824463, 5.164056777954102, 5.188007831573486, 5.145391464233398, 5.1857523918151855, 5.126996040344238, 5.115779876708984, 5.265334129333496, 5.189629554748535, 5.102872848510742, 5.149183750152588, 5.161287784576416, 5.142242431640625, 5.217270851135254, 5.190423011779785, 5.177441120147705, 5.150485515594482, 5.051453590393066, 5.066432952880859, 5.052683353424072, 5.17782735824585, 4.920627117156982, 5.139304161071777, 5.131524562835693, 5.112109184265137, 5.277780532836914, 5.134322166442871, 5.124073028564453, 5.176636695861816, 5.141321182250977, 5.120251655578613, 5.06049108505249, 5.053027629852295, 5.060403823852539, 5.080831527709961, 5.112944602966309, 5.0176472663879395, 5.079063892364502, 5.139845848083496, 5.11295747756958, 5.153772354125977, 5.075904369354248, 4.985989570617676, 5.037490367889404, 5.04079008102417, 5.155937671661377, 5.170769691467285, 5.2208147048950195, 5.056780815124512, 5.122748374938965, 5.131126403808594, 5.039307594299316, 5.160525798797607, 5.093586444854736, 5.083530902862549, 5.03158712387085, 5.105996608734131, 5.0959062576293945, 5.0776686668396, 5.050200462341309, 5.075728416442871, 5.026848316192627, 5.043578147888184, 4.95629358291626, 5.110536098480225, 4.985450744628906, 4.9411821365356445, 5.131749629974365, 5.065556049346924, 5.135838985443115, 5.036648273468018, 5.101701259613037, 4.998774528503418, 5.021974563598633, 5.031161308288574, 5.168808937072754, 5.006540298461914, 5.105757236480713, 5.091977596282959, 5.014554977416992, 5.07688570022583, 4.974833965301514, 5.002425670623779, 5.03425931930542, 5.053041458129883, 5.102443695068359, 4.977303981781006, 5.054732322692871, 5.0231428146362305, 5.0271501541137695, 5.06935453414917, 5.105351448059082, 4.9686079025268555, 5.127496242523193, 4.998944282531738, 5.038885593414307, 5.062430381774902, 5.065252304077148, 5.009901523590088, 4.950074672698975, 5.057125568389893, 5.109187126159668, 5.061272144317627, 5.113274574279785, 5.021711826324463, 5.157948970794678, 5.0382561683654785, 4.993192195892334, 5.092644214630127, 4.950291156768799, 4.986235618591309, 5.039493083953857, 4.971157073974609, 5.057917594909668, 5.042235374450684, 4.8824381828308105, 4.973464012145996, 5.093026638031006, 5.039593696594238, 4.945916175842285, 4.879037380218506, 5.035401821136475, 5.037309646606445, 4.995059490203857, 4.980670928955078, 5.051012992858887, 4.945258140563965, 5.083864688873291, 4.992556095123291, 5.054835796356201, 4.943033218383789, 4.99575138092041, 5.003300666809082, 4.962317943572998, 4.96967887878418, 5.008167743682861, 4.997619152069092, 5.102545738220215, 5.061532020568848, 5.006335258483887]\n",
      "[0.6929770708084106, 1.1238685846328735, 2.2970142364501953, 1.4763668775558472, 0.813361644744873, 0.7139677405357361, 0.854887068271637, 0.7674857378005981, 0.6950122117996216, 0.8106515407562256, 0.7621591687202454, 0.6931519508361816, 0.6998929977416992, 0.7363852858543396, 0.7244699001312256, 0.7074806690216064, 0.6947083473205566, 0.7304673790931702, 0.7331804633140564, 0.6958314776420593, 0.7003998756408691, 0.7177497744560242, 0.7066391706466675, 0.6925491094589233, 0.6965760588645935, 0.7048106789588928, 0.6884549260139465, 0.7066236138343811, 0.6949429512023926, 0.7066715955734253, 0.7031509280204773, 0.6935129761695862, 0.7034544944763184, 0.6965124011039734, 0.694106936454773, 0.6935907602310181, 0.6934067010879517, 0.6988702416419983, 0.692651093006134, 0.6922971606254578, 0.6926413178443909, 0.6925995349884033, 0.7023797035217285, 0.7017972469329834, 0.6937182545661926, 0.6962341666221619, 0.6995202898979187, 0.6918249130249023, 0.6923536062240601, 0.6902569532394409, 0.6917064189910889, 0.6994366645812988, 0.6949073076248169, 0.6943750977516174, 0.6931651830673218, 0.7021971940994263, 0.6939457058906555, 0.6904623508453369, 0.7143658995628357, 0.6921443343162537, 0.6928514242172241, 0.6930252313613892, 0.6949594616889954, 0.6967379450798035, 0.6940816640853882, 0.6958511471748352, 0.700160562992096, 0.6885218620300293, 0.6918877959251404, 0.6938961744308472, 0.6948925256729126, 0.6901750564575195, 0.691796600818634, 0.7075551748275757, 0.7019937634468079, 0.6932546496391296, 0.7116952538490295, 0.699397623538971, 0.6987213492393494, 0.6994016766548157, 0.7066239714622498, 0.6948294639587402, 0.6941757798194885, 0.6949127316474915, 0.6995757818222046, 0.6966817378997803, 0.6924420595169067, 0.7059156894683838, 0.6978276371955872, 0.6954312324523926, 0.6954927444458008, 0.7122833728790283, 0.6990821361541748, 0.6918725967407227, 0.6985138058662415, 0.7081744074821472, 0.696921169757843, 0.7082962989807129, 0.7195385694503784, 0.6935237050056458, 0.6987878680229187, 0.7107442617416382, 0.6966784000396729, 0.709635853767395, 0.6928918957710266, 0.6924521923065186, 0.6927857995033264, 0.6933500170707703, 0.69344562292099, 0.6956626772880554, 0.693970263004303, 0.6949264407157898, 0.6946430802345276, 0.6937808990478516, 0.6951421499252319, 0.6949222683906555, 0.6932310461997986, 0.6936370134353638, 0.695014238357544, 0.6956558227539062, 0.6952505111694336, 0.6938554048538208, 0.693311333656311, 0.6913194060325623, 0.6927039623260498, 0.697891116142273, 0.6932865381240845, 0.6951012015342712, 0.6913240551948547, 0.6947785019874573, 0.6977719068527222, 0.6898291110992432, 0.700404703617096, 0.6997261047363281, 0.7052969336509705, 0.6999964714050293, 0.7501087188720703, 0.7013794779777527, 0.7158718705177307, 0.7175028920173645, 0.7033220529556274, 0.6979247331619263, 0.7268441915512085, 0.7037308812141418, 0.69990074634552, 0.6946518421173096, 0.7060754299163818, 0.6949011087417603, 0.7076219320297241, 0.7064201235771179, 0.6922920942306519, 0.6940498352050781, 0.6914927363395691, 0.6944901347160339, 0.6919859647750854, 0.6948767304420471, 0.6975747346878052, 0.6926487684249878, 0.6962741017341614, 0.6918899416923523, 0.6932303309440613, 0.6935631036758423, 0.6984965801239014, 0.7048778533935547, 0.6923533082008362, 0.6942551136016846, 0.712632417678833, 0.7007006406784058, 0.7087980508804321, 0.7168524861335754, 0.6963458061218262, 0.7013537287712097, 0.7242050766944885, 0.6953070163726807, 0.710435152053833, 0.6992823481559753, 0.6962438225746155, 0.7011991143226624, 0.7147966027259827, 0.6942866444587708, 0.708159327507019, 0.7135109305381775, 0.696110188961029, 0.698248565196991, 0.7414810061454773, 0.6957048177719116, 0.7034823894500732, 0.7057973146438599, 0.7010665535926819, 0.6952008008956909, 0.6922170519828796, 0.7069262862205505, 0.6927686929702759, 0.6999480128288269, 0.699461817741394, 0.692013144493103, 0.6952883005142212, 0.6967423558235168, 0.6928843259811401, 0.6928146481513977, 0.7017409801483154, 0.6961421370506287, 0.6913213729858398, 0.6960603594779968, 0.6921208500862122, 0.6947400569915771, 0.6926708221435547, 0.6974619030952454, 0.6928029656410217, 0.6904688477516174, 0.6926100850105286, 0.6947360634803772, 0.6929729580879211, 0.6943405866622925, 0.6931931376457214, 0.6935110092163086, 0.6953029036521912, 0.6936729550361633, 0.6934584379196167, 0.6931875348091125, 0.6925714612007141, 0.6909016966819763, 0.7022177577018738, 0.692482054233551, 0.6960362792015076, 0.6942750811576843, 0.6928834319114685, 0.6945683360099792, 0.6929581165313721, 0.6924882531166077, 0.6971389651298523, 0.6932406425476074, 0.6941370368003845, 0.6940772533416748, 0.6924141049385071, 0.6933285593986511, 0.6937504410743713, 0.695219099521637, 0.6936894655227661, 0.6899142265319824, 0.7132875919342041, 0.7034021615982056, 0.7030011415481567, 0.7048230171203613, 0.6985676884651184, 0.7022705674171448, 0.6996819972991943, 0.6920874714851379, 0.6930286288261414, 0.7016464471817017, 0.6920198202133179, 0.7023724913597107, 0.7047238349914551, 0.6928749084472656, 0.7135140895843506, 0.7290723323822021, 0.6934729814529419, 0.6845385432243347, 0.7753505110740662, 0.6917712688446045, 0.6978029012680054, 0.7658747434616089, 0.6907713413238525, 0.6923384666442871, 0.728392481803894, 0.7048361897468567, 0.6916713714599609, 0.7351356148719788, 0.71034836769104, 0.6938714981079102, 0.7045658230781555, 0.721963107585907, 0.6977162957191467, 0.7122534513473511, 0.756525993347168, 0.7020295858383179, 0.712162435054779, 0.7227909564971924, 0.6895375847816467, 0.6923381686210632, 0.7042040824890137, 0.7012385129928589, 0.694452166557312, 0.704353928565979, 0.6915613412857056, 0.6951207518577576, 0.6992161870002747, 0.6897013783454895, 0.6950321793556213, 0.6947474479675293, 0.6869498491287231, 0.709904134273529, 0.7101786732673645, 0.693317711353302, 0.698270320892334, 0.7093538045883179, 0.6971935629844666, 0.6916314363479614, 0.7006677389144897, 0.7137945890426636, 0.6946972012519836, 0.6983682513237, 0.7245306968688965, 0.7058919072151184, 0.7035449743270874, 0.7372441291809082, 0.708305835723877, 0.6989402770996094, 0.7228389978408813, 0.7002344727516174, 0.6875919103622437, 0.7396178841590881, 0.711841881275177, 0.6982590556144714, 0.712285578250885, 0.7162591814994812, 0.6976380944252014, 0.7008093595504761, 0.7054762840270996, 0.6971908211708069, 0.6936480402946472, 0.6947137117385864, 0.6988347172737122, 0.6932146549224854, 0.6950794458389282, 0.6963121891021729, 0.6929930448532104, 0.6934249997138977, 0.6939865946769714, 0.69950270652771, 0.6925041675567627, 0.7109490633010864, 0.6927056908607483, 0.6935005187988281, 0.6871201395988464, 0.7109338641166687, 0.6980497241020203, 0.7020828127861023, 0.7038801312446594, 0.694074273109436, 0.7025689482688904, 0.6979655623435974, 0.6934235692024231, 0.6963953375816345, 0.7022815942764282, 0.694892168045044, 0.6994483470916748, 0.6947596669197083, 0.6923011541366577, 0.7006270885467529, 0.7007864713668823, 0.6996420621871948, 0.7055432796478271, 0.6926538944244385, 0.7023492455482483, 0.7041515707969666, 0.6938938498497009, 0.6941841244697571, 0.6928097605705261, 0.6939988732337952, 0.6936655640602112, 0.6953428983688354, 0.6925444602966309, 0.6931757926940918, 0.6936063766479492, 0.6948325634002686, 0.6929567456245422, 0.6967921853065491, 0.6929267048835754, 0.698790967464447, 0.693622887134552, 0.6914896965026855, 0.6941304802894592, 0.693149745464325, 0.6950212121009827, 0.6941767334938049, 0.6923097968101501, 0.6964138150215149, 0.6954516172409058, 0.7008194327354431, 0.6928810477256775, 0.6922322511672974, 0.7006325721740723, 0.6941531300544739, 0.7021690011024475, 0.7003058195114136, 0.6963969469070435, 0.6963150501251221, 0.7071548700332642, 0.6924078464508057, 0.6918668746948242, 0.6988669037818909, 0.7065445184707642, 0.6986002922058105, 0.7087571024894714, 0.6952645778656006, 0.6937679052352905, 0.710090160369873, 0.701765775680542, 0.6952507495880127, 0.7271625995635986, 0.6992698311805725, 0.6983813047409058, 0.7225505709648132, 0.6984460949897766, 0.6931177973747253, 0.7164362668991089, 0.6988415122032166, 0.6935287714004517, 0.6932582855224609, 0.6963674426078796, 0.7088035941123962, 0.6942322254180908, 0.7025144100189209, 0.6967288255691528, 0.6927131414413452, 0.6935773491859436, 0.7181375622749329, 0.6928744316101074, 0.7045844793319702, 0.7065764665603638, 0.6944338083267212, 0.7150806784629822, 0.6958394646644592, 0.69259113073349, 0.6975672841072083, 0.7094326019287109, 0.6924967765808105, 0.6947407722473145, 0.7190878391265869, 0.6939796209335327, 0.7085123062133789, 0.6973628401756287, 0.6926107406616211, 0.6957116723060608, 0.7008611559867859, 0.6913423538208008, 0.6948255896568298, 0.696711540222168, 0.6935123801231384, 0.6942214965820312, 0.7006886601448059, 0.692205548286438, 0.7063376307487488, 0.6926041841506958, 0.6927028894424438, 0.6960527300834656, 0.7132571935653687, 0.6921594142913818, 0.721095621585846, 0.7087635397911072, 0.6947461366653442, 0.6989704370498657, 0.7057455778121948, 0.6980752944946289, 0.7020650506019592, 0.7020017504692078, 0.6998085975646973, 0.6914220452308655, 0.6988606452941895, 0.7065691351890564, 0.6897256970405579, 0.6959506869316101, 0.6950534582138062, 0.6963776350021362, 0.6924285292625427, 0.7022390365600586, 0.6947873830795288, 0.6938759684562683, 0.6931117177009583, 0.7011401653289795, 0.6919170022010803, 0.7204408049583435, 0.6947035789489746, 0.7047606706619263, 0.6959759593009949, 0.6931391954421997, 0.6908731460571289, 0.7034220099449158, 0.6981359124183655, 0.69334876537323, 0.694036602973938, 0.6976140737533569, 0.6960974931716919, 0.6931370496749878, 0.6930835247039795, 0.6955680251121521, 0.6928669810295105, 0.6933586597442627, 0.6924501657485962, 0.6949683427810669, 0.6940710544586182, 0.6966056227684021, 0.6932134032249451, 0.6998295783996582, 0.7024347186088562, 0.6931883692741394, 0.6938281655311584, 0.699202835559845, 0.6945309638977051, 0.6919428110122681, 0.7095544338226318, 0.6930270791053772, 0.6964517831802368, 0.6917823553085327, 0.6918147206306458, 0.6947275400161743, 0.6956120133399963, 0.6922972202301025, 0.6924857497215271, 0.7013169527053833, 0.6953971982002258, 0.7001338601112366, 0.7173532843589783, 0.6941816210746765, 0.7038206458091736, 0.7196975946426392, 0.6925541162490845, 0.7095685601234436, 0.6957157850265503, 0.6943224668502808, 0.6912052035331726, 0.6966491937637329, 0.6951215267181396, 0.6932965517044067, 0.7004998326301575, 0.6949419379234314, 0.6933990716934204, 0.69444739818573, 0.693929135799408, 0.692820131778717, 0.6937589049339294, 0.6958450078964233, 0.6926569938659668, 0.6926863193511963, 0.6953334212303162, 0.6909719109535217, 0.7080767154693604, 0.6954192519187927, 0.6977839469909668, 0.6945956945419312, 0.6961945295333862, 0.694776713848114, 0.6934877038002014, 0.6942070126533508, 0.6942338347434998, 0.7107621431350708, 0.691118061542511, 0.6932097673416138, 0.6941972970962524, 0.691883385181427, 0.6956096291542053, 0.6972149610519409, 0.7024445533752441, 0.6921625137329102, 0.6918709874153137, 0.6915560364723206, 0.6971707940101624, 0.6931268572807312, 0.6931198835372925, 0.6930638551712036, 0.6893687844276428, 0.6924458742141724, 0.6934641599655151, 0.6960604786872864, 0.6934292316436768, 0.6964029669761658, 0.6948087215423584, 0.6934950947761536, 0.6983211636543274, 0.6933847069740295, 0.6947715878486633, 0.6940551996231079, 0.6969566345214844, 0.6948860287666321, 0.694709837436676, 0.6932398676872253, 0.6936076879501343, 0.6922458410263062, 0.6899687051773071, 0.6866000294685364, 0.7024949193000793, 0.7002555131912231, 0.6973055005073547, 0.696685791015625, 0.6951800584793091, 0.6946317553520203, 0.6950929760932922, 0.6938374638557434, 0.6930821537971497, 0.6990261673927307, 0.6949127912521362, 0.6933566927909851, 0.6978354454040527, 0.6936970949172974, 0.6933786869049072, 0.7013543844223022, 0.6927011013031006, 0.6900967955589294, 0.6966699361801147, 0.700124204158783, 0.6937064528465271, 0.7344977259635925, 0.7259598970413208, 0.6899917125701904, 0.7329034805297852, 0.7397947311401367, 0.6956292986869812, 0.7150241732597351, 0.751030683517456, 0.6929203867912292, 0.7096353769302368, 0.7058394551277161, 0.6990264654159546, 0.6992337107658386, 0.6956909894943237, 0.6947722434997559, 0.6937387585639954, 0.6934805512428284, 0.6953802704811096, 0.6932258605957031, 0.6981871724128723, 0.6930373907089233, 0.6911181807518005, 0.6974211931228638, 0.6908851861953735, 0.7011280059814453, 0.6957707405090332, 0.7046560049057007, 0.7114191055297852, 0.7056301832199097, 0.7330678701400757, 0.6934647560119629, 0.6932967901229858, 0.7268220782279968, 0.7036903500556946, 0.6922756433486938, 0.7156667709350586, 0.7028582096099854, 0.6938900947570801, 0.6972532868385315, 0.715725839138031, 0.6901667714118958, 0.6958863139152527, 0.6939806938171387, 0.6930738091468811, 0.6933335661888123, 0.6920843124389648, 0.6917036175727844, 0.693835973739624, 0.6969817280769348, 0.6931681036949158, 0.701811671257019, 0.6949688196182251, 0.6895105838775635, 0.7051395773887634, 0.7027274370193481, 0.6951597929000854, 0.6920331716537476, 0.70527583360672, 0.6949487328529358, 0.6952794790267944, 0.6837465763092041, 0.7009519934654236, 0.6963229775428772, 0.6981055736541748, 0.6971419453620911, 0.6965502500534058, 0.695219874382019, 0.6977179050445557, 0.693854033946991, 0.6921280026435852, 0.6951707601547241, 0.6969811320304871, 0.6944931149482727, 0.6937008500099182, 0.6949734091758728, 0.6937271356582642, 0.704406201839447, 0.6944195032119751, 0.6944292187690735, 0.6957314014434814, 0.6949844360351562, 0.6920269727706909, 0.7059087157249451, 0.7008568644523621, 0.697014570236206, 0.6818727254867554, 0.6952542066574097, 0.6932732462882996, 0.6949522495269775, 0.6910318732261658, 0.6940008997917175, 0.7058638334274292, 0.7089956998825073, 0.6928565502166748, 0.6927366256713867, 0.6939948201179504, 0.6903652548789978, 0.696281373500824, 0.6928572058677673, 0.6950870156288147, 0.6921969652175903, 0.6878455281257629, 0.7239238023757935, 0.6933059692382812, 0.7136520743370056, 0.6986510157585144, 0.6935155391693115, 0.7125695943832397, 0.6943091750144958, 0.6964860558509827, 0.7032591104507446, 0.691990077495575, 0.7068743109703064, 0.688042402267456, 0.7050175666809082, 0.6949514150619507, 0.7254979610443115, 0.6919981837272644, 0.6916460394859314, 0.7002917528152466, 0.7106297016143799, 0.695572018623352, 0.699590265750885, 0.7176047563552856, 0.6940522193908691, 0.6974877119064331, 0.7125477194786072, 0.6931841969490051, 0.7034657597541809, 0.7008681893348694, 0.6920812726020813, 0.697313666343689, 0.7044209241867065, 0.6958171129226685, 0.708592414855957, 0.7043954133987427, 0.6953943371772766, 0.7116165161132812, 0.714638888835907, 0.6930451393127441, 0.7008682489395142, 0.7024022340774536, 0.6928925514221191, 0.6918020248413086, 0.701546847820282, 0.7022269368171692, 0.69306880235672, 0.6945542097091675, 0.6930888891220093, 0.6976422667503357, 0.6952311992645264, 0.6892176270484924, 0.6938769817352295, 0.7042626738548279, 0.6936870217323303, 0.7065855264663696, 0.696708083152771, 0.6934800744056702, 0.6986913084983826, 0.6906260251998901, 0.6928213238716125, 0.6921894550323486, 0.6938114762306213, 0.695645809173584, 0.6933259963989258, 0.6912765502929688, 0.6917055249214172, 0.69517582654953, 0.6917427778244019, 0.6942304372787476, 0.6942071914672852, 0.6980301737785339, 0.6921523809432983, 0.69696044921875, 0.6924649477005005, 0.6984411478042603, 0.6941797733306885, 0.6978139877319336, 0.6929406523704529, 0.6936619877815247, 0.6934104561805725, 0.6931471228599548, 0.6932732462882996, 0.6938396692276001, 0.6938661932945251, 0.6901217699050903, 0.692579984664917, 0.6901242733001709, 0.7030727863311768, 0.692064642906189, 0.6993290781974792, 0.6986503601074219, 0.6924932599067688, 0.6963022351264954, 0.6927998065948486, 0.696507453918457, 0.6915715336799622, 0.6912459135055542, 0.6970298290252686, 0.6957075595855713, 0.6901054978370667, 0.6919810771942139, 0.7087677121162415, 0.694399356842041, 0.6959262490272522, 0.6994895935058594, 0.6923025846481323, 0.6945183873176575, 0.7065682411193848, 0.6952273845672607, 0.7029277086257935, 0.6947551369667053, 0.6955233812332153, 0.696284830570221, 0.6974883079528809, 0.696697473526001, 0.6952998638153076, 0.7014167308807373, 0.6957269310951233, 0.697877049446106, 0.6960635781288147, 0.693900465965271, 0.698972225189209, 0.6917678713798523, 0.693359375, 0.6967170238494873, 0.6943759918212891, 0.6921954154968262, 0.6888447999954224, 0.7052721977233887, 0.6925216913223267, 0.6979134678840637, 0.7029022574424744, 0.6944000124931335, 0.7169069051742554, 0.7151473164558411, 0.6930276155471802, 0.7117878794670105, 0.7033398151397705, 0.692696213722229, 0.7014107704162598, 0.7136802673339844, 0.6925829648971558, 0.7112241387367249, 0.6953561902046204, 0.695469081401825, 0.6953253746032715, 0.6956931352615356, 0.6935521960258484, 0.69675612449646, 0.6987833976745605, 0.6955134868621826, 0.6934601068496704, 0.6872599720954895, 0.7099854946136475, 0.6914029121398926, 0.6946966648101807, 0.7007637023925781, 0.6981576681137085, 0.6927478909492493, 0.7157039046287537, 0.6933750510215759, 0.6963909268379211, 0.6972607374191284, 0.6937286257743835, 0.6968275308609009, 0.6960984468460083, 0.6934380531311035, 0.6903836131095886, 0.6986376643180847, 0.7039740085601807, 0.6914588809013367, 0.7117449045181274, 0.6958354711532593, 0.6941478848457336, 0.6991292834281921, 0.7107385396957397, 0.6929275989532471, 0.6960814595222473, 0.7069520354270935, 0.6895712018013, 0.6928505897521973, 0.6918166875839233, 0.6921088099479675, 0.6908705234527588, 0.7018160820007324, 0.6925724148750305, 0.6925626993179321, 0.6900771260261536, 0.7073875665664673, 0.6976933479309082, 0.7060853838920593, 0.6918888688087463, 0.6871771812438965, 0.7069410681724548, 0.6909021139144897, 0.6917685270309448, 0.6953710913658142, 0.7055333256721497, 0.6967473030090332, 0.7114377617835999, 0.6935559511184692, 0.6961354613304138, 0.6983986496925354, 0.7008793354034424, 0.6961161494255066, 0.6977807283401489, 0.705007016658783, 0.7047040462493896, 0.70076584815979, 0.706932544708252, 0.7011130452156067, 0.7013720870018005, 0.7002980709075928, 0.6901357173919678, 0.6941938996315002, 0.6945496797561646, 0.6973051428794861, 0.6902784109115601, 0.6957658529281616, 0.6909480094909668, 0.7021550536155701, 0.7003415822982788, 0.696927011013031, 0.7142850756645203, 0.7050785422325134, 0.6923497319221497, 0.7213869094848633, 0.7043476700782776, 0.6940479278564453, 0.7162764668464661, 0.7026733160018921, 0.6949492692947388, 0.6990830302238464, 0.6918225288391113, 0.6932957172393799, 0.6941614151000977, 0.6940575838088989, 0.6949073672294617, 0.6959543228149414, 0.6928744316101074, 0.6923710107803345, 0.6931618452072144, 0.697105884552002, 0.6943081617355347, 0.6929654479026794, 0.6955714225769043, 0.6966310739517212, 0.6914414763450623, 0.6970086693763733, 0.7115522027015686, 0.6944576501846313, 0.7032005786895752, 0.7026907205581665, 0.6926463842391968, 0.702364444732666, 0.7028846740722656, 0.6968064904212952, 0.7137674689292908, 0.691344678401947, 0.6929440498352051, 0.7006618976593018, 0.6973912715911865, 0.7005758881568909, 0.6909500360488892, 0.6935210824012756, 0.6931115388870239, 0.6946945786476135, 0.6944786906242371, 0.6919607520103455, 0.697192370891571, 0.6976637840270996, 0.6941770315170288, 0.6956542730331421, 0.7000725269317627, 0.6950000524520874, 0.6964020729064941, 0.7073650360107422, 0.6932448148727417, 0.6986200213432312, 0.7019723057746887, 0.6978800892829895, 0.6962459683418274, 0.7085583806037903, 0.6952550411224365, 0.6946531534194946, 0.693511426448822, 0.7012875080108643, 0.6935771703720093, 0.689204752445221, 0.698116660118103, 0.6974690556526184, 0.6940855383872986, 0.7021867036819458, 0.6938860416412354, 0.6944973468780518, 0.6941755414009094, 0.6906602382659912, 0.7013818621635437, 0.6949732303619385, 0.7058923244476318, 0.7015720009803772, 0.7041776776313782, 0.6975474953651428, 0.6947388648986816, 0.6916726231575012, 0.7044540643692017, 0.6939771771430969, 0.6925456523895264, 0.6958369016647339, 0.6939924359321594, 0.6897985935211182, 0.6923338174819946, 0.6969430446624756, 0.6928399205207825, 0.6929076313972473, 0.6992281079292297, 0.6921185255050659, 0.6955094337463379, 0.6921967267990112, 0.6890195608139038, 0.7026286125183105, 0.709256112575531, 0.6980366110801697, 0.7142476439476013, 0.7032384872436523, 0.7041523456573486, 0.7374215126037598, 0.691789984703064, 0.689006507396698, 0.7436563372612, 0.696550190448761, 0.7034868597984314, 0.697724461555481, 0.6968206763267517, 0.694450855255127, 0.6940284967422485, 0.7041347622871399, 0.6923918724060059, 0.6933949589729309, 0.7110392451286316, 0.693230390548706, 0.7065032124519348, 0.7095494866371155, 0.6929400563240051, 0.7233119010925293, 0.7105426788330078, 0.7056664824485779, 0.731976330280304, 0.6941182613372803, 0.6992560029029846, 0.7144902944564819, 0.6938475966453552, 0.6928836107254028, 0.7114898562431335, 0.7009608745574951, 0.6946520805358887, 0.7001085877418518, 0.6960541009902954, 0.6921877861022949, 0.6943435072898865, 0.6938732862472534, 0.6920425891876221, 0.6875584125518799, 0.7056409120559692, 0.6934938430786133, 0.6922236680984497, 0.7238664627075195, 0.6938296556472778, 0.6915118098258972, 0.7072160840034485, 0.7238246202468872, 0.7002096176147461, 0.7452617287635803, 0.6995674967765808, 0.7102664113044739, 0.7013830542564392, 0.7029814720153809, 0.690475583076477, 0.7219087481498718, 0.7235077023506165, 0.6980736255645752, 0.7196572422981262, 0.6974000930786133, 0.6913995742797852, 0.7095908522605896, 0.7092663049697876, 0.6920945048332214, 0.7105231881141663, 0.7271236181259155, 0.6931910514831543, 0.717686653137207, 0.7045566439628601, 0.6955113410949707, 0.7070311307907104, 0.7014690637588501, 0.6971744894981384, 0.6965398788452148, 0.7268625497817993, 0.6932687163352966, 0.7075102925300598, 0.7184404730796814, 0.6924550533294678, 0.7038610577583313, 0.7053105235099792, 0.6913263201713562, 0.6926708221435547, 0.6972424387931824, 0.6927309036254883, 0.6927409768104553, 0.6934233903884888, 0.6952016949653625, 0.6931652426719666, 0.6968333721160889, 0.6944690942764282, 0.6930801272392273, 0.6916165947914124, 0.6950024366378784, 0.6938301920890808, 0.6923136115074158, 0.6912750601768494, 0.6932532787322998, 0.6891742944717407, 0.7007752060890198, 0.6933122873306274, 0.6951760649681091, 0.6936614513397217, 0.695857584476471, 0.6933255791664124, 0.6905370950698853, 0.7118051648139954, 0.6941435933113098, 0.7107678055763245, 0.6962916254997253, 0.6995245218276978, 0.7067391276359558, 0.6894245147705078, 0.7118784785270691, 0.7094330787658691, 0.6943297982215881, 0.6857883930206299, 0.7111015319824219, 0.6876265406608582, 0.6922745108604431, 0.70681232213974, 0.7092491388320923, 0.6983533501625061, 0.7067086100578308, 0.697708785533905, 0.6932594776153564, 0.6956756711006165, 0.6915807723999023, 0.6971837282180786, 0.6910524368286133, 0.7068348526954651, 0.7066903114318848, 0.694723904132843, 0.6999448537826538, 0.7008136510848999, 0.6933175325393677, 0.7080633044242859, 0.6838812828063965, 0.7011417150497437, 0.6935920119285583, 0.6908441781997681, 0.7111605405807495, 0.6966748237609863, 0.6924591064453125, 0.6971519589424133, 0.7028889656066895, 0.6936201453208923, 0.7066769003868103, 0.6911346316337585, 0.6932095885276794, 0.6971859335899353, 0.6881976127624512, 0.6948307752609253, 0.696398913860321, 0.6965274810791016, 0.6964817047119141, 0.7031292915344238, 0.6965194344520569, 0.6953069567680359, 0.6931442618370056, 0.7036160826683044, 0.6952823400497437, 0.6931441426277161, 0.6961520910263062, 0.6919767260551453, 0.6941428184509277, 0.6929643750190735, 0.6977471113204956, 0.6933692693710327, 0.6935275793075562, 0.6920790076255798, 0.6931352019309998, 0.6935389637947083, 0.6942748427391052, 0.6937214732170105, 0.6948705315589905, 0.6928458213806152, 0.6895972490310669, 0.7001092433929443, 0.6965653896331787, 0.6924687623977661, 0.7021656036376953, 0.700975775718689, 0.6955778002738953, 0.7030901312828064, 0.6995823979377747, 0.6936278343200684, 0.6913872361183167, 0.714375913143158, 0.6963551640510559, 0.7014344930648804, 0.6964753866195679, 0.7038059830665588, 0.6959347724914551, 0.701340913772583, 0.6959658265113831, 0.6937568783760071, 0.6929609775543213, 0.6879072785377502, 0.7106641530990601, 0.6930400133132935, 0.7098156213760376, 0.7055058479309082, 0.6930474638938904, 0.7032101154327393, 0.6971050500869751, 0.6912811398506165, 0.6939066052436829, 0.6941542029380798, 0.6929255723953247, 0.700901985168457, 0.6948773264884949, 0.698828935623169, 0.692938506603241, 0.6944098472595215, 0.6930558085441589, 0.693669319152832, 0.6921542882919312, 0.6940701603889465, 0.6952715516090393, 0.6952913403511047, 0.6925023198127747, 0.6983485817909241, 0.6943426132202148, 0.6914299726486206, 0.6922685503959656, 0.697812020778656, 0.6945818066596985, 0.6932032704353333, 0.6933631896972656, 0.6921128034591675, 0.6954403519630432, 0.6918644309043884, 0.6960023641586304, 0.6922078728675842, 0.6939228177070618, 0.6988581418991089, 0.6946204900741577, 0.6990650296211243, 0.6932483911514282, 0.690279483795166, 0.7059309482574463, 0.6940279603004456, 0.692987322807312, 0.7034568786621094, 0.692641019821167, 0.6962421536445618, 0.6863850951194763, 0.6999613642692566, 0.6934008598327637, 0.6963064670562744, 0.6988893747329712, 0.693668007850647, 0.6938602924346924, 0.7006209492683411, 0.6954339742660522, 0.6922447681427002, 0.693571150302887, 0.6926838159561157, 0.6943276524543762, 0.6924375891685486, 0.6921648979187012, 0.6931066513061523, 0.6923961043357849, 0.6947469115257263, 0.7004643678665161, 0.7009599804878235, 0.6941648125648499, 0.6943071484565735, 0.6944688558578491, 0.6955292224884033, 0.6935281157493591, 0.6925670504570007, 0.7019555568695068, 0.6966564655303955, 0.6929759383201599, 0.710371196269989, 0.6958427429199219, 0.6867703795433044, 0.7260566353797913, 0.7059444785118103, 0.6935100555419922, 0.7359554767608643, 0.6977529525756836, 0.6907421350479126, 0.7273623943328857, 0.694530189037323, 0.6924232244491577, 0.7006025910377502, 0.7155905961990356, 0.6935431361198425, 0.7149615287780762, 0.6952944397926331, 0.693588376045227, 0.7045873403549194, 0.6941713094711304, 0.6931481957435608, 0.6928918361663818, 0.6981942057609558, 0.6937388181686401, 0.6986407041549683, 0.6943058967590332, 0.6946109533309937, 0.6933660507202148, 0.7025542259216309, 0.6940640211105347, 0.701377272605896, 0.6914393901824951, 0.6930015683174133, 0.6941132545471191, 0.69373619556427, 0.6894463896751404, 0.6925918459892273, 0.7006921768188477, 0.693705677986145, 0.7050989866256714, 0.6902840733528137, 0.6911196708679199, 0.6991623640060425, 0.6967725157737732, 0.6995370984077454, 0.6932888627052307, 0.7023195028305054, 0.7097044587135315, 0.6945855021476746, 0.714347779750824, 0.6990587711334229, 0.6954604983329773, 0.6978294253349304, 0.7021287083625793, 0.693566620349884, 0.6912621855735779, 0.7063007354736328, 0.6970716118812561, 0.6971866488456726, 0.707797110080719, 0.7011094689369202, 0.6995421051979065, 0.7032577395439148, 0.700280487537384, 0.6959561705589294, 0.7139580845832825, 0.6983919739723206, 0.691046953201294, 0.7179511785507202, 0.7223334908485413, 0.7015541195869446, 0.7371454834938049, 0.6971872448921204, 0.6928461790084839, 0.718931257724762, 0.7084164023399353, 0.6953663229942322, 0.7176186442375183, 0.6950711011886597, 0.6979551911354065, 0.6996368169784546, 0.69143146276474, 0.6940880417823792, 0.6927390694618225, 0.6947774887084961, 0.6994240880012512, 0.6902375817298889, 0.7024433016777039, 0.7236812710762024, 0.6918733716011047, 0.7402474284172058, 0.7084466218948364, 0.6948400735855103, 0.7082314491271973, 0.703559398651123, 0.6933943033218384, 0.6999175548553467, 0.6983681917190552, 0.6972664594650269, 0.7001736164093018, 0.7122283577919006, 0.6930997967720032, 0.7047539353370667, 0.7213851809501648, 0.6931056380271912, 0.6967624425888062, 0.7227906584739685, 0.6897669434547424, 0.6973550915718079, 0.7023864984512329, 0.6930577158927917, 0.6946999430656433, 0.6983662247657776, 0.7040042281150818, 0.6995429992675781, 0.7037641406059265, 0.7097132802009583, 0.6915826797485352, 0.6920567750930786, 0.7213243246078491, 0.6988574862480164, 0.707462728023529, 0.7010880708694458, 0.6982330679893494, 0.7018317580223083, 0.6972535252571106, 0.6987897157669067, 0.6934627890586853, 0.6970221996307373, 0.7245956063270569, 0.6938027739524841, 0.7096341252326965, 0.7099340558052063, 0.6927482485771179, 0.7042544484138489, 0.7106915712356567, 0.6921273469924927, 0.704984188079834, 0.7130248546600342, 0.6991859078407288, 0.6959204077720642, 0.7200939059257507, 0.6950210928916931, 0.7040259838104248, 0.7016832232475281, 0.6897825598716736, 0.6922622323036194, 0.6981600522994995, 0.6937273740768433, 0.692765474319458, 0.6972604393959045, 0.6916724443435669, 0.6982640624046326, 0.6930170655250549, 0.7079885601997375, 0.7001456618309021, 0.6997982263565063, 0.691781759262085, 0.6901359558105469, 0.6981829404830933, 0.6922041177749634, 0.7095369696617126, 0.6987701058387756, 0.692508339881897, 0.7095382809638977, 0.6932188868522644, 0.6936375498771667, 0.690767228603363, 0.71089106798172, 0.6914660930633545, 0.6939741373062134, 0.7013559937477112, 0.7005077004432678, 0.693606436252594, 0.7032886147499084, 0.6937941312789917, 0.6966530084609985, 0.7011395692825317, 0.6958796977996826, 0.7026978731155396, 0.6991235017776489, 0.6953930854797363, 0.7050424218177795, 0.7067291736602783, 0.6955899596214294, 0.6992636322975159, 0.7039102911949158, 0.696185827255249, 0.6952561736106873, 0.695849597454071, 0.7020785808563232, 0.6934420466423035, 0.7039036750793457, 0.6932343244552612, 0.6922505497932434, 0.6885013580322266, 0.6997808814048767, 0.6968280076980591, 0.6923718452453613, 0.694491982460022, 0.6997636556625366, 0.697533905506134, 0.6935376524925232, 0.7023271322250366, 0.6955474019050598, 0.7011477947235107, 0.7007735967636108, 0.7036260962486267, 0.6922075152397156, 0.7367215156555176, 0.7015098929405212, 0.7078837156295776, 0.7108473777770996, 0.6943417191505432, 0.6959156394004822, 0.7017853260040283, 0.7070139646530151, 0.6915082335472107, 0.7247118353843689, 0.7218117117881775, 0.6967721581459045, 0.7173714637756348, 0.7012048363685608, 0.6929885149002075, 0.7023422718048096, 0.7060384154319763, 0.6906523704528809, 0.7064180374145508, 0.7106035947799683, 0.6924978494644165, 0.6968922019004822, 0.7089179754257202, 0.6979782581329346, 0.7047666311264038, 0.7099659442901611, 0.6911888122558594, 0.6929236650466919, 0.6943641304969788, 0.711968183517456, 0.6998997926712036, 0.6946267485618591, 0.7549331784248352, 0.6999592781066895, 0.7177446484565735, 0.705024242401123, 0.6977105736732483, 0.701466977596283, 0.6979344487190247, 0.7024803757667542, 0.6933829188346863, 0.7109169363975525, 0.6965631246566772, 0.6961243748664856, 0.6928773522377014, 0.6961148381233215, 0.6938067674636841, 0.6886679530143738, 0.7046301960945129, 0.6939303874969482, 0.6965556740760803, 0.6949372887611389, 0.6901994347572327, 0.6891605257987976, 0.6971486210823059, 0.6920384168624878, 0.6971951723098755, 0.7053319215774536, 0.6942148208618164, 0.7020296454429626, 0.6928173899650574, 0.6944829225540161, 0.6987203359603882, 0.6951053738594055, 0.6944525241851807, 0.6936556100845337, 0.6948718428611755, 0.693253755569458, 0.6940323710441589, 0.6933137774467468, 0.6959601044654846, 0.691677451133728, 0.6923011541366577, 0.7126559615135193, 0.692402720451355, 0.7111790180206299, 0.7168715000152588, 0.6943353414535522, 0.6925275325775146, 0.7304574251174927, 0.7091744542121887, 0.7091668844223022, 0.7181943655014038, 0.6993744969367981, 0.6934275031089783, 0.7260202169418335, 0.7004891037940979, 0.6951802968978882, 0.7016668319702148, 0.7008699178695679, 0.6954901218414307, 0.6969672441482544, 0.6951514482498169, 0.6948156952857971, 0.6948618292808533, 0.6926811933517456, 0.6940574645996094, 0.6950668692588806, 0.6931069493293762, 0.6960257291793823, 0.693426787853241, 0.6924906969070435, 0.6878256797790527, 0.7177413105964661, 0.6900737881660461, 0.7000921368598938, 0.6978794932365417, 0.693778395652771, 0.6959900856018066, 0.7048152089118958, 0.6920231580734253, 0.6904906034469604, 0.7019086480140686, 0.6990967392921448, 0.6963008046150208, 0.718952476978302, 0.6955637335777283, 0.7016744613647461, 0.707530677318573, 0.6935004591941833, 0.707774817943573, 0.7028331160545349, 0.6900750398635864, 0.7070337533950806, 0.7152680158615112, 0.6935830116271973, 0.6916509866714478, 0.7095219492912292, 0.7040152549743652, 0.6940669417381287, 0.7127700448036194, 0.7117184400558472, 0.6947542428970337, 0.7172060012817383, 0.7135847806930542, 0.6931047439575195, 0.6992185115814209, 0.7031524777412415, 0.6942072510719299, 0.6921494007110596, 0.7030491232872009, 0.6940650343894958, 0.7060587406158447, 0.7079920172691345, 0.6953144669532776, 0.6903148293495178, 0.7077001929283142, 0.6945127844810486, 0.6964995265007019, 0.705505907535553, 0.692450225353241, 0.6931206583976746, 0.7132763266563416, 0.6972596049308777, 0.6951601505279541, 0.7031086683273315, 0.697272777557373, 0.6936511993408203, 0.6896888017654419, 0.7253826260566711, 0.6956909894943237, 0.6958086490631104, 0.7063834071159363, 0.6975101828575134, 0.6976964473724365, 0.6930381059646606, 0.6955161690711975, 0.6940019130706787, 0.7014853954315186, 0.6963210701942444, 0.6935070753097534, 0.6950380802154541, 0.6961209774017334, 0.7002720832824707, 0.6987630724906921, 0.6931961178779602, 0.6919339895248413, 0.693679690361023, 0.6910618543624878, 0.6942282319068909, 0.6929079294204712, 0.6911105513572693, 0.7092867493629456, 0.6934772729873657, 0.7091434597969055, 0.6927417516708374, 0.6899136304855347, 0.7067259550094604, 0.6976235508918762, 0.7101076245307922, 0.7005274891853333, 0.6926408410072327, 0.6990917325019836, 0.6969289183616638, 0.6947427988052368, 0.6870861053466797, 0.7028198838233948, 0.6890108585357666, 0.6935845613479614, 0.6918575167655945, 0.6991881132125854, 0.6899357438087463, 0.6939651966094971, 0.6937820911407471, 0.7037209272384644, 0.6932375431060791, 0.6920062899589539, 0.6960659623146057, 0.6945013999938965, 0.6952449679374695, 0.6936312317848206, 0.6954168081283569, 0.6983082890510559, 0.6933189630508423, 0.6927012801170349, 0.7058582901954651, 0.6989910006523132, 0.7012126445770264, 0.6920082569122314, 0.6917937397956848, 0.6925068497657776, 0.6947932243347168, 0.6908722519874573, 0.6939885020256042, 0.6942296624183655, 0.6951279640197754, 0.6940511465072632, 0.6984321475028992, 0.7014483213424683, 0.6954324245452881, 0.6922407150268555, 0.69222092628479, 0.696628212928772, 0.692089319229126, 0.6922413110733032, 0.7006857991218567, 0.6915127635002136, 0.7055351734161377, 0.6923703551292419, 0.6925217509269714, 0.6930156350135803, 0.6924369931221008, 0.693668007850647, 0.6959171891212463, 0.6943110227584839, 0.695209801197052, 0.6946262121200562, 0.693812370300293, 0.7007191777229309, 0.6923938393592834, 0.6946760416030884, 0.6977636218070984, 0.6895683407783508, 0.6964699625968933, 0.7062645554542542, 0.6953996419906616, 0.6936549544334412, 0.6895079016685486, 0.6971209049224854, 0.6910789608955383, 0.6962962746620178, 0.6951149106025696, 0.6937259435653687, 0.6938490867614746, 0.6958540678024292, 0.6965276002883911, 0.6902895569801331, 0.6976668834686279, 0.6924323439598083, 0.6947745084762573, 0.6943861246109009, 0.6988797187805176, 0.7014511227607727, 0.6988347768783569, 0.6972466707229614, 0.7064932584762573, 0.7221028804779053, 0.6935120224952698, 0.7010568976402283, 0.6959825158119202, 0.6933240294456482, 0.6992608308792114, 0.6928340792655945, 0.695777177810669, 0.6931824684143066, 0.6928460001945496, 0.6949784755706787, 0.6929925084114075, 0.694433867931366, 0.6931931972503662, 0.6931766271591187, 0.6955767273902893, 0.6919691562652588, 0.6919941306114197, 0.6931614875793457, 0.6988731622695923, 0.6933292150497437, 0.7055462598800659, 0.6927269697189331, 0.6964254975318909, 0.6938236355781555, 0.694118857383728, 0.6929823756217957, 0.6901680827140808, 0.7129664421081543, 0.6988936066627502, 0.7118927240371704, 0.7079144716262817, 0.6919864416122437, 0.6998032927513123, 0.6883091926574707, 0.6947694420814514, 0.6903743743896484, 0.6940277218818665, 0.6928966641426086, 0.6941947937011719, 0.6953144073486328, 0.6889027953147888, 0.7022179365158081, 0.6974039673805237, 0.6952869892120361, 0.6928483843803406, 0.6974359750747681, 0.6930139064788818, 0.7074685096740723, 0.6931604146957397, 0.7015889286994934, 0.6946689486503601, 0.6924436092376709, 0.7019825577735901, 0.6953224539756775, 0.6955611109733582, 0.6980667114257812, 0.6910644173622131, 0.7081891298294067, 0.6887053847312927, 0.6964733600616455, 0.6936442255973816, 0.7132918834686279, 0.6959890127182007, 0.6993098855018616, 0.6913948059082031, 0.7010148763656616, 0.698523759841919, 0.71771240234375, 0.6926253437995911, 0.6887854337692261, 0.7116620540618896, 0.7165265679359436, 0.7010062336921692, 0.7178795337677002, 0.69929039478302, 0.6882144808769226, 0.7177618145942688, 0.6958367228507996, 0.6946600079536438, 0.6942639946937561, 0.7090122699737549, 0.7028149962425232, 0.6940908432006836, 0.6966700553894043, 0.7052469253540039, 0.6939478516578674, 0.6935251355171204, 0.7000819444656372, 0.6939569711685181, 0.6927177309989929, 0.6989883184432983, 0.700143039226532, 0.6932786107063293, 0.69415283203125, 0.6965117454528809, 0.6943137645721436, 0.6906031966209412, 0.7104967832565308, 0.69157874584198, 0.6954399943351746, 0.6974320411682129, 0.6924247741699219, 0.699880063533783, 0.6981813907623291, 0.6945489645004272, 0.6935097575187683, 0.6951849460601807, 0.6910640597343445, 0.7013369202613831, 0.6959864497184753, 0.6974109411239624, 0.6987256407737732, 0.6959607601165771, 0.6964360475540161, 0.6933915615081787, 0.6988824009895325, 0.6888566613197327, 0.7009509801864624, 0.69624924659729, 0.6947547197341919, 0.6929329633712769, 0.6951548457145691, 0.695959210395813, 0.6947456002235413, 0.6956312656402588, 0.7028709650039673, 0.692861795425415, 0.7220897078514099, 0.6925753355026245, 0.6940421462059021, 0.7039176225662231, 0.6927421689033508, 0.6931622624397278, 0.7085129022598267, 0.6923011541366577, 0.6923428177833557, 0.6887626647949219, 0.7130443453788757, 0.6961122751235962, 0.695408821105957, 0.6996721625328064, 0.691325306892395, 0.7132093906402588, 0.6935969591140747, 0.6937846541404724, 0.705420732498169, 0.6934195756912231, 0.6925920248031616, 0.6920561790466309, 0.6982201933860779, 0.6981798410415649, 0.6899087429046631, 0.692845344543457, 0.6923874020576477, 0.6948055028915405, 0.6948192119598389, 0.6923052072525024, 0.6938333511352539, 0.7080997228622437, 0.6943917274475098, 0.6948962807655334, 0.6973962783813477, 0.7065257430076599, 0.6942665576934814, 0.7104407548904419, 0.6971188187599182, 0.6926522850990295, 0.7065662145614624, 0.7019616961479187, 0.6934735178947449, 0.6968746185302734, 0.6952158212661743, 0.6927874684333801, 0.6928929090499878, 0.692531943321228, 0.6981018781661987, 0.6934847831726074, 0.6922763586044312, 0.7042961716651917, 0.6946179270744324, 0.6930178999900818, 0.6964267492294312, 0.6957505941390991, 0.6976327300071716, 0.7181310057640076, 0.6931370496749878, 0.6935386061668396, 0.6979230642318726, 0.6927793622016907, 0.6897768974304199, 0.6888667941093445, 0.6998021602630615, 0.6945874691009521, 0.695634663105011, 0.6955959796905518, 0.6968110203742981, 0.6967642903327942, 0.6993135213851929, 0.6932942867279053, 0.6980160474777222, 0.6975787878036499, 0.6942737698554993, 0.6955308318138123, 0.7060557007789612, 0.6927833557128906, 0.6922892928123474, 0.6946855187416077, 0.6935582756996155, 0.6933608651161194, 0.6921826004981995, 0.6928420662879944, 0.6951941847801208]\n"
     ]
    }
   ],
   "source": [
    "devices = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss = nn.CrossEntropyLoss()\n",
    "mlm_l_list, nsp_l_list = train_bert(train_iter, net, loss, len(vocab), devices, num_steps=2000)\n",
    "print(mlm_l_list)\n",
    "print(nsp_l_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlm_l_list, color='red', label=\"mlm_l loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nsp_l_list, color='b', label=\"nsp_l loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 6, 128]),\n torch.Size([1, 128]),\n tensor([ 0.0251,  0.9377, -0.0153], device='cuda:0', grad_fn=<SliceBackward0>))"
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bert_encoding(net, devices, tokens_a, tokens_b=None):\n",
    "    tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "    token_ids = torch.tensor(vocab[tokens], device=devices).unsqueeze(0)\n",
    "    segments = torch.tensor(segments, device=devices).unsqueeze(0)\n",
    "    valid_len = torch.tensor(len(tokens), device=devices).unsqueeze(0)\n",
    "    # encoded_X表示BERT encoder层的是输出\n",
    "    encoded_X, _, _ = net(token_ids, segments, valid_len)\n",
    "    return encoded_X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 6, 128]),\n torch.Size([1, 128]),\n tensor([ 0.0831,  1.2807, -0.6537], device='cuda:0', grad_fn=<SliceBackward0>))"
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a = ['a', 'crane', 'is', 'flying']\n",
    "# 序列长度为6\n",
    "encoded_text = get_bert_encoding(net, devices, tokens_a)\n",
    "# 词元序列:'<cls>','a','crane','is','flying','<sep>'\n",
    "encoded_text_cls = encoded_text[:, 0, :]  # 第一个词元为\"<cls>\"\n",
    "encoded_text_crane = encoded_text[:, 2, :]\n",
    "encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 10, 128]),\n torch.Size([1, 128]),\n tensor([0.0504, 1.4805, 0.0231], device='cuda:0', grad_fn=<SliceBackward0>))"
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']\n",
    "encoded_pair = get_bert_encoding(net, devices, tokens_a, tokens_b)\n",
    "# 词元序列:'<cls>','a','crane','driver','came','<sep>','he','just', 'left','<sep>'\n",
    "encoded_pair_cls = encoded_pair[:, 0, :]\n",
    "encoded_pair_crane = encoded_pair[:, 2, :]\n",
    "encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}