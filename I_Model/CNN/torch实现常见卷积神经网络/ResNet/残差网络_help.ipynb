{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;残差网络(Residual Network,ResNet)(残差网络的思想并不局限于卷积神经网络]).通过给非线性的卷积层增加直连边\n",
    "(Shortcut Connection)(也称为残差连接(Residual Connection))的方式来提高信息的传播效率．\n",
    "\n",
    "&emsp;&emsp;假设在一个深度网络中,我们期望一个非线性单元(可以为一层或多层的卷积层)$ f(x;\\theta) $去逼近一个目标函数为$h(x)$.\n",
    "如果将目标函数拆分成两部分:恒等函数(Identity Function)$x$ 和残差函数(Residue Function)$h(x) -x$．\n",
    "\n",
    "$$ h(x) = x + \\left( h(x) -x \\right)  $$\n",
    "\n",
    "根据通用近似定理(说明了神经网络的计算能力可以去近似一个给定的连续函数),一个由神经网络构成的非线性单元有足够的能力来近似逼近原始目标函数或残差函数,\n",
    "但实际中后者更容易学习.因此,原来的优化问题可以转换为:让非线性单元$f(x;\\theta)$去近似残差函数$h(x) -x$,\n",
    "用$f(x;\\theta) + x$去逼近$h(x)$.\n",
    "\n",
    "&emsp;&emsp;下图给出了一个典型的残差单元示例.残差单元由多个级联的(等宽)卷积层和一个跨层的直连边组成,\n",
    "再经过ReLU激活后得到输出.残差网络就是将很多个残差单元串联起来构成的一个非常深的网络.\n",
    "\n",
    "<img src='../../../../Other/img/残差单元.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
