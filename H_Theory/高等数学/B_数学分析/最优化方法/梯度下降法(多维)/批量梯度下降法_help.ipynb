{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;假设$ m(\\mathbf{\\mathbf{x}}) $是$ \\mathbf{R}^n $上具有一阶连续偏导数的函数.要求解的无约束最优化问题是     \n",
    "$$ \\min \\limits _{\\mathbf{x} \\in \\mathbf{R}^n} m(\\mathbf{\\mathbf{x}}) $$    \n",
    "$ \\mathbf{x}^* $表示目标函数$ m(\\mathbf{\\mathbf{x}})$的极小点.   \n",
    "&emsp;&emsp;梯度下降法是一种迭代算法.选取适当的初值$ \\mathbf{x}^{(0)} $,不断迭代,更新$\\mathbf{x}$的值,进行目标函数的极小化,直到\n",
    "收敛.由于负梯度方向是使函数值下降最快的方向,在迭代的每一步,以负梯度方向更新$ \\mathbf{x}$的值,从而达到减少函数值的目的.   \n",
    "&emsp;&emsp;由于$m(\\mathbf{x})$具有一阶连续偏导数,若第$ k$ 次迭代为$  \\mathbf{x}^{(k)} $,则可将$ m(\\mathbf{x}) $在 $ \\mathbf{x}^{(k)} $附近进行\n",
    "一阶泰勒展开:      \n",
    "$$ m(\\mathbf{x}) = f(\\mathbf{x}^{(k)}) + g_k^T (\\mathbf{x} - \\mathbf{x}^{(k)}) $$     \n",
    "这里,$ g_k = g(\\mathbf{x}^{(k)}) = \\nabla f(\\mathbf{x}^{(k)}) $为$ m(\\mathbf{x})$ 在 $ \\mathbf{x}^{(k)} $的梯度.    \n",
    "&emsp;&emsp;求出第$ k +1 $次迭代值$ \\mathbf{x}^{(k+1)} $ :    \n",
    "$$ \\mathbf{x}^{(k+1)} \\leftarrow \\mathbf{x}^{(k)} + \\lambda_k p_k $$   \n",
    "其中,$  p_k $是搜索方向,取负梯度方向$ p_k = -\\nabla m(\\mathbf{x}^{(k)}) $, $ \\lambda _k $是步长,由一维搜索确定,即$ \\lambda_k $使得\n",
    "$$ f(\\mathbf{x}^{(k)} + \\lambda_k p_k) = \\min_{\\lambda \\geq0} f(\\mathbf{x}^{(k)} + \\lambda_k p_k) $$  \n",
    "\n",
    "<font color='red' size=4>算法:</font>   \n",
    "输入:目标函数$ m(\\mathbf{x}) $,梯度函数$ g(\\mathbf{x})=\\nabla g(\\mathbf{x}) $,计算精度$ \\epsilon$;  \n",
    "输出:$ m(\\mathbf{x})$的极小点$ \\mathbf{x}* $ \n",
    "1. 取初始值$ \\mathbf{x}{(0)} \\in \\mathbf{R}^n$,置$ k =0 $\n",
    "2. 计算$ f(\\mathbf{x}{k}) $\n",
    "3. 计算梯度$ g_k = g(\\mathbf{x}{k}) $,当 $ \\|g_k \\|  < \\epsilon$时,停止迭代,令 $ \\mathbf{x}* = \\mathbf{x}{(k)} $;否则,令$ p_k = -g(\\mathbf{x}{(k)}) $,求\n",
    "$ \\lambda_k  $,使\n",
    "$$ m(\\mathbf{x}{(k)} + \\lambda_k p_k) = \\min_{\\lambda \\geq0} m(\\mathbf{x}{(k)} + \\lambda_k p_k) $$\n",
    "4. 置$ \\mathbf{x}{(k+1)} = \\mathbf{x}{(k)} + \\lambda_k p_k$,计算$ m(\\mathbf{x}{k+1)}) $   \n",
    "当$ \\| m(\\mathbf{x}{k+1}) - m(\\mathbf{x}{(k)})  < \\epsilon \\| $ 或$ \\| \\mathbf{x}{k+1} - \\mathbf{x}{(k)}  < \\epsilon \\| $时,停止迭代,令\n",
    "$ \\mathbf{x}* = \\mathbf{x}{(k+1)} $ \n",
    "5. 否则,置 $ k = k+1 $,转3\n",
    "\n",
    "<font color='red' size=4>结论:</font>  \n",
    "1. 当目标函数是凸函数时,梯度下降法的解是全局最优解.一般情况下,其解不保证是全局最优解.梯度下降法\n",
    "的收敛速度也未必是很快的(牛顿法,拟牛顿法一般收敛速度更快).\n",
    "\n",
    "2. 批量梯度下降法在每次迭代时需要计算每个样本上损失函数的梯度并求和.当训练集中的样本数量$ N $很\n",
    "大时,空间复杂度比较高,每次迭代的计算开销也很大\n",
    "\n",
    "3. 批量梯度下降相当于是从真实数据分布中采集$N$个样本,并由它们计算出来的经验风险的梯度来近似期望风险的梯度\n",
    "\n",
    "4. 多维特征问题时,使这些特征具有相同或相近的尺度,可以使梯度下降法算法更快地收敛\n",
    "\n",
    "例:线性回归的梯度下降法迭代公式为:\n",
    "\n",
    "$$ \\hat{\\boldsymbol{\\beta}}^{j+1} = \\hat{\\boldsymbol{\\beta}}^{j} - \\lambda^{j} \\frac{1}{m}\\left( X^T  \\left( X  \\hat{\\boldsymbol{\\beta}}^{j} - \\mathbf{y}  \\right) \\right)   $$\n",
    "\n",
    "注意到,在每轮的迭代过程中,$ \\hat{\\boldsymbol{\\beta}}^{j+1} $的更新幅度是和$ X^T $相关的,\n",
    "因此如果$ X^T $中某个特征对应的参数$ \\beta_i $的尺度相较其他特征对应的参数$ \\beta_i, i \\neq k$的尺度大很多的话,\n",
    "必定造成该特征对应的参数$ \\beta_{k} $更新十分剧烈,而其他维度特征对应的参数更新则相对缓和,这样就造成迭代过程中很多轮次实际上是为了消除特征尺度上的不一致,\n",
    "故使这些特征具有相同或相近的尺度,可以使梯度下降法算法更快地收敛\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}