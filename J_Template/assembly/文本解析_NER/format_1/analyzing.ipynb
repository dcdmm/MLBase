{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'text': '【病原和流行病学】狂犬病病毒（rabiesvirus）属弹状病毒科狂犬病病毒属。',\n  'entities': [{'start_idx': 9,\n    'end_idx': 13,\n    'type': 'mic',\n    'entity': '狂犬病病毒'},\n   {'start_idx': 15, 'end_idx': 25, 'type': 'mic', 'entity': 'rabiesvirus'},\n   {'start_idx': 28, 'end_idx': 31, 'type': 'mic', 'entity': '弹状病毒'},\n   {'start_idx': 33, 'end_idx': 37, 'type': 'mic', 'entity': '狂犬病病毒'}]},\n {'text': '对儿童SARST细胞亚群的研究表明，与成人SARS相比，儿童细胞下降不明显，证明上述推测成立。',\n  'entities': [{'start_idx': 3,\n    'end_idx': 9,\n    'type': 'bod',\n    'entity': 'SARST细胞'},\n   {'start_idx': 19, 'end_idx': 24, 'type': 'dis', 'entity': '成人SARS'}]},\n {'text': '研究证实，细胞减少与肺内病变程度及肺内炎性病变吸收程度密切相关。',\n  'entities': [{'start_idx': 10, 'end_idx': 10, 'type': 'bod', 'entity': '肺'},\n   {'start_idx': 10, 'end_idx': 13, 'type': 'sym', 'entity': '肺内病变'},\n   {'start_idx': 17, 'end_idx': 17, 'type': 'bod', 'entity': '肺'},\n   {'start_idx': 17, 'end_idx': 22, 'type': 'sym', 'entity': '肺内炎性病变'}]}]"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('CMeEE_train.json', encoding='utf-8') as f:\n",
    "    data_raw = json.load(f)\n",
    "\n",
    "data_raw  # 列表"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': '【病原和流行病学】狂犬病病毒（rabiesvirus）属弹状病毒科狂犬病病毒属。',\n 'entities': [{'start_idx': 9,\n   'end_idx': 13,\n   'type': 'mic',\n   'entity': '狂犬病病毒'},\n  {'start_idx': 15, 'end_idx': 25, 'type': 'mic', 'entity': 'rabiesvirus'},\n  {'start_idx': 28, 'end_idx': 31, 'type': 'mic', 'entity': '弹状病毒'},\n  {'start_idx': 33, 'end_idx': 37, 'type': 'mic', 'entity': '狂犬病病毒'}]}"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "狂\n",
      "\n",
      "毒\n",
      "s\n",
      "毒\n",
      "毒\n",
      "\n",
      "狂犬病病毒\n",
      "rabiesvirus\n",
      "弹状病毒\n",
      "狂犬病病毒\n"
     ]
    }
   ],
   "source": [
    "# 9表示开始字符('狂')的位置\n",
    "print(data_raw[0]['text'][9], end='\\n\\n')\n",
    "\n",
    "# 13表示结尾字符('毒')的位置\n",
    "print(data_raw[0]['text'][13])\n",
    "print(data_raw[0]['text'][25])\n",
    "print(data_raw[0]['text'][31])\n",
    "print(data_raw[0]['text'][37], end='\\n\\n')\n",
    "\n",
    "# 字符串索引不包括结尾位置元素\n",
    "print(data_raw[0]['text'][9:13 + 1])\n",
    "print(data_raw[0]['text'][15:25 + 1])\n",
    "print(data_raw[0]['text'][28:31 + 1])\n",
    "print(data_raw[0]['text'][33:37 + 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizerFast(name_or_path='IDEA-CCNL/Erlangshen-DeBERTa-v2-97M-Chinese', vocab_size=12800, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-97M-Chinese')\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "data": {
      "text/plain": "['【',\n '病',\n '原',\n '和',\n '流',\n '行',\n '病',\n '学',\n '】',\n '狂',\n '犬',\n '病',\n '病',\n '毒',\n '（',\n 'ra',\n '##bi',\n '##e',\n '##s',\n '##vi',\n '##r',\n '##us',\n '）',\n '属',\n '弹',\n '状',\n '病',\n '毒',\n '科',\n '狂',\n '犬',\n '病',\n '病',\n '毒',\n '属',\n '。']"
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(data_raw[0]['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "data": {
      "text/plain": "[[(0, 0),\n  (0, 1),\n  (1, 2),\n  (2, 3),\n  (3, 4),\n  (4, 5),\n  (5, 6),\n  (6, 7),\n  (7, 8),\n  (8, 9),\n  (9, 10),\n  (10, 11),\n  (11, 12),\n  (12, 13),\n  (13, 14),\n  (14, 15),\n  (15, 17),\n  (17, 19),\n  (19, 20),\n  (20, 21),\n  (21, 23),\n  (23, 24),\n  (24, 26),\n  (26, 27),\n  (27, 28),\n  (28, 29),\n  (29, 30),\n  (30, 31),\n  (31, 32),\n  (32, 33),\n  (33, 34),\n  (34, 35),\n  (35, 36),\n  (36, 37),\n  (37, 38),\n  (38, 39),\n  (39, 40),\n  (0, 0)]]"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokenizer([data_raw[0]['text']],\n",
    "                    max_length=512, truncation=True, padding=True,\n",
    "                    return_offsets_mapping=True)\n",
    "offset_mapping = outputs[\"offset_mapping\"]\n",
    "offset_mapping  # return (char_start, char_end) for each token."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 17: 17, 19: 18, 20: 19, 21: 20, 23: 21, 24: 22, 26: 23, 27: 24, 28: 25, 29: 26, 30: 27, 31: 28, 32: 29, 33: 30, 34: 31, 35: 32, 36: 33, 37: 34, 38: 35, 39: 36}]\n",
      "[{0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 16: 16, 18: 17, 19: 18, 20: 19, 22: 20, 23: 21, 25: 22, 26: 23, 27: 24, 28: 25, 29: 26, 30: 27, 31: 28, 32: 29, 33: 30, 34: 31, 35: 32, 36: 33, 37: 34, 38: 35, 39: 36}]\n"
     ]
    }
   ],
   "source": [
    "# (0, 0)表示特殊token(如:'[CLS]','[SEP'], '[PAD]'等)\n",
    "# i表示第几个token(从0开始计数,包含特殊token)\n",
    "# j[1] - 1表示该token结尾字符的位置\n",
    "start_mapping = [{j[0]: i for i, j in enumerate(i) if j != (0, 0)} for i in offset_mapping]\n",
    "end_mapping = [{j[1] - 1: i for i, j in enumerate(i) if j != (0, 0)} for i in offset_mapping]\n",
    "print(start_mapping)\n",
    "print(end_mapping)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start_idx': 9, 'end_idx': 13, 'type': 'mic', 'entity': '狂犬病病毒'}\n",
      "start_span: 10,end_span: 14\n",
      "{'start_idx': 15, 'end_idx': 25, 'type': 'mic', 'entity': 'rabiesvirus'}\n",
      "start_span: 16,end_span: 22\n",
      "{'start_idx': 28, 'end_idx': 31, 'type': 'mic', 'entity': '弹状病毒'}\n",
      "start_span: 25,end_span: 28\n",
      "{'start_idx': 33, 'end_idx': 37, 'type': 'mic', 'entity': '狂犬病病毒'}\n",
      "start_span: 30,end_span: 34\n"
     ]
    }
   ],
   "source": [
    "for i in data_raw[0]['entities']:\n",
    "    print(i)\n",
    "    start_idx, end_idx, entity_type, entity_text = i['start_idx'], i['end_idx'], i['type'], i['entity']\n",
    "    if start_idx in start_mapping[0] and end_idx in end_mapping[0]:\n",
    "        start_span = start_mapping[0][start_idx]\n",
    "        end_span = end_mapping[0][end_idx]\n",
    "        print(\"start_span: {},end_span: {}\".format(start_span, end_span))  # 该实体由第[start_span, end_span]的token组成(从0开始)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 注意"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "test_text = 'variety of diseases，     想要4atm和5sim!'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['var', '##i', '##e', '##ty', 'of', 'dis', '##e', '##as', '##e', '##s', '，', '想', '要', '4', '##at', '##m', '和', '5', '##s', '##im', '!']\n",
      "[10933, 11902, 11898, 12386, 9091, 10710, 11898, 12299, 11898, 11912, 183, 2454, 6459, 214, 12300, 11906, 1091, 216, 11912, 12331, 104]\n",
      "['var', '##i', '##e', '##ty', 'of', 'dis', '##e', '##as', '##e', '##s', '，', '想', '要', '4', '##at', '##m', '和', '5', '##s', '##im', '!']\n"
     ]
    }
   ],
   "source": [
    "# 多空格('    ')视为单个空格(' ')\n",
    "# '4atm'======拆分为======>['4', '##at', '##m']('atm'为一个实体,此时被错误拆分)\n",
    "init_t = tokenizer(test_text, add_special_tokens=False)['input_ids']\n",
    "print(init_t)\n",
    "print(tokenizer.convert_ids_to_tokens(init_t))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8273, 8252, 8269, 8260, 8256, 8271, 8276, 12800, 8266, 8257, 12800, 8255, 8260, 8270, 8256, 8252, 8270, 8256, 8270, 183, 12800, 12800, 12800, 12800, 12800, 2454, 6459, 214, 8252, 8271, 8264, 1091, 216, 8270, 8260, 8264, 104, 2]\n",
      "['[CLS]', 'v', 'a', 'r', 'i', 'e', 't', 'y', '[SP]', 'o', 'f', '[SP]', 'd', 'i', 's', 'e', 'a', 's', 'e', 's', '，', '[SP]', '[SP]', '[SP]', '[SP]', '[SP]', '想', '要', '4', 'a', 't', 'm', '和', '5', 's', 'i', 'm', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 解决:\n",
    "test_text_sp = np.array(list(test_text))\n",
    "test_text_sp = np.where(test_text_sp == ' ', '[SP]', test_text_sp).tolist()\n",
    "tokenizer.add_tokens(new_tokens=['[SP]'])\n",
    "sp_t = tokenizer(test_text_sp, is_split_into_words=True)['input_ids']\n",
    "print(sp_t)\n",
    "print(tokenizer.convert_ids_to_tokens(sp_t))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}