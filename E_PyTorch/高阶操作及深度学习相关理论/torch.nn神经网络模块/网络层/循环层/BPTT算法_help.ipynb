{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;BPTT 算法将循环神经网络看作一个展开的多层前馈网络,其中“每一层”对应循环网络中的\"每个时刻\".这样，循环神经网络就可以按照前馈网络中的反向传播算法计算参数梯度.在\"展开\"的前馈网络中,所有层的参数是共享的,因此参数的真实梯度是所有\"展开层\"\"的参数梯度之和.\n",
    "\n",
    "&emsp;&emsp;给定一个训练训练样本$(\\mathbf{x},y)$,其中$X_{1: T}=\\left(\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{t}, \\ldots, \\mathbf{x}_{T}\\right)$为长度是$T$的输入序列,易知循环网络在时刻$t$的更新公式为\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{z}_t &= W_{hh} \\mathbf{h}_{t-1} + W_{ih} \\mathbf{x}_t + \\mathbf{b}_{ih} +  \\mathbf{b}_{hh}  \\\\\n",
    "\\mathbf{h}_{t} &= f(\\mathbf{z}_t)\n",
    "\\end{align}\n",
    "\n",
    "其中$f(*)$是非线性激活函数,$\\mathbf{y}_{1: M}=\\left(y_{1}, y_{2}, \\ldots, y_{m}, \\ldots, y_{M}\\right)$是长度为$T$的标签序列,即在每个时刻$t$,都有一个监督信息$y_t$,定义时刻$t$的损失函数为\n",
    "\n",
    "$$ \\mathcal{L}_t = \\mathcal{L}(y_t, g(\\mathbf{h}_t)) $$\n",
    "\n",
    "其中$g(\\mathbf{h}_t)$为第$t$时刻的输出,$\\mathcal{L}$为可微分的损失函数,比如交叉熵.那么整个序列的损失函数为\n",
    "\n",
    "$$ \\mathcal{L} = \\sum_{t=1}^T \\mathcal{L}_t $$\n",
    "\n",
    "整个训练的损失函数关于参数$W_{hh}$的梯度为\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{hh}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}_{t}}{\\partial W_{hh}}  \\tag{0}$$\n",
    "\n",
    "即每个时刻损失$\\mathcal{L}_t $对参数$W_{hh}$的偏导数之和.\n",
    "\n",
    "&emsp;&emsp;先计算第$t$时刻损失函数$\\mathcal{L}_t$关于每个参数$[w_{hh}]_{ij}$的偏导数为:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}_{t}}{\\partial [w_{hh}]_{ij}}=\\sum_{k=1}^{t} \\frac{\\partial \\mathbf{z}_{k}}{\\partial [w_{hh}]_{ij}} \\frac{\\partial \\mathcal{L}_{t}}{\\partial \\mathbf{z}_{k}} \\tag{1} $$\n",
    "\n",
    "首先\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{z}_{k}}{\\partial [w_{hh}]_{ij}} &= \\left[0, \\cdots, \\mathbf{h}_{k-1;j},\\cdots, 0 \\right]_i  \\qquad \\qquad \\text{注:分布布局}  \\\\\n",
    "&= \\mathbb{I}_i( [\\mathbf{h}_{k-1}]_j) \\tag{2}\n",
    "\\end{align}\n",
    "\n",
    "其中$\\mathbb{I}_i( [\\mathbf{h}_{k-1}]_j)$表示第$i$个元素值为$\\mathbf{h}_{k-1}$的第$j$个元素,其余都为0的行向量\n",
    "\n",
    "其次,定义误差项$\\boldsymbol{\\delta}_{t, k}=\\frac{\\partial \\mathcal{L}_{t}}{\\partial \\mathbf{z}_{k}} $为第$t$时刻的损失对第$k$时刻隐藏神经层的净输入$\\mathbf{z}_k$的导数,则当$1 \\leq k <t$时,有\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\delta}_{t, k} &=\\frac{\\partial \\mathcal{L}_{t}}{\\partial \\mathbf{z}_{k}} \\\\\n",
    "              &=\\frac{\\partial \\mathbf{h}_{k}}{\\partial \\mathbf{z}_{k}} \\frac{\\partial \\mathbf{z}_{k+1}}{\\partial \\mathbf{h}_{k}} \\frac{\\partial \\mathcal{L}_{t}}{\\partial \\mathbf{z}_{k+1}} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{ \\partial f([z_{k}]_1)}{\\partial [z_{k}]_1} & 0 & \\cdots & 0  \\\\\n",
    "0 & \\frac{ \\partial f([z_{k}]_2)}{\\partial [z_{k}]_2} & \\cdots & 0 \\\\\n",
    "\\vdots& \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\frac{ \\partial f([z_{k}]_{-1})}{\\partial [z_{k}]_{-1}}\n",
    "\\end{bmatrix}  W_{hh}^{T} \\boldsymbol{\\delta_{\\mathit{t, k+1}}} \\\\ \\\\\\\n",
    "&=A_k W_{hh}^{\\top} \\boldsymbol{\\delta}_{t, k+1} \\qquad \\qquad \\text{注:列向量}\n",
    "\\end{align}\n",
    "\n",
    "将公式(2)公式(3)代入公式(1)可得\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}_{t}}{\\partial [w_{hh}]_{ij}}=\\sum_{k=1}^{t}\\left[\\delta_{t, k}\\right]_{i}\\left[\\mathbf{h}_{k-1}\\right]_j  \\tag{3}$$\n",
    "\n",
    "将上式写成矩阵形式有\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}_{t}}{\\partial W_{hh}}=\\sum_{k=1}^{t}\\boldsymbol{\\delta}_{t, k} \\mathbf{h}_{k-1}^T \\tag{4} $$\n",
    "\n",
    "下图给出了误差项随时间进行反向传播算法的示例\n",
    "\n",
    "<img src='../../../../../Other/img/BPTT.png'>\n",
    "\n",
    "将公式(4)代入公式(0),得到整个序列的损失函数$\\mathcal{L}$关于参数$W_{hh}$的梯度\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{hh}}= \\sum_{t=1}^T \\sum_{k=1}^{t}\\boldsymbol{\\delta}_{t, k} \\mathbf{h}_{k-1}^T \\tag{5}  $$\n",
    "\n",
    "\n",
    "同理可得,$\\mathcal{L}$关于权重$W_{ih}$和偏置$\\mathbf{b}_{hh},\\mathbf{b}_{ih}$的梯度为\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_{ih}}    &= \\sum_{t=1}^T  \\sum_{k=1}^{t} \\boldsymbol{\\delta}_{t, k} \\mathbf{x}_{k}^T   \\tag{6} \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}} &= \\sum_{t=1}^T  \\sum_{k=1}^{t} \\boldsymbol{\\delta}_{t, k}  \\tag{7} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}