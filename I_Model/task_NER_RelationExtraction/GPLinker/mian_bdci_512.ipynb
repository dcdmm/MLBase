{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa3c2ab-22ea-4eb8-aa00-67834dd2be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "\n",
    "from model import RawGlobalPointer, ERENet\n",
    "from utils import sparse_multilabel_categorical_crossentropy, MetricsCalculator_bdci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c58614-db6c-4826-ba07-ed60b054c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b1cd15-57b0-4fb3-a4a5-9fc7e98f233d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff540639-02de-4bf3-b393-e2ccde267a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': '部件故障', '1': '性能故障', '2': '检测工具', '3': '组成'}\n",
      "{'部件故障': 0, '性能故障': 1, '检测工具': 2, '组成': 3}\n"
     ]
    }
   ],
   "source": [
    "with open('datasets_bdci/rel2id.json', 'r', encoding='utf-8') as f:\n",
    "    id_to_rel, rel_to_id = json.load(f)\n",
    "    print(id_to_rel)\n",
    "    print(rel_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc8a8d1-7907-4711-bb0f-eb6954d19cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(text, spo_list, sign='。', max_len=510):\n",
    "    \"\"\"长句子根据标点符号(sign)截断\"\"\"\n",
    "    asf = [0, 0]\n",
    "\n",
    "    def sen_position(data):\n",
    "        \"\"\"句子长度之和不超过max_len的分组为同一组\"\"\"\n",
    "        asf[1] += data\n",
    "        if asf[1] > max_len:\n",
    "            asf[1] = data\n",
    "            asf[0] += 1\n",
    "        return pd.Series([data, asf[0]])\n",
    "\n",
    "    def sen_join(data):\n",
    "        return pd.Series([''.join(data['text'].tolist()), data['txt_len'].sum()], index=['join_text', 'join_text_len'])\n",
    "\n",
    "    text_s = text.split(sign)  # split函数结果不包含分割字符sign\n",
    "    if text_s[-1] == '':\n",
    "        text_ser = pd.Series(dict([(i + sign, len(i) + 1) for i in text_s[:-1]]))  # 最后一句以sign结尾时\n",
    "    else:\n",
    "        text_tup = [(i + sign, len(i) + 1) for i in text_s]\n",
    "        text_tup[-1] = (text_tup[-1][0][:-1], text_tup[-1][1] - 1)  # 最后一句不以sign结尾时\n",
    "        text_ser = pd.Series(dict(text_tup))\n",
    "    text_df = text_ser.apply(sen_position)\n",
    "    text_df = text_df.reset_index()\n",
    "    text_df.columns = ['text', 'txt_len', 'group']\n",
    "    result = text_df.groupby(text_df['group']).apply(sen_join)\n",
    "    result['text_cumsum_len'] = result['join_text_len'].cumsum()\n",
    "\n",
    "    spo_list_split = [[] for _ in range(result.shape[0])]\n",
    "    for spo in spo_list:\n",
    "        for i in result.iterrows():\n",
    "            # 根据句子长度分组情况重新计算spo_list\n",
    "            if spo['t']['pos'][-1] <= i[1]['text_cumsum_len']:\n",
    "                if i[1]['text_cumsum_len'] == i[1]['join_text_len']:\n",
    "                    spo_list_split[i[0]].append(\n",
    "                        (spo['h']['name'], spo['h']['pos'], spo['t']['name'], spo['t']['pos'], spo['relation']))\n",
    "                else:\n",
    "                    cut_number = i[1]['text_cumsum_len'] - i[1]['join_text_len']\n",
    "                    spo_h_pos = [spo['h']['pos'][0] - cut_number, spo['h']['pos'][1] - cut_number]  # 更新pos\n",
    "                    spo_t_pos = [spo['t']['pos'][0] - cut_number, spo['t']['pos'][1] - cut_number]  # 更新pos\n",
    "                    spo_list_split[i[0]].append(\n",
    "                        (spo['h']['name'], spo_h_pos, spo['t']['name'], spo_t_pos, spo['relation']))\n",
    "                break\n",
    "\n",
    "    result['spo_list'] = spo_list_split\n",
    "    return result\n",
    "\n",
    "\n",
    "def split_one(text, spo_list, sen_lst, max_len=510):\n",
    "    \"\"\"长句子(不含标点符号)根据实体位置截断\"\"\"\n",
    "    if len(text) <= max_len:\n",
    "        sen_lst.append([text, spo_list])\n",
    "        return\n",
    "    span = []\n",
    "    # i example:('蓄电池', [496, 499], '电压低', [499, 502], '部件故障')\n",
    "    for i in spo_list:\n",
    "        span.append([i, i[1][0], i[3][1]])\n",
    "    span.sort(key=lambda x: x[-1])\n",
    "    for i in range(len(span) - 1):\n",
    "        if span[i][-1] <= max_len < span[i + 1][-2]:\n",
    "            sen_lst.append([text[:span[i][-1]], [tuple(k[0]) for k in span[:i + 1]]])\n",
    "            new_spo_list = [(k[0][0], [k[0][1][0] - span[i][-1], k[0][1][1] - span[i][-1]],\n",
    "                             k[0][2], [k[0][3][0] - span[i][-1], k[0][3][1] - span[i][-1]], k[0][-1]) for k in\n",
    "                            span[i + 1:]]  # 更新pos\n",
    "            split_one(text[span[i][-1]:], new_spo_list, sen_lst)  # 递归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84bee069-3d7e-4df3-b091-480d8e43629f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '62号汽车故障报告综合情况:故障现象:加速后，丢开油门，发动机熄火。', 'spo_list': [('发动机', [28, 31], '熄火', [31, 33], '部件故障')]}\n",
      "1559\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    # example=[{'text': text0, 'spo_list': [(h_name00, hpos00, t_name00, tpos00, r00), (h_name01, hpos01, t_name01, tpos01, r01), xxxxxx]}, xxxxxx]\n",
    "    D = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line)\n",
    "            if len(line[\"text\"]) <= 510:\n",
    "                D.append({\n",
    "                    \"text\": line[\"text\"],\n",
    "                    \"spo_list\": [(spo['h']['name'], spo['h']['pos'], spo['t']['name'], spo['t']['pos'], spo['relation'])\n",
    "                                 for spo in line[\"spo_list\"]]})\n",
    "            else:\n",
    "                line_text = line['text'].replace('；', '。')  # 分号替换为句号(切分句子优先级相等)\n",
    "                if line_text.find('。') == -1 or line_text.find('。') == len(\n",
    "                        line_text) - 1:  # 长句子(不含标点符号'。',或只含一个标点符号'。'且位于句子结尾)\n",
    "                    spo_list = [(spo['h']['name'], spo['h']['pos'], spo['t']['name'], spo['t']['pos'], spo['relation'])\n",
    "                                for spo in line[\"spo_list\"]]\n",
    "                    afa = []\n",
    "                    split_one(line_text, spo_list, afa)\n",
    "                    for i in afa:\n",
    "                        D.append({\"text\": i[0], \"spo_list\": i[1]})\n",
    "                else:\n",
    "                    split_sentence_reuslt = split_sentence(line_text, line['spo_list'])\n",
    "                    for i in split_sentence_reuslt.iterrows():\n",
    "                        if len(i[1][\"join_text\"]) <= 510:\n",
    "                            D.append({\"text\": i[1][\"join_text\"], \"spo_list\": i[1][\"spo_list\"]})\n",
    "                        else:\n",
    "                            # 长句子根据标点符号(sign)截断仍存在长句子\n",
    "                            afa_ssr = []\n",
    "                            split_one(i[1][\"join_text\"], i[1][\"spo_list\"], afa_ssr)\n",
    "                            for i in afa_ssr:\n",
    "                                D.append({\"text\": i[0], \"spo_list\": i[1]})\n",
    "        return D\n",
    "\n",
    "\n",
    "data = load_data('datasets_bdci/train_bdci.json')\n",
    "print(data[0])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04fe119b-af01-44fe-9459-68640493db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, items):\n",
    "        self._items = items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._items)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self._items[index]\n",
    "\n",
    "        return {'text': item['text'],\n",
    "                'spo_list': item['spo_list']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828046de-c9ef-48cd-b939-d2c410908680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='junnyu/uer_large', vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at junnyu/uer_large were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325523456\n"
     ]
    }
   ],
   "source": [
    "tokenizer_fast = AutoTokenizer.from_pretrained('junnyu/uer_large', use_fast=False)\n",
    "tokenizer_fast.add_tokens(new_tokens=['[SP]'])\n",
    "print(tokenizer_fast)\n",
    "\n",
    "pretrained = AutoModel.from_pretrained('junnyu/uer_large')\n",
    "pretrained.resize_token_embeddings(len(tokenizer_fast))\n",
    "print(pretrained.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2621cba-696a-4e91-88eb-da70b901b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 361])\n",
      "torch.Size([4, 2, 9, 2])\n",
      "torch.Size([4, 4, 9, 2])\n"
     ]
    }
   ],
   "source": [
    "def padding_space(d):\n",
    "    \"\"\"将句子转换为字符列表,并将列表中的空格(' ')替换为'[SP]'\"\"\"\n",
    "    if d.find(' ') == -1:\n",
    "        return list(d)\n",
    "    else:\n",
    "        d_arr = np.array(list(d))\n",
    "        d_arr = np.where(d_arr == ' ', '[SP]', d_arr).tolist()\n",
    "        return d_arr\n",
    "\n",
    "\n",
    "def get_collate_fn(tokenizer, max_len=512):\n",
    "    \"\"\"返回collate_fun函数(通过闭包函数引入形参)\"\"\"\n",
    "\n",
    "    def collate_fn(data):\n",
    "        batch_size = len(data)\n",
    "        texts = [padding_space(i['text']) for i in data]\n",
    "        spo_lists = [i['spo_list'] for i in data]\n",
    "\n",
    "        encoder_text = tokenizer(texts, padding=True, max_length=max_len, truncation=True, is_split_into_words=True,\n",
    "                                 return_tensors='pt')\n",
    "        input_ids, token_type_ids, attention_mask = encoder_text.values()\n",
    "\n",
    "        entity_labels, head_labels, tail_labels = [], [], []\n",
    "        for i in range(batch_size):\n",
    "            entity_labels_temp, head_labels_temp, tail_labels_temp = [[], []], [[], [], [], []], [[], [], [], []]\n",
    "            if spo_lists[i]:\n",
    "                for _, p_index, _, o_index, r in spo_lists[i]:\n",
    "                    entity_labels_temp[0].append((p_index[0], p_index[1] - 1))\n",
    "                    entity_labels_temp[1].append((o_index[0], o_index[1] - 1))\n",
    "                    head_labels_temp[rel_to_id[r]].append((p_index[0], o_index[0]))\n",
    "                    tail_labels_temp[rel_to_id[r]].append((p_index[1] - 1, o_index[1] - 1))\n",
    "            else:\n",
    "                # spo_lists为空列表时\n",
    "                entity_labels_temp[0].append((0, 0))\n",
    "                entity_labels_temp[1].append((0, 0))\n",
    "\n",
    "            _, _ = [i.append((0, 0)) for i in head_labels_temp if not i], [i.append((0, 0)) for i in tail_labels_temp if\n",
    "                                                                           not i]\n",
    "\n",
    "            entity_labels_temp = torch.transpose(torch.tensor(entity_labels_temp), 0, 1)\n",
    "            entity_labels.append(entity_labels_temp)\n",
    "\n",
    "            head_labels_temp = [torch.tensor(i) for i in head_labels_temp]\n",
    "            head_labels_temp = torch.transpose(pad_sequence(head_labels_temp, batch_first=True), 0, 1)\n",
    "            head_labels.append(head_labels_temp)\n",
    "            tail_labels_temp = [torch.tensor(i) for i in tail_labels_temp]\n",
    "            tail_labels_temp = torch.transpose(pad_sequence(tail_labels_temp, batch_first=True), 0, 1)\n",
    "            tail_labels.append(tail_labels_temp)\n",
    "\n",
    "        entity_labels = torch.transpose(pad_sequence(entity_labels, batch_first=True), 1, 2)\n",
    "        head_labels = torch.transpose(pad_sequence(head_labels, batch_first=True), 1, 2)\n",
    "        tail_labels = torch.transpose(pad_sequence(tail_labels, batch_first=True), 1, 2)\n",
    "        return input_ids, attention_mask, token_type_ids, entity_labels, head_labels, tail_labels, texts, spo_lists\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "train_loader = DataLoader(CustomDataset(items=data), batch_size=4, shuffle=True,\n",
    "                          collate_fn=get_collate_fn(tokenizer_fast))\n",
    "for i in train_loader:\n",
    "    print(i[0].shape)\n",
    "    print(i[3].shape)\n",
    "    print(i[4].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4dde6a4-3558-460b-821f-68a5b1b85fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = pretrained.config.hidden_size\n",
    "mention_detect = RawGlobalPointer(hidden_size, 2, 64).to(device)  # 不提取实体类型(只识别subject、object对应的实体)\n",
    "s_o_head = RawGlobalPointer(hidden_size, len(id_to_rel), 64, RoPE=False, tril_mask=False).to(\n",
    "    device)  # 不需要设置tril_mask=False\n",
    "s_o_tail = RawGlobalPointer(hidden_size, len(id_to_rel), 64, RoPE=False, tril_mask=False).to(\n",
    "    device)  # 不需要设置tril_mask=False\n",
    "net = ERENet(pretrained, mention_detect, s_o_head, s_o_tail).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "015e1746-f036-457f-8ec2-5c77ea069626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spoes(logits1, logits2, logits3, texts, id2predicate):\n",
    "    logits1 = logits1.data.cpu().numpy()\n",
    "    logits2 = logits2.data.cpu().numpy()\n",
    "    logits3 = logits3.data.cpu().numpy()\n",
    "    batch_size = logits1.shape[0]\n",
    "\n",
    "    # 序列开头与结尾特殊token('[CLS]', '[SEP]')处元素设置为无穷小\n",
    "    logits1[:, :, [0, -1]] -= np.inf\n",
    "    logits1[:, :, :, [0, -1]] -= np.inf\n",
    "    subjects, objects = [[] for _ in range(batch_size)], [[] for _ in range(batch_size)]\n",
    "    for b, l, h, t in zip(*np.where(logits1 > 0.0)):  # 阈值(threshold)设置为0.0\n",
    "        if l == 0:  # 不提取实体类型(只识别subject、objects对应的实体)\n",
    "            subjects[b].append((int(h), int(t)))\n",
    "        else:\n",
    "            objects[b].append((int(h), int(t)))\n",
    "\n",
    "    spoes = [[] for _ in range(batch_size)]\n",
    "    for b in range(batch_size):\n",
    "        text_b = texts[b]\n",
    "        for sh, st in subjects[b]:\n",
    "            for oh, ot in objects[b]:\n",
    "                p1s = np.where(logits2[b, :, sh, oh] > 0.0)[0]  # 阈值(threshold)设置为0.0\n",
    "                p2s = np.where(logits3[b, :, st, ot] > 0.0)[0]  # 阈值(threshold)设置为0.0\n",
    "                ps = set(p1s) & set(p2s)\n",
    "                for p in ps:\n",
    "                    sht_str = ''.join(text_b[sh: st + 1])\n",
    "                    oht_str = ''.join(text_b[oh: ot + 1])\n",
    "                    spoes[b].append((sht_str, (sh, st + 1), oht_str, (oh, ot + 1),\n",
    "                                     id2predicate[str(p)]))  # 添加预测结果:(h_name, hpos, t_name, tpos, r)\n",
    "    return spoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6789da71-a1dd-420b-b03e-bd59546836f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(model, dataloader, optimizer, id2predicate, device):\n",
    "    model.train()\n",
    "\n",
    "    for idx, (\n",
    "    input_ids, attention_mask, token_type_ids, entity_labels, head_labels, tail_labels, texts, spo_lists) in enumerate(\n",
    "            dataloader, start=1):\n",
    "        # 数据设备切换\n",
    "        # input_ids.shape=[batch_size, seq_len]\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        entity_labels = entity_labels.to(device)\n",
    "        head_labels = head_labels.to(device)\n",
    "        tail_labels = tail_labels.to(device)\n",
    "\n",
    "        # logits1.shape=[batch_size, 2, seq_len, seq_len]\n",
    "        # logits2.shape=[batch_size, len(schema) seq_len, seq_len]\n",
    "        # logits3.shape=[batch_size, len(schema), seq_len, seq_len]\n",
    "        logits1, logits2, logits3 = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        loss1 = sparse_multilabel_categorical_crossentropy(y_true=entity_labels, y_pred=logits1)\n",
    "        loss2 = sparse_multilabel_categorical_crossentropy(y_true=head_labels, y_pred=logits2)\n",
    "        loss3 = sparse_multilabel_categorical_crossentropy(y_true=tail_labels, y_pred=logits3)\n",
    "        loss = sum([loss1, loss2, loss3]) / 3  # entities和relations之间的信息共享和交互\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            extract_spoes(logits1, logits2, logits3, texts, id_to_rel)\n",
    "            y_pred = extract_spoes(logits1, logits2, logits3, texts, id2predicate)\n",
    "            mc = MetricsCalculator_bdci()  # 计算查准率、查全率、F1 score \n",
    "            mc.calc_confusion_matrix(y_pred, spo_lists)\n",
    "            print('| step {:5d} | loss {:9.5f} | precision {:8.5f} | recall {:8.5f} | f1 {:8.5f} |'.format(idx,\n",
    "                                                                                                           loss.item(),\n",
    "                                                                                                           mc.precision,\n",
    "                                                                                                           mc.recall,\n",
    "                                                                                                           mc.f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4547519e-0624-440c-a487-42589de619ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型验证\n",
    "def evaluate(model, dataloader, id2predicate, device):\n",
    "    model.eval()\n",
    "\n",
    "    mc = MetricsCalculator_bdci()  # 计算查准率、查全率、F1 score \n",
    "    with torch.no_grad():\n",
    "        for idx, (input_ids, attention_mask, token_type_ids, entity_labels, head_labels, tail_labels, texts,\n",
    "                  spo_lists) in enumerate(dataloader):\n",
    "            # 数据设备切换\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "\n",
    "            logits1, logits2, logits3 = model(input_ids, attention_mask, token_type_ids)\n",
    "            y_pred = extract_spoes(logits1, logits2, logits3, texts, id2predicate)\n",
    "\n",
    "            mc.calc_confusion_matrix(y_pred, spo_lists)\n",
    "    return mc.precision, mc.recall, mc.f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e179acaf-b292-42b7-90d8-7fbb9d6349e8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------0--------------------------------------------------\n",
      "| step   100 | loss  22.28444 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   200 | loss  22.22444 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   300 | loss  23.07591 | precision  0.33333 | recall  0.06667 | f1  0.11111 |\n",
      "--------------------------------------------------1--------------------------------------------------\n",
      "| step   100 | loss  27.81215 | precision  0.50000 | recall  0.06250 | f1  0.11111 |\n",
      "| step   200 | loss  27.93113 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   300 | loss  10.15987 | precision  0.21429 | recall  0.13043 | f1  0.16216 |\n",
      "--------------------------------------------------2--------------------------------------------------\n",
      "| step   100 | loss  19.36986 | precision  1.00000 | recall  0.13333 | f1  0.23529 |\n",
      "| step   200 | loss   1.13598 | precision  0.50000 | recall  0.21429 | f1  0.30000 |\n",
      "| step   300 | loss  25.97929 | precision  0.22727 | recall  0.21739 | f1  0.22222 |\n",
      "--------------------------------------------------3--------------------------------------------------\n",
      "| step   100 | loss  11.25628 | precision  0.31250 | recall  0.62500 | f1  0.41667 |\n",
      "| step   200 | loss  10.09508 | precision  1.00000 | recall  0.16667 | f1  0.28571 |\n",
      "| step   300 | loss  14.02247 | precision  0.50000 | recall  0.04762 | f1  0.08696 |\n",
      "--------------------------------------------------4--------------------------------------------------\n",
      "| step   100 | loss  19.64416 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   200 | loss  20.69500 | precision  0.00000 | recall  0.00000 | f1  0.00000 |\n",
      "| step   300 | loss   9.31698 | precision  1.00000 | recall  0.14286 | f1  0.25000 |\n",
      "--------------------------------------------------5--------------------------------------------------\n",
      "| step   100 | loss   6.58361 | precision  1.00000 | recall  0.16667 | f1  0.28571 |\n",
      "| step   200 | loss   0.18724 | precision  1.00000 | recall  0.14286 | f1  0.25000 |\n",
      "| step   300 | loss   8.92265 | precision  1.00000 | recall  0.07692 | f1  0.14286 |\n",
      "--------------------------------------------------6--------------------------------------------------\n",
      "| step   100 | loss   2.01538 | precision  0.75000 | recall  0.30000 | f1  0.42857 |\n",
      "| step   200 | loss   1.98751 | precision  1.00000 | recall  0.57143 | f1  0.72727 |\n",
      "| step   300 | loss   9.16895 | precision  0.60000 | recall  0.16667 | f1  0.26087 |\n",
      "--------------------------------------------------7--------------------------------------------------\n",
      "| step   100 | loss  -6.28468 | precision  1.00000 | recall  0.20000 | f1  0.33333 |\n",
      "| step   200 | loss   4.43810 | precision  0.83333 | recall  0.33333 | f1  0.47619 |\n",
      "| step   300 | loss   4.22893 | precision  0.87500 | recall  0.30435 | f1  0.45161 |\n",
      "--------------------------------------------------8--------------------------------------------------\n",
      "| step   100 | loss  -3.32934 | precision  1.00000 | recall  0.04545 | f1  0.08696 |\n",
      "| step   200 | loss  -1.19242 | precision  0.66667 | recall  0.30769 | f1  0.42105 |\n",
      "| step   300 | loss  -3.18791 | precision  0.80000 | recall  0.22222 | f1  0.34783 |\n",
      "--------------------------------------------------9--------------------------------------------------\n",
      "| step   100 | loss  12.47422 | precision  1.00000 | recall  0.21053 | f1  0.34783 |\n",
      "| step   200 | loss   1.96666 | precision  0.63158 | recall  0.66667 | f1  0.64865 |\n",
      "| step   300 | loss   7.29690 | precision  0.76923 | recall  0.66667 | f1  0.71429 |\n",
      "--------------------------------------------------10--------------------------------------------------\n",
      "| step   100 | loss -10.80716 | precision  1.00000 | recall  0.75000 | f1  0.85714 |\n",
      "| step   200 | loss   6.07992 | precision  0.73333 | recall  0.73333 | f1  0.73333 |\n",
      "| step   300 | loss   4.78757 | precision  1.00000 | recall  0.25000 | f1  0.40000 |\n",
      "--------------------------------------------------11--------------------------------------------------\n",
      "| step   100 | loss   1.61986 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss  -6.48685 | precision  0.75000 | recall  0.25000 | f1  0.37500 |\n",
      "| step   300 | loss -18.18840 | precision  1.00000 | recall  0.41667 | f1  0.58824 |\n",
      "--------------------------------------------------12--------------------------------------------------\n",
      "| step   100 | loss  -2.58965 | precision  0.91667 | recall  0.50000 | f1  0.64706 |\n",
      "| step   200 | loss  -3.05094 | precision  1.00000 | recall  0.33333 | f1  0.50000 |\n",
      "| step   300 | loss   2.30395 | precision  1.00000 | recall  0.43333 | f1  0.60465 |\n",
      "--------------------------------------------------13--------------------------------------------------\n",
      "| step   100 | loss  -1.94598 | precision  0.94118 | recall  1.00000 | f1  0.96970 |\n",
      "| step   200 | loss  11.53832 | precision  1.00000 | recall  0.29412 | f1  0.45455 |\n",
      "| step   300 | loss   3.09672 | precision  0.55556 | recall  0.50000 | f1  0.52632 |\n",
      "--------------------------------------------------14--------------------------------------------------\n",
      "| step   100 | loss   1.71129 | precision  1.00000 | recall  0.26667 | f1  0.42105 |\n",
      "| step   200 | loss  -9.22341 | precision  0.71429 | recall  0.35714 | f1  0.47619 |\n",
      "| step   300 | loss   0.55977 | precision  1.00000 | recall  0.61538 | f1  0.76190 |\n",
      "--------------------------------------------------15--------------------------------------------------\n",
      "| step   100 | loss   2.03068 | precision  1.00000 | recall  0.71429 | f1  0.83333 |\n",
      "| step   200 | loss   4.40630 | precision  0.91667 | recall  0.84615 | f1  0.88000 |\n",
      "| step   300 | loss  -0.37086 | precision  0.75000 | recall  0.30000 | f1  0.42857 |\n",
      "--------------------------------------------------16--------------------------------------------------\n",
      "| step   100 | loss  -1.37046 | precision  0.91667 | recall  0.57895 | f1  0.70968 |\n",
      "| step   200 | loss  -0.38987 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   300 | loss -13.11260 | precision  1.00000 | recall  0.86364 | f1  0.92683 |\n",
      "--------------------------------------------------17--------------------------------------------------\n",
      "| step   100 | loss -11.96064 | precision  0.92857 | recall  0.76471 | f1  0.83871 |\n",
      "| step   200 | loss  10.44480 | precision  0.57143 | recall  0.37500 | f1  0.45283 |\n",
      "| step   300 | loss  -1.24756 | precision  0.86667 | recall  0.81250 | f1  0.83871 |\n",
      "--------------------------------------------------18--------------------------------------------------\n",
      "| step   100 | loss   0.80187 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss  30.06746 | precision  1.00000 | recall  0.20000 | f1  0.33333 |\n",
      "| step   300 | loss   9.25385 | precision  0.75000 | recall  0.31579 | f1  0.44444 |\n",
      "--------------------------------------------------19--------------------------------------------------\n",
      "| step   100 | loss   7.15514 | precision  1.00000 | recall  0.57895 | f1  0.73333 |\n",
      "| step   200 | loss  -4.70566 | precision  1.00000 | recall  0.85714 | f1  0.92308 |\n",
      "| step   300 | loss   3.05194 | precision  1.00000 | recall  0.81818 | f1  0.90000 |\n",
      "--------------------------------------------------20--------------------------------------------------\n",
      "| step   100 | loss   0.94957 | precision  1.00000 | recall  0.83333 | f1  0.90909 |\n",
      "| step   200 | loss -15.96169 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   300 | loss -10.58134 | precision  1.00000 | recall  0.92857 | f1  0.96296 |\n",
      "--------------------------------------------------21--------------------------------------------------\n",
      "| step   100 | loss   8.35721 | precision  0.75000 | recall  0.11765 | f1  0.20339 |\n",
      "| step   200 | loss  -2.20557 | precision  1.00000 | recall  0.71429 | f1  0.83333 |\n",
      "| step   300 | loss   3.11479 | precision  1.00000 | recall  0.71429 | f1  0.83333 |\n",
      "--------------------------------------------------22--------------------------------------------------\n",
      "| step   100 | loss  -1.65890 | precision  0.94444 | recall  1.00000 | f1  0.97143 |\n",
      "| step   200 | loss -16.96282 | precision  0.94118 | recall  0.72727 | f1  0.82051 |\n",
      "| step   300 | loss -11.33216 | precision  1.00000 | recall  0.90909 | f1  0.95238 |\n",
      "--------------------------------------------------23--------------------------------------------------\n",
      "| step   100 | loss -10.25806 | precision  1.00000 | recall  0.85714 | f1  0.92308 |\n",
      "| step   200 | loss -14.60156 | precision  0.94444 | recall  0.89474 | f1  0.91892 |\n",
      "| step   300 | loss   1.12521 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "--------------------------------------------------24--------------------------------------------------\n",
      "| step   100 | loss  -1.80486 | precision  1.00000 | recall  0.53333 | f1  0.69565 |\n",
      "| step   200 | loss  -6.44814 | precision  0.96774 | recall  0.85714 | f1  0.90909 |\n",
      "| step   300 | loss -10.29459 | precision  1.00000 | recall  0.80000 | f1  0.88889 |\n",
      "--------------------------------------------------25--------------------------------------------------\n",
      "| step   100 | loss -31.55070 | precision  1.00000 | recall  0.81081 | f1  0.89552 |\n",
      "| step   200 | loss   1.57829 | precision  0.84615 | recall  0.75862 | f1  0.80000 |\n",
      "| step   300 | loss -12.30375 | precision  0.77778 | recall  0.58333 | f1  0.66667 |\n",
      "--------------------------------------------------26--------------------------------------------------\n",
      "| step   100 | loss -14.62654 | precision  0.85714 | recall  1.00000 | f1  0.92308 |\n",
      "| step   200 | loss   0.38805 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   300 | loss   0.40098 | precision  1.00000 | recall  0.75000 | f1  0.85714 |\n",
      "--------------------------------------------------27--------------------------------------------------\n",
      "| step   100 | loss   2.49424 | precision  0.88889 | recall  0.88889 | f1  0.88889 |\n",
      "| step   200 | loss  -4.32910 | precision  1.00000 | recall  0.82353 | f1  0.90323 |\n",
      "| step   300 | loss -16.04180 | precision  0.93333 | recall  0.87500 | f1  0.90323 |\n",
      "--------------------------------------------------28--------------------------------------------------\n",
      "| step   100 | loss  -4.00431 | precision  1.00000 | recall  0.93750 | f1  0.96774 |\n",
      "| step   200 | loss  -2.50102 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   300 | loss -10.80550 | precision  0.92857 | recall  0.86667 | f1  0.89655 |\n",
      "--------------------------------------------------29--------------------------------------------------\n",
      "| step   100 | loss   2.71385 | precision  1.00000 | recall  1.00000 | f1  1.00000 |\n",
      "| step   200 | loss   6.23092 | precision  1.00000 | recall  0.47059 | f1  0.64000 |\n",
      "| step   300 | loss  -6.55629 | precision  0.90909 | recall  0.90909 | f1  0.90909 |\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    print('-' * 50 + str(epoch) + '-' * 50)\n",
    "    train(net, train_loader, optimizer, id_to_rel, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3812",
   "language": "python",
   "name": "env_3812"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}